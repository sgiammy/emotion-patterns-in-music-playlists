{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#LyricsManager.py\" data-toc-modified-id=\"LyricsManager.py-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LyricsManager.py</a></span></li><li><span><a href=\"#New-features\" data-toc-modified-id=\"New-features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>New features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset-Stats\" data-toc-modified-id=\"Dataset-Stats-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dataset Stats</a></span></li><li><span><a href=\"#Artificial-Neural-Network\" data-toc-modified-id=\"Artificial-Neural-Network-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Artificial Neural Network</a></span></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>SVM</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Previously on Sara&Mario project**: \n",
    "<ol>\n",
    "    <li>*MoodyLyrics* stats analysis</li>\n",
    "    <li>Lyrics classification using the main classifiers</li>\n",
    "    <li>Emotion classification by just considering the song title</li> \n",
    "</ol>\n",
    "Now, following the last meeting discussion we: \n",
    "<ol>\n",
    "    <li>Wrote a script to create the dataset *SpotifyURI*, *List of PlaylistIDs*, *MoodyLyric_Emotion*</li>\n",
    "    <li>Added new features for the classification task</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LyricsManager.py\n",
    "After downloading the Spotify RecSys Challenge dataset we wrote a script that: <br>\n",
    "<ul>\n",
    "    <li>Given as input: the *Spotify playlist dataset folder*, and an output folder, creates a data structure to store for each song:<br>\n",
    "        <*SpotifyURI*, *PlaylistIDs*, *TrackInformation*, *Emotion*><br>\n",
    "        where: \n",
    "            <ul>\n",
    "                <li>*SpotifyURI* is the songID</li>\n",
    "                <li>*PlaylistsIDs* is the list of playlist in which the song appear</li>\n",
    "                <li>*TrackInformation* is the list of information taken from Spotify dataset</li>\n",
    "                <li>*Emotion* is an optional field, present only if the song is also contained in the MoodyLyrics dataset, that contains the emotion label for the song\n",
    "    </li>\n",
    "            </ul>\n",
    "      <li>Can load Spotify songs datastructure (if already existing)</li>\n",
    "            <li>Given as input a SpotifyURI it can download lyrics from lyricwikia</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New features\n",
    "Starting from MoodyLyrics we are going to create a new dataset with a lot of new features. <br>\n",
    "<ul>\n",
    "    <li>**Title_vector**</li>\n",
    "    <li>**Lyric_vector**</li>\n",
    "    <li>**%Rhymes**:<br> defined as the percentage of the number of rhymes over the number of total lines. A rhyme is defined as a rhyme between two following lines.</li>\n",
    "    <li>**%Past_tense_verbs**:<br> defined as the the percentage of the number of past tense verbs over the total number of verbs.</li>\n",
    "    <li>**%Present_tense_verbs**:<br>  defined as the the percentage of the number of present tense verbs over the total number of verbs.</li>\n",
    "    <li>**%Future_tense_verbs**:<br>  defined as the the percentage of the number of future tense verbs over the total number of verbs, where future is just will + base form.</li>\n",
    "    <li>**%ADJ**:<br> Percentage of adjectives over the total number of words.</li>\n",
    "    <li>**%ADP**:<br> Percentage of adpositions (e.g. in, to, during) over the total number of words.</li>\n",
    "    <li>**%ADV**:<br> Percentage of adverbs (e.g. very, tomorrow, down, where, there) over the total number of words.</li>\n",
    "    <li>**%AUX**:<br> Percentage of auxiliaries (e.g. is, has (done), will (do), should (do)) over the total number of words.</li>\n",
    "    <li>**%INTJ**:<br> Percentage of interjections (e.g. psst, ouch, bravo, hello) over the total number of words.</li>\n",
    "    <li>**%NOUN**:<br> Percentage of nouns over the total number of words.</li>\n",
    "    <li>**%NUM**:<br> Percentage of numerals over the total number of words.</li>\n",
    "    <li>**%PRON**:<br> Percentage of pronouns (e.g. I, you, he, she, myself, themselves, somebody,...) over the total number of words.</li> \n",
    "    <li>**%PROPN**:<br> Percentage of proper nouns (e.g. Mary, John) over the total number of words.</li>\n",
    "    <li>**%PUNCT**:<br> Percentage of puntuctuation (e.g. ., (, ), ?) over the total number of words.</li>\n",
    "    <li>**%VERB**:<br> Percentage of verbs over the total number of words.</li>\n",
    "    <li>**Selfish_degree**:<br> Percentage of 'I' pronouns over the total number of pronouns</li>\n",
    "    <li>**%Echoism**:<br> Percentage of echoism over the total number of words, where an echoism is either a sequence of two subsequent repeated words or the repetition of a vowel in a word. </li>\n",
    "    <li>**%Duplicates**:<br> Percentage of duplicate words over the total number of words</li>\n",
    "    <li>**isTitleInLyric**:<br> Boolean, true if the title string is also a substring of the lyric</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = dataset.drop(['Vector1','Vector2'],axis=1)\n",
    "\n",
    "f, axarr = plt.subplots(5, 4, figsize=(25,40))\n",
    "k = 0\n",
    "for feature in tmp_df.columns:\n",
    "    (i, j) = divmod(k, 4)\n",
    "    axarr[i,j] = sns.boxplot(y=tmp_df[feature], ax=axarr[i,j])\n",
    "    axarr[i,j].set_title('Boxplot for {}'.format(feature), fontsize=16, weight='bold')\n",
    "    k += 1\n",
    "plt.tight_layout()\n",
    "f.delaxes(axarr[-1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare array for sklearn classifiers\n",
    "X_vect = dataset['Vector'].as_matrix().T\n",
    "X_vect = np.array([np.array(x) for x in X_vect])\n",
    "X_norm = dataset['Vector_Norm'].as_matrix()\n",
    "y = dataset['Emotion'].as_matrix()\n",
    "\n",
    "#1 Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#2 y_nn should be a vector (len(X_vect),4), with a 1 in the right class\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "y_nn = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "#3 Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# we need to scale because we don't want one feature to predomine the others\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Importing the Keras libraries and packages\n",
    "import keras\n",
    "# Sequential module is required to initialize our ANN\n",
    "from keras.models import Sequential\n",
    "# Dense module is required to create the layers\n",
    "from keras.layers import Dense, Dropout\n",
    "    \n",
    "def build_ann(optimizer='adam', input_size=618):\n",
    "    classifier = Sequential()\n",
    "    #2 Adding first hidden layer\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'random_normal', activation = 'sigmoid', input_dim = input_size))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    # Adding second hidden layer\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    # Adding output layer\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'random_normal', activation = 'softmax'))\n",
    "\n",
    "    #3 Compiling the ANN\n",
    "    classifier.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "keras_classifier = KerasClassifier(build_fn=build_ann)\n",
    "parameters = {'batch_size': [64, 128],\n",
    "              'epochs': [25, 50],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = keras_classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5)\n",
    "grid_search = grid_search.fit(X_train, y_train, verbose=0)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print('Accuracy: %0.2f' % (best_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build model\n",
    "clf = SVC()\n",
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['linear'], 'C': [ 0.01, 0.05, 1, 10, 100 ]},\n",
    "    { 'kernel': ['rbf', 'sigmoid'], 'C': [ 0.01, 0.05, 0.1, 0.3, 0.8, 1, 3, 10, 50, 100, 150, 200 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params = parameters_grid_search(SVC, params, X_vect, y, verbose=1)\n",
    "print('Parameters:', best_params)\n",
    "scores = cross_val_score(svm_best, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
