{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#EmoInt-dataset-for-emotion-detection-in-lyrics\" data-toc-modified-id=\"EmoInt-dataset-for-emotion-detection-in-lyrics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>EmoInt dataset for emotion detection in lyrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#EmoInt-statistics\" data-toc-modified-id=\"EmoInt-statistics-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>EmoInt statistics</a></span></li><li><span><a href=\"#Merge-with-MoodyLyrics\" data-toc-modified-id=\"Merge-with-MoodyLyrics-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Merge with MoodyLyrics</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>SVM</a></span></li><li><span><a href=\"#Gradient-Boost\" data-toc-modified-id=\"Gradient-Boost-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Gradient Boost</a></span></li><li><span><a href=\"#Artificial-Neural-Network\" data-toc-modified-id=\"Artificial-Neural-Network-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Artificial Neural Network</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:32:29.176469Z",
     "start_time": "2018-04-21T10:32:29.172558Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoInt dataset for emotion detection in lyrics\n",
    "\n",
    "Existing emotion datasets are mainly annotated categorically without an indication of degree of emotion. EmoInt, instead, provides several tweets annotated according to an emotion (anger, fear, joy, sadness) and to the degree at which the emotion is expressed in text.\n",
    "\n",
    "It is important to mention that EmoInt was manually annotated, using [Best-Worst Scaling](https://nparc.nrc-cnrc.gc.ca/eng/view/fulltext/?id=b132b0af-2ae0-4964-ac3a-493e7292a37a) (BWS), an annotation scheme shown to obtain very reliable scores.\n",
    "\n",
    "For our purpose, we will consider each tweet to be like a lyric and, on top of that, we will perform our feature engineering using spaCy and the other tools we used so far.\n",
    "\n",
    "Our original dataset, MoodyLyrics, contains \"happy\", \"sad\", \"angry\" and \"relaxed\" as labels. Therefore, in order to perform a sort of interjection with EmoInt, we will just use the tweets corresponding to the anger, joy and sadness emotions.\n",
    "\n",
    "The remaining part of this notebook assumes that we have already parsed EmoInt dataset in a .csv file which we can use to train some machine learning models as we did when we performed our feature engineering on lyrics. For more information about how this .csv was generated, please refer to the `src/emoint_parser.py` script.\n",
    "\n",
    "## EmoInt statistics\n",
    "\n",
    "As EmoInt provide intensity levels together with emotion labels, we decided to take into account only those tweets for which the intensity was greater that 0.50 (50%). Also, we dropped tags and hashtags (e.g. \"Hey @MrTwitter how are you? #cool\" became \"Hey how are you?\") because we will have to compare those tweets with songs and songs do not have those kind of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:16.959547Z",
     "start_time": "2018-04-21T10:23:16.838572Z"
    }
   },
   "outputs": [],
   "source": [
    "emoint = pd.read_csv('datasets/emoint_featurized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:17.148894Z",
     "start_time": "2018-04-21T10:23:17.142913Z"
    }
   },
   "outputs": [],
   "source": [
    "useless_columns = [ 'ID','ARTIST', 'SONG_TITLE', 'X_FREQUENCIES', 'SPACE_FREQUENCIES']\n",
    "emoint.drop(useless_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:17.580859Z",
     "start_time": "2018-04-21T10:23:17.539149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LYRICS_VECTOR</th>\n",
       "      <th>TITLE_VECTOR</th>\n",
       "      <th>LINE_COUNT</th>\n",
       "      <th>WORD_COUNT</th>\n",
       "      <th>ECHOISMS</th>\n",
       "      <th>SELFISH_DEGREE</th>\n",
       "      <th>DUPLICATE_LINES</th>\n",
       "      <th>IS_TITLE_IN_LYRICS</th>\n",
       "      <th>RHYMES</th>\n",
       "      <th>VERB_PRESENT</th>\n",
       "      <th>...</th>\n",
       "      <th>NOUN_FREQUENCIES</th>\n",
       "      <th>NUM_FREQUENCIES</th>\n",
       "      <th>PART_FREQUENCIES</th>\n",
       "      <th>PRON_FREQUENCIES</th>\n",
       "      <th>PROPN_FREQUENCIES</th>\n",
       "      <th>PUNCT_FREQUENCIES</th>\n",
       "      <th>SCONJ_FREQUENCIES</th>\n",
       "      <th>SYM_FREQUENCIES</th>\n",
       "      <th>VERB_FREQUENCIES</th>\n",
       "      <th>EMOTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.58756495e-01  1.27405643e-01 -1.87897816e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-5.92244938e-02  1.76795915e-01 -1.86805829e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 5.27095608e-03  1.50894225e-01 -9.51976478e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-8.42130557e-02  2.85120517e-01 -2.86448717e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.01087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-2.13992037e-03  2.37986907e-01 -1.79613903e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       LYRICS_VECTOR  \\\n",
       "0  [-1.58756495e-01  1.27405643e-01 -1.87897816e-...   \n",
       "1  [-5.92244938e-02  1.76795915e-01 -1.86805829e-...   \n",
       "2  [ 5.27095608e-03  1.50894225e-01 -9.51976478e-...   \n",
       "3  [-8.42130557e-02  2.85120517e-01 -2.86448717e-...   \n",
       "4  [-2.13992037e-03  2.37986907e-01 -1.79613903e-...   \n",
       "\n",
       "                                        TITLE_VECTOR  LINE_COUNT  WORD_COUNT  \\\n",
       "0  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          16   \n",
       "1  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          20   \n",
       "2  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1           9   \n",
       "3  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          23   \n",
       "4  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          21   \n",
       "\n",
       "   ECHOISMS  SELFISH_DEGREE  DUPLICATE_LINES  IS_TITLE_IN_LYRICS  RHYMES  \\\n",
       "0       0.0        0.000000              0.0               False       0   \n",
       "1       0.0        1.000000              0.0               False       0   \n",
       "2       0.0        0.000000              0.0               False       0   \n",
       "3       0.0        0.200000              0.0               False       0   \n",
       "4       0.0        0.666667              0.0               False       0   \n",
       "\n",
       "   VERB_PRESENT   ...     NOUN_FREQUENCIES  NUM_FREQUENCIES  PART_FREQUENCIES  \\\n",
       "0          0.75   ...             0.000000              0.0          0.000000   \n",
       "1          0.60   ...             0.030000              0.0          0.000000   \n",
       "2          0.50   ...             0.222222              0.0          0.000000   \n",
       "3          0.75   ...             0.021739              0.0          0.000000   \n",
       "4          0.50   ...             0.071429              0.0          0.011905   \n",
       "\n",
       "   PRON_FREQUENCIES  PROPN_FREQUENCIES  PUNCT_FREQUENCIES  SCONJ_FREQUENCIES  \\\n",
       "0          0.000000            0.03125                0.0                0.0   \n",
       "1          0.020000            0.00000                0.0                0.0   \n",
       "2          0.000000            0.00000                0.0                0.0   \n",
       "3          0.054348            0.01087                0.0                0.0   \n",
       "4          0.035714            0.00000                0.0                0.0   \n",
       "\n",
       "   SYM_FREQUENCIES  VERB_FREQUENCIES  EMOTION  \n",
       "0              0.0          0.062500    happy  \n",
       "1              0.0          0.050000    happy  \n",
       "2              0.0          0.111111    happy  \n",
       "3              0.0          0.043478    happy  \n",
       "4              0.0          0.047619    happy  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoint.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the same columns naming convention we used in the past notebooks with MoodyLyrics just for compatibility reasons (we will have to put them together). Since tweets do not have title, the `TITLE_VECTOR` was just left there as a vector of 0s, with the same shape of the `LYRICS_VECTOR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with MoodyLyrics\n",
    "Let's now merge EmoInt and MoodyLyrics featurized datasets in order to be able to proceed with further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:18.921198Z",
     "start_time": "2018-04-21T10:23:18.916074Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'datasets/emotion_detection_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:19.777422Z",
     "start_time": "2018-04-21T10:23:19.555587Z"
    }
   },
   "outputs": [],
   "source": [
    "moodylyrics = pd.read_csv(path)\n",
    "moodylyrics.columns = ['ID', 'ARTIST', 'SONG_TITLE', 'LYRICS_VECTOR', 'TITLE_VECTOR', \n",
    "                   'LINE_COUNT', 'WORD_COUNT', 'ECHOISMS', 'SELFISH_DEGREE', \n",
    "                   'DUPLICATE_LINES', 'IS_TITLE_IN_LYRICS', 'RHYMES', 'VERB_PRESENT', \n",
    "                   'VERB_PAST', 'VERB_FUTURE', 'ADJ_FREQUENCIES', 'CONJUCTION_FREQUENCIES', \n",
    "                   'ADV_FREQUENCIES', 'AUX_FREQUENCIES', 'CONJ_FREQUENCIES', 'CCONJ_FREQUENCIES', \n",
    "                   'DETERMINER_FREQUENCIES', 'INTERJECTION_FREQUENCIES', 'NOUN_FREQUENCIES', \n",
    "                   'NUM_FREQUENCIES', 'PART_FREQUENCIES', 'PRON_FREQUENCIES', 'PROPN_FREQUENCIES', \n",
    "                   'PUNCT_FREQUENCIES', 'SCONJ_FREQUENCIES', 'SYM_FREQUENCIES', 'VERB_FREQUENCIES', \n",
    "                   'X_FREQUENCIES', 'SPACE_FREQUENCIES', 'EMOTION']\n",
    "moodylyrics.drop(useless_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:20.514335Z",
     "start_time": "2018-04-21T10:23:20.503145Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = emoint.append(moodylyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:21.054939Z",
     "start_time": "2018-04-21T10:23:20.954960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINE_COUNT</th>\n",
       "      <th>WORD_COUNT</th>\n",
       "      <th>ECHOISMS</th>\n",
       "      <th>SELFISH_DEGREE</th>\n",
       "      <th>DUPLICATE_LINES</th>\n",
       "      <th>RHYMES</th>\n",
       "      <th>VERB_PRESENT</th>\n",
       "      <th>VERB_PAST</th>\n",
       "      <th>VERB_FUTURE</th>\n",
       "      <th>ADJ_FREQUENCIES</th>\n",
       "      <th>...</th>\n",
       "      <th>INTERJECTION_FREQUENCIES</th>\n",
       "      <th>NOUN_FREQUENCIES</th>\n",
       "      <th>NUM_FREQUENCIES</th>\n",
       "      <th>PART_FREQUENCIES</th>\n",
       "      <th>PRON_FREQUENCIES</th>\n",
       "      <th>PROPN_FREQUENCIES</th>\n",
       "      <th>PUNCT_FREQUENCIES</th>\n",
       "      <th>SCONJ_FREQUENCIES</th>\n",
       "      <th>SYM_FREQUENCIES</th>\n",
       "      <th>VERB_FREQUENCIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.0</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.0</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.0</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.634722</td>\n",
       "      <td>118.501062</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.046397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.353133</td>\n",
       "      <td>3.478670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>0.016783</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.043912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.537418</td>\n",
       "      <td>127.585458</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.308315</td>\n",
       "      <td>0.062092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.179584</td>\n",
       "      <td>6.630785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.103561</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.061004</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.066435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.086462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.062307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>189.000000</td>\n",
       "      <td>1162.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LINE_COUNT   WORD_COUNT     ECHOISMS  SELFISH_DEGREE  DUPLICATE_LINES  \\\n",
       "count  4706.000000  4706.000000  4706.000000     4706.000000      4706.000000   \n",
       "mean     19.634722   118.501062     0.003040        0.257467         0.046397   \n",
       "std      22.537418   127.585458     0.018688        0.308315         0.062092   \n",
       "min       1.000000     1.000000     0.000000        0.000000         0.000000   \n",
       "25%       1.000000    17.000000     0.000000        0.000000         0.000000   \n",
       "50%      13.000000    64.000000     0.000000        0.142857         0.000000   \n",
       "75%      34.000000   202.000000     0.000000        0.451613         0.086462   \n",
       "max     189.000000  1162.000000     0.500000        1.000000         1.000000   \n",
       "\n",
       "       RHYMES  VERB_PRESENT    VERB_PAST  VERB_FUTURE  ADJ_FREQUENCIES  \\\n",
       "count  4706.0   4706.000000  4706.000000       4706.0      4706.000000   \n",
       "mean      0.0     12.353133     3.478670          0.0         0.034587   \n",
       "std       0.0     17.179584     6.630785          0.0         0.075121   \n",
       "min       0.0      0.000000     0.000000          0.0         0.000000   \n",
       "25%       0.0      0.500000     0.000000          0.0         0.001356   \n",
       "50%       0.0      1.000000     0.500000          0.0         0.004329   \n",
       "75%       0.0     21.000000     4.000000          0.0         0.031250   \n",
       "max       0.0    142.000000    97.000000          0.0         1.000000   \n",
       "\n",
       "             ...         INTERJECTION_FREQUENCIES  NOUN_FREQUENCIES  \\\n",
       "count        ...                      4706.000000       4706.000000   \n",
       "mean         ...                         0.003442          0.056307   \n",
       "std          ...                         0.022703          0.103561   \n",
       "min          ...                         0.000000          0.000000   \n",
       "25%          ...                         0.000000          0.003262   \n",
       "50%          ...                         0.000000          0.013400   \n",
       "75%          ...                         0.000296          0.062307   \n",
       "max          ...                         0.500000          1.000000   \n",
       "\n",
       "       NUM_FREQUENCIES  PART_FREQUENCIES  PRON_FREQUENCIES  PROPN_FREQUENCIES  \\\n",
       "count      4706.000000       4706.000000       4706.000000        4706.000000   \n",
       "mean          0.002523          0.003810          0.017707           0.016783   \n",
       "std           0.015115          0.012375          0.039051           0.061004   \n",
       "min           0.000000          0.000000          0.000000           0.000000   \n",
       "25%           0.000000          0.000000          0.001751           0.000000   \n",
       "50%           0.000000          0.000217          0.004305           0.000000   \n",
       "75%           0.000000          0.000991          0.018519           0.005952   \n",
       "max           0.444444          0.200000          0.666667           1.000000   \n",
       "\n",
       "       PUNCT_FREQUENCIES  SCONJ_FREQUENCIES  SYM_FREQUENCIES  VERB_FREQUENCIES  \n",
       "count        4706.000000             4706.0      4706.000000       4706.000000  \n",
       "mean            0.002929                0.0         0.000031          0.043912  \n",
       "std             0.021154                0.0         0.001528          0.066435  \n",
       "min             0.000000                0.0         0.000000          0.000000  \n",
       "25%             0.000000                0.0         0.000000          0.005114  \n",
       "50%             0.000000                0.0         0.000000          0.012512  \n",
       "75%             0.001526                0.0         0.000000          0.059881  \n",
       "max             0.875000                0.0         0.100000          1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Before starting we should flatten the dataset's features which are vectors at the moment (title vector and content vector). Let's do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:47.184497Z",
     "start_time": "2018-04-21T10:23:45.108667Z"
    }
   },
   "outputs": [],
   "source": [
    "X_vect = list()\n",
    "for (i, row) in dataset.drop('EMOTION', axis=1).iterrows():\n",
    "    sub_list = list()\n",
    "    for field in row:\n",
    "        if type(field) == str:\n",
    "            field = field[1:-1].split()\n",
    "            sub_list += [float(x.replace('\\n','')) for x in field]\n",
    "        else:\n",
    "            sub_list.append(field)\n",
    "    X_vect.append(np.array(sub_list))\n",
    "X_vect = np.array(X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:47.964271Z",
     "start_time": "2018-04-21T10:23:47.956516Z"
    }
   },
   "outputs": [],
   "source": [
    "y = dataset.EMOTION.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:23:49.269630Z",
     "start_time": "2018-04-21T10:23:49.263656Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4706, 627)\n",
      "(4706,)\n"
     ]
    }
   ],
   "source": [
    "print(X_vect.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:26:04.898286Z",
     "start_time": "2018-04-21T10:26:04.891735Z"
    }
   },
   "outputs": [],
   "source": [
    "def parameters_grid_search(classifier, params, x, y, cv=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Grid Search to find best parameters for a certain classifier whose\n",
    "    performances are evaluated using cross-validation\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(classifier(), params, cv=cv, n_jobs=-1, verbose=verbose)\n",
    "    gs.fit(x, y)    \n",
    "    return (gs.best_estimator_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:31:39.586623Z",
     "start_time": "2018-04-21T10:26:36.403450Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build model\n",
    "clf = SVC()\n",
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['rbf'], 'C': [ 0.1, 1 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params = parameters_grid_search(SVC, params, X_vect, y, verbose=1)\n",
    "print('Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T10:35:40.105241Z",
     "start_time": "2018-04-21T10:32:38.474954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm_best, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[EmoInt paper](http://saifmohammad.com/WebDocs/TweetEmotionIntensities-starsem2017.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
