{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#EmoInt-dataset-for-emotion-detection-in-lyrics\" data-toc-modified-id=\"EmoInt-dataset-for-emotion-detection-in-lyrics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>EmoInt dataset for emotion detection in lyrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#EmoInt-statistics\" data-toc-modified-id=\"EmoInt-statistics-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>EmoInt statistics</a></span></li><li><span><a href=\"#Merge-with-MoodyLyrics\" data-toc-modified-id=\"Merge-with-MoodyLyrics-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Merge with MoodyLyrics</a></span></li></ul></li><li><span><a href=\"#Features-Selection\" data-toc-modified-id=\"Features-Selection-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Features Selection</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#k-Nearest-Neighbour\" data-toc-modified-id=\"k-Nearest-Neighbour-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>k-Nearest Neighbour</a></span></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>SVM</a></span></li><li><span><a href=\"#Gradient-Boost\" data-toc-modified-id=\"Gradient-Boost-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Gradient Boost</a></span></li><li><span><a href=\"#Artificial-Neural-Network\" data-toc-modified-id=\"Artificial-Neural-Network-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Artificial Neural Network</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T13:35:26.182656Z",
     "start_time": "2018-04-21T13:35:26.176701Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoInt dataset for emotion detection in lyrics\n",
    "\n",
    "Existing emotion datasets are mainly annotated categorically without an indication of degree of emotion. EmoInt, instead, provides several tweets annotated according to an emotion (anger, fear, joy, sadness) and to the degree at which the emotion is expressed in text.\n",
    "\n",
    "It is important to mention that EmoInt was manually annotated, using [Best-Worst Scaling](https://nparc.nrc-cnrc.gc.ca/eng/view/fulltext/?id=b132b0af-2ae0-4964-ac3a-493e7292a37a) (BWS), an annotation scheme shown to obtain very reliable scores.\n",
    "\n",
    "For our purpose, we will consider each tweet to be like a lyric and, on top of that, we will perform our feature engineering using spaCy and the other tools we used so far.\n",
    "\n",
    "Our original dataset, MoodyLyrics, contains \"happy\", \"sad\", \"angry\" and \"relaxed\" as labels. Therefore, in order to perform a sort of interjection with EmoInt, we will just use the tweets corresponding to the anger, joy and sadness emotions.\n",
    "\n",
    "The remaining part of this notebook assumes that we have already parsed EmoInt dataset in a .csv file which we can use to train some machine learning models as we did when we performed our feature engineering on lyrics. For more information about how this .csv was generated, please refer to the `src/emoint_parser.py` script.\n",
    "\n",
    "## EmoInt statistics\n",
    "\n",
    "As EmoInt provide intensity levels together with emotion labels, we decided to take into account only those tweets for which the intensity was greater that 0.50 (50%). Also, we dropped hashtags a remove the tag characters (e.g. \"Hey @MrTwitter how are you? #cool\" became \"Hey MrTwitter how are you?\") because we will have to compare those tweets with songs and songs do not have those kind of things. Also, this sort of preprocessing should maximize the chances that everything is recognized properly by spaCy's POS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T14:00:46.038528Z",
     "start_time": "2018-04-21T14:00:45.907181Z"
    }
   },
   "outputs": [],
   "source": [
    "emoint = pd.read_csv('datasets/emoint_featurized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T14:00:46.194880Z",
     "start_time": "2018-04-21T14:00:46.188297Z"
    }
   },
   "outputs": [],
   "source": [
    "useless_columns = [ 'ID','ARTIST', 'SONG_TITLE', 'X_FREQUENCIES', 'SPACE_FREQUENCIES']\n",
    "emoint.drop(useless_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T14:00:46.521129Z",
     "start_time": "2018-04-21T14:00:46.472191Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LYRICS_VECTOR</th>\n",
       "      <th>TITLE_VECTOR</th>\n",
       "      <th>LINE_COUNT</th>\n",
       "      <th>WORD_COUNT</th>\n",
       "      <th>ECHOISMS</th>\n",
       "      <th>SELFISH_DEGREE</th>\n",
       "      <th>DUPLICATE_LINES</th>\n",
       "      <th>IS_TITLE_IN_LYRICS</th>\n",
       "      <th>RHYMES</th>\n",
       "      <th>VERB_PRESENT</th>\n",
       "      <th>...</th>\n",
       "      <th>NOUN_FREQUENCIES</th>\n",
       "      <th>NUM_FREQUENCIES</th>\n",
       "      <th>PART_FREQUENCIES</th>\n",
       "      <th>PRON_FREQUENCIES</th>\n",
       "      <th>PROPN_FREQUENCIES</th>\n",
       "      <th>PUNCT_FREQUENCIES</th>\n",
       "      <th>SCONJ_FREQUENCIES</th>\n",
       "      <th>SYM_FREQUENCIES</th>\n",
       "      <th>VERB_FREQUENCIES</th>\n",
       "      <th>EMOTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.26710683e-01  1.60194725e-01 -1.36762261e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-6.28133714e-02  1.90393195e-01 -1.95530921e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 9.66307223e-02  2.91245524e-02 -1.42218113e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1.13483094e-01  3.13860744e-01 -2.05740720e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 3.85632203e-03  2.41273686e-01 -1.58885673e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       LYRICS_VECTOR  \\\n",
       "0  [-1.26710683e-01  1.60194725e-01 -1.36762261e-...   \n",
       "1  [-6.28133714e-02  1.90393195e-01 -1.95530921e-...   \n",
       "2  [ 9.66307223e-02  2.91245524e-02 -1.42218113e-...   \n",
       "3  [-1.13483094e-01  3.13860744e-01 -2.05740720e-...   \n",
       "4  [ 3.85632203e-03  2.41273686e-01 -1.58885673e-...   \n",
       "\n",
       "                                        TITLE_VECTOR  LINE_COUNT  WORD_COUNT  \\\n",
       "0  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          16   \n",
       "1  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          19   \n",
       "2  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          11   \n",
       "3  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          23   \n",
       "4  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          22   \n",
       "\n",
       "   ECHOISMS  SELFISH_DEGREE  DUPLICATE_LINES  IS_TITLE_IN_LYRICS  RHYMES  \\\n",
       "0       0.0        0.000000              0.0               False       0   \n",
       "1       0.0        1.000000              0.0               False       0   \n",
       "2       0.0        0.000000              0.0               False       0   \n",
       "3       0.0        0.200000              0.0               False       0   \n",
       "4       0.0        0.666667              0.0               False       0   \n",
       "\n",
       "   VERB_PRESENT   ...     NOUN_FREQUENCIES  NUM_FREQUENCIES  PART_FREQUENCIES  \\\n",
       "0      0.750000   ...             0.015625              0.0          0.000000   \n",
       "1      0.666667   ...             0.052632              0.0          0.000000   \n",
       "2      0.500000   ...             0.272727              0.0          0.000000   \n",
       "3      0.750000   ...             0.021739              0.0          0.000000   \n",
       "4      1.000000   ...             0.136364              0.0          0.022727   \n",
       "\n",
       "   PRON_FREQUENCIES  PROPN_FREQUENCIES  PUNCT_FREQUENCIES  SCONJ_FREQUENCIES  \\\n",
       "0          0.000000           0.031250           0.046875                0.0   \n",
       "1          0.035088           0.000000           0.017544                0.0   \n",
       "2          0.000000           0.181818           0.000000                0.0   \n",
       "3          0.054348           0.010870           0.076087                0.0   \n",
       "4          0.068182           0.022727           0.068182                0.0   \n",
       "\n",
       "   SYM_FREQUENCIES  VERB_FREQUENCIES  EMOTION  \n",
       "0              0.0          0.062500    happy  \n",
       "1              0.0          0.087719    happy  \n",
       "2              0.0          0.090909    happy  \n",
       "3              0.0          0.043478    happy  \n",
       "4              0.0          0.113636    happy  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoint.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the same columns naming convention we used in the past notebooks with MoodyLyrics just for compatibility reasons (we will have to put them together). Since tweets do not have title, the `TITLE_VECTOR` was just left there as a vector of 0s, with the same shape of the `LYRICS_VECTOR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with MoodyLyrics\n",
    "Let's now merge EmoInt and MoodyLyrics featurized datasets in order to be able to proceed with further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:01.065504Z",
     "start_time": "2018-04-21T15:08:01.061303Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'datasets/moodylyrics_featurized.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:01.866691Z",
     "start_time": "2018-04-21T15:08:01.640234Z"
    }
   },
   "outputs": [],
   "source": [
    "moodylyrics = pd.read_csv(path)\n",
    "moodylyrics.columns = ['ID', 'ARTIST', 'SONG_TITLE', 'LYRICS_VECTOR', 'TITLE_VECTOR', \n",
    "                   'LINE_COUNT', 'WORD_COUNT', 'ECHOISMS', 'SELFISH_DEGREE', \n",
    "                   'DUPLICATE_LINES', 'IS_TITLE_IN_LYRICS', 'RHYMES', 'VERB_PRESENT', \n",
    "                   'VERB_PAST', 'VERB_FUTURE', 'ADJ_FREQUENCIES', 'CONJUCTION_FREQUENCIES', \n",
    "                   'ADV_FREQUENCIES', 'AUX_FREQUENCIES', 'CONJ_FREQUENCIES', 'CCONJ_FREQUENCIES', \n",
    "                   'DETERMINER_FREQUENCIES', 'INTERJECTION_FREQUENCIES', 'NOUN_FREQUENCIES', \n",
    "                   'NUM_FREQUENCIES', 'PART_FREQUENCIES', 'PRON_FREQUENCIES', 'PROPN_FREQUENCIES', \n",
    "                   'PUNCT_FREQUENCIES', 'SCONJ_FREQUENCIES', 'SYM_FREQUENCIES', 'VERB_FREQUENCIES', \n",
    "                   'X_FREQUENCIES', 'SPACE_FREQUENCIES', 'EMOTION']\n",
    "moodylyrics.drop(useless_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:02.001032Z",
     "start_time": "2018-04-21T15:08:01.986653Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = emoint.append(moodylyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:03.000798Z",
     "start_time": "2018-04-21T15:08:02.901918Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINE_COUNT</th>\n",
       "      <th>WORD_COUNT</th>\n",
       "      <th>ECHOISMS</th>\n",
       "      <th>SELFISH_DEGREE</th>\n",
       "      <th>DUPLICATE_LINES</th>\n",
       "      <th>RHYMES</th>\n",
       "      <th>VERB_PRESENT</th>\n",
       "      <th>VERB_PAST</th>\n",
       "      <th>VERB_FUTURE</th>\n",
       "      <th>ADJ_FREQUENCIES</th>\n",
       "      <th>...</th>\n",
       "      <th>INTERJECTION_FREQUENCIES</th>\n",
       "      <th>NOUN_FREQUENCIES</th>\n",
       "      <th>NUM_FREQUENCIES</th>\n",
       "      <th>PART_FREQUENCIES</th>\n",
       "      <th>PRON_FREQUENCIES</th>\n",
       "      <th>PROPN_FREQUENCIES</th>\n",
       "      <th>PUNCT_FREQUENCIES</th>\n",
       "      <th>SCONJ_FREQUENCIES</th>\n",
       "      <th>SYM_FREQUENCIES</th>\n",
       "      <th>VERB_FREQUENCIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.0</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.0</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.771143</td>\n",
       "      <td>115.487675</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.257142</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681412</td>\n",
       "      <td>0.223734</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>0.038996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.069006</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.024745</td>\n",
       "      <td>0.041535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.062544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.636621</td>\n",
       "      <td>123.785535</td>\n",
       "      <td>0.014999</td>\n",
       "      <td>0.307904</td>\n",
       "      <td>0.062358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.327729</td>\n",
       "      <td>0.276788</td>\n",
       "      <td>0.102573</td>\n",
       "      <td>0.074652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020679</td>\n",
       "      <td>0.111933</td>\n",
       "      <td>0.017120</td>\n",
       "      <td>0.020067</td>\n",
       "      <td>0.046580</td>\n",
       "      <td>0.081443</td>\n",
       "      <td>0.104194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011572</td>\n",
       "      <td>0.086848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.124038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451455</td>\n",
       "      <td>0.086786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.010677</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>188.000000</td>\n",
       "      <td>1149.000000</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LINE_COUNT   WORD_COUNT     ECHOISMS  SELFISH_DEGREE  DUPLICATE_LINES  \\\n",
       "count  4706.000000  4706.000000  4706.000000     4706.000000      4706.000000   \n",
       "mean     18.771143   115.487675     0.001933        0.257142         0.046589   \n",
       "std      21.636621   123.785535     0.014999        0.307904         0.062358   \n",
       "min       1.000000     1.000000     0.000000        0.000000         0.000000   \n",
       "25%       1.000000    16.000000     0.000000        0.000000         0.000000   \n",
       "50%      11.000000    64.000000     0.000000        0.142857         0.000000   \n",
       "75%      33.000000   198.000000     0.000000        0.451455         0.086786   \n",
       "max     188.000000  1149.000000     0.341463        1.000000         1.000000   \n",
       "\n",
       "       RHYMES  VERB_PRESENT    VERB_PAST  VERB_FUTURE  ADJ_FREQUENCIES  \\\n",
       "count  4706.0   4706.000000  4706.000000  4706.000000      4706.000000   \n",
       "mean      0.0      0.681412     0.223734     0.032805         0.038996   \n",
       "std       0.0      0.327729     0.276788     0.102573         0.074652   \n",
       "min       0.0      0.000000     0.000000     0.000000         0.000000   \n",
       "25%       0.0      0.500000     0.000000     0.000000         0.001886   \n",
       "50%       0.0      0.769231     0.124038     0.000000         0.005696   \n",
       "75%       0.0      1.000000     0.333333     0.000000         0.041667   \n",
       "max       0.0      1.000000     1.000000     1.000000         0.666667   \n",
       "\n",
       "             ...         INTERJECTION_FREQUENCIES  NOUN_FREQUENCIES  \\\n",
       "count        ...                      4706.000000       4706.000000   \n",
       "mean         ...                         0.003894          0.069006   \n",
       "std          ...                         0.020679          0.111933   \n",
       "min          ...                         0.000000          0.000000   \n",
       "25%          ...                         0.000000          0.004949   \n",
       "50%          ...                         0.000000          0.018131   \n",
       "75%          ...                         0.000460          0.083333   \n",
       "max          ...                         0.400000          1.000000   \n",
       "\n",
       "       NUM_FREQUENCIES  PART_FREQUENCIES  PRON_FREQUENCIES  PROPN_FREQUENCIES  \\\n",
       "count      4706.000000       4706.000000       4706.000000        4706.000000   \n",
       "mean          0.003092          0.006647          0.025388           0.024745   \n",
       "std           0.017120          0.020067          0.046580           0.081443   \n",
       "min           0.000000          0.000000          0.000000           0.000000   \n",
       "25%           0.000000          0.000000          0.002778           0.000000   \n",
       "50%           0.000000          0.000329          0.006885           0.000038   \n",
       "75%           0.000000          0.001757          0.027778           0.010677   \n",
       "max           0.444444          0.250000          0.500000           1.500000   \n",
       "\n",
       "       PUNCT_FREQUENCIES  SCONJ_FREQUENCIES  SYM_FREQUENCIES  VERB_FREQUENCIES  \n",
       "count        4706.000000             4706.0      4706.000000       4706.000000  \n",
       "mean            0.041535                0.0         0.000593          0.062544  \n",
       "std             0.104194                0.0         0.011572          0.086848  \n",
       "min             0.000000                0.0         0.000000          0.000000  \n",
       "25%             0.000589                0.0         0.000000          0.007813  \n",
       "50%             0.003652                0.0         0.000000          0.021222  \n",
       "75%             0.039216                0.0         0.000000          0.086957  \n",
       "max             2.000000                0.0         0.714286          1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Selection\n",
    "\n",
    "Based on our experience with previous models and feature engineering strategies, we believe that building models using all our available features is a waste of time. We already noticed that the models which achieved better results were those using just the content of the lyrics. Therefore we will work on just the content of our input texts (either lyrics or tweets) plus some additional features. Among all the available features we decided to pick the followings:\n",
    "- SELFISH_DEGREE\n",
    "- VERB_PRESENT\n",
    "- VERB_PAST\n",
    "- VERB_FUTURE\n",
    "\n",
    "In fact we believe that we can not use other features which may seem to be useful for our purpose ,e.g. \"RHYMES\", because, as we are considering a broader dataset, those kind of features are not general enough (not suitable for tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:04.322053Z",
     "start_time": "2018-04-21T15:08:04.317605Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset[['LYRICS_VECTOR', 'SELFISH_DEGREE', 'VERB_PRESENT', 'VERB_PAST', 'VERB_FUTURE', 'EMOTION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:04.649908Z",
     "start_time": "2018-04-21T15:08:04.634321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LYRICS_VECTOR</th>\n",
       "      <th>SELFISH_DEGREE</th>\n",
       "      <th>VERB_PRESENT</th>\n",
       "      <th>VERB_PAST</th>\n",
       "      <th>VERB_FUTURE</th>\n",
       "      <th>EMOTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.26710683e-01  1.60194725e-01 -1.36762261e-...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-6.28133714e-02  1.90393195e-01 -1.95530921e-...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 9.66307223e-02  2.91245524e-02 -1.42218113e-...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1.13483094e-01  3.13860744e-01 -2.05740720e-...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 3.85632203e-03  2.41273686e-01 -1.58885673e-...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       LYRICS_VECTOR  SELFISH_DEGREE  \\\n",
       "0  [-1.26710683e-01  1.60194725e-01 -1.36762261e-...        0.000000   \n",
       "1  [-6.28133714e-02  1.90393195e-01 -1.95530921e-...        1.000000   \n",
       "2  [ 9.66307223e-02  2.91245524e-02 -1.42218113e-...        0.000000   \n",
       "3  [-1.13483094e-01  3.13860744e-01 -2.05740720e-...        0.200000   \n",
       "4  [ 3.85632203e-03  2.41273686e-01 -1.58885673e-...        0.666667   \n",
       "\n",
       "   VERB_PRESENT  VERB_PAST  VERB_FUTURE EMOTION  \n",
       "0      0.750000   0.250000          0.0   happy  \n",
       "1      0.666667   0.333333          0.0   happy  \n",
       "2      0.500000   0.500000          0.0   happy  \n",
       "3      0.750000   0.250000          0.0   happy  \n",
       "4      1.000000   0.000000          0.0   happy  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Before starting we should flatten the dataset's features which are vectors at the moment (title vector and content vector). Let's do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:07.824778Z",
     "start_time": "2018-04-21T15:08:06.601130Z"
    }
   },
   "outputs": [],
   "source": [
    "X_vect = list()\n",
    "for (i, row) in dataset.drop('EMOTION', axis=1).iterrows():\n",
    "    sub_list = list()\n",
    "    for field in row:\n",
    "        if type(field) == str:\n",
    "            field = field[1:-1].split()\n",
    "            sub_list += [float(x.replace('\\n','')) for x in field]\n",
    "        else:\n",
    "            sub_list.append(field)\n",
    "    X_vect.append(np.array(sub_list))\n",
    "X_vect = np.array(X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:07.833616Z",
     "start_time": "2018-04-21T15:08:07.827586Z"
    }
   },
   "outputs": [],
   "source": [
    "y = dataset.EMOTION.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:08:07.957171Z",
     "start_time": "2018-04-21T15:08:07.952163Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4706, 304)\n",
      "(4706,)\n"
     ]
    }
   ],
   "source": [
    "print(X_vect.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we will have 4706 entries in our dataset, each of them having 304 different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:09:19.951524Z",
     "start_time": "2018-04-21T15:08:10.777811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1: 0.60 (+/- 0.14)\n",
      "Accuracy for k=3: 0.59 (+/- 0.15)\n",
      "Accuracy for k=5: 0.60 (+/- 0.17)\n",
      "Accuracy for k=7: 0.61 (+/- 0.18)\n",
      "Accuracy for k=9: 0.62 (+/- 0.17)\n",
      "Accuracy for k=11: 0.62 (+/- 0.19)\n",
      "Accuracy for k=13: 0.61 (+/- 0.18)\n",
      "Accuracy for k=15: 0.62 (+/- 0.18)\n",
      "Accuracy for k=17: 0.61 (+/- 0.17)\n",
      "Accuracy for k=19: 0.62 (+/- 0.17)\n",
      "Accuracy for k=21: 0.62 (+/- 0.17)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ks = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "\n",
    "for k in ks:\n",
    "    # Build model\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, algorithm='auto', \n",
    "                           metric='euclidean', n_jobs=-1)\n",
    "    # Evaluate accuracy\n",
    "    scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "    print('Accuracy for k=%d: %0.2f (+/- %0.2f)' % (k, scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:11:41.016582Z",
     "start_time": "2018-04-21T15:11:41.009507Z"
    }
   },
   "outputs": [],
   "source": [
    "def parameters_grid_search(classifier, params, x, y, cv=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Grid Search to find best parameters for a certain classifier whose\n",
    "    performances are evaluated using cross-validation\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(classifier(), params, cv=cv, n_jobs=-1, verbose=verbose)\n",
    "    gs.fit(x, y)    \n",
    "    return (gs.best_estimator_, gs.best_params_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:13:06.218806Z",
     "start_time": "2018-04-21T15:11:42.315044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'C': 1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build model\n",
    "clf = SVC()\n",
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['rbf'], 'C': [ 1 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params, best_score = parameters_grid_search(SVC, params, X_vect, y, verbose=1)\n",
    "print('Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T15:14:40.308784Z",
     "start_time": "2018-04-21T15:13:08.544371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm_best, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T14:30:08.449811Z",
     "start_time": "2018-04-21T14:13:15.866197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Build model\n",
    "clf = GradientBoostingClassifier(learning_rate=0.7, n_estimators=200)\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[EmoInt](http://saifmohammad.com/WebDocs/TweetEmotionIntensities-starsem2017.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
