{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#EmoInt-dataset-for-emotion-detection-in-lyrics\" data-toc-modified-id=\"EmoInt-dataset-for-emotion-detection-in-lyrics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>EmoInt dataset for emotion detection in lyrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#EmoInt-statistics\" data-toc-modified-id=\"EmoInt-statistics-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>EmoInt statistics</a></span></li><li><span><a href=\"#Merge-with-MoodyLyrics\" data-toc-modified-id=\"Merge-with-MoodyLyrics-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Merge with MoodyLyrics</a></span></li></ul></li><li><span><a href=\"#Features-Selection\" data-toc-modified-id=\"Features-Selection-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Features Selection</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#k-Nearest-Neighbour\" data-toc-modified-id=\"k-Nearest-Neighbour-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>k-Nearest Neighbour</a></span></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>SVM</a></span></li><li><span><a href=\"#Gradient-Boost\" data-toc-modified-id=\"Gradient-Boost-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Gradient Boost</a></span></li><li><span><a href=\"#Artificial-Neural-Network\" data-toc-modified-id=\"Artificial-Neural-Network-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Artificial Neural Network</a></span></li><li><span><a href=\"#Naive-Bayes-Classifier\" data-toc-modified-id=\"Naive-Bayes-Classifier-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Naive Bayes Classifier</a></span></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:40.536823Z",
     "start_time": "2018-04-26T12:50:40.111343Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.datasets import load_dataset_from_path, split_train_validation\n",
    "\n",
    "emotion_labels = ['happy', 'sad', 'angry', 'relaxed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:40.745603Z",
     "start_time": "2018-04-26T12:50:40.685563Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes = emotion_labels,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoInt dataset for emotion detection in lyrics\n",
    "\n",
    "Existing emotion datasets are mainly annotated categorically without an indication of degree of emotion. EmoInt, instead, provides several tweets annotated according to an emotion (anger, fear, joy, sadness) and to the degree at which the emotion is expressed in text.\n",
    "\n",
    "It is important to mention that EmoInt was manually annotated, using [Best-Worst Scaling](https://nparc.nrc-cnrc.gc.ca/eng/view/fulltext/?id=b132b0af-2ae0-4964-ac3a-493e7292a37a) (BWS), an annotation scheme shown to obtain very reliable scores.\n",
    "\n",
    "For our purpose, we will consider each tweet to be like a lyric and, on top of that, we will perform our feature engineering using spaCy and the other tools we used so far.\n",
    "\n",
    "Our original dataset, MoodyLyrics, contains \"happy\", \"sad\", \"angry\" and \"relaxed\" as labels. Therefore, in order to perform a sort of interjection with EmoInt, we will just use the tweets corresponding to the anger, joy and sadness emotions.\n",
    "\n",
    "The remaining part of this notebook assumes that we have already parsed EmoInt dataset in a .csv file which we can use to train some machine learning models as we did when we performed our feature engineering on lyrics. For more information about how this .csv was generated, please refer to the `src/emoint_parser.py` script.\n",
    "\n",
    "## EmoInt statistics\n",
    "\n",
    "As EmoInt provide intensity levels together with emotion labels, we decided to take into account only those tweets for which the intensity was greater that 0.50 (50%). Also, we dropped hashtags a remove the tag characters (e.g. \"Hey @MrTwitter how are you? #cool\" became \"Hey MrTwitter how are you?\") because we will have to compare those tweets with songs and songs do not have those kind of things. Also, this sort of preprocessing should maximize the chances that everything is recognized properly by spaCy's POS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:51.735401Z",
     "start_time": "2018-04-26T12:50:51.588134Z"
    }
   },
   "outputs": [],
   "source": [
    "emoint = pd.read_csv('datasets/emoint_featurized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:52.231355Z",
     "start_time": "2018-04-26T12:50:52.223157Z"
    }
   },
   "outputs": [],
   "source": [
    "useless_columns = [ 'ID','ARTIST', 'SONG_TITLE', 'X_FREQUENCIES', 'SPACE_FREQUENCIES']\n",
    "emoint.drop(useless_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:53.069844Z",
     "start_time": "2018-04-26T12:50:53.008418Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LYRICS_VECTOR</th>\n",
       "      <th>TITLE_VECTOR</th>\n",
       "      <th>LINE_COUNT</th>\n",
       "      <th>WORD_COUNT</th>\n",
       "      <th>ECHOISMS</th>\n",
       "      <th>SELFISH_DEGREE</th>\n",
       "      <th>DUPLICATE_LINES</th>\n",
       "      <th>IS_TITLE_IN_LYRICS</th>\n",
       "      <th>RHYMES</th>\n",
       "      <th>VERB_PRESENT</th>\n",
       "      <th>...</th>\n",
       "      <th>NOUN_FREQUENCIES</th>\n",
       "      <th>NUM_FREQUENCIES</th>\n",
       "      <th>PART_FREQUENCIES</th>\n",
       "      <th>PRON_FREQUENCIES</th>\n",
       "      <th>PROPN_FREQUENCIES</th>\n",
       "      <th>PUNCT_FREQUENCIES</th>\n",
       "      <th>SCONJ_FREQUENCIES</th>\n",
       "      <th>SYM_FREQUENCIES</th>\n",
       "      <th>VERB_FREQUENCIES</th>\n",
       "      <th>EMOTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.26710683e-01  1.60194725e-01 -1.36762261e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-6.28133714e-02  1.90393195e-01 -1.95530921e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 9.66307223e-02  2.91245524e-02 -1.42218113e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1.13483094e-01  3.13860744e-01 -2.05740720e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 3.85632203e-03  2.41273686e-01 -1.58885673e-...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       LYRICS_VECTOR  \\\n",
       "0  [-1.26710683e-01  1.60194725e-01 -1.36762261e-...   \n",
       "1  [-6.28133714e-02  1.90393195e-01 -1.95530921e-...   \n",
       "2  [ 9.66307223e-02  2.91245524e-02 -1.42218113e-...   \n",
       "3  [-1.13483094e-01  3.13860744e-01 -2.05740720e-...   \n",
       "4  [ 3.85632203e-03  2.41273686e-01 -1.58885673e-...   \n",
       "\n",
       "                                        TITLE_VECTOR  LINE_COUNT  WORD_COUNT  \\\n",
       "0  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          16   \n",
       "1  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          19   \n",
       "2  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          11   \n",
       "3  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          23   \n",
       "4  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...           1          22   \n",
       "\n",
       "   ECHOISMS  SELFISH_DEGREE  DUPLICATE_LINES  IS_TITLE_IN_LYRICS  RHYMES  \\\n",
       "0       0.0        0.000000              0.0               False     0.0   \n",
       "1       0.0        1.000000              0.0               False     0.0   \n",
       "2       0.0        0.000000              0.0               False     0.0   \n",
       "3       0.0        0.200000              0.0               False     0.0   \n",
       "4       0.0        0.666667              0.0               False     0.0   \n",
       "\n",
       "   VERB_PRESENT   ...     NOUN_FREQUENCIES  NUM_FREQUENCIES  PART_FREQUENCIES  \\\n",
       "0      0.750000   ...             0.062500              0.0          0.000000   \n",
       "1      0.666667   ...             0.157895              0.0          0.000000   \n",
       "2      0.500000   ...             0.545455              0.0          0.000000   \n",
       "3      0.750000   ...             0.086957              0.0          0.000000   \n",
       "4      1.000000   ...             0.272727              0.0          0.045455   \n",
       "\n",
       "   PRON_FREQUENCIES  PROPN_FREQUENCIES  PUNCT_FREQUENCIES  SCONJ_FREQUENCIES  \\\n",
       "0          0.000000           0.125000           0.187500                0.0   \n",
       "1          0.105263           0.000000           0.052632                0.0   \n",
       "2          0.000000           0.363636           0.000000                0.0   \n",
       "3          0.217391           0.043478           0.304348                0.0   \n",
       "4          0.136364           0.045455           0.136364                0.0   \n",
       "\n",
       "   SYM_FREQUENCIES  VERB_FREQUENCIES  EMOTION  \n",
       "0              0.0          0.250000    happy  \n",
       "1              0.0          0.263158    happy  \n",
       "2              0.0          0.181818    happy  \n",
       "3              0.0          0.173913    happy  \n",
       "4              0.0          0.227273    happy  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoint.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the same columns naming convention we used in the past notebooks with MoodyLyrics just for compatibility reasons (we will have to put them together). Since tweets do not have title, the `TITLE_VECTOR` was just left there as a vector of 0s, with the same shape of the `LYRICS_VECTOR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with MoodyLyrics\n",
    "Let's now merge EmoInt and MoodyLyrics featurized datasets in order to be able to proceed with further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:57.893627Z",
     "start_time": "2018-04-26T12:50:57.889912Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'datasets/moodylyrics_featurized.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:58.673562Z",
     "start_time": "2018-04-26T12:50:58.428914Z"
    }
   },
   "outputs": [],
   "source": [
    "moodylyrics = pd.read_csv(path)\n",
    "moodylyrics.columns = ['ID', 'ARTIST', 'SONG_TITLE', 'LYRICS_VECTOR', 'TITLE_VECTOR', \n",
    "                   'LINE_COUNT', 'WORD_COUNT', 'ECHOISMS', 'SELFISH_DEGREE', \n",
    "                   'DUPLICATE_LINES', 'IS_TITLE_IN_LYRICS', 'RHYMES', 'VERB_PRESENT', \n",
    "                   'VERB_PAST', 'VERB_FUTURE', 'ADJ_FREQUENCIES', 'CONJUCTION_FREQUENCIES', \n",
    "                   'ADV_FREQUENCIES', 'AUX_FREQUENCIES', 'CONJ_FREQUENCIES', 'CCONJ_FREQUENCIES', \n",
    "                   'DETERMINER_FREQUENCIES', 'INTERJECTION_FREQUENCIES', 'NOUN_FREQUENCIES', \n",
    "                   'NUM_FREQUENCIES', 'PART_FREQUENCIES', 'PRON_FREQUENCIES', 'PROPN_FREQUENCIES', \n",
    "                   'PUNCT_FREQUENCIES', 'SCONJ_FREQUENCIES', 'SYM_FREQUENCIES', 'VERB_FREQUENCIES', \n",
    "                   'X_FREQUENCIES', 'SPACE_FREQUENCIES', 'EMOTION']\n",
    "moodylyrics.drop(useless_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:59.172063Z",
     "start_time": "2018-04-26T12:50:59.161552Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = emoint.append(moodylyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:50:59.530528Z",
     "start_time": "2018-04-26T12:50:59.390477Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINE_COUNT</th>\n",
       "      <th>WORD_COUNT</th>\n",
       "      <th>ECHOISMS</th>\n",
       "      <th>SELFISH_DEGREE</th>\n",
       "      <th>DUPLICATE_LINES</th>\n",
       "      <th>RHYMES</th>\n",
       "      <th>VERB_PRESENT</th>\n",
       "      <th>VERB_PAST</th>\n",
       "      <th>VERB_FUTURE</th>\n",
       "      <th>ADJ_FREQUENCIES</th>\n",
       "      <th>...</th>\n",
       "      <th>INTERJECTION_FREQUENCIES</th>\n",
       "      <th>NOUN_FREQUENCIES</th>\n",
       "      <th>NUM_FREQUENCIES</th>\n",
       "      <th>PART_FREQUENCIES</th>\n",
       "      <th>PRON_FREQUENCIES</th>\n",
       "      <th>PROPN_FREQUENCIES</th>\n",
       "      <th>PUNCT_FREQUENCIES</th>\n",
       "      <th>SCONJ_FREQUENCIES</th>\n",
       "      <th>SYM_FREQUENCIES</th>\n",
       "      <th>VERB_FREQUENCIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.0</td>\n",
       "      <td>4706.000000</td>\n",
       "      <td>4706.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.771143</td>\n",
       "      <td>115.487675</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.257160</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.034445</td>\n",
       "      <td>0.681493</td>\n",
       "      <td>0.223661</td>\n",
       "      <td>0.032797</td>\n",
       "      <td>0.103892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014189</td>\n",
       "      <td>0.190628</td>\n",
       "      <td>0.007121</td>\n",
       "      <td>0.025846</td>\n",
       "      <td>0.118723</td>\n",
       "      <td>0.046162</td>\n",
       "      <td>0.097045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.235702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.636621</td>\n",
       "      <td>123.785535</td>\n",
       "      <td>0.014999</td>\n",
       "      <td>0.307920</td>\n",
       "      <td>0.062358</td>\n",
       "      <td>0.076869</td>\n",
       "      <td>0.327669</td>\n",
       "      <td>0.276698</td>\n",
       "      <td>0.102555</td>\n",
       "      <td>0.080279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031877</td>\n",
       "      <td>0.105343</td>\n",
       "      <td>0.023027</td>\n",
       "      <td>0.035716</td>\n",
       "      <td>0.085248</td>\n",
       "      <td>0.092595</td>\n",
       "      <td>0.128123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013569</td>\n",
       "      <td>0.104105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011205</td>\n",
       "      <td>0.114544</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451455</td>\n",
       "      <td>0.086786</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041301</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.060647</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>188.000000</td>\n",
       "      <td>1149.000000</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LINE_COUNT   WORD_COUNT     ECHOISMS  SELFISH_DEGREE  DUPLICATE_LINES  \\\n",
       "count  4706.000000  4706.000000  4706.000000     4706.000000      4706.000000   \n",
       "mean     18.771143   115.487675     0.001933        0.257160         0.046589   \n",
       "std      21.636621   123.785535     0.014999        0.307920         0.062358   \n",
       "min       1.000000     1.000000     0.000000        0.000000         0.000000   \n",
       "25%       1.000000    16.000000     0.000000        0.000000         0.000000   \n",
       "50%      11.000000    64.000000     0.000000        0.142857         0.000000   \n",
       "75%      33.000000   198.000000     0.000000        0.451455         0.086786   \n",
       "max     188.000000  1149.000000     0.341463        1.000000         1.000000   \n",
       "\n",
       "            RHYMES  VERB_PRESENT    VERB_PAST  VERB_FUTURE  ADJ_FREQUENCIES  \\\n",
       "count  4706.000000   4706.000000  4706.000000  4706.000000      4706.000000   \n",
       "mean      0.034445      0.681493     0.223661     0.032797         0.103892   \n",
       "std       0.076869      0.327669     0.276698     0.102555         0.080279   \n",
       "min       0.000000      0.000000     0.000000     0.000000         0.000000   \n",
       "25%       0.000000      0.500000     0.000000     0.000000         0.051323   \n",
       "50%       0.000000      0.769231     0.125000     0.000000         0.090909   \n",
       "75%       0.031250      1.000000     0.333333     0.000000         0.142857   \n",
       "max       0.735294      1.000000     1.000000     1.000000         0.666667   \n",
       "\n",
       "             ...         INTERJECTION_FREQUENCIES  NOUN_FREQUENCIES  \\\n",
       "count        ...                      4706.000000       4706.000000   \n",
       "mean         ...                         0.014189          0.190628   \n",
       "std          ...                         0.031877          0.105343   \n",
       "min          ...                         0.000000          0.000000   \n",
       "25%          ...                         0.000000          0.127972   \n",
       "50%          ...                         0.000000          0.178983   \n",
       "75%          ...                         0.013502          0.240000   \n",
       "max          ...                         0.400000          1.000000   \n",
       "\n",
       "       NUM_FREQUENCIES  PART_FREQUENCIES  PRON_FREQUENCIES  PROPN_FREQUENCIES  \\\n",
       "count      4706.000000       4706.000000       4706.000000        4706.000000   \n",
       "mean          0.007121          0.025846          0.118723           0.046162   \n",
       "std           0.023027          0.035716          0.085248           0.092595   \n",
       "min           0.000000          0.000000          0.000000           0.000000   \n",
       "25%           0.000000          0.000000          0.055556           0.000000   \n",
       "50%           0.000000          0.011205          0.114544           0.002660   \n",
       "75%           0.000000          0.041301          0.173913           0.060647   \n",
       "max           0.444444          0.333333          0.500000           1.500000   \n",
       "\n",
       "       PUNCT_FREQUENCIES  SCONJ_FREQUENCIES  SYM_FREQUENCIES  VERB_FREQUENCIES  \n",
       "count        4706.000000             4706.0      4706.000000       4706.000000  \n",
       "mean            0.097045                0.0         0.001023          0.235702  \n",
       "std             0.128123                0.0         0.013569          0.104105  \n",
       "min             0.000000                0.0         0.000000          0.000000  \n",
       "25%             0.016854                0.0         0.000000          0.178571  \n",
       "50%             0.068182                0.0         0.000000          0.241379  \n",
       "75%             0.135135                0.0         0.000000          0.295567  \n",
       "max             2.777778                0.0         0.714286          1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Selection\n",
    "\n",
    "Based on our experience with previous models and feature engineering strategies, we believe that building models using all our available features is a waste of time. We already noticed that the models which achieved better results were those using just the content of the lyrics. Therefore we will work on just the content of our input texts (either lyrics or tweets) plus some additional features. Among all the available features we decided to pick the followings:\n",
    "- SELFISH_DEGREE\n",
    "- VERB_PRESENT\n",
    "- VERB_PAST\n",
    "- VERB_FUTURE\n",
    "\n",
    "In fact we believe that we can not use other features which may seem to be useful for our purpose ,e.g. \"RHYMES\", because, as we are considering a broader dataset, those kind of features are not general enough (not suitable for tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:51:19.857936Z",
     "start_time": "2018-04-26T12:51:19.849219Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "   'LYRICS_VECTOR',\n",
    "   'WORD_COUNT', 'ECHOISMS', 'SELFISH_DEGREE', \n",
    "   'DUPLICATE_LINES', 'IS_TITLE_IN_LYRICS', 'VERB_PRESENT', \n",
    "   'VERB_PAST', 'VERB_FUTURE', 'ADJ_FREQUENCIES',\n",
    "   'PUNCT_FREQUENCIES', 'EMOTION'\n",
    "]\n",
    "dataset = dataset[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:51:20.640445Z",
     "start_time": "2018-04-26T12:51:20.604450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LYRICS_VECTOR</th>\n",
       "      <th>WORD_COUNT</th>\n",
       "      <th>ECHOISMS</th>\n",
       "      <th>SELFISH_DEGREE</th>\n",
       "      <th>DUPLICATE_LINES</th>\n",
       "      <th>IS_TITLE_IN_LYRICS</th>\n",
       "      <th>VERB_PRESENT</th>\n",
       "      <th>VERB_PAST</th>\n",
       "      <th>VERB_FUTURE</th>\n",
       "      <th>ADJ_FREQUENCIES</th>\n",
       "      <th>PUNCT_FREQUENCIES</th>\n",
       "      <th>EMOTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.26710683e-01  1.60194725e-01 -1.36762261e-...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-6.28133714e-02  1.90393195e-01 -1.95530921e-...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 9.66307223e-02  2.91245524e-02 -1.42218113e-...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1.13483094e-01  3.13860744e-01 -2.05740720e-...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 3.85632203e-03  2.41273686e-01 -1.58885673e-...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       LYRICS_VECTOR  WORD_COUNT  ECHOISMS  \\\n",
       "0  [-1.26710683e-01  1.60194725e-01 -1.36762261e-...          16       0.0   \n",
       "1  [-6.28133714e-02  1.90393195e-01 -1.95530921e-...          19       0.0   \n",
       "2  [ 9.66307223e-02  2.91245524e-02 -1.42218113e-...          11       0.0   \n",
       "3  [-1.13483094e-01  3.13860744e-01 -2.05740720e-...          23       0.0   \n",
       "4  [ 3.85632203e-03  2.41273686e-01 -1.58885673e-...          22       0.0   \n",
       "\n",
       "   SELFISH_DEGREE  DUPLICATE_LINES  IS_TITLE_IN_LYRICS  VERB_PRESENT  \\\n",
       "0        0.000000              0.0               False      0.750000   \n",
       "1        1.000000              0.0               False      0.666667   \n",
       "2        0.000000              0.0               False      0.500000   \n",
       "3        0.200000              0.0               False      0.750000   \n",
       "4        0.666667              0.0               False      1.000000   \n",
       "\n",
       "   VERB_PAST  VERB_FUTURE  ADJ_FREQUENCIES  PUNCT_FREQUENCIES EMOTION  \n",
       "0   0.250000          0.0         0.062500           0.187500   happy  \n",
       "1   0.333333          0.0         0.105263           0.052632   happy  \n",
       "2   0.500000          0.0         0.000000           0.000000   happy  \n",
       "3   0.250000          0.0         0.173913           0.304348   happy  \n",
       "4   0.000000          0.0         0.090909           0.136364   happy  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Before starting we should flatten the dataset's features which are vectors at the moment (title vector and content vector). Let's do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:51:24.601135Z",
     "start_time": "2018-04-26T12:51:23.219391Z"
    }
   },
   "outputs": [],
   "source": [
    "X_vect = list()\n",
    "for (i, row) in dataset.drop('EMOTION', axis=1).iterrows():\n",
    "    sub_list = list()\n",
    "    for field in row:\n",
    "        if type(field) == str:\n",
    "            field = field[1:-1].split()\n",
    "            sub_list += [float(x.replace('\\n','')) for x in field]\n",
    "        else:\n",
    "            sub_list.append(field)\n",
    "    X_vect.append(np.array(sub_list))\n",
    "X_vect = np.array(X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:51:26.242670Z",
     "start_time": "2018-04-26T12:51:26.230184Z"
    }
   },
   "outputs": [],
   "source": [
    "y = dataset.EMOTION.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:51:26.856090Z",
     "start_time": "2018-04-26T12:51:26.848673Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4706, 310)\n",
      "(4706,)\n"
     ]
    }
   ],
   "source": [
    "print(X_vect.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:56:26.100696Z",
     "start_time": "2018-04-26T12:56:26.093557Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we will have 4706 entries in our dataset, each of them having 304 different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:51:48.611597Z",
     "start_time": "2018-04-26T12:51:29.246084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1: 0.44 (+/- 0.11)\n",
      "Accuracy for k=3: 0.43 (+/- 0.09)\n",
      "Accuracy for k=5: 0.43 (+/- 0.09)\n",
      "Accuracy for k=7: 0.42 (+/- 0.10)\n",
      "Accuracy for k=9: 0.42 (+/- 0.13)\n",
      "Accuracy for k=11: 0.42 (+/- 0.13)\n",
      "Accuracy for k=13: 0.41 (+/- 0.13)\n",
      "Accuracy for k=15: 0.40 (+/- 0.14)\n",
      "Accuracy for k=17: 0.41 (+/- 0.14)\n",
      "Accuracy for k=19: 0.40 (+/- 0.15)\n",
      "Accuracy for k=21: 0.40 (+/- 0.16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ks = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "\n",
    "for k in ks:\n",
    "    # Build model\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, algorithm='auto', \n",
    "                           metric='euclidean', n_jobs=-1)\n",
    "    # Evaluate accuracy\n",
    "    scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "    print('Accuracy for k=%d: %0.2f (+/- %0.2f)' % (k, scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T17:10:48.375783Z",
     "start_time": "2018-04-21T17:10:48.367095Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def parameters_grid_search(classifier, params, x, y, cv=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Grid Search to find best parameters for a certain classifier whose\n",
    "    performances are evaluated using cross-validation\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(classifier(), params, cv=cv, n_jobs=-1, verbose=verbose)\n",
    "    gs.fit(x, y)    \n",
    "    return (gs.best_estimator_, gs.best_params_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:53:19.028587Z",
     "start_time": "2018-04-26T12:53:19.025039Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T17:12:15.472597Z",
     "start_time": "2018-04-21T17:10:48.380152Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'kernel': 'rbf', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "clf = SVC()\n",
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['rbf'], 'C': [ 1 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params, best_score = parameters_grid_search(SVC, params, X_vect, y, verbose=1)\n",
    "print('Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T17:13:48.196076Z",
     "start_time": "2018-04-21T17:12:15.476014Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm_best, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:56:39.530037Z",
     "start_time": "2018-04-26T12:56:30.412884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C=1, kernel='rbf')\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T13:00:08.178303Z",
     "start_time": "2018-04-26T13:00:06.483537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T17:29:42.745409Z",
     "start_time": "2018-04-21T17:13:48.198592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Build model\n",
    "clf = GradientBoostingClassifier(learning_rate=0.7, n_estimators=200)\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T17:42:35.312042Z",
     "start_time": "2018-04-21T17:42:35.151390Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1 Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#2 y_nn should be a vector (len(X_vect),4), with a 1 in the right class\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "y_nn = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "#3 Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# we need to scale because we don't want one feature to predomine the others\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T17:45:06.879528Z",
     "start_time": "2018-04-21T17:45:06.790140Z"
    }
   },
   "outputs": [],
   "source": [
    "#1 Importing the Keras libraries and packages\n",
    "import keras\n",
    "# Sequential module is required to initialize our ANN\n",
    "from keras.models import Sequential\n",
    "# Dense module is required to create the layers\n",
    "from keras.layers import Dense, Dropout\n",
    "    \n",
    "def build_ann(optimizer='adam', input_size=X_vect.shape[1]):\n",
    "    classifier = Sequential()\n",
    "    #2 Adding first hidden layer\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'random_normal', activation = 'sigmoid', input_dim = input_size))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    #2 Adding second hidden layer\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'random_normal', activation = 'sigmoid', input_dim = input_size))\n",
    "    classifier.add(Dropout(0.5))\n",
    "   \n",
    "\n",
    "    # Adding output layer\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'random_normal', activation = 'softmax'))\n",
    "\n",
    "    #3 Compiling the ANN\n",
    "    classifier.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T17:58:44.643703Z",
     "start_time": "2018-04-21T17:58:13.875056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3764/3764 [==============================] - 1s 308us/step - loss: 1.3520 - acc: 0.3127\n",
      "Epoch 2/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 1.3255 - acc: 0.3398\n",
      "Epoch 3/300\n",
      "3764/3764 [==============================] - 0s 47us/step - loss: 1.3031 - acc: 0.3882\n",
      "Epoch 4/300\n",
      "3764/3764 [==============================] - 0s 37us/step - loss: 1.2691 - acc: 0.4485\n",
      "Epoch 5/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 1.1969 - acc: 0.5197\n",
      "Epoch 6/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 1.1203 - acc: 0.5584\n",
      "Epoch 7/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 1.0344 - acc: 0.5882\n",
      "Epoch 8/300\n",
      "3764/3764 [==============================] - 0s 28us/step - loss: 0.9709 - acc: 0.6079\n",
      "Epoch 9/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.9262 - acc: 0.6124\n",
      "Epoch 10/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.8768 - acc: 0.6469\n",
      "Epoch 11/300\n",
      "3764/3764 [==============================] - 0s 30us/step - loss: 0.8424 - acc: 0.6589\n",
      "Epoch 12/300\n",
      "3764/3764 [==============================] - 0s 36us/step - loss: 0.8219 - acc: 0.6655\n",
      "Epoch 13/300\n",
      "3764/3764 [==============================] - 0s 35us/step - loss: 0.7832 - acc: 0.6796\n",
      "Epoch 14/300\n",
      "3764/3764 [==============================] - 0s 32us/step - loss: 0.7654 - acc: 0.6929\n",
      "Epoch 15/300\n",
      "3764/3764 [==============================] - 0s 32us/step - loss: 0.7423 - acc: 0.7056\n",
      "Epoch 16/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 0.7124 - acc: 0.7168\n",
      "Epoch 17/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.6995 - acc: 0.7303\n",
      "Epoch 18/300\n",
      "3764/3764 [==============================] - 0s 30us/step - loss: 0.6820 - acc: 0.7386\n",
      "Epoch 19/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.6749 - acc: 0.7372\n",
      "Epoch 20/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.6642 - acc: 0.7396\n",
      "Epoch 21/300\n",
      "3764/3764 [==============================] - 0s 34us/step - loss: 0.6410 - acc: 0.7540\n",
      "Epoch 22/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.6305 - acc: 0.7667\n",
      "Epoch 23/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.6219 - acc: 0.7646\n",
      "Epoch 24/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 0.6212 - acc: 0.7588\n",
      "Epoch 25/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.6093 - acc: 0.7617\n",
      "Epoch 26/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.5955 - acc: 0.7747\n",
      "Epoch 27/300\n",
      "3764/3764 [==============================] - 0s 28us/step - loss: 0.5938 - acc: 0.7670\n",
      "Epoch 28/300\n",
      "3764/3764 [==============================] - 0s 31us/step - loss: 0.5812 - acc: 0.7760\n",
      "Epoch 29/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.5714 - acc: 0.7880\n",
      "Epoch 30/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.5671 - acc: 0.7837\n",
      "Epoch 31/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.5632 - acc: 0.7901\n",
      "Epoch 32/300\n",
      "3764/3764 [==============================] - 0s 39us/step - loss: 0.5373 - acc: 0.7970\n",
      "Epoch 33/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.5497 - acc: 0.7925\n",
      "Epoch 34/300\n",
      "3764/3764 [==============================] - 0s 33us/step - loss: 0.5490 - acc: 0.7920\n",
      "Epoch 35/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.5287 - acc: 0.8026\n",
      "Epoch 36/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.5392 - acc: 0.7960\n",
      "Epoch 37/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.5296 - acc: 0.7970\n",
      "Epoch 38/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.5152 - acc: 0.7994\n",
      "Epoch 39/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.5159 - acc: 0.8053\n",
      "Epoch 40/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.5080 - acc: 0.8119\n",
      "Epoch 41/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.5252 - acc: 0.8005\n",
      "Epoch 42/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.5123 - acc: 0.8050\n",
      "Epoch 43/300\n",
      "3764/3764 [==============================] - 0s 30us/step - loss: 0.5144 - acc: 0.8031\n",
      "Epoch 44/300\n",
      "3764/3764 [==============================] - 0s 28us/step - loss: 0.5028 - acc: 0.8122\n",
      "Epoch 45/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.4938 - acc: 0.8162\n",
      "Epoch 46/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.4995 - acc: 0.8124\n",
      "Epoch 47/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4923 - acc: 0.8151\n",
      "Epoch 48/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.4887 - acc: 0.8241\n",
      "Epoch 49/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.4927 - acc: 0.8124\n",
      "Epoch 50/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.4942 - acc: 0.8146\n",
      "Epoch 51/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.4851 - acc: 0.8156\n",
      "Epoch 52/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.4869 - acc: 0.8172\n",
      "Epoch 53/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4850 - acc: 0.8223\n",
      "Epoch 54/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 0.4737 - acc: 0.8239\n",
      "Epoch 55/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4655 - acc: 0.8260\n",
      "Epoch 56/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4583 - acc: 0.8340\n",
      "Epoch 57/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.4668 - acc: 0.8308\n",
      "Epoch 58/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4558 - acc: 0.8268\n",
      "Epoch 59/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4636 - acc: 0.8207\n",
      "Epoch 60/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4624 - acc: 0.8249\n",
      "Epoch 61/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.4702 - acc: 0.8185\n",
      "Epoch 62/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4573 - acc: 0.8268\n",
      "Epoch 63/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.4650 - acc: 0.8278\n",
      "Epoch 64/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.4421 - acc: 0.8300\n",
      "Epoch 65/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.4506 - acc: 0.8316\n",
      "Epoch 66/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.4429 - acc: 0.8329\n",
      "Epoch 67/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.4371 - acc: 0.8353\n",
      "Epoch 68/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.4479 - acc: 0.8324\n",
      "Epoch 69/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.4258 - acc: 0.8488\n",
      "Epoch 70/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.4358 - acc: 0.8411\n",
      "Epoch 71/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.4357 - acc: 0.8382\n",
      "Epoch 72/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4278 - acc: 0.8387\n",
      "Epoch 73/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4337 - acc: 0.8379\n",
      "Epoch 74/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.4403 - acc: 0.8401\n",
      "Epoch 75/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4245 - acc: 0.8377\n",
      "Epoch 76/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4373 - acc: 0.8350\n",
      "Epoch 77/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4219 - acc: 0.8448\n",
      "Epoch 78/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4212 - acc: 0.8433\n",
      "Epoch 79/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.4091 - acc: 0.8446\n",
      "Epoch 80/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4205 - acc: 0.8377\n",
      "Epoch 81/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4213 - acc: 0.8435\n",
      "Epoch 82/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4124 - acc: 0.8515\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4000 - acc: 0.8491\n",
      "Epoch 84/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.4115 - acc: 0.8456\n",
      "Epoch 85/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.3946 - acc: 0.8475\n",
      "Epoch 86/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4020 - acc: 0.8510\n",
      "Epoch 87/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.4079 - acc: 0.8433\n",
      "Epoch 88/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.3985 - acc: 0.8571\n",
      "Epoch 89/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.4008 - acc: 0.8520\n",
      "Epoch 90/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 0.3915 - acc: 0.8531\n",
      "Epoch 91/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.3912 - acc: 0.8512\n",
      "Epoch 92/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.3945 - acc: 0.8515\n",
      "Epoch 93/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.3968 - acc: 0.8472\n",
      "Epoch 94/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.3956 - acc: 0.8486\n",
      "Epoch 95/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.3983 - acc: 0.8518\n",
      "Epoch 96/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.3946 - acc: 0.8496\n",
      "Epoch 97/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.3796 - acc: 0.8549\n",
      "Epoch 98/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3889 - acc: 0.8499\n",
      "Epoch 99/300\n",
      "3764/3764 [==============================] - 0s 30us/step - loss: 0.3781 - acc: 0.8576\n",
      "Epoch 100/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3869 - acc: 0.8568\n",
      "Epoch 101/300\n",
      "3764/3764 [==============================] - 0s 30us/step - loss: 0.3749 - acc: 0.8640\n",
      "Epoch 102/300\n",
      "3764/3764 [==============================] - 0s 30us/step - loss: 0.3827 - acc: 0.8573\n",
      "Epoch 103/300\n",
      "3764/3764 [==============================] - 0s 28us/step - loss: 0.3746 - acc: 0.8626\n",
      "Epoch 104/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3789 - acc: 0.8531\n",
      "Epoch 105/300\n",
      "3764/3764 [==============================] - 0s 28us/step - loss: 0.3931 - acc: 0.8512\n",
      "Epoch 106/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3881 - acc: 0.8557\n",
      "Epoch 107/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3700 - acc: 0.8690\n",
      "Epoch 108/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3753 - acc: 0.8589\n",
      "Epoch 109/300\n",
      "3764/3764 [==============================] - 0s 28us/step - loss: 0.3613 - acc: 0.8648\n",
      "Epoch 110/300\n",
      "3764/3764 [==============================] - 0s 30us/step - loss: 0.3695 - acc: 0.8605\n",
      "Epoch 111/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3645 - acc: 0.8629\n",
      "Epoch 112/300\n",
      "3764/3764 [==============================] - 0s 31us/step - loss: 0.3661 - acc: 0.8632\n",
      "Epoch 113/300\n",
      "3764/3764 [==============================] - 0s 34us/step - loss: 0.3684 - acc: 0.8629\n",
      "Epoch 114/300\n",
      "3764/3764 [==============================] - 0s 34us/step - loss: 0.3775 - acc: 0.8616\n",
      "Epoch 115/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 0.3574 - acc: 0.8661\n",
      "Epoch 116/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3453 - acc: 0.8722\n",
      "Epoch 117/300\n",
      "3764/3764 [==============================] - 0s 30us/step - loss: 0.3493 - acc: 0.8682\n",
      "Epoch 118/300\n",
      "3764/3764 [==============================] - 0s 37us/step - loss: 0.3545 - acc: 0.8656\n",
      "Epoch 119/300\n",
      "3764/3764 [==============================] - 0s 36us/step - loss: 0.3470 - acc: 0.8664\n",
      "Epoch 120/300\n",
      "3764/3764 [==============================] - 0s 35us/step - loss: 0.3572 - acc: 0.8682\n",
      "Epoch 121/300\n",
      "3764/3764 [==============================] - 0s 26us/step - loss: 0.3656 - acc: 0.8661\n",
      "Epoch 122/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.3573 - acc: 0.8685\n",
      "Epoch 123/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 0.3538 - acc: 0.8653\n",
      "Epoch 124/300\n",
      "3764/3764 [==============================] - 0s 32us/step - loss: 0.3531 - acc: 0.8669\n",
      "Epoch 125/300\n",
      "3764/3764 [==============================] - 0s 31us/step - loss: 0.3561 - acc: 0.8642\n",
      "Epoch 126/300\n",
      "3764/3764 [==============================] - 0s 35us/step - loss: 0.3552 - acc: 0.8656\n",
      "Epoch 127/300\n",
      "3764/3764 [==============================] - 0s 33us/step - loss: 0.3487 - acc: 0.8688\n",
      "Epoch 128/300\n",
      "3764/3764 [==============================] - 0s 37us/step - loss: 0.3437 - acc: 0.8765\n",
      "Epoch 129/300\n",
      "3764/3764 [==============================] - 0s 44us/step - loss: 0.3468 - acc: 0.8770\n",
      "Epoch 130/300\n",
      "3764/3764 [==============================] - 0s 40us/step - loss: 0.3538 - acc: 0.8587\n",
      "Epoch 131/300\n",
      "3764/3764 [==============================] - 0s 33us/step - loss: 0.3543 - acc: 0.8648\n",
      "Epoch 132/300\n",
      "3764/3764 [==============================] - 0s 35us/step - loss: 0.3490 - acc: 0.8738\n",
      "Epoch 133/300\n",
      "3764/3764 [==============================] - 0s 50us/step - loss: 0.3417 - acc: 0.8727\n",
      "Epoch 134/300\n",
      "3764/3764 [==============================] - 0s 50us/step - loss: 0.3423 - acc: 0.8677\n",
      "Epoch 135/300\n",
      "3764/3764 [==============================] - 0s 45us/step - loss: 0.3466 - acc: 0.8719\n",
      "Epoch 136/300\n",
      "3764/3764 [==============================] - 0s 35us/step - loss: 0.3397 - acc: 0.8738\n",
      "Epoch 137/300\n",
      "3764/3764 [==============================] - 0s 33us/step - loss: 0.3510 - acc: 0.8738\n",
      "Epoch 138/300\n",
      "3764/3764 [==============================] - 0s 43us/step - loss: 0.3308 - acc: 0.8746\n",
      "Epoch 139/300\n",
      "3764/3764 [==============================] - 0s 43us/step - loss: 0.3489 - acc: 0.8791\n",
      "Epoch 140/300\n",
      "3764/3764 [==============================] - 0s 45us/step - loss: 0.3330 - acc: 0.8711\n",
      "Epoch 141/300\n",
      "3764/3764 [==============================] - 0s 33us/step - loss: 0.3316 - acc: 0.8709\n",
      "Epoch 142/300\n",
      "3764/3764 [==============================] - 0s 33us/step - loss: 0.3344 - acc: 0.8794\n",
      "Epoch 143/300\n",
      "3764/3764 [==============================] - 0s 38us/step - loss: 0.3403 - acc: 0.8738\n",
      "Epoch 144/300\n",
      "3764/3764 [==============================] - 0s 42us/step - loss: 0.3344 - acc: 0.8754\n",
      "Epoch 145/300\n",
      "3764/3764 [==============================] - 0s 43us/step - loss: 0.3134 - acc: 0.8831\n",
      "Epoch 146/300\n",
      "3764/3764 [==============================] - 0s 39us/step - loss: 0.3306 - acc: 0.8749\n",
      "Epoch 147/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.3346 - acc: 0.8717\n",
      "Epoch 148/300\n",
      "3764/3764 [==============================] - 0s 34us/step - loss: 0.3321 - acc: 0.8778\n",
      "Epoch 149/300\n",
      "3764/3764 [==============================] - 0s 36us/step - loss: 0.3246 - acc: 0.8794\n",
      "Epoch 150/300\n",
      "3764/3764 [==============================] - 0s 29us/step - loss: 0.3199 - acc: 0.8823\n",
      "Epoch 151/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 0.3280 - acc: 0.8770\n",
      "Epoch 152/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.3233 - acc: 0.8796\n",
      "Epoch 153/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.3189 - acc: 0.8839\n",
      "Epoch 154/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.3137 - acc: 0.8799\n",
      "Epoch 155/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3188 - acc: 0.8786\n",
      "Epoch 156/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3179 - acc: 0.8812\n",
      "Epoch 157/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.3228 - acc: 0.8757\n",
      "Epoch 158/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3210 - acc: 0.8823\n",
      "Epoch 159/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.3141 - acc: 0.8778\n",
      "Epoch 160/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.3158 - acc: 0.8823\n",
      "Epoch 161/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.3109 - acc: 0.8812\n",
      "Epoch 162/300\n",
      "3764/3764 [==============================] - 0s 28us/step - loss: 0.3080 - acc: 0.8868\n",
      "Epoch 163/300\n",
      "3764/3764 [==============================] - 0s 36us/step - loss: 0.3152 - acc: 0.8802\n",
      "Epoch 164/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.3071 - acc: 0.8908\n",
      "Epoch 165/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.3035 - acc: 0.8871\n",
      "Epoch 166/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.3089 - acc: 0.8879\n",
      "Epoch 167/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3224 - acc: 0.8847\n",
      "Epoch 168/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3094 - acc: 0.8844\n",
      "Epoch 169/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.3154 - acc: 0.8852\n",
      "Epoch 170/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3015 - acc: 0.8879\n",
      "Epoch 171/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3152 - acc: 0.8858\n",
      "Epoch 172/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.3078 - acc: 0.8823\n",
      "Epoch 173/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3084 - acc: 0.8850\n",
      "Epoch 174/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2859 - acc: 0.8921\n",
      "Epoch 175/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2978 - acc: 0.8908\n",
      "Epoch 176/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2965 - acc: 0.8882\n",
      "Epoch 177/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.2951 - acc: 0.8953\n",
      "Epoch 178/300\n",
      "3764/3764 [==============================] - 0s 27us/step - loss: 0.2958 - acc: 0.8905\n",
      "Epoch 179/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.3092 - acc: 0.8879\n",
      "Epoch 180/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.3029 - acc: 0.8826\n",
      "Epoch 181/300\n",
      "3764/3764 [==============================] - 0s 24us/step - loss: 0.2972 - acc: 0.8831\n",
      "Epoch 182/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.3036 - acc: 0.8834\n",
      "Epoch 183/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.2889 - acc: 0.8940\n",
      "Epoch 184/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.3000 - acc: 0.8866\n",
      "Epoch 185/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2903 - acc: 0.8943\n",
      "Epoch 186/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2858 - acc: 0.8943\n",
      "Epoch 187/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2944 - acc: 0.8921\n",
      "Epoch 188/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2849 - acc: 0.8943\n",
      "Epoch 189/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.2836 - acc: 0.8929\n",
      "Epoch 190/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.2865 - acc: 0.8921\n",
      "Epoch 191/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2902 - acc: 0.8892\n",
      "Epoch 192/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.2839 - acc: 0.8943\n",
      "Epoch 193/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2892 - acc: 0.8945\n",
      "Epoch 194/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2806 - acc: 0.8967\n",
      "Epoch 195/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2921 - acc: 0.8895\n",
      "Epoch 196/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2954 - acc: 0.8916\n",
      "Epoch 197/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2764 - acc: 0.8969\n",
      "Epoch 198/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.2767 - acc: 0.8921\n",
      "Epoch 199/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.2740 - acc: 0.9004\n",
      "Epoch 200/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2836 - acc: 0.8858\n",
      "Epoch 201/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2919 - acc: 0.8943\n",
      "Epoch 202/300\n",
      "3764/3764 [==============================] - 0s 19us/step - loss: 0.2942 - acc: 0.8866\n",
      "Epoch 203/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2900 - acc: 0.8889\n",
      "Epoch 204/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2783 - acc: 0.8982\n",
      "Epoch 205/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2799 - acc: 0.8921\n",
      "Epoch 206/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2903 - acc: 0.8903\n",
      "Epoch 207/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2687 - acc: 0.9022\n",
      "Epoch 208/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2788 - acc: 0.8953\n",
      "Epoch 209/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2800 - acc: 0.8940\n",
      "Epoch 210/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2794 - acc: 0.8953\n",
      "Epoch 211/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2817 - acc: 0.8948\n",
      "Epoch 212/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2694 - acc: 0.8996\n",
      "Epoch 213/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2606 - acc: 0.9038\n",
      "Epoch 214/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2629 - acc: 0.9012\n",
      "Epoch 215/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2806 - acc: 0.8969\n",
      "Epoch 216/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2702 - acc: 0.8990\n",
      "Epoch 217/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2753 - acc: 0.9014\n",
      "Epoch 218/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2731 - acc: 0.8948\n",
      "Epoch 219/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2761 - acc: 0.8985\n",
      "Epoch 220/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2672 - acc: 0.8969\n",
      "Epoch 221/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2526 - acc: 0.9001\n",
      "Epoch 222/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2678 - acc: 0.9001\n",
      "Epoch 223/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2795 - acc: 0.9001\n",
      "Epoch 224/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2663 - acc: 0.8953\n",
      "Epoch 225/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2586 - acc: 0.9038\n",
      "Epoch 226/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2659 - acc: 0.9001\n",
      "Epoch 227/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2674 - acc: 0.9004\n",
      "Epoch 228/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2761 - acc: 0.8964\n",
      "Epoch 229/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2637 - acc: 0.9004\n",
      "Epoch 230/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2506 - acc: 0.8980\n",
      "Epoch 231/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2572 - acc: 0.9086\n",
      "Epoch 232/300\n",
      "3764/3764 [==============================] - 0s 19us/step - loss: 0.2452 - acc: 0.9067\n",
      "Epoch 233/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2578 - acc: 0.9049\n",
      "Epoch 234/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2520 - acc: 0.9046\n",
      "Epoch 235/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2576 - acc: 0.9102\n",
      "Epoch 236/300\n",
      "3764/3764 [==============================] - 0s 19us/step - loss: 0.2610 - acc: 0.8996\n",
      "Epoch 237/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2645 - acc: 0.9036\n",
      "Epoch 238/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2597 - acc: 0.9022\n",
      "Epoch 239/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2649 - acc: 0.9020\n",
      "Epoch 240/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2511 - acc: 0.9086\n",
      "Epoch 241/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.2571 - acc: 0.9083\n",
      "Epoch 242/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2459 - acc: 0.9089\n",
      "Epoch 243/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2657 - acc: 0.9036\n",
      "Epoch 244/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2633 - acc: 0.8993\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2418 - acc: 0.9123\n",
      "Epoch 246/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2440 - acc: 0.9097\n",
      "Epoch 247/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2433 - acc: 0.9139\n",
      "Epoch 248/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2522 - acc: 0.9022\n",
      "Epoch 249/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2507 - acc: 0.9025\n",
      "Epoch 250/300\n",
      "3764/3764 [==============================] - 0s 19us/step - loss: 0.2501 - acc: 0.9070\n",
      "Epoch 251/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2489 - acc: 0.9054\n",
      "Epoch 252/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2448 - acc: 0.9062\n",
      "Epoch 253/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2307 - acc: 0.9158\n",
      "Epoch 254/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2530 - acc: 0.9097\n",
      "Epoch 255/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2499 - acc: 0.9118\n",
      "Epoch 256/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2507 - acc: 0.9115\n",
      "Epoch 257/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2419 - acc: 0.9086\n",
      "Epoch 258/300\n",
      "3764/3764 [==============================] - 0s 19us/step - loss: 0.2314 - acc: 0.9160\n",
      "Epoch 259/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2544 - acc: 0.9054\n",
      "Epoch 260/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2308 - acc: 0.9166\n",
      "Epoch 261/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2406 - acc: 0.9115\n",
      "Epoch 262/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2459 - acc: 0.9102\n",
      "Epoch 263/300\n",
      "3764/3764 [==============================] - 0s 19us/step - loss: 0.2517 - acc: 0.8998\n",
      "Epoch 264/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2475 - acc: 0.9057\n",
      "Epoch 265/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2509 - acc: 0.9089\n",
      "Epoch 266/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.2410 - acc: 0.9086\n",
      "Epoch 267/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.2458 - acc: 0.9115\n",
      "Epoch 268/300\n",
      "3764/3764 [==============================] - 0s 25us/step - loss: 0.2377 - acc: 0.9123\n",
      "Epoch 269/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2332 - acc: 0.9126\n",
      "Epoch 270/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2410 - acc: 0.9134\n",
      "Epoch 271/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2329 - acc: 0.9139\n",
      "Epoch 272/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2325 - acc: 0.9118\n",
      "Epoch 273/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2342 - acc: 0.9123\n",
      "Epoch 274/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2430 - acc: 0.9044\n",
      "Epoch 275/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2420 - acc: 0.9075\n",
      "Epoch 276/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2409 - acc: 0.9091\n",
      "Epoch 277/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2420 - acc: 0.9121\n",
      "Epoch 278/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2354 - acc: 0.9158\n",
      "Epoch 279/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2326 - acc: 0.9147\n",
      "Epoch 280/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2320 - acc: 0.9137\n",
      "Epoch 281/300\n",
      "3764/3764 [==============================] - 0s 22us/step - loss: 0.2360 - acc: 0.9174\n",
      "Epoch 282/300\n",
      "3764/3764 [==============================] - 0s 36us/step - loss: 0.2338 - acc: 0.9155\n",
      "Epoch 283/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2243 - acc: 0.9174\n",
      "Epoch 284/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2212 - acc: 0.9198\n",
      "Epoch 285/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2475 - acc: 0.9121\n",
      "Epoch 286/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2382 - acc: 0.9176\n",
      "Epoch 287/300\n",
      "3764/3764 [==============================] - 0s 19us/step - loss: 0.2353 - acc: 0.9097\n",
      "Epoch 288/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2219 - acc: 0.9174\n",
      "Epoch 289/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2259 - acc: 0.9187\n",
      "Epoch 290/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2337 - acc: 0.9176\n",
      "Epoch 291/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2188 - acc: 0.9214\n",
      "Epoch 292/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2292 - acc: 0.9155\n",
      "Epoch 293/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2257 - acc: 0.9134\n",
      "Epoch 294/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2183 - acc: 0.9190\n",
      "Epoch 295/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2212 - acc: 0.9150\n",
      "Epoch 296/300\n",
      "3764/3764 [==============================] - 0s 23us/step - loss: 0.2268 - acc: 0.9176\n",
      "Epoch 297/300\n",
      "3764/3764 [==============================] - 0s 20us/step - loss: 0.2277 - acc: 0.9158\n",
      "Epoch 298/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2238 - acc: 0.9142\n",
      "Epoch 299/300\n",
      "3764/3764 [==============================] - 0s 21us/step - loss: 0.2171 - acc: 0.9243\n",
      "Epoch 300/300\n",
      "3764/3764 [==============================] - 0s 39us/step - loss: 0.2225 - acc: 0.9206\n"
     ]
    }
   ],
   "source": [
    "classifier = build_ann('adam')\n",
    "classifier.fit(X_train, y_nn, batch_size = 256, epochs = 300, verbose=True)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred1 = np.argmax(y_pred,axis=1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "y_nn_pred = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T17:58:44.672323Z",
     "start_time": "2018-04-21T17:58:44.653600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.35\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_pred1, y_nn_pred.argmax(axis=1))\n",
    "accuracy = (sum([cm[i,i] for i in range(len(cm))])) / len(y_nn_pred)\n",
    "print('Accuracy: %0.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy of Naive-Bayes classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_extra)\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred_extra))\n",
    "print('Accuracy of Naive-Bayes classifier on extra test set: {:.2f}'.format(clf.score(X_test_extra, y_test_extra)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Having a bigger dataset at our disposal may certainly make our predictions more stable. However EmoInt is probably too much different from what we have classify (tweets vs lyrics) and it may not improve our predictive abilities at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[EmoInt](http://saifmohammad.com/WebDocs/TweetEmotionIntensities-starsem2017.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
