{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Patterns in Music Playlists\n",
    "\n",
    "Please refer to [https://github.com/sgiammy/emotion-patterns-in-music-playlists](https://github.com/sgiammy/emotion-patterns-in-music-playlists) for more details on the code which is used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:05.552845Z",
     "start_time": "2018-04-17T10:41:04.158780Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.datasets import load_dataset_from_path, split_train_validation\n",
    "\n",
    "lyrics_path = './ml_lyrics'\n",
    "\n",
    "emotion_labels = ['happy', 'sad', 'relaxed', 'angry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:05.568629Z",
     "start_time": "2018-04-17T10:41:05.555838Z"
    }
   },
   "outputs": [],
   "source": [
    "def parameters_grid_search(classifier, params, x, y, cv=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Grid Search to find best parameters for a certain classifier whose\n",
    "    performances are evaluated using cross-validation\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(classifier(), params, cv=cv, n_jobs=-1, verbose=verbose)\n",
    "    gs.fit(x, y)    \n",
    "    return (gs.best_estimator_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has the sole purpose of providing us an experimental environment in which we can quickly test our intuitions and ideas. Therefore, even though for the final algorithm we plan to use FastText, for this notebook we will use a pre-trainend language model available in spaCy containing 685k unique vectors trained on Common Crawl.\n",
    "\n",
    "For more information about the language model we are using, please refer to [this](https://spacy.io/models/en) link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:16.470279Z",
     "start_time": "2018-04-17T10:41:05.571274Z"
    }
   },
   "outputs": [],
   "source": [
    "# For this notebook we will use a simple spacy vocabulary\n",
    "# because we just need to do some experiments\n",
    "spacy_lang = 'en_core_web_lg'\n",
    "nlp = spacy.load(spacy_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:20.439340Z",
     "start_time": "2018-04-17T10:41:16.472641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The language model we are using has some issues with stop words.\n",
    "# Basically we need to grab stopwords from the 'en' language model\n",
    "# and add them back to the model we are using.\n",
    "# https://github.com/explosion/spaCy/issues/922\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Emotion-Patterns-in-Music-Playlists\" data-toc-modified-id=\"Emotion-Patterns-in-Music-Playlists-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Emotion Patterns in Music Playlists</a></span></li><li><span><a href=\"#The-Dataset:-MoodyLyrics\" data-toc-modified-id=\"The-Dataset:-MoodyLyrics-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The Dataset: MoodyLyrics</a></span></li><li><span><a href=\"#Lyrics-Preprocessing\" data-toc-modified-id=\"Lyrics-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Lyrics Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lyrics-Download\" data-toc-modified-id=\"Lyrics-Download-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Lyrics Download</a></span></li><li><span><a href=\"#Stopwords-deletion\" data-toc-modified-id=\"Stopwords-deletion-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stopwords deletion</a></span></li><li><span><a href=\"#Preprocessing-Function\" data-toc-modified-id=\"Preprocessing-Function-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Preprocessing Function</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Principal-Component-Analysis\" data-toc-modified-id=\"Principal-Component-Analysis-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Principal Component Analysis</a></span></li><li><span><a href=\"#Feature-Engineering-Function\" data-toc-modified-id=\"Feature-Engineering-Function-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Feature Engineering Function</a></span></li></ul></li><li><span><a href=\"#Classifiers-on-Lyrics-Content\" data-toc-modified-id=\"Classifiers-on-Lyrics-Content-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classifiers on Lyrics Content</a></span><ul class=\"toc-item\"><li><span><a href=\"#Supervised-K-Means\" data-toc-modified-id=\"Supervised-K-Means-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Supervised K-Means</a></span></li><li><span><a href=\"#k-Nearest-Neighbour\" data-toc-modified-id=\"k-Nearest-Neighbour-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>k-Nearest Neighbour</a></span></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>SVM</a></span></li><li><span><a href=\"#Gradient-Boost\" data-toc-modified-id=\"Gradient-Boost-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Gradient Boost</a></span></li><li><span><a href=\"#Artifical-Neural-Network\" data-toc-modified-id=\"Artifical-Neural-Network-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Artifical Neural Network</a></span></li></ul></li><li><span><a href=\"#What-if-we-just-consider-the-song-title?\" data-toc-modified-id=\"What-if-we-just-consider-the-song-title?-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>What if we just consider the song title?</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM-with-title-only\" data-toc-modified-id=\"SVM-with-title-only-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>SVM with title only</a></span></li><li><span><a href=\"#Gradient-Boost-with-title-only\" data-toc-modified-id=\"Gradient-Boost-with-title-only-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Gradient Boost with title only</a></span></li><li><span><a href=\"#Artificial-Neural-Network-with-title-only\" data-toc-modified-id=\"Artificial-Neural-Network-with-title-only-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Artificial Neural Network with title only</a></span></li></ul></li><li><span><a href=\"#Song-Content-+-Song-Title\" data-toc-modified-id=\"Song-Content-+-Song-Title-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Song Content + Song Title</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>SVM</a></span></li><li><span><a href=\"#Artificial-Neural-Network\" data-toc-modified-id=\"Artificial-Neural-Network-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Artificial Neural Network</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset: MoodyLyrics\n",
    "\n",
    "In the followign section we will provide some interesting statistics on the dataset we will be using throughtout the notebook: MoodyLyrics.<br>\n",
    "ModdyLyrics has the following header: <br>\n",
    "*<Index, Artist, Song, Emotion>*\n",
    "where:\n",
    "<ul>\n",
    "    <li>*Index*: monotonically increasing ID</li>\n",
    "    <li>*Artist*: name of the artist</li>\n",
    "    <li>*Song*: title of the song</li>\n",
    "    <li>*Emotion*: one between ['happy', 'sad', 'relaxed', 'angry']</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's see how many rows we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:20.463191Z",
     "start_time": "2018-04-17T10:41:20.444118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2595, 4)\n"
     ]
    }
   ],
   "source": [
    "moodyLyricsDF = pd.read_csv('./datasets/moodylyrics_raw.csv')\n",
    "print(moodyLyricsDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 2596 rows, but the first one of course is the header. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MoodyLyrics we have 4 different emotion labels for our songs: happy, sad, relaxed and angry. Let's see how those 4 classes are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T08:55:36.239656Z",
     "start_time": "2018-04-16T08:55:36.209988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2595</td>\n",
       "      <td>2595</td>\n",
       "      <td>2595</td>\n",
       "      <td>2595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2595</td>\n",
       "      <td>1672</td>\n",
       "      <td>2229</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ML825</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Silent Night</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>24</td>\n",
       "      <td>819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Index       Artist          Song Emotion\n",
       "count    2595         2595          2595    2595\n",
       "unique   2595         1672          2229       4\n",
       "top     ML825  The Beatles  Silent Night   happy\n",
       "freq        1           52            24     819"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moodyLyricsDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice something weird! Some songs, such as \"Silent Night\" appear more than once.<br>\n",
    "Let's check if this is because there are duplicated rows or just because there are songs with the same title but different artist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T08:55:37.552786Z",
     "start_time": "2018-04-16T08:55:37.519176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2509, 3)\n",
      "(82, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>ML654</td>\n",
       "      <td>Akon</td>\n",
       "      <td>Don't Matter</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>ML962</td>\n",
       "      <td>Akon</td>\n",
       "      <td>Lonely</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>ML2418</td>\n",
       "      <td>Akon</td>\n",
       "      <td>Lonely</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>ML2539</td>\n",
       "      <td>Akon</td>\n",
       "      <td>Don't Matter</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index Artist          Song Emotion\n",
       "653    ML654   Akon  Don't Matter   angry\n",
       "961    ML962   Akon        Lonely     sad\n",
       "2417  ML2418   Akon        Lonely     sad\n",
       "2538  ML2539   Akon  Don't Matter   angry"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(moodyLyricsDF.Song.value_counts()[:10])\n",
    "duplicatedCheck = moodyLyricsDF.groupby(['Artist','Song']).size().reset_index(name='count')\n",
    "print(duplicatedCheck.shape)\n",
    "duplicatedRows = duplicatedCheck [(duplicatedCheck ['count']>1)]\n",
    "print(duplicatedRows.shape)\n",
    "moodyLyricsDF[(moodyLyricsDF['Artist']=='Akon')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 82 duplicated rows. <br>\n",
    "We now group by 'Artist' and 'Song', and we label each song with the most frequent emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T08:55:39.741672Z",
     "start_time": "2018-04-16T08:55:38.303602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2509, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Akon</td>\n",
       "      <td>Don't Matter</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Akon</td>\n",
       "      <td>Lonely</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Artist          Song Emotion\n",
       "34   Akon  Don't Matter   angry\n",
       "35   Akon        Lonely     sad"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moodyLyricsDF = moodyLyricsDF.groupby(['Artist','Song'],as_index=False)['Emotion'].agg(lambda x:x.value_counts().index[0])\n",
    "print(moodyLyricsDF.shape)\n",
    "moodyLyricsDF[(moodyLyricsDF['Artist']==\"Akon\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T08:55:40.038198Z",
     "start_time": "2018-04-16T08:55:39.744082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy      790\n",
      "sad        584\n",
      "relaxed    579\n",
      "angry      556\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotionDistribution = moodyLyricsDF.Emotion.value_counts()\n",
    "print(emotionDistribution)\n",
    "ax = sns.barplot(x=np.array(range(4)),y=emotionDistribution,palette='magma')\n",
    "ax.set_title('Emotion distribution',fontsize=20)\n",
    "ax.set_xticklabels(emotion_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our lyrics are quite balanced among the 4 different classes. Only the happy class has some more lyrics with respect to the other threes. However we believe that it is not going to be a problem as the different between happy and other lyrics should not be too relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the cleaned dataset into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T08:55:41.279957Z",
     "start_time": "2018-04-16T08:55:41.258075Z"
    }
   },
   "outputs": [],
   "source": [
    "moodyLyricsDF.to_csv('./datasets/moodylyrics_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Preprocessing\n",
    "\n",
    "Now we will move on analyzing several techniques for lyrics preprocessing, eventually evaluating their effects and their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the lyrics from lyricwikia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:04:53.614882Z",
     "start_time": "2018-04-11T21:04:53.352578Z"
    }
   },
   "outputs": [],
   "source": [
    "import lyricwikia\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from utils.progress import progress\n",
    "\n",
    "def download_lyrics(input,output,skipHeader=True):\n",
    "\n",
    "    def songs_count(path):\n",
    "        with open(path) as f:\n",
    "            count = len(f.readlines()) - 1\n",
    "            if skipHeader:\n",
    "                count -= 1\n",
    "            return count\n",
    "\n",
    "    # Generator function which reads the lyrics from a csv file line by line\n",
    "    def lyric_entries_generator(path):\n",
    "        with open(path) as lp:\n",
    "            l = lp.readline()\n",
    "            if skipHeader:\n",
    "                l = lp.readline()\n",
    "            while l:\n",
    "                yield l.rstrip().split(',')\n",
    "                l = lp.readline()\n",
    "\n",
    "    def create_output_dir(path):\n",
    "        if os.path.exists(path) and os.path.isdir(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "    LOG_FILE = '.'.join([input, 'log'])\n",
    "    try:\n",
    "        os.remove(os.path.join('.', LOG_FILE))\n",
    "    except OSError:\n",
    "      # Log file did not exists...not too bad\n",
    "        pass\n",
    "\n",
    "    def err(msg):\n",
    "        with open(os.path.join('.', LOG_FILE), 'a') as log:\n",
    "            log.write(msg)\n",
    "            log.write('\\n')\n",
    "\n",
    "    def download_lyric(song):\n",
    "        try:\n",
    "            lyric = lyricwikia.get_lyrics(song[1], song[2])\n",
    "            filename = '_'.join([song[3], song[1], song[2]])\n",
    "            filename = filename.replace('/', '-') # The '/' should never appear\n",
    "            with open(os.path.join(output, filename), 'w') as sfile:\n",
    "                sfile.write(lyric)\n",
    "                return True\n",
    "        except lyricwikia.LyricsNotFound:\n",
    "            err('Could not download {}'.format(song))\n",
    "            return False\n",
    "\n",
    "\n",
    "    # Get the number of songs we are going to download\n",
    "    totalSongs = songs_count(input)\n",
    "\n",
    "    # Create output directory\n",
    "    create_output_dir(output)\n",
    "\n",
    "    # Download songs\n",
    "    count = 0 \n",
    "    errCount = 0\n",
    "    for lyric in lyric_entries_generator(input):\n",
    "        progress(count, totalSongs, 'Errors encountered: {}'.format(errCount))\n",
    "        if not download_lyric(lyric):\n",
    "            errCount += 1\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:12:41.509559Z",
     "start_time": "2018-04-11T21:12:41.503423Z"
    }
   },
   "outputs": [],
   "source": [
    "inputCsv = './datasets/moodylyrics_cleaned.csv'\n",
    "outputDir = './ml_lyrics'\n",
    "if os.path.exists(outputDir) and os.path.isdir(outputDir):\n",
    "    pass\n",
    "else:\n",
    "    download_lyrics(inputCsv,outputDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many lyrics we have actually downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:12:42.648587Z",
     "start_time": "2018-04-11T21:12:42.589695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2453\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -1 ml_lyrics/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's analyse the emotion distribution considering only the song for which we have downloaded the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:12:43.386279Z",
     "start_time": "2018-04-11T21:12:43.365503Z"
    }
   },
   "outputs": [],
   "source": [
    "count = dict(list(zip(emotion_labels, [0 for x in range(len(emotion_labels))])))\n",
    "\n",
    "# Traverse the dataset directory\n",
    "for root, dirs, files in os.walk(lyrics_path):\n",
    "    for f in files:\n",
    "        fields = f.split('_')\n",
    "        if len(fields) > 0:\n",
    "            count[fields[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:12:44.892814Z",
     "start_time": "2018-04-11T21:12:44.683312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the obtained counts\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "#fig.set_size_inches(10, 7)\n",
    "ax = sns.barplot(x=list(count.keys()), y=list(count.values()), ax=ax,palette='magma')\n",
    "ax.set_title('Amount of lyrics for each emotion',fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the emotion distribution looks alike the previous one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords deletion\n",
    "\n",
    "In one of our meetings for our project an interesting question was raised: does it really make sense to remove stopwords from lyrics? Would we have enough words after removing stopwords? Let's see what is the percentage change in the amount of words in our lyrics after removing stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:20.473838Z",
     "start_time": "2018-04-17T10:41:20.466625Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    tks = list(filter(lambda tk: not tk.is_stop, doc))\n",
    "    return spacy.tokens.Doc(nlp.vocab, words=[tk.text for tk in tks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:58:33.941710Z",
     "start_time": "2018-04-16T09:54:31.929012Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Lyric  Word_Count  \\\n",
      "0  /home/mario/dev/emotion-patterns-in-music-play...         161   \n",
      "1  /home/mario/dev/emotion-patterns-in-music-play...        1151   \n",
      "2  /home/mario/dev/emotion-patterns-in-music-play...         391   \n",
      "3  /home/mario/dev/emotion-patterns-in-music-play...         297   \n",
      "4  /home/mario/dev/emotion-patterns-in-music-play...         759   \n",
      "\n",
      "   Word_Count_After  Percentage_Change  \n",
      "0                85          47.204969  \n",
      "1               562          51.172893  \n",
      "2               245          37.340153  \n",
      "3               186          37.373737  \n",
      "4               417          45.059289  \n"
     ]
    }
   ],
   "source": [
    "paths = load_dataset_from_path(lyrics_path)['Lyric_Path'].as_matrix()\n",
    "\n",
    "# Build a dataframe with the following schema:\n",
    "# <Song, word_count, words_after_stopwords_removal, percentage_change>\n",
    "rows = list()\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        doc = nlp(f.read())\n",
    "        n_words_before = len(doc)\n",
    "        doc = remove_stopwords(doc)\n",
    "        n_words_after = len(doc)\n",
    "        perc = (n_words_before - n_words_after) / n_words_before * 100\n",
    "        row = (path, n_words_before, n_words_after, perc)\n",
    "        rows.append(row)\n",
    "\n",
    "# Create a dataframe with the found information\n",
    "df = pd.DataFrame(rows, columns=['Lyric', 'Word_Count', \n",
    "                  'Word_Count_After', 'Percentage_Change'])\n",
    "print(df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:08:06.633245Z",
     "start_time": "2018-04-09T21:08:06.530430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of change in lyrics after removing stopwords:\n",
      "                0     1\n",
      "0            < 25   225\n",
      "1  >= 25 and < 30    74\n",
      "2  >= 30 and < 40   495\n",
      "3  >= 40 and < 50  1009\n",
      "4  >= 50 and < 60   585\n",
      "5  >= 60 and < 75    59\n",
      "6           >= 75     0\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics\n",
    "percs = [ 25, 30, 40, 50, 60, 75 ]\n",
    "print('Percentage of change in lyrics after removing stopwords:')\n",
    "it = enumerate(percs)\n",
    "plt_data = list()\n",
    "for (i, perc) in it:\n",
    "    if i == 0: \n",
    "        count = len(df[df['Percentage_Change'] < perc])\n",
    "        #print(' - < {}:\\t\\t\\t{}'.format(perc, count))\n",
    "        plt_data.append(('< {}'.format(perc), count))\n",
    "    elif i == len(percs) - 1:\n",
    "        prev_p = percs[i-1]\n",
    "        count = len(df[(df['Percentage_Change'] >= prev_p) & (df['Percentage_Change'] < perc)])\n",
    "        #print(' - between {} and {}:\\t{}'.format(prev_p, perc, count))\n",
    "        plt_data.append(('>= {} and < {}'.format(prev_p, perc, count), count))\n",
    "        \n",
    "        count = len(df[df['Percentage_Change'] >= perc])\n",
    "        #print(' - >= {}:\\t\\t\\t{}'.format(perc, count))\n",
    "        plt_data.append(('>= {}'.format(perc), count))\n",
    "    else:\n",
    "        prev_p = percs[i-1]\n",
    "        count = len(df[(df['Percentage_Change'] >= prev_p) & (df['Percentage_Change'] < perc)])\n",
    "        #print(' - between {} and {}:\\t{}'.format(prev_p, perc, count))\n",
    "        plt_data.append(('>= {} and < {}'.format(prev_p, perc, count), count))\n",
    "\n",
    "pltDf = pd.DataFrame(plt_data)\n",
    "print(pltDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:08:13.872095Z",
     "start_time": "2018-04-09T21:08:13.482903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a12b7d9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 7)\n",
    "ax = sns.barplot(pltDf[0], pltDf[1], ax=ax, palette='magma')\n",
    "ax.set_title('Percentage change in number of words after stopwords pruning')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above plot, there is a minimum (but still relevant to our purpose) impact on lyrics, which still have a large number of words to be analyzed. Very few songs reduced their size of more then the 50% and for those songs we may probably have some issues. However this is a really small amount of lyrics that does not prevent us from saying that stopword deletion can be used as a good pre-processing technique for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function\n",
    "Now that we have defined which kind of preprocessing is good to apply to the text of our songs, let's define a function which will be used throughout the notebook to perform lyrics preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:20.482537Z",
     "start_time": "2018-04-17T10:41:20.477898Z"
    }
   },
   "outputs": [],
   "source": [
    "def doc_preprocess(doc):\n",
    "    d = remove_stopwords(doc)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "We will now move on trying to extract some interesting feature from our dataset. \n",
    "\n",
    "An important thing we need to state before starting analyzing this section, is that, by using spaCy's language model, each document has a word vector of length 300. SpaCy's language model has vectors of length 300 because it uses [GloVe](https://nlp.stanford.edu/projects/glove/) to train its models which is based on word2vec and assigns vectors of length 300 to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "As we already said, the language model we are usin assigns word vectors of length 300 to documents and words. However this dimension may be too huge for our problem. In fact it is possible that, by performing some dimensionality reduction, we could still obtain good results from our model.\n",
    "\n",
    "First of all we will perform PCA with 2 components, just because we want to visualize the effect of dimensionality reduction on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:53:52.732337Z",
     "start_time": "2018-04-16T09:49:56.314769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read dataset into pandas dataframe\n",
    "pca_dataset = load_dataset_from_path(lyrics_path)\n",
    "\n",
    "# Turn emotion labels into numerical features\n",
    "mapping = dict(zip(emotion_labels, range(len(emotion_labels)))) # { 'happy' : 0, 'sad': 1, 'relaxed': 2, 'angry': 3 }\n",
    "pca_dataset['Emotion'] = pca_dataset['Emotion'].map(mapping)\n",
    "\n",
    "# Make the dataset to follow this scema:\n",
    "# <Lyric_Path, Emotion, Vector, Vector_Norm>\n",
    "rows = list()\n",
    "pca_dataset['Vector'] = np.nan\n",
    "for index, row in pca_dataset.iterrows():\n",
    "    lyric = row['Lyric_Path']\n",
    "    emotion = row['Emotion']\n",
    "    with open(lyric, 'r') as lyric_file: \n",
    "        doc = nlp(lyric_file.read())\n",
    "        doc = doc_preprocess(doc)\n",
    "        # Consider only those vectors with the same length\n",
    "        # This will be avoided when we will have proper PCA\n",
    "        if len(doc.vector) == 300:\n",
    "            rows.append((\n",
    "                emotion, doc.vector\n",
    "            ))\n",
    "pca_dataset = pd.DataFrame(rows, columns=['Emotion', 'Vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:53:52.753299Z",
     "start_time": "2018-04-16T09:53:52.738507Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_X_vect = pca_dataset['Vector'].as_matrix()\n",
    "pca_X_vect = np.array([np.array(x) for x in pca_X_vect])\n",
    "pca_y = pca_dataset['Emotion'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:26.204219Z",
     "start_time": "2018-04-17T10:41:26.198083Z"
    }
   },
   "outputs": [],
   "source": [
    "def pca(n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(pca_X_vect)\n",
    "    return pca.transform(pca_X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:53:52.858949Z",
     "start_time": "2018-04-16T09:53:52.774285Z"
    }
   },
   "outputs": [],
   "source": [
    "components = pca(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:53:52.879266Z",
     "start_time": "2018-04-16T09:53:52.867771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put reduced components and labels together for plotting\n",
    "comps = list(zip(components, pca_y))\n",
    "pca_df = pd.DataFrame(comps, columns=['Vector', 'Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:59:43.347161Z",
     "start_time": "2018-04-16T09:59:43.051173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot points for each class\n",
    "for i in range(4):\n",
    "    emo_df = pca_df[pca_df['Emotion'] == i]\n",
    "    x = emo_df['Vector'].as_matrix()\n",
    "    x = np.array([np.array(k) for k in x])\n",
    "    plt.scatter(x[:,0], x[:,1])\n",
    "plt.legend(emotion_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we were not expecting to obtain fantastic results. However we believed that doing a PCA with two components could have been great, especially because of the arousal-valence based songs classification model which was used in MoodyLyrics. \n",
    "\n",
    "We will abandon PCA for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Function\n",
    "Now that we have defined which kind of feature engineering we want to, let's define a function which will be used throughout the notebook to perform the desired operations on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:29.515590Z",
     "start_time": "2018-04-17T10:41:29.511438Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: we still have to properly define which feature \n",
    "# engineering we want to perform\n",
    "def feature_engineer(dataset):\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers on Lyrics Content\n",
    "In the next sections we will provide the implementation of several classification algorithms we used. Those classifiers work on the 300-shaped vector assigned by our language model to the lyrics of each song.\n",
    "\n",
    "First, let's prepare the data for those classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:41:37.192455Z",
     "start_time": "2018-04-17T10:41:37.138837Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(lyrics_path):\n",
    "    # Read dataset into pandas dataframe\n",
    "    dataset = load_dataset_from_path(lyrics_path)\n",
    "\n",
    "    # Turn emotion labels into numerical features\n",
    "    mapping = dict(zip(emotion_labels, range(len(emotion_labels)))) # { 'happy' : 0, 'sad': 1, 'relaxed': 2, 'angry': 3 }\n",
    "    dataset['Emotion'] = dataset['Emotion'].map(mapping)\n",
    "\n",
    "    # Make the dataset to follow this scema:\n",
    "    # <Lyric_Path, Emotion, Vector, Vector_Norm>\n",
    "    rows = list()\n",
    "    dataset['Vector'] = np.nan\n",
    "    dataset['Vector_Norm'] = np.nan\n",
    "    for index, row in dataset.iterrows():\n",
    "        lyric = row['Lyric_Path']\n",
    "        emotion = row['Emotion']\n",
    "        with open(lyric, 'r') as lyric_file: \n",
    "            doc = nlp(lyric_file.read())\n",
    "            doc = doc_preprocess(doc) # Preprocessing step\n",
    "            # Consider only those vectors with the same length\n",
    "            # This will be avoided when we will have proper PCA\n",
    "            if len(doc.vector) == 300:\n",
    "                rows.append((\n",
    "                    lyric,\n",
    "                    emotion, doc.vector,\n",
    "                    doc.vector_norm\n",
    "                ))\n",
    "    dataset = pd.DataFrame(rows, columns=['Lyric_Path', 'Emotion', 'Vector', 'Vector_Norm'])\n",
    "    #dataset = feature_engineer(dataset) # Do feature engineering\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:45:18.376403Z",
     "start_time": "2018-04-17T10:41:40.292210Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric_Path</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Vector</th>\n",
       "      <th>Vector_Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0574794, 0.185014, -0.141049, -0.0462526, 0...</td>\n",
       "      <td>2.859903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0134395, 0.177813, -0.158519, -0.020895, 0...</td>\n",
       "      <td>2.633056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0307511, 0.310463, -0.151425, -0.0138566, ...</td>\n",
       "      <td>3.019833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0625424, 0.0874376, -0.0896016, -0.096178, ...</td>\n",
       "      <td>2.446515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.0777918, 0.17095, -0.165166, -0.0325069, 0...</td>\n",
       "      <td>2.737937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Lyric_Path  Emotion  \\\n",
       "0  /home/mario/dev/emotion-patterns-in-music-play...        3   \n",
       "1  /home/mario/dev/emotion-patterns-in-music-play...        0   \n",
       "2  /home/mario/dev/emotion-patterns-in-music-play...        0   \n",
       "3  /home/mario/dev/emotion-patterns-in-music-play...        1   \n",
       "4  /home/mario/dev/emotion-patterns-in-music-play...        3   \n",
       "\n",
       "                                              Vector  Vector_Norm  \n",
       "0  [0.0574794, 0.185014, -0.141049, -0.0462526, 0...     2.859903  \n",
       "1  [-0.0134395, 0.177813, -0.158519, -0.020895, 0...     2.633056  \n",
       "2  [-0.0307511, 0.310463, -0.151425, -0.0138566, ...     3.019833  \n",
       "3  [0.0625424, 0.0874376, -0.0896016, -0.096178, ...     2.446515  \n",
       "4  [-0.0777918, 0.17095, -0.165166, -0.0325069, 0...     2.737937  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(lyrics_path)\n",
    "# Show some dataset's values\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:45:18.420976Z",
     "start_time": "2018-04-17T10:45:18.384990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare array for sklearn classifiers\n",
    "X_vect = dataset['Vector'].as_matrix().T\n",
    "X_vect = np.array([np.array(x) for x in X_vect])\n",
    "X_norm = dataset['Vector_Norm'].as_matrix()\n",
    "y = dataset['Emotion'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:00:43.017200Z",
     "start_time": "2018-04-16T09:00:42.992566Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2453, 300)\n",
      "(2453,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_vect))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide a function to perform some additional tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:48:51.851065Z",
     "start_time": "2018-04-16T09:48:51.694047Z"
    }
   },
   "outputs": [],
   "source": [
    "import lyricwikia\n",
    "\n",
    "def extra_test(classifier):\n",
    "    songs = [\n",
    "        ('Bobby McFerrin', 'Don\\'t Worry, Be Happy', 'happy'),\n",
    "        ('Queen', 'Don\\'t Stop me Now', 'happy'),\n",
    "        ('Pharrell Williams', 'Happy', 'happy'),\n",
    "        ('The Monkees', 'I\\'m a believer', 'happy'),\n",
    "        \n",
    "        ('R.E.M.', 'Everybody Hurts', 'sad'),\n",
    "        ('Adele', 'Someone Like You', 'sad'),\n",
    "        ('Pink Floyd', 'Wish you were here', 'sad'),\n",
    "        ('Johnny Cash', 'Hurt', 'sad'),\n",
    "        ('Nirvana', 'Smells like teen spirit', 'sad'),\n",
    "        \n",
    "        ('Rage Against the Machine', 'Killing in the name', 'angry'),\n",
    "        ('Kanye West', 'Stronger', 'angry'),\n",
    "        ('Smash Mouth', 'All Star', 'angry'),\n",
    "        ('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry'),\n",
    "        \n",
    "        ('Blur', 'Song 2', 'relaxed') # I'm not quite confident about this labeling\n",
    "    ]\n",
    "\n",
    "    count_correct = 0\n",
    "    for s in songs:\n",
    "        # Download the lyric\n",
    "        lyric = lyricwikia.get_lyrics(s[0], s[1])\n",
    "        # Convert lyric to spacy Doc and preproces it\n",
    "        doc = nlp(lyric)\n",
    "        doc = doc_preprocess(doc)\n",
    "        # Classify\n",
    "        vect = np.array([doc.vector])\n",
    "        label = classifier.predict(vect)\n",
    "        if emotion_labels[label[0]] == s[2]:\n",
    "            count_correct += 1\n",
    "        print(s, '->', emotion_labels[label[0]], '(was supposed to be {})'.format(s[2]))\n",
    "    print('We got {} predictions our of {} songs'.format(count_correct, len(songs)))\n",
    "    print('Accuracy: %0.2f' % (count_correct / len(songs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised K-Means\n",
    "This is the first and the easieast classifier idea we came up. We used this idea just to verify what we could do and if we were on the right path. \n",
    "\n",
    "The name we gave to this classifier could sound ambiguous but we believe that, once its functioning is explained, the name will sound more decent.\n",
    "\n",
    "The idea behind this classifier is quite simple. The first thing we do is to compute the centroids for each emotion class. Then, to classify a lyric, we simply compare its word vector norm to the 4 centroids. At the end we will assign our lyric with the label of the closest \"cluster\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:40:02.719222Z",
     "start_time": "2018-04-11T09:39:36.344927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31\n"
     ]
    }
   ],
   "source": [
    "from classifier.LyricsSupervisedKMeans import LyricsSupervisedKMeans\n",
    "\n",
    "clf = LyricsSupervisedKMeans()\n",
    "clf.set_lang(nlp)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "trainDf, testDf = split_train_validation(dataset)\n",
    "\n",
    "X_sup_kmeans_train = trainDf['Vector_Norm'].as_matrix().T\n",
    "X_sup_kmeans_train = np.array([np.array(x) for x in X_sup_kmeans_train])\n",
    "\n",
    "y_sup_kmeans_train = trainDf['Emotion'].as_matrix()\n",
    "\n",
    "# Train our model\n",
    "clf.train(X_sup_kmeans_train, y_sup_kmeans_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "acc = clf.score(testDf)\n",
    "print('Accuracy: %0.2f' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we did not even implement cross-validation. We just split the dataset into a training set (90%) and a test set (10%) and we evaluated the classification accuracy on that test set. We decided not to implement cross-validation for this classifier as we believe it is not worth to spend time on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were expecting poor results from this classifier but at least it served to the purpose of giving us a hint of which direction to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbour\n",
    "We will now build a k-NN model which is basically a generalization of what we called \"Supervised K-Means\". We will evaluate our model for several different k values. The parameters passed to our model are quite self-explicative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:37:07.317982Z",
     "start_time": "2018-04-11T09:36:41.271097Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1: 0.81 (+/- 0.04)\n",
      "Accuracy for k=3: 0.81 (+/- 0.05)\n",
      "Accuracy for k=5: 0.81 (+/- 0.05)\n",
      "Accuracy for k=7: 0.81 (+/- 0.05)\n",
      "Accuracy for k=9: 0.82 (+/- 0.05)\n",
      "Accuracy for k=11: 0.81 (+/- 0.04)\n",
      "Accuracy for k=13: 0.82 (+/- 0.04)\n",
      "Accuracy for k=15: 0.81 (+/- 0.04)\n",
      "Accuracy for k=17: 0.81 (+/- 0.06)\n",
      "Accuracy for k=19: 0.81 (+/- 0.06)\n",
      "Accuracy for k=21: 0.81 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ks = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "\n",
    "for k in ks:\n",
    "    # Build model\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, algorithm='auto', \n",
    "                           metric='euclidean', n_jobs=-1)\n",
    "    # Evaluate accuracy\n",
    "    scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "    print('Accuracy for k=%d: %0.2f (+/- %0.2f)' % (k, scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-NN algorithm produced unexpectedly good results. As we can see the algorithm never reaches an accuracy value higher than 82%. From our observations we can conclude that `k=9` could be an good parameter for that model.\n",
    "\n",
    "An interesting thing we noticed while running our experiments is that, when we read our dataset without preprocessing our lyrics, the accuracy was around 10% lower.\n",
    "\n",
    "Let's do some more tests and see if this classifier is really that good. We will now try to classify some very popular songs which were labelled according to our personal tastes and to [IBM Tone Analyzer](https://tone-analyzer-demo.ng.bluemix.net/?cm_mc_uid=56761301373215210511228&cm_mc_sid_50200000=91001461523311389617&cm_mc_sid_52640000=29317781523311389622) ones. Those songs we are trying to classify are not available in MoodyLyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:38:22.907601Z",
     "start_time": "2018-04-11T09:38:13.799310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> sad (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> angry (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> sad (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> angry (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> angry (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> sad (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> relaxed (was supposed to be relaxed)\n",
      "We got 9 predictions our of 14 songs\n",
      "Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=9, algorithm='auto', \n",
    "                           metric='euclidean', n_jobs=-1)\n",
    "clf.fit(X_vect, y)\n",
    "extra_test(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results are encouraging and make us believe that we can certainly do even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "We will now train a Support Vector Machine using the algorithm available in sklearn. For this first experiment we did not change the default parameters of the SVM algorithm provided in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:15:16.369778Z",
     "start_time": "2018-04-09T21:14:50.158328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build model\n",
    "clf = SVC()\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can improve our model's accuracy just my properly tuning our parameters. We will perform a [Grid Search](http://scikit-learn.org/stable/modules/grid_search.html) to properly tune our model using a cross validation approach to evaluating accuracy. We will operate on both the kernel function and the penalty parameter C.\n",
    "\n",
    "Beware that the below cell takes quite a long time to run as the `SVC` train function complexity is more than quadratic and, having to run it several times, is quite expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:22:15.533515Z",
     "start_time": "2018-04-09T21:15:21.742694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 29 candidates, totalling 290 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 290 out of 290 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'C': 10, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['linear'], 'C': [ 0.01, 0.05, 1, 10, 100 ]},\n",
    "    { 'kernel': ['rbf', 'sigmoid'], 'C': [ 0.01, 0.05, 0.1, 0.3, 0.8, 1, 3, 10, 50, 100, 150, 200 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params = parameters_grid_search(SVC, params, X_vect, y, verbose=1)\n",
    "print('Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:22:26.843539Z",
     "start_time": "2018-04-09T21:22:20.992884Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm_best, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly we were not expecting the SVM model to be so good. Let's see how our optimal SVM performs on the extra test set as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T22:14:54.516665Z",
     "start_time": "2018-04-09T22:14:36.130397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> angry (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> angry (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> angry (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> angry (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> sad (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> sad (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> happy (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 3 predictions our of 14 songs\n",
      "Accuracy: 0.21\n"
     ]
    }
   ],
   "source": [
    "extra_test(svm_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not properly what we would expect from a predictor having 91% accuracy. Probably (you don't say?) we are overfitting. In fact, we would get better results even by using a random classifier instead of our \"optimal\" SVM.\n",
    "\n",
    "One interesting thing we can notice is that a \"Don't Worry, Be Happy\" is labelled as angry. However it is quite explicit from the title (and also from the lyrics) that the song is about happiness. This obeservation suggests us that we should probably be considering also the song's title when computing our word vectors.\n",
    "\n",
    "Let's now move on to some ensemble methods which are supposed to be even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function ([reference link](https://en.wikipedia.org/wiki/Gradient_boosting)).\n",
    "\n",
    "Since we already obtained great results using our SVM model, we will omit any grid search for parameters tuning on this model. In fact, a grid search on a Gradient Boosting Classifier would be quite expensive and would slow down our experiments a lot. Instead, some manual tuning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:26:35.695954Z",
     "start_time": "2018-04-09T21:22:32.508973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Build model\n",
    "clf = GradientBoostingClassifier(learning_rate=0.7, n_estimators=200)\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several attempts we did never succeed in obtaining higher accuracy values w.r.t. our SVM model. Therefore, given the slow training time for the GradientBoostingClassifier algorithm, we will keep using the SVM model.\n",
    "\n",
    "However, let's have a look on Gradient Boost performances on our \"extra\" test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:57:55.163898Z",
     "start_time": "2018-04-09T21:57:26.262125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.7, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_vect, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T22:16:54.795941Z",
     "start_time": "2018-04-09T22:16:43.952136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> angry (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> angry (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> angry (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> sad (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> angry (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> angry (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> angry (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> sad (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> angry (was supposed to be relaxed)\n",
      "We got 6 predictions our of 14 songs\n",
      "Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "extra_test(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thst's definitivelly better but we still need to improve a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Neural Network\n",
    "Additionally to what we have already done we will now try to tune, train and evaluate an artificial neural network model with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:11:48.673849Z",
     "start_time": "2018-04-11T13:11:48.667411Z"
    }
   },
   "outputs": [],
   "source": [
    "#1 Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:11:53.657745Z",
     "start_time": "2018-04-11T13:11:50.677108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# y_nn should be a vector (len(X_vect),4), with a 1 in the right class\n",
    "from keras.utils import np_utils\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "y_nn = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:11:55.794676Z",
     "start_time": "2018-04-11T13:11:55.783221Z"
    }
   },
   "outputs": [],
   "source": [
    "#2 Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# we need to scale because we don't want one feature to predomine the others\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T10:03:20.840116Z",
     "start_time": "2018-04-16T10:03:20.812700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1 Importing the Keras libraries and packages\n",
    "import keras\n",
    "# Sequential module is required to initialize our ANN\n",
    "from keras.models import Sequential\n",
    "# Dense module is required to create the layers\n",
    "from keras.layers import Dense, Dropout\n",
    "    \n",
    "def build_ann(optimizer='adam', input_size=300):\n",
    "    classifier = Sequential()\n",
    "    #2 Adding first hidden layer\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'random_normal', activation = 'sigmoid', input_dim = input_size))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    # Adding second hidden layer\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    # Adding output layer\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'random_normal', activation = 'softmax'))\n",
    "\n",
    "    #3 Compiling the ANN\n",
    "    classifier.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:14:47.269277Z",
     "start_time": "2018-04-11T13:14:41.410764Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1962/1962 [==============================] - 0s 199us/step - loss: 1.3823 - acc: 0.3012\n",
      "Epoch 2/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 1.2945 - acc: 0.4154\n",
      "Epoch 3/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 1.1512 - acc: 0.5377\n",
      "Epoch 4/50\n",
      "1962/1962 [==============================] - 0s 56us/step - loss: 0.9740 - acc: 0.6055\n",
      "Epoch 5/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.8528 - acc: 0.6519\n",
      "Epoch 6/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.7660 - acc: 0.7130\n",
      "Epoch 7/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.6951 - acc: 0.7543\n",
      "Epoch 8/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.6213 - acc: 0.7803\n",
      "Epoch 9/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.5755 - acc: 0.8109\n",
      "Epoch 10/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.5100 - acc: 0.8318\n",
      "Epoch 11/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.4745 - acc: 0.8486\n",
      "Epoch 12/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.4406 - acc: 0.8527\n",
      "Epoch 13/50\n",
      "1962/1962 [==============================] - 0s 60us/step - loss: 0.4139 - acc: 0.8665\n",
      "Epoch 14/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.3953 - acc: 0.8726\n",
      "Epoch 15/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.3587 - acc: 0.8818\n",
      "Epoch 16/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.3432 - acc: 0.8930\n",
      "Epoch 17/50\n",
      "1962/1962 [==============================] - 0s 53us/step - loss: 0.3293 - acc: 0.8976\n",
      "Epoch 18/50\n",
      "1962/1962 [==============================] - 0s 57us/step - loss: 0.3127 - acc: 0.8991\n",
      "Epoch 19/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.3098 - acc: 0.9006\n",
      "Epoch 20/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.3080 - acc: 0.8950\n",
      "Epoch 21/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2885 - acc: 0.9042\n",
      "Epoch 22/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.2715 - acc: 0.9103\n",
      "Epoch 23/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2692 - acc: 0.9128\n",
      "Epoch 24/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.2673 - acc: 0.9164\n",
      "Epoch 25/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.2523 - acc: 0.9169\n",
      "Epoch 26/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.2416 - acc: 0.9286\n",
      "Epoch 27/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.2272 - acc: 0.9251\n",
      "Epoch 28/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.2322 - acc: 0.9185\n",
      "Epoch 29/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.2254 - acc: 0.9256\n",
      "Epoch 30/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.2307 - acc: 0.9235\n",
      "Epoch 31/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.2202 - acc: 0.9297\n",
      "Epoch 32/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2059 - acc: 0.9373\n",
      "Epoch 33/50\n",
      "1962/1962 [==============================] - 0s 53us/step - loss: 0.2096 - acc: 0.9358\n",
      "Epoch 34/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.2119 - acc: 0.9276\n",
      "Epoch 35/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.2090 - acc: 0.9348\n",
      "Epoch 36/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.2114 - acc: 0.9327\n",
      "Epoch 37/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1957 - acc: 0.9353\n",
      "Epoch 38/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2027 - acc: 0.9353\n",
      "Epoch 39/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1775 - acc: 0.9383\n",
      "Epoch 40/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.1895 - acc: 0.9343\n",
      "Epoch 41/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1883 - acc: 0.9429\n",
      "Epoch 42/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.1765 - acc: 0.9501\n",
      "Epoch 43/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.1676 - acc: 0.9485\n",
      "Epoch 44/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1706 - acc: 0.9424\n",
      "Epoch 45/50\n",
      "1962/1962 [==============================] - 0s 42us/step - loss: 0.1737 - acc: 0.9439\n",
      "Epoch 46/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.1677 - acc: 0.9490\n",
      "Epoch 47/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.1596 - acc: 0.9465\n",
      "Epoch 48/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.1670 - acc: 0.9465\n",
      "Epoch 49/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.1608 - acc: 0.9501\n",
      "Epoch 50/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.1721 - acc: 0.9460\n"
     ]
    }
   ],
   "source": [
    "classifier = build_ann('adam')\n",
    "classifier.fit(X_train, y_nn, batch_size = 64, epochs = 50)\n",
    "\n",
    "#1 Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred1 = np.argmax(y_pred,axis=1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "y_nn_pred = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:16:21.039165Z",
     "start_time": "2018-04-11T13:16:21.033560Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred1, y_nn_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:16:30.264689Z",
     "start_time": "2018-04-11T13:16:30.258757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.87\n"
     ]
    }
   ],
   "source": [
    "accuracy = (sum([cm[i,i] for i in range(len(cm))])) / len(y_nn_pred)\n",
    "print('Accuracy: %0.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:19:02.350499Z",
     "start_time": "2018-04-11T13:16:44.656541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.06\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "keras_classifier = KerasClassifier(build_fn=build_ann)\n",
    "parameters = {'batch_size': [64, 128],\n",
    "              'epochs': [25, 50],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = keras_classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5)\n",
    "grid_search = grid_search.fit(X_train, y_train, verbose=0)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print('Accuracy: %0.2f' % (best_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T10:05:50.598608Z",
     "start_time": "2018-04-16T10:05:50.547601Z"
    }
   },
   "outputs": [],
   "source": [
    "def extra_test_ann(classifier):\n",
    "    songs = [\n",
    "        ('Bobby McFerrin', 'Don\\'t Worry, Be Happy', 'happy'),\n",
    "        ('Queen', 'Don\\'t Stop me Now', 'happy'),\n",
    "        ('Pharrell Williams', 'Happy', 'happy'),\n",
    "        ('The Monkees', 'I\\'m a believer', 'happy'),\n",
    "        \n",
    "        ('R.E.M.', 'Everybody Hurts', 'sad'),\n",
    "        ('Adele', 'Someone Like You', 'sad'),\n",
    "        ('Pink Floyd', 'Wish you were here', 'sad'),\n",
    "        ('Johnny Cash', 'Hurt', 'sad'),\n",
    "        ('Nirvana', 'Smells like teen spirit', 'sad'),\n",
    "        \n",
    "        ('Rage Against the Machine', 'Killing in the name', 'angry'),\n",
    "        ('Kanye West', 'Stronger', 'angry'),\n",
    "        ('Smash Mouth', 'All Star', 'angry'),\n",
    "        ('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry'),\n",
    "        \n",
    "        ('Blur', 'Song 2', 'relaxed') # I'm not quite confident about this labeling\n",
    "    ]\n",
    "\n",
    "    count_correct = 0\n",
    "    for s in songs:\n",
    "        # Download the lyric\n",
    "        lyric = lyricwikia.get_lyrics(s[0], s[1])\n",
    "        # Convert lyric to spacy Doc and preproces it\n",
    "        doc = nlp(lyric)\n",
    "        doc = doc_preprocess(doc)\n",
    "        # Classify\n",
    "        vect = np.array([doc.vector])\n",
    "        label = classifier.predict(sc.transform(vect))\n",
    "        lbl = np.argmax(label[0])\n",
    "        if emotion_labels[lbl] == s[2]:\n",
    "            count_correct += 1\n",
    "        print(s, '->', emotion_labels[lbl], '(was supposed to be {})'.format(s[2]))\n",
    "    print('We got {} predictions our of {} songs'.format(count_correct, len(songs)))\n",
    "    print('Accuracy: %0.2f' % (count_correct / len(songs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:27:57.894355Z",
     "start_time": "2018-04-11T13:27:51.751289Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> sad (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> sad (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> happy (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 4 predictions our of 14 songs\n",
      "Accuracy: 0.29\n"
     ]
    }
   ],
   "source": [
    "extra_test_ann(best_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we just consider the song title? \n",
    "Now let's try to perform the classification just considering the song title. We'll try SVM, Gradient Boost and an ANN to compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Artist                Song  Emotion\n",
      "0        .38 Special        Fantasy Girl  relaxed\n",
      "1        .38 Special       Second Chance    happy\n",
      "2             22-20s             Hold On    angry\n",
      "3  3 Inches Of Blood  Trial Of Champions    angry\n"
     ]
    }
   ],
   "source": [
    "print(moodyLyricsDF[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:51:56.618732Z",
     "start_time": "2018-04-11T09:51:56.604426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Artist                Song  Emotion\n",
      "0        .38 Special        Fantasy Girl        2\n",
      "1        .38 Special       Second Chance        0\n",
      "2             22-20s             Hold On        3\n",
      "3  3 Inches Of Blood  Trial Of Champions        3\n"
     ]
    }
   ],
   "source": [
    "# First we map the emotion labels into number: Happy=0, Sad=1, Relaxed=2, Angry=3\n",
    "mapping = dict(zip(emotion_labels, range(len(emotion_labels)))) \n",
    "moodyLyricsDF['Emotion'] = moodyLyricsDF['Emotion'].map(mapping)\n",
    "print(moodyLyricsDF[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:52:37.477954Z",
     "start_time": "2018-04-11T09:51:57.740621Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then we transform the Song title with spaCy\n",
    "rows = list()\n",
    "for index,row in moodyLyricsDF.iterrows():\n",
    "    doc = nlp(row['Song'])\n",
    "    rows.append((\n",
    "                    row['Artist'],\n",
    "                    row['Song'],\n",
    "                    row['Emotion'], doc.vector,\n",
    "                    doc.vector_norm\n",
    "                ))\n",
    "\n",
    "dataset = pd.DataFrame(rows, columns=['Artist', 'Song', 'Emotion','Vector', 'Vector_Norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:52:37.534393Z",
     "start_time": "2018-04-11T09:52:37.509879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Artist           Song  Emotion  \\\n",
      "0  .38 Special   Fantasy Girl        2   \n",
      "1  .38 Special  Second Chance        0   \n",
      "\n",
      "                                              Vector  Vector_Norm  \n",
      "0  [-0.1325517, 0.070274994, -0.353215, 0.0470585...     5.674618  \n",
      "1  [-0.02172, 0.336335, -0.17991, 0.38281, 0.1225...     4.670355  \n"
     ]
    }
   ],
   "source": [
    "print(dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:52:37.569952Z",
     "start_time": "2018-04-11T09:52:37.541928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare array for sklearn classifiers\n",
    "X_vect_name = dataset['Vector'].as_matrix().T\n",
    "X_vect_name = np.array([np.array(x) for x in X_vect])\n",
    "X_norm_name = dataset['Vector_Norm'].as_matrix()\n",
    "y_name = dataset['Emotion'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 29 candidates, totalling 290 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 290 out of 290 | elapsed:  9.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'C': 100, 'kernel': 'sigmoid'}\n",
      "Accuracy: 0.67 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['linear'], 'C': [ 0.01, 0.05, 1, 10, 100 ]},\n",
    "    { 'kernel': ['rbf', 'sigmoid'], 'C': [ 0.01, 0.05, 0.1, 0.3, 0.8, 1, 3, 10, 50, 100, 150, 200 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params = parameters_grid_search(SVC, params, X_vect_name, y_name, verbose=1)\n",
    "print('Parameters:', best_params)\n",
    "scores = cross_val_score(svm_best, X_vect_name, y_name, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> sad (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> sad (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> sad (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> angry (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> sad (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> sad (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 6 predictions our of 14 songs\n",
      "Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "extra_test(svm_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost with title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "clf = GradientBoostingClassifier(learning_rate=0.7, n_estimators=50)\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect_name, y_name, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.7, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_vect_name, y_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> angry (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> angry (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> happy (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> happy (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 6 predictions our of 14 songs\n",
      "Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "extra_test(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network with title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:20:34.555233Z",
     "start_time": "2018-04-11T13:20:34.542863Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "# we need to scale because we don't want one feature to predomine the others\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X_vect = sc.fit_transform(X_vect_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:20:35.754648Z",
     "start_time": "2018-04-11T13:20:35.749380Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_name)\n",
    "encoded_Y = encoder.transform(y_name)\n",
    "y_nn = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:21:24.980277Z",
     "start_time": "2018-04-11T13:20:51.361745Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1962/1962 [==============================] - 2s 814us/step - loss: 1.3902 - acc: 0.2732\n",
      "Epoch 2/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 1.3482 - acc: 0.3430\n",
      "Epoch 3/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 1.3003 - acc: 0.4062\n",
      "Epoch 4/50\n",
      "1962/1962 [==============================] - 0s 42us/step - loss: 1.2210 - acc: 0.4954\n",
      "Epoch 5/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 1.1190 - acc: 0.5591\n",
      "Epoch 6/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 1.0033 - acc: 0.6081\n",
      "Epoch 7/50\n",
      "1962/1962 [==============================] - 0s 34us/step - loss: 0.8989 - acc: 0.6606\n",
      "Epoch 8/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.8277 - acc: 0.6896\n",
      "Epoch 9/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.7522 - acc: 0.7380\n",
      "Epoch 10/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.6978 - acc: 0.7589\n",
      "Epoch 11/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.6499 - acc: 0.7808\n",
      "Epoch 12/50\n",
      "1962/1962 [==============================] - 0s 35us/step - loss: 0.6027 - acc: 0.7977\n",
      "Epoch 13/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.5577 - acc: 0.8354\n",
      "Epoch 14/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.5277 - acc: 0.8303\n",
      "Epoch 15/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.4814 - acc: 0.8563\n",
      "Epoch 16/50\n",
      "1962/1962 [==============================] - 0s 41us/step - loss: 0.4587 - acc: 0.8558\n",
      "Epoch 17/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 0.4372 - acc: 0.8573\n",
      "Epoch 18/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 0.4214 - acc: 0.8629\n",
      "Epoch 19/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.3976 - acc: 0.8710\n",
      "Epoch 20/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.3877 - acc: 0.8828\n",
      "Epoch 21/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.3647 - acc: 0.8823\n",
      "Epoch 22/50\n",
      "1962/1962 [==============================] - 0s 35us/step - loss: 0.3541 - acc: 0.8863\n",
      "Epoch 23/50\n",
      "1962/1962 [==============================] - 0s 35us/step - loss: 0.3493 - acc: 0.8889\n",
      "Epoch 24/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.3338 - acc: 0.8940\n",
      "Epoch 25/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.3437 - acc: 0.8914\n",
      "Epoch 26/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.3061 - acc: 0.9093\n",
      "Epoch 27/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.3012 - acc: 0.9103\n",
      "Epoch 28/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.2902 - acc: 0.9154\n",
      "Epoch 29/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.3025 - acc: 0.9067\n",
      "Epoch 30/50\n",
      "1962/1962 [==============================] - 0s 35us/step - loss: 0.2851 - acc: 0.9093\n",
      "Epoch 31/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.2758 - acc: 0.9190\n",
      "Epoch 32/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.2821 - acc: 0.9052\n",
      "Epoch 33/50\n",
      "1962/1962 [==============================] - 0s 41us/step - loss: 0.2775 - acc: 0.9159\n",
      "Epoch 34/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.2663 - acc: 0.9149\n",
      "Epoch 35/50\n",
      "1962/1962 [==============================] - 0s 34us/step - loss: 0.2691 - acc: 0.9134\n",
      "Epoch 36/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 0.2628 - acc: 0.9169\n",
      "Epoch 37/50\n",
      "1962/1962 [==============================] - 0s 35us/step - loss: 0.2436 - acc: 0.9225\n",
      "Epoch 38/50\n",
      "1962/1962 [==============================] - 0s 35us/step - loss: 0.2427 - acc: 0.9246\n",
      "Epoch 39/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.2499 - acc: 0.9205\n",
      "Epoch 40/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.2409 - acc: 0.9256\n",
      "Epoch 41/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.2400 - acc: 0.9281\n",
      "Epoch 42/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.2359 - acc: 0.9241\n",
      "Epoch 43/50\n",
      "1962/1962 [==============================] - 0s 41us/step - loss: 0.2425 - acc: 0.9251\n",
      "Epoch 44/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.2280 - acc: 0.9286\n",
      "Epoch 45/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.2226 - acc: 0.9266\n",
      "Epoch 46/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.2263 - acc: 0.9266\n",
      "Epoch 47/50\n",
      "1962/1962 [==============================] - 0s 41us/step - loss: 0.2166 - acc: 0.9297\n",
      "Epoch 48/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.1992 - acc: 0.9373\n",
      "Epoch 49/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.2110 - acc: 0.9286\n",
      "Epoch 50/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.2097 - acc: 0.9353\n",
      "491/491 [==============================] - 1s 1ms/step\n",
      "Epoch 1/50\n",
      "1962/1962 [==============================] - 2s 921us/step - loss: 1.3811 - acc: 0.3002\n",
      "Epoch 2/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 1.3396 - acc: 0.3665\n",
      "Epoch 3/50\n",
      "1962/1962 [==============================] - 0s 41us/step - loss: 1.2770 - acc: 0.4526\n",
      "Epoch 4/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 1.1811 - acc: 0.5194\n",
      "Epoch 5/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 1.0741 - acc: 0.5617\n",
      "Epoch 6/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.9659 - acc: 0.6009\n",
      "Epoch 7/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 0.8902 - acc: 0.6330\n",
      "Epoch 8/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.8259 - acc: 0.6549\n",
      "Epoch 9/50\n",
      "1962/1962 [==============================] - 0s 34us/step - loss: 0.7707 - acc: 0.6962\n",
      "Epoch 10/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.7299 - acc: 0.7125\n",
      "Epoch 11/50\n",
      "1962/1962 [==============================] - 0s 56us/step - loss: 0.6799 - acc: 0.7528\n",
      "Epoch 12/50\n",
      "1962/1962 [==============================] - 0s 45us/step - loss: 0.6439 - acc: 0.7696\n",
      "Epoch 13/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.6004 - acc: 0.7890\n",
      "Epoch 14/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.5652 - acc: 0.8053\n",
      "Epoch 15/50\n",
      "1962/1962 [==============================] - 0s 61us/step - loss: 0.5228 - acc: 0.8298\n",
      "Epoch 16/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.5083 - acc: 0.8231\n",
      "Epoch 17/50\n",
      "1962/1962 [==============================] - 0s 45us/step - loss: 0.4677 - acc: 0.8502\n",
      "Epoch 18/50\n",
      "1962/1962 [==============================] - 0s 53us/step - loss: 0.4483 - acc: 0.8512\n",
      "Epoch 19/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.4335 - acc: 0.8609\n",
      "Epoch 20/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.4016 - acc: 0.8670\n",
      "Epoch 21/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.3956 - acc: 0.8700\n",
      "Epoch 22/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.3755 - acc: 0.8797\n",
      "Epoch 23/50\n",
      "1962/1962 [==============================] - 0s 55us/step - loss: 0.3663 - acc: 0.8823\n",
      "Epoch 24/50\n",
      "1962/1962 [==============================] - 0s 45us/step - loss: 0.3441 - acc: 0.8899\n",
      "Epoch 25/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.3501 - acc: 0.8828\n",
      "Epoch 26/50\n",
      "1962/1962 [==============================] - 0s 46us/step - loss: 0.3218 - acc: 0.8940\n",
      "Epoch 27/50\n",
      "1962/1962 [==============================] - 0s 58us/step - loss: 0.3145 - acc: 0.8955\n",
      "Epoch 28/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2994 - acc: 0.9037\n",
      "Epoch 29/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.2968 - acc: 0.9103\n",
      "Epoch 30/50\n",
      "1962/1962 [==============================] - 0s 42us/step - loss: 0.3003 - acc: 0.9011\n",
      "Epoch 31/50\n",
      "1962/1962 [==============================] - 0s 41us/step - loss: 0.2903 - acc: 0.9062\n",
      "Epoch 32/50\n",
      "1962/1962 [==============================] - 0s 46us/step - loss: 0.2821 - acc: 0.9083\n",
      "Epoch 33/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.2630 - acc: 0.9118\n",
      "Epoch 34/50\n",
      "1962/1962 [==============================] - 0s 41us/step - loss: 0.2666 - acc: 0.9103\n",
      "Epoch 35/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.2491 - acc: 0.9185\n",
      "Epoch 36/50\n",
      "1962/1962 [==============================] - 0s 35us/step - loss: 0.2541 - acc: 0.9246\n",
      "Epoch 37/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.2408 - acc: 0.9261\n",
      "Epoch 38/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.2431 - acc: 0.9210\n",
      "Epoch 39/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.2369 - acc: 0.9251\n",
      "Epoch 40/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.2265 - acc: 0.9281\n",
      "Epoch 41/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.2245 - acc: 0.9210\n",
      "Epoch 42/50\n",
      "1962/1962 [==============================] - 0s 46us/step - loss: 0.2287 - acc: 0.9266\n",
      "Epoch 43/50\n",
      "1962/1962 [==============================] - 0s 31us/step - loss: 0.2190 - acc: 0.9276\n",
      "Epoch 44/50\n",
      "1962/1962 [==============================] - 0s 46us/step - loss: 0.2147 - acc: 0.9343\n",
      "Epoch 45/50\n",
      "1962/1962 [==============================] - 0s 46us/step - loss: 0.2221 - acc: 0.9235\n",
      "Epoch 46/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.2143 - acc: 0.9358\n",
      "Epoch 47/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.2129 - acc: 0.9343\n",
      "Epoch 48/50\n",
      "1962/1962 [==============================] - 0s 46us/step - loss: 0.2036 - acc: 0.9358\n",
      "Epoch 49/50\n",
      "1962/1962 [==============================] - 0s 45us/step - loss: 0.1973 - acc: 0.9343\n",
      "Epoch 50/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1926 - acc: 0.9414\n",
      "491/491 [==============================] - 1s 2ms/step\n",
      "Epoch 1/50\n",
      "1962/1962 [==============================] - 2s 820us/step - loss: 1.3825 - acc: 0.2910\n",
      "Epoch 2/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 1.3487 - acc: 0.3547\n",
      "Epoch 3/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 1.2911 - acc: 0.4332\n",
      "Epoch 4/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 1.2062 - acc: 0.5076\n",
      "Epoch 5/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 1.0976 - acc: 0.5505\n",
      "Epoch 6/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.9831 - acc: 0.5810\n",
      "Epoch 7/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.9014 - acc: 0.6264\n",
      "Epoch 8/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.8331 - acc: 0.6616\n",
      "Epoch 9/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.7809 - acc: 0.7029\n",
      "Epoch 10/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.7447 - acc: 0.7090\n",
      "Epoch 11/50\n",
      "1962/1962 [==============================] - 0s 42us/step - loss: 0.6919 - acc: 0.7401\n",
      "Epoch 12/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.6593 - acc: 0.7706\n",
      "Epoch 13/50\n",
      "1962/1962 [==============================] - 0s 42us/step - loss: 0.6259 - acc: 0.7701\n",
      "Epoch 14/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.5765 - acc: 0.8063\n",
      "Epoch 15/50\n",
      "1962/1962 [==============================] - 0s 45us/step - loss: 0.5394 - acc: 0.8216\n",
      "Epoch 16/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.5219 - acc: 0.8308\n",
      "Epoch 17/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.5042 - acc: 0.8293\n",
      "Epoch 18/50\n",
      "1962/1962 [==============================] - 0s 46us/step - loss: 0.4702 - acc: 0.8456\n",
      "Epoch 19/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.4519 - acc: 0.8568\n",
      "Epoch 20/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.4194 - acc: 0.8660\n",
      "Epoch 21/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.4082 - acc: 0.8614\n",
      "Epoch 22/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.3893 - acc: 0.8741\n",
      "Epoch 23/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.3835 - acc: 0.8756\n",
      "Epoch 24/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.3637 - acc: 0.8848\n",
      "Epoch 25/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.3534 - acc: 0.8746\n",
      "Epoch 26/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.3391 - acc: 0.8996\n",
      "Epoch 27/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.3315 - acc: 0.8909\n",
      "Epoch 28/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.3158 - acc: 0.8976\n",
      "Epoch 29/50\n",
      "1962/1962 [==============================] - 0s 42us/step - loss: 0.3012 - acc: 0.9027\n",
      "Epoch 30/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.3026 - acc: 0.9103\n",
      "Epoch 31/50\n",
      "1962/1962 [==============================] - 0s 41us/step - loss: 0.2853 - acc: 0.9098\n",
      "Epoch 32/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.2759 - acc: 0.9169\n",
      "Epoch 33/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.2823 - acc: 0.9057\n",
      "Epoch 34/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.2629 - acc: 0.9113\n",
      "Epoch 35/50\n",
      "1962/1962 [==============================] - 0s 39us/step - loss: 0.2609 - acc: 0.9179\n",
      "Epoch 36/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.2539 - acc: 0.9251\n",
      "Epoch 37/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.2506 - acc: 0.9205\n",
      "Epoch 38/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.2491 - acc: 0.9220\n",
      "Epoch 39/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.2342 - acc: 0.9286\n",
      "Epoch 40/50\n",
      "1962/1962 [==============================] - 0s 43us/step - loss: 0.2357 - acc: 0.9246\n",
      "Epoch 41/50\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 0.2426 - acc: 0.9169\n",
      "Epoch 42/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.2328 - acc: 0.9241\n",
      "Epoch 43/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.2223 - acc: 0.9317\n",
      "Epoch 44/50\n",
      "1962/1962 [==============================] - 0s 40us/step - loss: 0.2293 - acc: 0.9276\n",
      "Epoch 45/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 0.2268 - acc: 0.9220\n",
      "Epoch 46/50\n",
      "1962/1962 [==============================] - 0s 36us/step - loss: 0.2011 - acc: 0.9327\n",
      "Epoch 47/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 0.1949 - acc: 0.9414\n",
      "Epoch 48/50\n",
      "1962/1962 [==============================] - 0s 38us/step - loss: 0.2048 - acc: 0.9348\n",
      "Epoch 49/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.2005 - acc: 0.9383\n",
      "Epoch 50/50\n",
      "1962/1962 [==============================] - 0s 45us/step - loss: 0.2096 - acc: 0.9337\n",
      "491/491 [==============================] - 1s 1ms/step\n",
      "Epoch 1/50\n",
      "1963/1963 [==============================] - 2s 835us/step - loss: 1.3825 - acc: 0.2990\n",
      "Epoch 2/50\n",
      "1963/1963 [==============================] - 0s 41us/step - loss: 1.3426 - acc: 0.3530\n",
      "Epoch 3/50\n",
      "1963/1963 [==============================] - 0s 42us/step - loss: 1.2782 - acc: 0.4350\n",
      "Epoch 4/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 1.1848 - acc: 0.5150\n",
      "Epoch 5/50\n",
      "1963/1963 [==============================] - 0s 52us/step - loss: 1.0738 - acc: 0.5690\n",
      "Epoch 6/50\n",
      "1963/1963 [==============================] - 0s 47us/step - loss: 0.9590 - acc: 0.6322\n",
      "Epoch 7/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.8744 - acc: 0.6510\n",
      "Epoch 8/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.8142 - acc: 0.7025\n",
      "Epoch 9/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.7454 - acc: 0.7417\n",
      "Epoch 10/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.6922 - acc: 0.7652\n",
      "Epoch 11/50\n",
      "1963/1963 [==============================] - 0s 40us/step - loss: 0.6389 - acc: 0.7799\n",
      "Epoch 12/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.5814 - acc: 0.8085\n",
      "Epoch 13/50\n",
      "1963/1963 [==============================] - 0s 39us/step - loss: 0.5466 - acc: 0.8161\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963/1963 [==============================] - 0s 39us/step - loss: 0.5059 - acc: 0.8375\n",
      "Epoch 15/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.4830 - acc: 0.8451\n",
      "Epoch 16/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.4534 - acc: 0.8538\n",
      "Epoch 17/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.4277 - acc: 0.8645\n",
      "Epoch 18/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.4065 - acc: 0.8742\n",
      "Epoch 19/50\n",
      "1963/1963 [==============================] - 0s 47us/step - loss: 0.4010 - acc: 0.8675\n",
      "Epoch 20/50\n",
      "1963/1963 [==============================] - 0s 42us/step - loss: 0.3924 - acc: 0.8691\n",
      "Epoch 21/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.3699 - acc: 0.8828\n",
      "Epoch 22/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.3478 - acc: 0.8961\n",
      "Epoch 23/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.3459 - acc: 0.8818\n",
      "Epoch 24/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.3316 - acc: 0.8915\n",
      "Epoch 25/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.3363 - acc: 0.8976\n",
      "Epoch 26/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.3311 - acc: 0.8900\n",
      "Epoch 27/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.3048 - acc: 0.9042\n",
      "Epoch 28/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2985 - acc: 0.9032\n",
      "Epoch 29/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2951 - acc: 0.9032\n",
      "Epoch 30/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.3035 - acc: 0.9032\n",
      "Epoch 31/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2706 - acc: 0.9088\n",
      "Epoch 32/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2729 - acc: 0.9098\n",
      "Epoch 33/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.2616 - acc: 0.9251\n",
      "Epoch 34/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2788 - acc: 0.9119\n",
      "Epoch 35/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.2553 - acc: 0.9226\n",
      "Epoch 36/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2582 - acc: 0.9159\n",
      "Epoch 37/50\n",
      "1963/1963 [==============================] - 0s 40us/step - loss: 0.2477 - acc: 0.9149\n",
      "Epoch 38/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.2497 - acc: 0.9180\n",
      "Epoch 39/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.2430 - acc: 0.9210\n",
      "Epoch 40/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2451 - acc: 0.9180\n",
      "Epoch 41/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2371 - acc: 0.9246\n",
      "Epoch 42/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2356 - acc: 0.9251\n",
      "Epoch 43/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2377 - acc: 0.9256\n",
      "Epoch 44/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.2282 - acc: 0.9256\n",
      "Epoch 45/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2271 - acc: 0.9307\n",
      "Epoch 46/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2136 - acc: 0.9353\n",
      "Epoch 47/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2180 - acc: 0.9358\n",
      "Epoch 48/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2046 - acc: 0.9379\n",
      "Epoch 49/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.2059 - acc: 0.9322\n",
      "Epoch 50/50\n",
      "1963/1963 [==============================] - 0s 39us/step - loss: 0.1995 - acc: 0.9394\n",
      "490/490 [==============================] - 1s 1ms/step\n",
      "Epoch 1/50\n",
      "1963/1963 [==============================] - 2s 844us/step - loss: 1.3858 - acc: 0.2909\n",
      "Epoch 2/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 1.3464 - acc: 0.3535\n",
      "Epoch 3/50\n",
      "1963/1963 [==============================] - 0s 39us/step - loss: 1.2919 - acc: 0.4187\n",
      "Epoch 4/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 1.1944 - acc: 0.5166\n",
      "Epoch 5/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 1.0791 - acc: 0.5690\n",
      "Epoch 6/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.9627 - acc: 0.6052\n",
      "Epoch 7/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.8743 - acc: 0.6470\n",
      "Epoch 8/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.8026 - acc: 0.6735\n",
      "Epoch 9/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.7413 - acc: 0.7208\n",
      "Epoch 10/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.6970 - acc: 0.7320\n",
      "Epoch 11/50\n",
      "1963/1963 [==============================] - 0s 39us/step - loss: 0.6431 - acc: 0.7590\n",
      "Epoch 12/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.6001 - acc: 0.7947\n",
      "Epoch 13/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.5738 - acc: 0.7937\n",
      "Epoch 14/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.5374 - acc: 0.8141\n",
      "Epoch 15/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.5034 - acc: 0.8334\n",
      "Epoch 16/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.4667 - acc: 0.8507\n",
      "Epoch 17/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.4458 - acc: 0.8574\n",
      "Epoch 18/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.4349 - acc: 0.8569\n",
      "Epoch 19/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.4111 - acc: 0.8681\n",
      "Epoch 20/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.3948 - acc: 0.8670\n",
      "Epoch 21/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.3651 - acc: 0.8793\n",
      "Epoch 22/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.3607 - acc: 0.8828\n",
      "Epoch 23/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.3437 - acc: 0.8895\n",
      "Epoch 24/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.3221 - acc: 0.8986\n",
      "Epoch 25/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.3355 - acc: 0.8910\n",
      "Epoch 26/50\n",
      "1963/1963 [==============================] - 0s 40us/step - loss: 0.3167 - acc: 0.8986\n",
      "Epoch 27/50\n",
      "1963/1963 [==============================] - 0s 39us/step - loss: 0.3160 - acc: 0.8930\n",
      "Epoch 28/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.2869 - acc: 0.9088\n",
      "Epoch 29/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.2810 - acc: 0.9129\n",
      "Epoch 30/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.2863 - acc: 0.9058\n",
      "Epoch 31/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2782 - acc: 0.9170\n",
      "Epoch 32/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2762 - acc: 0.9088\n",
      "Epoch 33/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2581 - acc: 0.9170\n",
      "Epoch 34/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2595 - acc: 0.9190\n",
      "Epoch 35/50\n",
      "1963/1963 [==============================] - 0s 38us/step - loss: 0.2402 - acc: 0.9256\n",
      "Epoch 36/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2468 - acc: 0.9159\n",
      "Epoch 37/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.2405 - acc: 0.9185\n",
      "Epoch 38/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2316 - acc: 0.9241\n",
      "Epoch 39/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.2371 - acc: 0.9200\n",
      "Epoch 40/50\n",
      "1963/1963 [==============================] - 0s 33us/step - loss: 0.2281 - acc: 0.9256\n",
      "Epoch 41/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2300 - acc: 0.9246\n",
      "Epoch 42/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.2166 - acc: 0.9379\n",
      "Epoch 43/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2251 - acc: 0.9282\n",
      "Epoch 44/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.2055 - acc: 0.9358\n",
      "Epoch 45/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.2085 - acc: 0.9373\n",
      "Epoch 46/50\n",
      "1963/1963 [==============================] - 0s 36us/step - loss: 0.1938 - acc: 0.9353\n",
      "Epoch 47/50\n",
      "1963/1963 [==============================] - 0s 37us/step - loss: 0.2048 - acc: 0.9353\n",
      "Epoch 48/50\n",
      "1963/1963 [==============================] - 0s 39us/step - loss: 0.2044 - acc: 0.9353\n",
      "Epoch 49/50\n",
      "1963/1963 [==============================] - 0s 34us/step - loss: 0.2062 - acc: 0.9348\n",
      "Epoch 50/50\n",
      "1963/1963 [==============================] - 0s 35us/step - loss: 0.1893 - acc: 0.9414\n",
      "490/490 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "kclf = KerasClassifier(build_fn = build_ann, batch_size = 128, epochs = 50)\n",
    "accuracies = cross_val_score(estimator = kclf, X = X_vect_name, y = y_nn, cv = 5, n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:21:47.769961Z",
     "start_time": "2018-04-11T13:21:47.763085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.8976665730848788\n",
      "Standard Deviation: 0.012339559335223823\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy:', accuracies.mean())\n",
    "print('Standard Deviation:', accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:21:51.440556Z",
     "start_time": "2018-04-11T13:21:51.431707Z"
    }
   },
   "outputs": [],
   "source": [
    "X_vect_name = dataset['Vector'].as_matrix().T\n",
    "X_vect_name = np.array([np.array(x) for x in X_vect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:21:59.853022Z",
     "start_time": "2018-04-11T13:21:52.837102Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2453/2453 [==============================] - 2s 759us/step - loss: 1.3868 - acc: 0.2951\n",
      "Epoch 2/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 1.3854 - acc: 0.3004\n",
      "Epoch 3/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.3816 - acc: 0.3033\n",
      "Epoch 4/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 1.3721 - acc: 0.3229\n",
      "Epoch 5/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.3556 - acc: 0.3424\n",
      "Epoch 6/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 1.3475 - acc: 0.3624\n",
      "Epoch 7/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.3283 - acc: 0.3702\n",
      "Epoch 8/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 1.2993 - acc: 0.3946\n",
      "Epoch 9/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.2595 - acc: 0.4350\n",
      "Epoch 10/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.1994 - acc: 0.4721\n",
      "Epoch 11/50\n",
      "2453/2453 [==============================] - 0s 45us/step - loss: 1.1424 - acc: 0.4974\n",
      "Epoch 12/50\n",
      "2453/2453 [==============================] - 0s 43us/step - loss: 1.0777 - acc: 0.5145\n",
      "Epoch 13/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 1.0243 - acc: 0.5218\n",
      "Epoch 14/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 0.9653 - acc: 0.5495\n",
      "Epoch 15/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.9282 - acc: 0.5850\n",
      "Epoch 16/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 0.9024 - acc: 0.5879\n",
      "Epoch 17/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.8899 - acc: 0.5874\n",
      "Epoch 18/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.8595 - acc: 0.6254\n",
      "Epoch 19/50\n",
      "2453/2453 [==============================] - 0s 36us/step - loss: 0.8498 - acc: 0.6066\n",
      "Epoch 20/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.8249 - acc: 0.6249\n",
      "Epoch 21/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.8121 - acc: 0.6298\n",
      "Epoch 22/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.8135 - acc: 0.6343\n",
      "Epoch 23/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.7859 - acc: 0.6592\n",
      "Epoch 24/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.7881 - acc: 0.6478\n",
      "Epoch 25/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.7707 - acc: 0.6572\n",
      "Epoch 26/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.7652 - acc: 0.6690\n",
      "Epoch 27/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.7590 - acc: 0.6657\n",
      "Epoch 28/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 0.7464 - acc: 0.6869\n",
      "Epoch 29/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.7368 - acc: 0.6747\n",
      "Epoch 30/50\n",
      "2453/2453 [==============================] - 0s 42us/step - loss: 0.7434 - acc: 0.6596\n",
      "Epoch 31/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.7171 - acc: 0.6918\n",
      "Epoch 32/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.7176 - acc: 0.6869\n",
      "Epoch 33/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 0.7054 - acc: 0.6967\n",
      "Epoch 34/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.6939 - acc: 0.6983\n",
      "Epoch 35/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.7085 - acc: 0.6918\n",
      "Epoch 36/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.6823 - acc: 0.7110\n",
      "Epoch 37/50\n",
      "2453/2453 [==============================] - 0s 35us/step - loss: 0.6822 - acc: 0.7008\n",
      "Epoch 38/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.6705 - acc: 0.7199\n",
      "Epoch 39/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.6731 - acc: 0.7142\n",
      "Epoch 40/50\n",
      "2453/2453 [==============================] - 0s 36us/step - loss: 0.6603 - acc: 0.7256\n",
      "Epoch 41/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.6549 - acc: 0.7195\n",
      "Epoch 42/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.6368 - acc: 0.7415\n",
      "Epoch 43/50\n",
      "2453/2453 [==============================] - 0s 36us/step - loss: 0.6409 - acc: 0.7419\n",
      "Epoch 44/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.6265 - acc: 0.7428\n",
      "Epoch 45/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.6412 - acc: 0.7395\n",
      "Epoch 46/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.6030 - acc: 0.7546\n",
      "Epoch 47/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.6057 - acc: 0.7583\n",
      "Epoch 48/50\n",
      "2453/2453 [==============================] - 0s 35us/step - loss: 0.6158 - acc: 0.7534\n",
      "Epoch 49/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.6147 - acc: 0.7538\n",
      "Epoch 50/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.5900 - acc: 0.7664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbda0fe07b8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = build_ann()\n",
    "ann.fit(X_vect_name, y_nn, batch_size = 128, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:35:07.076109Z",
     "start_time": "2018-04-11T13:34:59.611822Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> sad (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> sad (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> sad (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> sad (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> sad (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> sad (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 8 predictions our of 14 songs\n",
      "Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "extra_test_ann(ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Content + Song Title\n",
    "Here we propose an approach which is a sort of combination of the previous two. We will build classifiers which will receive as input 5 dimensional points. Those dimensions will be the 4 principal components of the song lyric + the vector norm of the song's title.\n",
    "\n",
    "We already understood that the best classifiers for our purpose are the SVM and the ANN so we will just use them.\n",
    "\n",
    "First of all we will do our feature engineering, and then we will proceed to the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:46:19.075885Z",
     "start_time": "2018-04-17T10:45:39.050995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare array for sklearn classifiers\n",
    "X_vect_pca = dataset['Vector'].as_matrix()\n",
    "X_vect_pca = np.array([np.array(x) for x in X_vect_pca])\n",
    "\n",
    "#X_vect_title\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_vect_pca)\n",
    "X_vect_comb = pca.transform(X_vect_pca)\n",
    "\n",
    "X_vect_nl = list()\n",
    "\n",
    "for (i, (index,row)) in enumerate(dataset.iterrows()):\n",
    "    song_title = row['Lyric_Path'].split('_')[-1]\n",
    "    title_doc = nlp(song_title)\n",
    "    title_comp = title_doc.vector_norm\n",
    "    \n",
    "    X_vect_nl.append(np.concatenate((X_vect_comb[i], [title_comp])))\n",
    "X_vect_nl = np.array(X_vect_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "We will repeat the same operations with did eariler. If you want more detailed explainations, please refer to the \"Classifiers on Lyrics Content\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:41:36.554935Z",
     "start_time": "2018-04-16T09:41:31.849343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build model\n",
    "clf = SVC()\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect_nl, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:48:32.689482Z",
     "start_time": "2018-04-16T09:41:53.212272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 29 candidates, totalling 290 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 290 out of 290 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'kernel': 'linear', 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['linear'], 'C': [ 0.01, 0.05, 1, 10, 100 ]},\n",
    "    { 'kernel': ['rbf', 'sigmoid'], 'C': [ 0.01, 0.05, 0.1, 0.3, 0.8, 1, 3, 10, 50, 100, 150, 200 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params = parameters_grid_search(SVC, params, X_vect, y, verbose=1)\n",
    "print('Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:48:38.543238Z",
     "start_time": "2018-04-16T09:48:32.693937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm_best, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:49:06.896711Z",
     "start_time": "2018-04-16T09:48:55.571082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> angry (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> angry (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> angry (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> angry (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> sad (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> sad (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> happy (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 3 predictions our of 14 songs\n",
      "Accuracy: 0.21\n"
     ]
    }
   ],
   "source": [
    "extra_test(svm_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network\n",
    "Again, if you want more detailed information on what we will be doing in this section, please refer to what we wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:46:23.376529Z",
     "start_time": "2018-04-17T10:46:23.368981Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "# we need to scale because we don't want one feature to predomine the others\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X_vect_nl_ann = sc.fit_transform(X_vect_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:46:57.420427Z",
     "start_time": "2018-04-17T10:46:57.410467Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "y_nn = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:46:57.612070Z",
     "start_time": "2018-04-17T10:46:57.559586Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_ann(optimizer='adam', input_size=300):\n",
    "    classifier = Sequential()\n",
    "    #2 Adding first hidden layer\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'random_normal', activation = 'sigmoid', input_dim = input_size))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    # Adding hidden layers\n",
    "    classifier.add(Dense(units = 35, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    classifier.add(Dense(units = 15, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    classifier.add(Dense(units = 10, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    # Adding output layer\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'random_normal', activation = 'softmax'))\n",
    "\n",
    "    #3 Compiling the ANN\n",
    "    classifier.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:48:07.212092Z",
     "start_time": "2018-04-17T10:47:38.062883Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1962/1962 [==============================] - 1s 399us/step - loss: 1.3888 - acc: 0.2472\n",
      "Epoch 2/100\n",
      "1962/1962 [==============================] - 0s 34us/step - loss: 1.3840 - acc: 0.2829\n",
      "Epoch 3/100\n",
      "1962/1962 [==============================] - 0s 25us/step - loss: 1.3813 - acc: 0.2951\n",
      "Epoch 4/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3803 - acc: 0.3129\n",
      "Epoch 5/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3775 - acc: 0.3119\n",
      "Epoch 6/100\n",
      "1962/1962 [==============================] - 0s 24us/step - loss: 1.3759 - acc: 0.3191\n",
      "Epoch 7/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3740 - acc: 0.3175\n",
      "Epoch 8/100\n",
      "1962/1962 [==============================] - 0s 25us/step - loss: 1.3774 - acc: 0.3150\n",
      "Epoch 9/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3746 - acc: 0.3180\n",
      "Epoch 10/100\n",
      "1962/1962 [==============================] - 0s 37us/step - loss: 1.3774 - acc: 0.3180\n",
      "Epoch 11/100\n",
      "1962/1962 [==============================] - 0s 24us/step - loss: 1.3735 - acc: 0.3180\n",
      "Epoch 12/100\n",
      "1962/1962 [==============================] - 0s 32us/step - loss: 1.3764 - acc: 0.3191\n",
      "Epoch 13/100\n",
      "1962/1962 [==============================] - 0s 26us/step - loss: 1.3758 - acc: 0.3175\n",
      "Epoch 14/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3774 - acc: 0.3170\n",
      "Epoch 15/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3748 - acc: 0.3180\n",
      "Epoch 16/100\n",
      "1962/1962 [==============================] - 0s 25us/step - loss: 1.3744 - acc: 0.3180\n",
      "Epoch 17/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3763 - acc: 0.3186\n",
      "Epoch 18/100\n",
      "1962/1962 [==============================] - 0s 25us/step - loss: 1.3744 - acc: 0.3180\n",
      "Epoch 19/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3737 - acc: 0.3180\n",
      "Epoch 20/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3739 - acc: 0.3180\n",
      "Epoch 21/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3762 - acc: 0.3180\n",
      "Epoch 22/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3780 - acc: 0.3180\n",
      "Epoch 23/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3766 - acc: 0.3180\n",
      "Epoch 24/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3769 - acc: 0.3180\n",
      "Epoch 25/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3750 - acc: 0.3180\n",
      "Epoch 26/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3746 - acc: 0.3180\n",
      "Epoch 27/100\n",
      "1962/1962 [==============================] - 0s 24us/step - loss: 1.3764 - acc: 0.3180\n",
      "Epoch 28/100\n",
      "1962/1962 [==============================] - 0s 27us/step - loss: 1.3761 - acc: 0.3180\n",
      "Epoch 29/100\n",
      "1962/1962 [==============================] - 0s 30us/step - loss: 1.3757 - acc: 0.3180\n",
      "Epoch 30/100\n",
      "1962/1962 [==============================] - 0s 31us/step - loss: 1.3772 - acc: 0.3180\n",
      "Epoch 31/100\n",
      "1962/1962 [==============================] - 0s 28us/step - loss: 1.3757 - acc: 0.3180\n",
      "Epoch 32/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3732 - acc: 0.3180\n",
      "Epoch 33/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3736 - acc: 0.3180\n",
      "Epoch 34/100\n",
      "1962/1962 [==============================] - 0s 24us/step - loss: 1.3758 - acc: 0.3180\n",
      "Epoch 35/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3739 - acc: 0.3180\n",
      "Epoch 36/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3738 - acc: 0.3180\n",
      "Epoch 37/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3752 - acc: 0.3180\n",
      "Epoch 38/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3750 - acc: 0.3180\n",
      "Epoch 39/100\n",
      "1962/1962 [==============================] - 0s 27us/step - loss: 1.3730 - acc: 0.3180\n",
      "Epoch 40/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3749 - acc: 0.3180\n",
      "Epoch 41/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3745 - acc: 0.3180\n",
      "Epoch 42/100\n",
      "1962/1962 [==============================] - 0s 24us/step - loss: 1.3745 - acc: 0.3180\n",
      "Epoch 43/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3729 - acc: 0.3180\n",
      "Epoch 44/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3751 - acc: 0.3180\n",
      "Epoch 45/100\n",
      "1962/1962 [==============================] - 0s 26us/step - loss: 1.3743 - acc: 0.3180\n",
      "Epoch 46/100\n",
      "1962/1962 [==============================] - 0s 26us/step - loss: 1.3737 - acc: 0.3180\n",
      "Epoch 47/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3749 - acc: 0.3180\n",
      "Epoch 48/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3735 - acc: 0.3180\n",
      "Epoch 49/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3740 - acc: 0.3180\n",
      "Epoch 50/100\n",
      "1962/1962 [==============================] - 0s 24us/step - loss: 1.3746 - acc: 0.3180\n",
      "Epoch 51/100\n",
      "1962/1962 [==============================] - 0s 25us/step - loss: 1.3747 - acc: 0.3180\n",
      "Epoch 52/100\n",
      "1962/1962 [==============================] - 0s 31us/step - loss: 1.3719 - acc: 0.3180\n",
      "Epoch 53/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3745 - acc: 0.3180\n",
      "Epoch 54/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3733 - acc: 0.3180\n",
      "Epoch 55/100\n",
      "1962/1962 [==============================] - 0s 31us/step - loss: 1.3751 - acc: 0.3180\n",
      "Epoch 56/100\n",
      "1962/1962 [==============================] - 0s 26us/step - loss: 1.3743 - acc: 0.3180\n",
      "Epoch 57/100\n",
      "1962/1962 [==============================] - 0s 25us/step - loss: 1.3742 - acc: 0.3180\n",
      "Epoch 58/100\n",
      "1962/1962 [==============================] - 0s 35us/step - loss: 1.3740 - acc: 0.3180\n",
      "Epoch 59/100\n",
      "1962/1962 [==============================] - 0s 30us/step - loss: 1.3741 - acc: 0.3180\n",
      "Epoch 60/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3743 - acc: 0.3180\n",
      "Epoch 61/100\n",
      "1962/1962 [==============================] - 0s 24us/step - loss: 1.3739 - acc: 0.3180\n",
      "Epoch 62/100\n",
      "1962/1962 [==============================] - 0s 34us/step - loss: 1.3745 - acc: 0.3180\n",
      "Epoch 63/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3744 - acc: 0.3180\n",
      "Epoch 64/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3739 - acc: 0.3180\n",
      "Epoch 65/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3731 - acc: 0.3180\n",
      "Epoch 66/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3743 - acc: 0.3180\n",
      "Epoch 67/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3740 - acc: 0.3180\n",
      "Epoch 68/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3737 - acc: 0.3180\n",
      "Epoch 69/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3732 - acc: 0.3180\n",
      "Epoch 70/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3737 - acc: 0.3180\n",
      "Epoch 71/100\n",
      "1962/1962 [==============================] - 0s 26us/step - loss: 1.3732 - acc: 0.3180\n",
      "Epoch 72/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3756 - acc: 0.3180\n",
      "Epoch 73/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3750 - acc: 0.3180\n",
      "Epoch 74/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3752 - acc: 0.3180\n",
      "Epoch 75/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3737 - acc: 0.3180\n",
      "Epoch 76/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3731 - acc: 0.3180\n",
      "Epoch 77/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3738 - acc: 0.3180\n",
      "Epoch 78/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3735 - acc: 0.3180\n",
      "Epoch 79/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3744 - acc: 0.3180\n",
      "Epoch 80/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3732 - acc: 0.3180\n",
      "Epoch 81/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3743 - acc: 0.3180\n",
      "Epoch 82/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3733 - acc: 0.3180\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3726 - acc: 0.3180\n",
      "Epoch 84/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3736 - acc: 0.3180\n",
      "Epoch 85/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3745 - acc: 0.3180\n",
      "Epoch 86/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3739 - acc: 0.3180\n",
      "Epoch 87/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3752 - acc: 0.3180\n",
      "Epoch 88/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3733 - acc: 0.3180\n",
      "Epoch 89/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3744 - acc: 0.3180\n",
      "Epoch 90/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3742 - acc: 0.3180\n",
      "Epoch 91/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3749 - acc: 0.3180\n",
      "Epoch 92/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3747 - acc: 0.3180\n",
      "Epoch 93/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3745 - acc: 0.3180\n",
      "Epoch 94/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3733 - acc: 0.3180\n",
      "Epoch 95/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3735 - acc: 0.3180\n",
      "Epoch 96/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3743 - acc: 0.3180\n",
      "Epoch 97/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3744 - acc: 0.3180\n",
      "Epoch 98/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3743 - acc: 0.3180\n",
      "Epoch 99/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3743 - acc: 0.3180\n",
      "Epoch 100/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3745 - acc: 0.3180\n",
      "491/491 [==============================] - 0s 479us/step\n",
      "Epoch 1/100\n",
      "1962/1962 [==============================] - 1s 356us/step - loss: 1.3918 - acc: 0.2421\n",
      "Epoch 2/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3837 - acc: 0.2645\n",
      "Epoch 3/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3855 - acc: 0.2655\n",
      "Epoch 4/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3810 - acc: 0.3007\n",
      "Epoch 5/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3783 - acc: 0.3053\n",
      "Epoch 6/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3785 - acc: 0.3084\n",
      "Epoch 7/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3761 - acc: 0.3104\n",
      "Epoch 8/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3758 - acc: 0.3084\n",
      "Epoch 9/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3776 - acc: 0.3089\n",
      "Epoch 10/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3798 - acc: 0.3099\n",
      "Epoch 11/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3802 - acc: 0.3094\n",
      "Epoch 12/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3788 - acc: 0.3094\n",
      "Epoch 13/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3793 - acc: 0.3094\n",
      "Epoch 14/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3801 - acc: 0.3094\n",
      "Epoch 15/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3773 - acc: 0.3094\n",
      "Epoch 16/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3802 - acc: 0.3094\n",
      "Epoch 17/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3813 - acc: 0.3094\n",
      "Epoch 18/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3792 - acc: 0.3094\n",
      "Epoch 19/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3791 - acc: 0.3094\n",
      "Epoch 20/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3776 - acc: 0.3094\n",
      "Epoch 21/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3772 - acc: 0.3094\n",
      "Epoch 22/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3790 - acc: 0.3094\n",
      "Epoch 23/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3777 - acc: 0.3094\n",
      "Epoch 24/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3778 - acc: 0.3094\n",
      "Epoch 25/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3780 - acc: 0.3094\n",
      "Epoch 26/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3767 - acc: 0.3094\n",
      "Epoch 27/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3780 - acc: 0.3094\n",
      "Epoch 28/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3788 - acc: 0.3094\n",
      "Epoch 29/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3788 - acc: 0.3094\n",
      "Epoch 30/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3780 - acc: 0.3094\n",
      "Epoch 31/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3778 - acc: 0.3094\n",
      "Epoch 32/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3786 - acc: 0.3094\n",
      "Epoch 33/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3796 - acc: 0.3094\n",
      "Epoch 34/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3780 - acc: 0.3094\n",
      "Epoch 35/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3778 - acc: 0.3094\n",
      "Epoch 36/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3782 - acc: 0.3094\n",
      "Epoch 37/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3778 - acc: 0.3094\n",
      "Epoch 38/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3774 - acc: 0.3094\n",
      "Epoch 39/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3778 - acc: 0.3094\n",
      "Epoch 40/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3782 - acc: 0.3094\n",
      "Epoch 41/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3786 - acc: 0.3094\n",
      "Epoch 42/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3753 - acc: 0.3094\n",
      "Epoch 43/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3792 - acc: 0.3094\n",
      "Epoch 44/100\n",
      "1962/1962 [==============================] - 0s 15us/step - loss: 1.3784 - acc: 0.3094\n",
      "Epoch 45/100\n",
      "1962/1962 [==============================] - 0s 15us/step - loss: 1.3778 - acc: 0.3094\n",
      "Epoch 46/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3773 - acc: 0.3094\n",
      "Epoch 47/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3786 - acc: 0.3094\n",
      "Epoch 48/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3779 - acc: 0.3094\n",
      "Epoch 49/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3776 - acc: 0.3094\n",
      "Epoch 50/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3781 - acc: 0.3094\n",
      "Epoch 51/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3766 - acc: 0.3094\n",
      "Epoch 52/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3782 - acc: 0.3094\n",
      "Epoch 53/100\n",
      "1962/1962 [==============================] - 0s 15us/step - loss: 1.3775 - acc: 0.3094\n",
      "Epoch 54/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3768 - acc: 0.3094\n",
      "Epoch 55/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3761 - acc: 0.3094\n",
      "Epoch 56/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3764 - acc: 0.3094\n",
      "Epoch 57/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3785 - acc: 0.3094\n",
      "Epoch 58/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3771 - acc: 0.3094\n",
      "Epoch 59/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3762 - acc: 0.3094\n",
      "Epoch 60/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3772 - acc: 0.3094\n",
      "Epoch 61/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3774 - acc: 0.3094\n",
      "Epoch 62/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3775 - acc: 0.3094\n",
      "Epoch 63/100\n",
      "1962/1962 [==============================] - 0s 15us/step - loss: 1.3773 - acc: 0.3094\n",
      "Epoch 64/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3778 - acc: 0.3094\n",
      "Epoch 65/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3776 - acc: 0.3094\n",
      "Epoch 66/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3786 - acc: 0.3094\n",
      "Epoch 67/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3775 - acc: 0.3094\n",
      "Epoch 68/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3778 - acc: 0.3094\n",
      "Epoch 69/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3763 - acc: 0.3094\n",
      "Epoch 70/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3781 - acc: 0.3094\n",
      "Epoch 71/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3780 - acc: 0.3094\n",
      "Epoch 72/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3770 - acc: 0.3094\n",
      "Epoch 73/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3775 - acc: 0.3094\n",
      "Epoch 74/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3782 - acc: 0.3094\n",
      "Epoch 75/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3774 - acc: 0.3094\n",
      "Epoch 76/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3777 - acc: 0.3094\n",
      "Epoch 77/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3777 - acc: 0.3094\n",
      "Epoch 78/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3772 - acc: 0.3094\n",
      "Epoch 79/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3779 - acc: 0.3094\n",
      "Epoch 80/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3779 - acc: 0.3094\n",
      "Epoch 81/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3771 - acc: 0.3094\n",
      "Epoch 82/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3791 - acc: 0.3094\n",
      "Epoch 83/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3775 - acc: 0.3094\n",
      "Epoch 84/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3768 - acc: 0.3094\n",
      "Epoch 85/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3773 - acc: 0.3094\n",
      "Epoch 86/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3761 - acc: 0.3094\n",
      "Epoch 87/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3765 - acc: 0.3094\n",
      "Epoch 88/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3773 - acc: 0.3094\n",
      "Epoch 89/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3775 - acc: 0.3094\n",
      "Epoch 90/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3767 - acc: 0.3094\n",
      "Epoch 91/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3772 - acc: 0.3094\n",
      "Epoch 92/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3767 - acc: 0.3094\n",
      "Epoch 93/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3772 - acc: 0.3094\n",
      "Epoch 94/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3775 - acc: 0.3094\n",
      "Epoch 95/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3765 - acc: 0.3094\n",
      "Epoch 96/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3770 - acc: 0.3094\n",
      "Epoch 97/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3771 - acc: 0.3094\n",
      "Epoch 98/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3774 - acc: 0.3094\n",
      "Epoch 99/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3767 - acc: 0.3094\n",
      "Epoch 100/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3774 - acc: 0.3094\n",
      "491/491 [==============================] - 0s 526us/step\n",
      "Epoch 1/100\n",
      "1962/1962 [==============================] - 1s 376us/step - loss: 1.3881 - acc: 0.2564\n",
      "Epoch 2/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3836 - acc: 0.2768\n",
      "Epoch 3/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3844 - acc: 0.2885\n",
      "Epoch 4/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3805 - acc: 0.3022\n",
      "Epoch 5/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3832 - acc: 0.2971\n",
      "Epoch 6/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3823 - acc: 0.3017\n",
      "Epoch 7/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3796 - acc: 0.3053\n",
      "Epoch 8/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3818 - acc: 0.2982\n",
      "Epoch 9/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3815 - acc: 0.3048\n",
      "Epoch 10/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3800 - acc: 0.3053\n",
      "Epoch 11/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3810 - acc: 0.3017\n",
      "Epoch 12/100\n",
      "1962/1962 [==============================] - 0s 15us/step - loss: 1.3805 - acc: 0.3043\n",
      "Epoch 13/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3809 - acc: 0.3017\n",
      "Epoch 14/100\n",
      "1962/1962 [==============================] - 0s 15us/step - loss: 1.3803 - acc: 0.3068\n",
      "Epoch 15/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3808 - acc: 0.3033\n",
      "Epoch 16/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3800 - acc: 0.3043\n",
      "Epoch 17/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3800 - acc: 0.3043\n",
      "Epoch 18/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3792 - acc: 0.3038\n",
      "Epoch 19/100\n",
      "1962/1962 [==============================] - 0s 15us/step - loss: 1.3803 - acc: 0.3028\n",
      "Epoch 20/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3798 - acc: 0.3043\n",
      "Epoch 21/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3806 - acc: 0.3043\n",
      "Epoch 22/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3792 - acc: 0.3043\n",
      "Epoch 23/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3794 - acc: 0.3043\n",
      "Epoch 24/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3795 - acc: 0.3043\n",
      "Epoch 25/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3805 - acc: 0.3043\n",
      "Epoch 26/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3786 - acc: 0.3043\n",
      "Epoch 27/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3814 - acc: 0.3043\n",
      "Epoch 28/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3792 - acc: 0.3043\n",
      "Epoch 29/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3786 - acc: 0.3043\n",
      "Epoch 30/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3790 - acc: 0.3043\n",
      "Epoch 31/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3785 - acc: 0.3043\n",
      "Epoch 32/100\n",
      "1962/1962 [==============================] - 0s 16us/step - loss: 1.3795 - acc: 0.3043\n",
      "Epoch 33/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3796 - acc: 0.3043\n",
      "Epoch 34/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3801 - acc: 0.3043\n",
      "Epoch 35/100\n",
      "1962/1962 [==============================] - 0s 32us/step - loss: 1.3807 - acc: 0.3043\n",
      "Epoch 36/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3784 - acc: 0.3043\n",
      "Epoch 37/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3797 - acc: 0.3043\n",
      "Epoch 38/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3803 - acc: 0.3043\n",
      "Epoch 39/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3794 - acc: 0.3043\n",
      "Epoch 40/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3789 - acc: 0.3043\n",
      "Epoch 41/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3800 - acc: 0.3043\n",
      "Epoch 42/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3810 - acc: 0.3043\n",
      "Epoch 43/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3792 - acc: 0.3043\n",
      "Epoch 44/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3808 - acc: 0.3043\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3804 - acc: 0.3043\n",
      "Epoch 46/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3790 - acc: 0.3043\n",
      "Epoch 47/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3795 - acc: 0.3043\n",
      "Epoch 48/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3790 - acc: 0.3043\n",
      "Epoch 49/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3796 - acc: 0.3043\n",
      "Epoch 50/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3786 - acc: 0.3043\n",
      "Epoch 51/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3803 - acc: 0.3043\n",
      "Epoch 52/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3799 - acc: 0.3043\n",
      "Epoch 53/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3805 - acc: 0.3043\n",
      "Epoch 54/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3797 - acc: 0.3043\n",
      "Epoch 55/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3793 - acc: 0.3043\n",
      "Epoch 56/100\n",
      "1962/1962 [==============================] - 0s 17us/step - loss: 1.3791 - acc: 0.3043\n",
      "Epoch 57/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3788 - acc: 0.3043\n",
      "Epoch 58/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3798 - acc: 0.3043\n",
      "Epoch 59/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3799 - acc: 0.3043\n",
      "Epoch 60/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3772 - acc: 0.3043\n",
      "Epoch 61/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3791 - acc: 0.3043\n",
      "Epoch 62/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3791 - acc: 0.3043\n",
      "Epoch 63/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3788 - acc: 0.3043\n",
      "Epoch 64/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3789 - acc: 0.3043\n",
      "Epoch 65/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3798 - acc: 0.3043\n",
      "Epoch 66/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3804 - acc: 0.3043\n",
      "Epoch 67/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3781 - acc: 0.3043\n",
      "Epoch 68/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3786 - acc: 0.3043\n",
      "Epoch 69/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3784 - acc: 0.3043\n",
      "Epoch 70/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3795 - acc: 0.3043\n",
      "Epoch 71/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3803 - acc: 0.3043\n",
      "Epoch 72/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3778 - acc: 0.3043\n",
      "Epoch 73/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3784 - acc: 0.3043\n",
      "Epoch 74/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3789 - acc: 0.3043\n",
      "Epoch 75/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3784 - acc: 0.3043\n",
      "Epoch 76/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3797 - acc: 0.3043\n",
      "Epoch 77/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3788 - acc: 0.3043\n",
      "Epoch 78/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3793 - acc: 0.3043\n",
      "Epoch 79/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3795 - acc: 0.3043\n",
      "Epoch 80/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3780 - acc: 0.3043\n",
      "Epoch 81/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3794 - acc: 0.3043\n",
      "Epoch 82/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3790 - acc: 0.3043\n",
      "Epoch 83/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3788 - acc: 0.3043\n",
      "Epoch 84/100\n",
      "1962/1962 [==============================] - 0s 25us/step - loss: 1.3790 - acc: 0.3043\n",
      "Epoch 85/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3791 - acc: 0.3043\n",
      "Epoch 86/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3788 - acc: 0.3043\n",
      "Epoch 87/100\n",
      "1962/1962 [==============================] - 0s 24us/step - loss: 1.3797 - acc: 0.3043\n",
      "Epoch 88/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3786 - acc: 0.3043\n",
      "Epoch 89/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3795 - acc: 0.3043\n",
      "Epoch 90/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3791 - acc: 0.3043\n",
      "Epoch 91/100\n",
      "1962/1962 [==============================] - 0s 22us/step - loss: 1.3793 - acc: 0.3043\n",
      "Epoch 92/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3788 - acc: 0.3043\n",
      "Epoch 93/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3796 - acc: 0.3043\n",
      "Epoch 94/100\n",
      "1962/1962 [==============================] - 0s 19us/step - loss: 1.3795 - acc: 0.3043\n",
      "Epoch 95/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3791 - acc: 0.3043\n",
      "Epoch 96/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3791 - acc: 0.3043\n",
      "Epoch 97/100\n",
      "1962/1962 [==============================] - 0s 21us/step - loss: 1.3789 - acc: 0.3043\n",
      "Epoch 98/100\n",
      "1962/1962 [==============================] - 0s 23us/step - loss: 1.3793 - acc: 0.3043\n",
      "Epoch 99/100\n",
      "1962/1962 [==============================] - 0s 18us/step - loss: 1.3791 - acc: 0.3043\n",
      "Epoch 100/100\n",
      "1962/1962 [==============================] - 0s 20us/step - loss: 1.3788 - acc: 0.3043\n",
      "491/491 [==============================] - 0s 575us/step\n",
      "Epoch 1/100\n",
      "1963/1963 [==============================] - 1s 497us/step - loss: 1.3905 - acc: 0.2292\n",
      "Epoch 2/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3863 - acc: 0.2593\n",
      "Epoch 3/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3826 - acc: 0.2914\n",
      "Epoch 4/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3823 - acc: 0.3001\n",
      "Epoch 5/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3795 - acc: 0.3016\n",
      "Epoch 6/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3793 - acc: 0.3107\n",
      "Epoch 7/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3767 - acc: 0.3072\n",
      "Epoch 8/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3786 - acc: 0.3092\n",
      "Epoch 9/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3795 - acc: 0.3102\n",
      "Epoch 10/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3800 - acc: 0.3113\n",
      "Epoch 11/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3785 - acc: 0.3113\n",
      "Epoch 12/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3787 - acc: 0.3102\n",
      "Epoch 13/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3777 - acc: 0.3118\n",
      "Epoch 14/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3786 - acc: 0.3102\n",
      "Epoch 15/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3764 - acc: 0.3113\n",
      "Epoch 16/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3780 - acc: 0.3102\n",
      "Epoch 17/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3776 - acc: 0.3107\n",
      "Epoch 18/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3773 - acc: 0.3107\n",
      "Epoch 19/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3776 - acc: 0.3107\n",
      "Epoch 20/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3782 - acc: 0.3107\n",
      "Epoch 21/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3786 - acc: 0.3107\n",
      "Epoch 22/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3786 - acc: 0.3107\n",
      "Epoch 23/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3799 - acc: 0.3107\n",
      "Epoch 24/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3775 - acc: 0.3107\n",
      "Epoch 25/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3774 - acc: 0.3107\n",
      "Epoch 26/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3764 - acc: 0.3107\n",
      "Epoch 27/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3778 - acc: 0.3107\n",
      "Epoch 28/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3784 - acc: 0.3107\n",
      "Epoch 29/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3773 - acc: 0.3107\n",
      "Epoch 30/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3788 - acc: 0.3107\n",
      "Epoch 31/100\n",
      "1963/1963 [==============================] - 0s 23us/step - loss: 1.3787 - acc: 0.3107\n",
      "Epoch 32/100\n",
      "1963/1963 [==============================] - 0s 28us/step - loss: 1.3778 - acc: 0.3107\n",
      "Epoch 33/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3786 - acc: 0.3107\n",
      "Epoch 34/100\n",
      "1963/1963 [==============================] - 0s 26us/step - loss: 1.3767 - acc: 0.3107\n",
      "Epoch 35/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3760 - acc: 0.3107\n",
      "Epoch 36/100\n",
      "1963/1963 [==============================] - 0s 23us/step - loss: 1.3791 - acc: 0.3107\n",
      "Epoch 37/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3788 - acc: 0.3107\n",
      "Epoch 38/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3778 - acc: 0.3107\n",
      "Epoch 39/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3786 - acc: 0.3107\n",
      "Epoch 40/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3786 - acc: 0.3107\n",
      "Epoch 41/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3781 - acc: 0.3107\n",
      "Epoch 42/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3768 - acc: 0.3107\n",
      "Epoch 43/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3780 - acc: 0.3107\n",
      "Epoch 44/100\n",
      "1963/1963 [==============================] - 0s 27us/step - loss: 1.3771 - acc: 0.3107\n",
      "Epoch 45/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3767 - acc: 0.3107\n",
      "Epoch 46/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3773 - acc: 0.3107\n",
      "Epoch 47/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3791 - acc: 0.3107\n",
      "Epoch 48/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3766 - acc: 0.3107\n",
      "Epoch 49/100\n",
      "1963/1963 [==============================] - 0s 23us/step - loss: 1.3771 - acc: 0.3107\n",
      "Epoch 50/100\n",
      "1963/1963 [==============================] - 0s 24us/step - loss: 1.3771 - acc: 0.3107\n",
      "Epoch 51/100\n",
      "1963/1963 [==============================] - 0s 25us/step - loss: 1.3786 - acc: 0.3107\n",
      "Epoch 52/100\n",
      "1963/1963 [==============================] - 0s 24us/step - loss: 1.3774 - acc: 0.3107\n",
      "Epoch 53/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3782 - acc: 0.3107\n",
      "Epoch 54/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3781 - acc: 0.3107\n",
      "Epoch 55/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3779 - acc: 0.3107\n",
      "Epoch 56/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3771 - acc: 0.3107\n",
      "Epoch 57/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3764 - acc: 0.3107\n",
      "Epoch 58/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3775 - acc: 0.3107\n",
      "Epoch 59/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3776 - acc: 0.3107\n",
      "Epoch 60/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3775 - acc: 0.3107\n",
      "Epoch 61/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3773 - acc: 0.3107\n",
      "Epoch 62/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3775 - acc: 0.3107\n",
      "Epoch 63/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3770 - acc: 0.3107\n",
      "Epoch 64/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3762 - acc: 0.3107\n",
      "Epoch 65/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3781 - acc: 0.3107\n",
      "Epoch 66/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3770 - acc: 0.3107\n",
      "Epoch 67/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3761 - acc: 0.3107\n",
      "Epoch 68/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3766 - acc: 0.3107\n",
      "Epoch 69/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3768 - acc: 0.3107\n",
      "Epoch 70/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3776 - acc: 0.3107\n",
      "Epoch 71/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3772 - acc: 0.3107\n",
      "Epoch 72/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3771 - acc: 0.3107\n",
      "Epoch 73/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3774 - acc: 0.3107\n",
      "Epoch 74/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3768 - acc: 0.3107\n",
      "Epoch 75/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3775 - acc: 0.3107\n",
      "Epoch 76/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3769 - acc: 0.3107\n",
      "Epoch 77/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3785 - acc: 0.3107\n",
      "Epoch 78/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3772 - acc: 0.3107\n",
      "Epoch 79/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3769 - acc: 0.3107\n",
      "Epoch 80/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3768 - acc: 0.3107\n",
      "Epoch 81/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3773 - acc: 0.3107\n",
      "Epoch 82/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3775 - acc: 0.3107\n",
      "Epoch 83/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3764 - acc: 0.3107\n",
      "Epoch 84/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3770 - acc: 0.3107\n",
      "Epoch 85/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3768 - acc: 0.3107\n",
      "Epoch 86/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3773 - acc: 0.3107\n",
      "Epoch 87/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3773 - acc: 0.3107\n",
      "Epoch 88/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3775 - acc: 0.3107\n",
      "Epoch 89/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3756 - acc: 0.3107\n",
      "Epoch 90/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3758 - acc: 0.3107\n",
      "Epoch 91/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3762 - acc: 0.3107\n",
      "Epoch 92/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3783 - acc: 0.3107\n",
      "Epoch 93/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3780 - acc: 0.3107\n",
      "Epoch 94/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3783 - acc: 0.3107\n",
      "Epoch 95/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3780 - acc: 0.3107\n",
      "Epoch 96/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3774 - acc: 0.3107\n",
      "Epoch 97/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3778 - acc: 0.3107\n",
      "Epoch 98/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3769 - acc: 0.3107\n",
      "Epoch 99/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3772 - acc: 0.3107\n",
      "Epoch 100/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3773 - acc: 0.3107\n",
      "490/490 [==============================] - 0s 539us/step\n",
      "Epoch 1/100\n",
      "1963/1963 [==============================] - 1s 416us/step - loss: 1.3987 - acc: 0.2338\n",
      "Epoch 2/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3900 - acc: 0.2471\n",
      "Epoch 3/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3868 - acc: 0.2685\n",
      "Epoch 4/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3819 - acc: 0.2868\n",
      "Epoch 5/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3797 - acc: 0.2919\n",
      "Epoch 6/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3786 - acc: 0.3092\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3796 - acc: 0.3199\n",
      "Epoch 8/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3796 - acc: 0.3133\n",
      "Epoch 9/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3776 - acc: 0.3169\n",
      "Epoch 10/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3743 - acc: 0.3179\n",
      "Epoch 11/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3763 - acc: 0.3164\n",
      "Epoch 12/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3757 - acc: 0.3189\n",
      "Epoch 13/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3764 - acc: 0.3179\n",
      "Epoch 14/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3762 - acc: 0.3189\n",
      "Epoch 15/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3755 - acc: 0.3194\n",
      "Epoch 16/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3753 - acc: 0.3179\n",
      "Epoch 17/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3758 - acc: 0.3179\n",
      "Epoch 18/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3786 - acc: 0.3189\n",
      "Epoch 19/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3755 - acc: 0.3189\n",
      "Epoch 20/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3766 - acc: 0.3189\n",
      "Epoch 21/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3737 - acc: 0.3189\n",
      "Epoch 22/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3760 - acc: 0.3189\n",
      "Epoch 23/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3772 - acc: 0.3189\n",
      "Epoch 24/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3754 - acc: 0.3189\n",
      "Epoch 25/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3753 - acc: 0.3189\n",
      "Epoch 26/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3746 - acc: 0.3189\n",
      "Epoch 27/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3755 - acc: 0.3189\n",
      "Epoch 28/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3751 - acc: 0.3189\n",
      "Epoch 29/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3754 - acc: 0.3189\n",
      "Epoch 30/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3738 - acc: 0.3189\n",
      "Epoch 31/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3736 - acc: 0.3189\n",
      "Epoch 32/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3764 - acc: 0.3189\n",
      "Epoch 33/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3756 - acc: 0.3189\n",
      "Epoch 34/100\n",
      "1963/1963 [==============================] - 0s 15us/step - loss: 1.3773 - acc: 0.3189\n",
      "Epoch 35/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3744 - acc: 0.3189\n",
      "Epoch 36/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3755 - acc: 0.3189\n",
      "Epoch 37/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3763 - acc: 0.3189\n",
      "Epoch 38/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3754 - acc: 0.3189\n",
      "Epoch 39/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3735 - acc: 0.3189\n",
      "Epoch 40/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3739 - acc: 0.3189\n",
      "Epoch 41/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3756 - acc: 0.3189\n",
      "Epoch 42/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3747 - acc: 0.3189\n",
      "Epoch 43/100\n",
      "1963/1963 [==============================] - 0s 15us/step - loss: 1.3749 - acc: 0.3189\n",
      "Epoch 44/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3743 - acc: 0.3189\n",
      "Epoch 45/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3744 - acc: 0.3189\n",
      "Epoch 46/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3760 - acc: 0.3189\n",
      "Epoch 47/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3736 - acc: 0.3189\n",
      "Epoch 48/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3742 - acc: 0.3189\n",
      "Epoch 49/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3739 - acc: 0.3189\n",
      "Epoch 50/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3756 - acc: 0.3189\n",
      "Epoch 51/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3760 - acc: 0.3189\n",
      "Epoch 52/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3760 - acc: 0.3189\n",
      "Epoch 53/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3747 - acc: 0.3189\n",
      "Epoch 54/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3760 - acc: 0.3189\n",
      "Epoch 55/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3752 - acc: 0.3189\n",
      "Epoch 56/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3748 - acc: 0.3189\n",
      "Epoch 57/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3756 - acc: 0.3189\n",
      "Epoch 58/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3743 - acc: 0.3189\n",
      "Epoch 59/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3747 - acc: 0.3189\n",
      "Epoch 60/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3740 - acc: 0.3189\n",
      "Epoch 61/100\n",
      "1963/1963 [==============================] - 0s 15us/step - loss: 1.3734 - acc: 0.3189\n",
      "Epoch 62/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3748 - acc: 0.3189\n",
      "Epoch 63/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3762 - acc: 0.3189\n",
      "Epoch 64/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3734 - acc: 0.3189\n",
      "Epoch 65/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3755 - acc: 0.3189\n",
      "Epoch 66/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3748 - acc: 0.3189\n",
      "Epoch 67/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3741 - acc: 0.3189\n",
      "Epoch 68/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3748 - acc: 0.3189\n",
      "Epoch 69/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3739 - acc: 0.3189\n",
      "Epoch 70/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3750 - acc: 0.3189\n",
      "Epoch 71/100\n",
      "1963/1963 [==============================] - 0s 20us/step - loss: 1.3738 - acc: 0.3189\n",
      "Epoch 72/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3759 - acc: 0.3189\n",
      "Epoch 73/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3759 - acc: 0.3189\n",
      "Epoch 74/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3753 - acc: 0.3189\n",
      "Epoch 75/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3746 - acc: 0.3189\n",
      "Epoch 76/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3745 - acc: 0.3189\n",
      "Epoch 77/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3752 - acc: 0.3189\n",
      "Epoch 78/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3740 - acc: 0.3189\n",
      "Epoch 79/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3748 - acc: 0.3189\n",
      "Epoch 80/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3741 - acc: 0.3189\n",
      "Epoch 81/100\n",
      "1963/1963 [==============================] - 0s 15us/step - loss: 1.3746 - acc: 0.3189\n",
      "Epoch 82/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3757 - acc: 0.3189\n",
      "Epoch 83/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3743 - acc: 0.3189\n",
      "Epoch 84/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3751 - acc: 0.3189\n",
      "Epoch 85/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3746 - acc: 0.3189\n",
      "Epoch 86/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3741 - acc: 0.3189\n",
      "Epoch 87/100\n",
      "1963/1963 [==============================] - 0s 19us/step - loss: 1.3754 - acc: 0.3189\n",
      "Epoch 88/100\n",
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3744 - acc: 0.3189\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963/1963 [==============================] - 0s 17us/step - loss: 1.3737 - acc: 0.3189\n",
      "Epoch 90/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3741 - acc: 0.3189\n",
      "Epoch 91/100\n",
      "1963/1963 [==============================] - 0s 21us/step - loss: 1.3747 - acc: 0.3189\n",
      "Epoch 92/100\n",
      "1963/1963 [==============================] - 0s 14us/step - loss: 1.3751 - acc: 0.3189\n",
      "Epoch 93/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3753 - acc: 0.3189\n",
      "Epoch 94/100\n",
      "1963/1963 [==============================] - 0s 22us/step - loss: 1.3749 - acc: 0.3189\n",
      "Epoch 95/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3745 - acc: 0.3189\n",
      "Epoch 96/100\n",
      "1963/1963 [==============================] - 0s 15us/step - loss: 1.3737 - acc: 0.3189\n",
      "Epoch 97/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3744 - acc: 0.3189\n",
      "Epoch 98/100\n",
      "1963/1963 [==============================] - 0s 15us/step - loss: 1.3751 - acc: 0.3189\n",
      "Epoch 99/100\n",
      "1963/1963 [==============================] - 0s 16us/step - loss: 1.3742 - acc: 0.3189\n",
      "Epoch 100/100\n",
      "1963/1963 [==============================] - 0s 18us/step - loss: 1.3744 - acc: 0.3189\n",
      "490/490 [==============================] - 0s 590us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "kclf = KerasClassifier(build_fn = build_ann, batch_size = 256, epochs = 100, input_size=5)\n",
    "accuracies = cross_val_score(estimator = kclf, X = X_vect_nl_ann, y = y_nn, cv = 5, n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T10:48:31.071867Z",
     "start_time": "2018-04-17T10:48:31.067732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.312262358417\n",
      "Standard Deviation: 0.0220326678423\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy:', accuracies.mean())\n",
    "print('Standard Deviation:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have very bad performances also on the training set. There is no reason in attempting any further tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": "1",
   "nav_menu": {
    "height": "260px",
    "width": "346px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "500px",
    "left": "45px",
    "top": "111px",
    "width": "295px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
