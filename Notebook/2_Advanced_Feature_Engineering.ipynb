{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#LyricsManager.py\" data-toc-modified-id=\"LyricsManager.py-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LyricsManager.py</a></span></li><li><span><a href=\"#MoodyLyrics-issue\" data-toc-modified-id=\"MoodyLyrics-issue-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>MoodyLyrics issue</a></span></li><li><span><a href=\"#MoodyLyrics-and-MoodyLyrics4Q-comparison\" data-toc-modified-id=\"MoodyLyrics-and-MoodyLyrics4Q-comparison-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>MoodyLyrics and MoodyLyrics4Q comparison</a></span></li><li><span><a href=\"#New-features\" data-toc-modified-id=\"New-features-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>New features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Analysis\" data-toc-modified-id=\"Feature-Analysis-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Feature Analysis</a></span></li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Feature Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#PCA-on-MoodyLyrics4Q\" data-toc-modified-id=\"PCA-on-MoodyLyrics4Q-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>PCA on MoodyLyrics4Q</a></span></li></ul></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Artificial-Neural-Network\" data-toc-modified-id=\"Artificial-Neural-Network-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Artificial Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Cross Validation</a></span></li><li><span><a href=\"#Analysis-on-Extra-Test\" data-toc-modified-id=\"Analysis-on-Extra-Test-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;</span>Analysis on Extra Test</a></span></li></ul></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis-on-Extra-Test\" data-toc-modified-id=\"Analysis-on-Extra-Test-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Analysis on Extra Test</a></span></li></ul></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>SVM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis-on-Extra-Test\" data-toc-modified-id=\"Analysis-on-Extra-Test-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>Analysis on Extra Test</a></span></li></ul></li><li><span><a href=\"#eXtreme-Gradient-Boost\" data-toc-modified-id=\"eXtreme-Gradient-Boost-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>eXtreme Gradient Boost</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis-on-Extra-Test\" data-toc-modified-id=\"Analysis-on-Extra-Test-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>Analysis on Extra Test</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:28:23.312499Z",
     "start_time": "2018-06-22T14:28:21.866664Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_color_codes()\n",
    "\n",
    "#The below code is used to install external packages\n",
    "import pip\n",
    "def install(package):\n",
    "    pip.main(['install', package])\n",
    "\n",
    "\n",
    "emotion_labels = ['happy', 'sad', 'angry', 'relaxed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:28:23.338874Z",
     "start_time": "2018-06-22T14:28:23.316219Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read extra test set\n",
    "extra_test_path = '../src/datasets/extra_test.csv'\n",
    "extra_test = pd.read_csv(extra_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:28:23.358072Z",
     "start_time": "2018-06-22T14:28:23.341523Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function computes and plots the confusion matrix.\n",
    "\"\"\"\n",
    "def plot_confusion_matrix(y_true, y_pred, title, labels, filename):\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cnf_matrix = pd.DataFrame(data=cnf_matrix)\n",
    "    plt.figure(figsize = (8,6))\n",
    "    #plt.title(title,fontsize=20)\n",
    "    ax = sns.heatmap(df_cnf_matrix, annot=True, cmap='Blues',fmt='g',square=True,linewidths=.7, cbar_kws={\"shrink\": .5}, annot_kws={\"size\": 20})\n",
    "    ax.set_xticklabels(labels,fontsize=20,  rotation=90)\n",
    "    b = ax.set_yticklabels(labels,fontsize=20, rotation=360)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Previously on Sara&Mario project**: \n",
    "<ol>\n",
    "    <li>*MoodyLyrics* stats analysis</li>\n",
    "    <li>Lyrics classification using the main classifiers</li>\n",
    "    <li>Emotion classification by just considering the song title</li> \n",
    "</ol>\n",
    "Now, following the last meeting discussion we: \n",
    "<ol>\n",
    "    <li>Wrote a script to create the dataset *SpotifyURI*, *List of PlaylistIDs*, *MoodyLyric_Emotion*</li>\n",
    "    <li>Wrote a script to detect all duplicated songs in MoodyLyrics to fill a bug report</li>\n",
    "    <li>Added new features for the classification task</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LyricsManager.py\n",
    "After downloading the Spotify RecSys Challenge dataset we wrote a script that: <br>\n",
    "<ul>\n",
    "    <li>Given as input: the *Spotify playlist dataset folder*, and an output folder, creates a data structure to store for each song:<br>\n",
    "        <*SpotifyURI*, *PlaylistIDs*, *TrackInformation*, *Emotion*><br>\n",
    "        where: \n",
    "            <ul>\n",
    "                <li>*SpotifyURI* is the songID</li>\n",
    "                <li>*PlaylistsIDs* is the list of playlist in which the song appear</li>\n",
    "                <li>*TrackInformation* is the list of information taken from Spotify dataset</li>\n",
    "                <li>*Emotion* is an optional field, present only if the song is also contained in the MoodyLyrics dataset, that contains the emotion label for the song\n",
    "    </li>\n",
    "            </ul>\n",
    "      <li>Can load Spotify songs datastructure (if already existing)</li>\n",
    "            <li>Given as input a SpotifyURI it can download lyrics from lyricwikia</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoodyLyrics issue\n",
    "Last week we detected duplicate information in MoodyLyrics. Now we can generate a .csv file capable of reporting those duplicates.\n",
    "\n",
    "The bug report can be found at `./src/datasets/moodylyrics_bug_report.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoodyLyrics and MoodyLyrics4Q comparison\n",
    "Here we are going to read MoodyLyrics and MoodyLyrics4Q and to compare the two class label distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:21.890227Z",
     "start_time": "2018-06-22T14:29:21.872558Z"
    }
   },
   "outputs": [],
   "source": [
    "moodyLyrics4qDF = pd.read_csv('../src/datasets/MoodyLyrics4Q.csv')\n",
    "moodyLyricsDF   = pd.read_csv('../src/datasets/MoodyLyrics_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:23.277667Z",
     "start_time": "2018-06-22T14:29:22.729165Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Advanced_feature_engineering_pictures/Stats.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-df0466eb00f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Emotion distribution in MoodyLyrics4Q'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Advanced_feature_engineering_pictures/Stats.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, **kwargs)\u001b[0m\n\u001b[1;32m   2033\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2035\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2259\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m                 **kwargs)\n\u001b[0m\u001b[1;32m   2262\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[1;32m    526\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Advanced_feature_engineering_pictures/Stats.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(12,4))\n",
    "emotionDistribution = moodyLyricsDF.Emotion.value_counts()\n",
    "ax = sns.barplot(x=np.array(range(4)),y=emotionDistribution, ax = axarr[0])\n",
    "ax.set_title('Emotion distribution in MoodyLyrics',fontsize=20)\n",
    "ax.set_xticklabels(emotion_labels,fontsize=13, fontweight='bold')\n",
    "\n",
    "emotionDistribution = moodyLyrics4qDF.Mood.value_counts()\n",
    "ax = sns.barplot(x=np.array(range(4)),y=emotionDistribution, ax = axarr[1])\n",
    "ax.set_title('Emotion distribution in MoodyLyrics4Q',fontsize=20)\n",
    "ax.set_xticklabels(emotion_labels,fontsize=13, fontweight='bold')\n",
    "plt.savefig('./Advanced_feature_engineering_pictures/Stats.png')\n",
    "plt.show()\n",
    "\n",
    "## Compare size\n",
    "print('MoodyLyrics4Q size:', len(moodyLyrics4qDF))\n",
    "print('MoodyLyrics size:',len(moodyLyricsDF))\n",
    "\n",
    "## Check if MoodyLyrics4q has duplicates as well\n",
    "duplicatedCheck = moodyLyrics4qDF.groupby(['Artist','Title']).size().reset_index(name='count')\n",
    "duplicatedRows = duplicatedCheck [(duplicatedCheck ['count']>1)]\n",
    "print('MoodyLyrics4Q has', duplicatedRows.shape[0], 'duplicates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New features\n",
    "Starting from MoodyLyrics and MoodyLyrics4Q we are going to create a new dataset with a lot of new features. <br>\n",
    "<ul>\n",
    "    <li>**Title_vector**</li>\n",
    "    <li>**Lyric_vector**</li>\n",
    "    <li>**%Rhymes**:<br> defined as the percentage of the number of rhymes over the number of total lines. A rhyme is defined as a rhyme between two following lines.</li>\n",
    "    <li>**%Past_tense_verbs**:<br> defined as the the percentage of the number of past tense verbs over the total number of verbs.</li>\n",
    "    <li>**%Present_tense_verbs**:<br>  defined as the the percentage of the number of present tense verbs over the total number of verbs.</li>\n",
    "    <li>**%Future_tense_verbs**:<br>  defined as the the percentage of the number of future tense verbs over the total number of verbs, where future is just will + base form.</li>\n",
    "    <li>**%ADJ**:<br> Percentage of adjectives over the total number of words.</li>\n",
    "    <li>**%ADP**:<br> Percentage of adpositions (e.g. in, to, during) over the total number of words.</li>\n",
    "    <li>**%ADV**:<br> Percentage of adverbs (e.g. very, tomorrow, down, where, there) over the total number of words.</li>\n",
    "    <li>**%AUX**:<br> Percentage of auxiliaries (e.g. is, has (done), will (do), should (do)) over the total number of words.</li>\n",
    "    <li>**%INTJ**:<br> Percentage of interjections (e.g. psst, ouch, bravo, hello) over the total number of words.</li>\n",
    "    <li>**%NOUN**:<br> Percentage of nouns over the total number of words.</li>\n",
    "    <li>**%NUM**:<br> Percentage of numerals over the total number of words.</li>\n",
    "    <li>**%PRON**:<br> Percentage of pronouns (e.g. I, you, he, she, myself, themselves, somebody,...) over the total number of words.</li> \n",
    "    <li>**%PROPN**:<br> Percentage of proper nouns (e.g. Mary, John) over the total number of words.</li>\n",
    "    <li>**%PUNCT**:<br> Percentage of puntuctuation (e.g. ., (, ), ?) over the total number of words.</li>\n",
    "    <li>**%VERB**:<br> Percentage of verbs over the total number of words.</li>\n",
    "    <li>**Selfish_degree**:<br> Percentage of 'I' pronouns over the total number of pronouns</li>\n",
    "    <li>**%Echoism**:<br> Percentage of echoism over the total number of words, where an echoism is either a sequence of two subsequent repeated words or the repetition of a vowel in a word. </li>\n",
    "    <li>**%Duplicates**:<br> Percentage of duplicate words over the total number of words</li>\n",
    "    <li>**isTitleInLyric**:<br> Boolean, true if the title string is also a substring of the lyric</li>\n",
    "    <li>**sentiment**:<br> Sentiment between -1 and 1</li>\n",
    "    <li>**subjectivity degree**:<br> Degree of subjectivity of the text</li>\n",
    "</ul>\n",
    "\n",
    "The sentiment and subjectivity analysis was done using [TextBlob](http://textblob.readthedocs.io/en/dev/index.html) Python NLP library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already featurized the songs in MoodyLyrics and in MoodyLyrics4Q using the script that can be found at `./src/song_featurized.py`. We saved the resulting datasets in `./src/datasets/moodylyrics4q_featurized.csv` and in `./src/dataset/moodylyrics_featurized.csv`. <br>\n",
    "Here we will build a model using both datasets. This behavior can be changed setting `use_both  = False` (in this case we'll use MoodyLyrics4Q alone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:41.174819Z",
     "start_time": "2018-06-22T14:29:40.947125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (1935, 38)\n",
      "**************************************************\n",
      "Final dataset header:  Index(['Unnamed: 0', 'ID', 'ARTIST', 'SONG_TITLE', 'LYRICS_VECTOR',\n",
      "       'TITLE_VECTOR', 'LINE_COUNT', 'WORD_COUNT', 'ECHOISMS',\n",
      "       'SELFISH_DEGREE', 'DUPLICATE_LINES', 'IS_TITLE_IN_LYRICS', 'RHYMES',\n",
      "       'VERB_PRESENT', 'VERB_PAST', 'VERB_FUTURE', 'ADJ_FREQUENCIES',\n",
      "       'CONJUCTION_FREQUENCIES', 'ADV_FREQUENCIES', 'AUX_FREQUENCIES',\n",
      "       'CONJ_FREQUENCIES', 'CCONJ_FREQUENCIES', 'DETERMINER_FREQUENCIES',\n",
      "       'INTERJECTION_FREQUENCIES', 'NOUN_FREQUENCIES', 'NUM_FREQUENCIES',\n",
      "       'PART_FREQUENCIES', 'PRON_FREQUENCIES', 'PROPN_FREQUENCIES',\n",
      "       'PUNCT_FREQUENCIES', 'SCONJ_FREQUENCIES', 'SYM_FREQUENCIES',\n",
      "       'VERB_FREQUENCIES', 'X_FREQUENCIES', 'SPACE_FREQUENCIES', 'SENTIMENT',\n",
      "       'SUBJECTIVITY', 'EMOTION'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "use_both = False\n",
    "moodyLyrics4q_F = pd.read_csv('../src/datasets/MoodyLyrics4Q_featurized.csv')\n",
    "if use_both:\n",
    "    print('Merging moodyLyrics and moodyLyrcs4Q...')\n",
    "    moodyLyrics_F = pd.read_csv('../src/datasets/MoodyLyrics_featurized.csv')\n",
    "    total_len = len(moodyLyrics4q_F ) + len(moodyLyrics_F )\n",
    "    dataset = pd.concat([moodyLyrics4q_F , moodyLyrics_F])\n",
    "    dataset.drop_duplicates(['ARTIST', 'SONG_TITLE'], inplace=True)\n",
    "    print('Deleted duplicated songs: ',total_len - len(dataset))\n",
    "else: \n",
    "    dataset = moodyLyrics4q_F\n",
    "print('Final dataset shape:',dataset.shape)\n",
    "print('*'*50)\n",
    "print('Final dataset header: ', dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we want to analyse the distribution of each variable.<br>\n",
    "There are some features that we find useless for this analysis, i.e. `ID`, `ARTIST`, `SONG_TITLE`, `EMOTION`, while there are some other features we don't want to use in our classification task, i.e. `X_FREQUENCIES` and `SPACE_FREQUENCIES`. <br>\n",
    "So the first step before this analysis is to drop all these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:37:25.407996Z",
     "start_time": "2018-06-05T20:37:19.547556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x2880 with 29 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "useless_columns = [ 'Unnamed: 0','ID','ARTIST', 'SONG_TITLE', 'EMOTION', 'X_FREQUENCIES', 'SPACE_FREQUENCIES']\n",
    "tmp_df = dataset.drop(useless_columns + ['LYRICS_VECTOR', 'TITLE_VECTOR'],axis=1)\n",
    "\n",
    "f, axarr = plt.subplots(8, 4, figsize=(25,40))\n",
    "k = 0\n",
    "for feature in tmp_df.columns:\n",
    "    (i, j) = divmod(k, 4)\n",
    "    axarr[i,j] = sns.boxplot(y=tmp_df[feature], ax=axarr[i,j])\n",
    "    axarr[i,j].set_title('Boxplot for {}'.format(feature), fontsize=16, weight='bold')\n",
    "    k += 1\n",
    "plt.tight_layout()\n",
    "f.delaxes(axarr[-1,-1])\n",
    "f.delaxes(axarr[-1,-2])\n",
    "f.delaxes(axarr[-1,-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we want to select the most important feature for our classification.<br>\n",
    "The final selection is the result of a lot trials that we don't want to show for the sake of brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:30.138305Z",
     "start_time": "2018-06-22T14:29:30.133389Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "   'LYRICS_VECTOR',\n",
    "   #'WORD_COUNT', \n",
    "   'ECHOISMS',\n",
    "   #'SELFISH_DEGREE', \n",
    "   'DUPLICATE_LINES', 'IS_TITLE_IN_LYRICS', \n",
    "   'VERB_PRESENT', \n",
    "   'VERB_PAST', 'VERB_FUTURE', 'ADJ_FREQUENCIES',\n",
    "   'PUNCT_FREQUENCIES',\n",
    "   'SENTIMENT', 'SUBJECTIVITY',\n",
    "   'EMOTION'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:45.628553Z",
     "start_time": "2018-06-22T14:29:45.597786Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get X and y of the correct shape. The function mainly transforms the lyrics array \n",
    "feature into 300 separate features. \n",
    " X = (..., 310)\n",
    " y = (...,)\n",
    "'''\n",
    "def get_X_y(dataset, emotion=True):\n",
    "    if emotion is True:\n",
    "        tmp_df = dataset.drop(['EMOTION'], axis=1)\n",
    "    else:\n",
    "        tmp_df = dataset\n",
    "    X = list()\n",
    "    for (i, row) in tmp_df.iterrows():\n",
    "        sub_list = list()\n",
    "        for field in row:\n",
    "            if type(field) == str:\n",
    "                field = field[1:-1].split()\n",
    "                sub_list += [float(x.replace('\\n','')) for x in field]\n",
    "            else:\n",
    "                sub_list.append(field)\n",
    "        X.append(np.array(sub_list))\n",
    "    X = np.array(X)\n",
    "    if emotion is True:\n",
    "        y = dataset['EMOTION'].as_matrix().T\n",
    "        return X, y\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:46.729304Z",
     "start_time": "2018-06-22T14:29:46.164835Z"
    }
   },
   "outputs": [],
   "source": [
    "#Reduced dataset and extra_test\n",
    "r_dataset = dataset[selected_columns]\n",
    "r_extra_test = extra_test[selected_columns]\n",
    "\n",
    "X, y = get_X_y(r_dataset)\n",
    "X_extra_test, y_extra_test = get_X_y(r_extra_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:47.236540Z",
     "start_time": "2018-06-22T14:29:47.230227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1935, 310)\n",
      "y shape: (1935,)\n",
      "X_extra_test shape: (20, 310)\n",
      "y_extra_test shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "print('X shape:', X.shape)\n",
    "print('y shape:',y.shape)\n",
    "print('X_extra_test shape:', X_extra_test.shape)\n",
    "print('y_extra_test shape:',y_extra_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on MoodyLyrics4Q\n",
    "\n",
    "Here I will try to project MoodyLyrics4Q lyrics vectors on a 2-dimensional space to see if the arousal-valance classification actually makes the lyrics content very separated in 4 distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:48.593214Z",
     "start_time": "2018-06-22T14:29:48.587325Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(x, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:29:50.860525Z",
     "start_time": "2018-06-22T14:29:49.838101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not use_both:\n",
    "    #Reduced dataset and extra_test\n",
    "    r1_dataset = dataset[['LYRICS_VECTOR', 'EMOTION']]\n",
    "\n",
    "    X1, y1 = get_X_y(r1_dataset)\n",
    "\n",
    "    components = pca(X1, 2)\n",
    "\n",
    "    # Put reduced components and labels together for plotting\n",
    "    encoder = LabelEncoder()\n",
    "    pca_y = encoder.fit_transform(y)\n",
    "\n",
    "    comps = list(zip(components, pca_y))\n",
    "    pca_df = pd.DataFrame(comps, columns=['Vector', 'Emotion'])\n",
    "    \n",
    "    # Plot points for each class\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 7)\n",
    "    for i in range(4):\n",
    "        emo_df = pca_df[pca_df['Emotion'] == i]\n",
    "        x = emo_df['Vector'].as_matrix()\n",
    "        x = np.array([np.array(k) for k in x])\n",
    "        plt.scatter(x[:,0], x[:,1])\n",
    "    plt.legend(emotion_labels)\n",
    "    plt.title('PCA with 2 components (MoodyLyrics4Q)', weight='bold', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use an ANN for our classification task. <br>\n",
    "In the first cell we define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:56:50.938379Z",
     "start_time": "2018-06-22T14:56:50.879274Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "    \n",
    "from sklearn.utils import class_weight\n",
    "    \n",
    "def build_ann(optimizer='adam', input_size=310):\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    # Add input layer\n",
    "    classifier = Sequential()\n",
    "    # Add input layer\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'random_normal', activation = 'sigmoid', input_dim = input_size))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    # Add hidden layers\n",
    "    #classifier.add(Dense(units = 30, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    #classifier.add(Dropout(0.5))\n",
    "                \n",
    "    #classifier.add(Dense(units = 30, kernel_initializer = 'random_normal', activation = 'relu'))\n",
    "    #classifier.add(Dropout(0.1))\n",
    "    \n",
    "        \n",
    "    # Add output layer\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'glorot_normal', activation = 'softmax'))\n",
    "    \n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will encode y with numerical label and we'll scale X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:56:51.597741Z",
     "start_time": "2018-06-22T14:56:51.558888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shapes...\n",
      "X shape (1935, 310)\n",
      "y shape (1935, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "'''\n",
    "We encode y (i.e. angry, happy, sad, relaxed) into numerical label (0,1,2,3) and \n",
    "then into dummy variables... e.g. 0 0 1 0 instead of 2. \n",
    "and we scale X with a standard scaler\n",
    "'''\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_nn = np_utils.to_categorical(encoder.fit_transform(y))\n",
    "sc = StandardScaler()\n",
    "X_nn = sc.fit_transform(X)\n",
    "print('New shapes...')\n",
    "print('X shape', X_nn.shape)\n",
    "print('y shape', y_nn.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nn, y_nn, test_size = 0.1, random_state = 13)\n",
    "emotion_labels = encoder.inverse_transform([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:33:55.334546Z",
     "start_time": "2018-06-22T14:33:50.043767Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07c0b6b3c8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = build_ann('adam', X_train.shape[1])\n",
    "classifier.fit(X_train, y_train, batch_size = 128, epochs = 100, \n",
    "               validation_split=0.2, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:33:55.991947Z",
     "start_time": "2018-06-22T14:33:55.337003Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.54\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_true = np.argmax(y_test,axis=1)\n",
    "filename = '../Report/chapters/chapter5/images/CM_ANN.png'\n",
    "plot_confusion_matrix(y_true, y_pred,'ANN - confusion matrix', emotion_labels, filename)\n",
    "\n",
    "print('Accuracy score', round(accuracy_score(y_pred,y_true),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T15:04:03.571134Z",
     "start_time": "2018-06-22T15:00:31.800589Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 4s 2ms/step - loss: 1.4990 - acc: 0.2803 - val_loss: 1.2364 - val_acc: 0.4400\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.3240 - acc: 0.3799 - val_loss: 1.1513 - val_acc: 0.5714\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.2809 - acc: 0.3940 - val_loss: 1.1002 - val_acc: 0.6114\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.2212 - acc: 0.4361 - val_loss: 1.0681 - val_acc: 0.6057\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.1920 - acc: 0.4655 - val_loss: 1.0347 - val_acc: 0.6400\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.1702 - acc: 0.4872 - val_loss: 1.0117 - val_acc: 0.6686\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1471 - acc: 0.4981 - val_loss: 0.9939 - val_acc: 0.6686\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 1.1132 - acc: 0.5185 - val_loss: 0.9801 - val_acc: 0.6514\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 1.0849 - acc: 0.5236 - val_loss: 0.9674 - val_acc: 0.6743\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.1040 - acc: 0.5102 - val_loss: 0.9607 - val_acc: 0.6743\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.0663 - acc: 0.5364 - val_loss: 0.9542 - val_acc: 0.6743\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0477 - acc: 0.5504 - val_loss: 0.9431 - val_acc: 0.6743\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0303 - acc: 0.5575 - val_loss: 0.9330 - val_acc: 0.6914\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.0344 - acc: 0.5587 - val_loss: 0.9250 - val_acc: 0.6914\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0103 - acc: 0.5773 - val_loss: 0.9253 - val_acc: 0.6743\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0022 - acc: 0.5543 - val_loss: 0.9185 - val_acc: 0.6743\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0211 - acc: 0.5715 - val_loss: 0.9134 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9997 - acc: 0.5715 - val_loss: 0.9091 - val_acc: 0.6971\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9907 - acc: 0.5670 - val_loss: 0.9067 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9727 - acc: 0.5932 - val_loss: 0.8992 - val_acc: 0.6971\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9744 - acc: 0.5875 - val_loss: 0.8971 - val_acc: 0.6686\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9521 - acc: 0.6117 - val_loss: 0.8993 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9677 - acc: 0.5792 - val_loss: 0.8975 - val_acc: 0.6457\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9567 - acc: 0.6003 - val_loss: 0.8899 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9428 - acc: 0.6117 - val_loss: 0.8900 - val_acc: 0.6743\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9162 - acc: 0.6264 - val_loss: 0.8878 - val_acc: 0.6743\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9306 - acc: 0.5983 - val_loss: 0.8875 - val_acc: 0.6857\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9357 - acc: 0.6143 - val_loss: 0.8846 - val_acc: 0.6857\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9360 - acc: 0.5964 - val_loss: 0.8874 - val_acc: 0.6800\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9285 - acc: 0.6073 - val_loss: 0.8902 - val_acc: 0.6743\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.9191 - acc: 0.6137 - val_loss: 0.8841 - val_acc: 0.6686\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9173 - acc: 0.6073 - val_loss: 0.8849 - val_acc: 0.6800\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9066 - acc: 0.6054 - val_loss: 0.8859 - val_acc: 0.6800\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8961 - acc: 0.6264 - val_loss: 0.8853 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9115 - acc: 0.6213 - val_loss: 0.8911 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8932 - acc: 0.6360 - val_loss: 0.8882 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9130 - acc: 0.6303 - val_loss: 0.8843 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8991 - acc: 0.6175 - val_loss: 0.8850 - val_acc: 0.6743\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9009 - acc: 0.6290 - val_loss: 0.8868 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8980 - acc: 0.6341 - val_loss: 0.8837 - val_acc: 0.6743\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9055 - acc: 0.6079 - val_loss: 0.8857 - val_acc: 0.6743\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8898 - acc: 0.6117 - val_loss: 0.8800 - val_acc: 0.6914\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8886 - acc: 0.6290 - val_loss: 0.8787 - val_acc: 0.6857\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8759 - acc: 0.6398 - val_loss: 0.8844 - val_acc: 0.6800\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8753 - acc: 0.6264 - val_loss: 0.8794 - val_acc: 0.6800\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8752 - acc: 0.6430 - val_loss: 0.8783 - val_acc: 0.6857\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8718 - acc: 0.6367 - val_loss: 0.8815 - val_acc: 0.6857\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8608 - acc: 0.6475 - val_loss: 0.8827 - val_acc: 0.6743\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8623 - acc: 0.6507 - val_loss: 0.8826 - val_acc: 0.6800\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8800 - acc: 0.6469 - val_loss: 0.8793 - val_acc: 0.6743\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8564 - acc: 0.6507 - val_loss: 0.8849 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 37us/step - loss: 0.8561 - acc: 0.6437 - val_loss: 0.8872 - val_acc: 0.6800\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8615 - acc: 0.6430 - val_loss: 0.8888 - val_acc: 0.6743\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8360 - acc: 0.6411 - val_loss: 0.8929 - val_acc: 0.6629\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8469 - acc: 0.6513 - val_loss: 0.8918 - val_acc: 0.6686\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8441 - acc: 0.6481 - val_loss: 0.8914 - val_acc: 0.6686\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8371 - acc: 0.6513 - val_loss: 0.8912 - val_acc: 0.6800\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8426 - acc: 0.6462 - val_loss: 0.8916 - val_acc: 0.6800\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8368 - acc: 0.6488 - val_loss: 0.8921 - val_acc: 0.6629\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8335 - acc: 0.6552 - val_loss: 0.8964 - val_acc: 0.6686\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8413 - acc: 0.6469 - val_loss: 0.8955 - val_acc: 0.6629\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8323 - acc: 0.6584 - val_loss: 0.8917 - val_acc: 0.6743\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8178 - acc: 0.6667 - val_loss: 0.8887 - val_acc: 0.6629\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8412 - acc: 0.6654 - val_loss: 0.8904 - val_acc: 0.6629\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8077 - acc: 0.6558 - val_loss: 0.8899 - val_acc: 0.6629\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8180 - acc: 0.6705 - val_loss: 0.8884 - val_acc: 0.6457\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8002 - acc: 0.6686 - val_loss: 0.8895 - val_acc: 0.6514\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8161 - acc: 0.6750 - val_loss: 0.8896 - val_acc: 0.6629\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8101 - acc: 0.6737 - val_loss: 0.8895 - val_acc: 0.6571\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.8073 - acc: 0.6756 - val_loss: 0.8866 - val_acc: 0.6686\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8073 - acc: 0.6628 - val_loss: 0.8898 - val_acc: 0.6514\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7902 - acc: 0.6833 - val_loss: 0.8971 - val_acc: 0.6743\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8107 - acc: 0.6648 - val_loss: 0.8947 - val_acc: 0.6571\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7923 - acc: 0.6794 - val_loss: 0.8961 - val_acc: 0.6514\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8027 - acc: 0.6833 - val_loss: 0.9002 - val_acc: 0.6514\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7975 - acc: 0.6769 - val_loss: 0.9023 - val_acc: 0.6514\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8057 - acc: 0.6756 - val_loss: 0.8967 - val_acc: 0.6514\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8005 - acc: 0.6673 - val_loss: 0.8991 - val_acc: 0.6571\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7833 - acc: 0.6916 - val_loss: 0.8972 - val_acc: 0.6686\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7994 - acc: 0.6731 - val_loss: 0.9052 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7865 - acc: 0.6890 - val_loss: 0.9060 - val_acc: 0.6571\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7957 - acc: 0.6750 - val_loss: 0.9029 - val_acc: 0.6571\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7943 - acc: 0.6743 - val_loss: 0.9072 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7929 - acc: 0.6737 - val_loss: 0.8981 - val_acc: 0.6400\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7879 - acc: 0.6814 - val_loss: 0.8956 - val_acc: 0.6571\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7881 - acc: 0.6826 - val_loss: 0.8981 - val_acc: 0.6629\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7827 - acc: 0.6788 - val_loss: 0.9065 - val_acc: 0.6571\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7993 - acc: 0.6737 - val_loss: 0.9036 - val_acc: 0.6571\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7761 - acc: 0.6928 - val_loss: 0.9043 - val_acc: 0.6457\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7639 - acc: 0.6980 - val_loss: 0.9071 - val_acc: 0.6514\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7764 - acc: 0.6935 - val_loss: 0.9076 - val_acc: 0.6457\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7826 - acc: 0.6788 - val_loss: 0.9142 - val_acc: 0.6457\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7645 - acc: 0.6928 - val_loss: 0.9153 - val_acc: 0.6457\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7608 - acc: 0.6973 - val_loss: 0.9104 - val_acc: 0.6571\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7797 - acc: 0.6897 - val_loss: 0.9150 - val_acc: 0.6571\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7421 - acc: 0.7005 - val_loss: 0.9205 - val_acc: 0.6571\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7726 - acc: 0.6871 - val_loss: 0.9127 - val_acc: 0.6571\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7542 - acc: 0.6980 - val_loss: 0.9151 - val_acc: 0.6457\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7711 - acc: 0.6852 - val_loss: 0.9131 - val_acc: 0.6514\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7534 - acc: 0.6973 - val_loss: 0.9218 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7683 - acc: 0.6814 - val_loss: 0.9184 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7639 - acc: 0.7024 - val_loss: 0.9183 - val_acc: 0.6571\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7440 - acc: 0.7095 - val_loss: 0.9206 - val_acc: 0.6343\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7619 - acc: 0.7139 - val_loss: 0.9150 - val_acc: 0.6457\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7374 - acc: 0.7165 - val_loss: 0.9175 - val_acc: 0.6400\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7314 - acc: 0.7011 - val_loss: 0.9210 - val_acc: 0.6343\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7446 - acc: 0.6922 - val_loss: 0.9182 - val_acc: 0.6457\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7541 - acc: 0.7031 - val_loss: 0.9208 - val_acc: 0.6343\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7447 - acc: 0.6941 - val_loss: 0.9184 - val_acc: 0.6514\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7592 - acc: 0.6935 - val_loss: 0.9192 - val_acc: 0.6514\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7398 - acc: 0.7095 - val_loss: 0.9205 - val_acc: 0.6514\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7394 - acc: 0.7139 - val_loss: 0.9205 - val_acc: 0.6514\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7421 - acc: 0.6935 - val_loss: 0.9232 - val_acc: 0.6343\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7251 - acc: 0.7178 - val_loss: 0.9231 - val_acc: 0.6514\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7341 - acc: 0.7031 - val_loss: 0.9242 - val_acc: 0.6629\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7178 - acc: 0.7158 - val_loss: 0.9230 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7204 - acc: 0.7095 - val_loss: 0.9274 - val_acc: 0.6286\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7073 - acc: 0.7280 - val_loss: 0.9258 - val_acc: 0.6457\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7236 - acc: 0.7146 - val_loss: 0.9228 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7208 - acc: 0.7126 - val_loss: 0.9247 - val_acc: 0.6400\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7344 - acc: 0.6999 - val_loss: 0.9287 - val_acc: 0.6571\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7451 - acc: 0.6871 - val_loss: 0.9377 - val_acc: 0.6400\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7403 - acc: 0.7082 - val_loss: 0.9407 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7134 - acc: 0.7235 - val_loss: 0.9300 - val_acc: 0.6343\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7347 - acc: 0.7063 - val_loss: 0.9194 - val_acc: 0.6400\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7178 - acc: 0.7037 - val_loss: 0.9243 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7076 - acc: 0.7152 - val_loss: 0.9249 - val_acc: 0.6457\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7163 - acc: 0.7139 - val_loss: 0.9299 - val_acc: 0.6457\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7169 - acc: 0.7101 - val_loss: 0.9316 - val_acc: 0.6457\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7180 - acc: 0.7165 - val_loss: 0.9352 - val_acc: 0.6400\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7164 - acc: 0.7063 - val_loss: 0.9345 - val_acc: 0.6457\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7252 - acc: 0.7031 - val_loss: 0.9364 - val_acc: 0.6400\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7188 - acc: 0.7235 - val_loss: 0.9402 - val_acc: 0.6400\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6905 - acc: 0.7107 - val_loss: 0.9401 - val_acc: 0.6343\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7047 - acc: 0.7292 - val_loss: 0.9393 - val_acc: 0.6571\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7151 - acc: 0.7069 - val_loss: 0.9415 - val_acc: 0.6514\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7006 - acc: 0.7075 - val_loss: 0.9485 - val_acc: 0.6343\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6943 - acc: 0.7235 - val_loss: 0.9469 - val_acc: 0.6571\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7208 - acc: 0.7273 - val_loss: 0.9462 - val_acc: 0.6514\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.7171 - val_loss: 0.9464 - val_acc: 0.6457\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7037 - acc: 0.7312 - val_loss: 0.9472 - val_acc: 0.6400\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.7273 - val_loss: 0.9448 - val_acc: 0.6514\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6836 - acc: 0.7216 - val_loss: 0.9524 - val_acc: 0.6400\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6999 - acc: 0.7165 - val_loss: 0.9552 - val_acc: 0.6400\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 37us/step - loss: 0.6851 - acc: 0.7318 - val_loss: 0.9533 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6664 - acc: 0.7375 - val_loss: 0.9513 - val_acc: 0.6343\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6776 - acc: 0.7305 - val_loss: 0.9501 - val_acc: 0.6400\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6919 - acc: 0.7235 - val_loss: 0.9541 - val_acc: 0.6514\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7014 - acc: 0.7248 - val_loss: 0.9532 - val_acc: 0.6514\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7047 - acc: 0.7209 - val_loss: 0.9528 - val_acc: 0.6400\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6884 - acc: 0.7241 - val_loss: 0.9581 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6779 - acc: 0.7350 - val_loss: 0.9576 - val_acc: 0.6286\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6841 - acc: 0.7324 - val_loss: 0.9584 - val_acc: 0.6286\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6817 - acc: 0.7356 - val_loss: 0.9546 - val_acc: 0.6514\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6719 - acc: 0.7382 - val_loss: 0.9574 - val_acc: 0.6457\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.6773 - acc: 0.7197 - val_loss: 0.9595 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6806 - acc: 0.7407 - val_loss: 0.9603 - val_acc: 0.6457\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6461 - acc: 0.7529 - val_loss: 0.9604 - val_acc: 0.6343\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6734 - acc: 0.7401 - val_loss: 0.9645 - val_acc: 0.6457\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6755 - acc: 0.7312 - val_loss: 0.9726 - val_acc: 0.6229\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6747 - acc: 0.7344 - val_loss: 0.9688 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6626 - acc: 0.7363 - val_loss: 0.9688 - val_acc: 0.6343\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6661 - acc: 0.7446 - val_loss: 0.9673 - val_acc: 0.6343\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6685 - acc: 0.7216 - val_loss: 0.9612 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6721 - acc: 0.7433 - val_loss: 0.9609 - val_acc: 0.6400\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6684 - acc: 0.7350 - val_loss: 0.9619 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6628 - acc: 0.7510 - val_loss: 0.9670 - val_acc: 0.6514\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6561 - acc: 0.7458 - val_loss: 0.9718 - val_acc: 0.6457\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6655 - acc: 0.7241 - val_loss: 0.9687 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6567 - acc: 0.7388 - val_loss: 0.9742 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.6611 - acc: 0.7395 - val_loss: 0.9776 - val_acc: 0.6286\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6451 - acc: 0.7465 - val_loss: 0.9740 - val_acc: 0.6400\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6518 - acc: 0.7299 - val_loss: 0.9735 - val_acc: 0.6400\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6569 - acc: 0.7478 - val_loss: 0.9738 - val_acc: 0.6343\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6392 - acc: 0.7503 - val_loss: 0.9697 - val_acc: 0.6514\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6411 - acc: 0.7554 - val_loss: 0.9718 - val_acc: 0.6400\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6342 - acc: 0.7388 - val_loss: 0.9748 - val_acc: 0.6400\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6542 - acc: 0.7446 - val_loss: 0.9739 - val_acc: 0.6343\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6408 - acc: 0.7407 - val_loss: 0.9789 - val_acc: 0.6400\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6329 - acc: 0.7503 - val_loss: 0.9792 - val_acc: 0.6514\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6468 - acc: 0.7427 - val_loss: 0.9836 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6387 - acc: 0.7401 - val_loss: 0.9815 - val_acc: 0.6457\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6376 - acc: 0.7458 - val_loss: 0.9863 - val_acc: 0.6629\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6319 - acc: 0.7567 - val_loss: 0.9912 - val_acc: 0.6400\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6177 - acc: 0.7465 - val_loss: 0.9980 - val_acc: 0.6400\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6010 - acc: 0.7650 - val_loss: 0.9952 - val_acc: 0.6400\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6317 - acc: 0.7458 - val_loss: 0.9937 - val_acc: 0.6629\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6092 - acc: 0.7605 - val_loss: 0.9971 - val_acc: 0.6686\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6515 - acc: 0.7503 - val_loss: 1.0035 - val_acc: 0.6457\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6170 - acc: 0.7573 - val_loss: 1.0008 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6332 - acc: 0.7599 - val_loss: 0.9989 - val_acc: 0.6571\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6096 - acc: 0.7548 - val_loss: 1.0064 - val_acc: 0.6457\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6183 - acc: 0.7605 - val_loss: 1.0059 - val_acc: 0.6400\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6110 - acc: 0.7586 - val_loss: 1.0049 - val_acc: 0.6457\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.6317 - acc: 0.7510 - val_loss: 1.0042 - val_acc: 0.6457\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6190 - acc: 0.7554 - val_loss: 0.9925 - val_acc: 0.6629\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6059 - acc: 0.7612 - val_loss: 0.9911 - val_acc: 0.6629\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6171 - acc: 0.7446 - val_loss: 1.0008 - val_acc: 0.6514\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6049 - acc: 0.7593 - val_loss: 0.9993 - val_acc: 0.6514\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6129 - acc: 0.7701 - val_loss: 1.0039 - val_acc: 0.6629\n",
      "194/194 [==============================] - 0s 64us/step\n",
      "1741/1741 [==============================] - 0s 29us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 4s 3ms/step - loss: 1.5327 - acc: 0.2886 - val_loss: 1.2388 - val_acc: 0.4457\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 1.3498 - acc: 0.3838 - val_loss: 1.1468 - val_acc: 0.5657\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.2378 - acc: 0.4413 - val_loss: 1.0946 - val_acc: 0.5771\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.2211 - acc: 0.4623 - val_loss: 1.0561 - val_acc: 0.5714\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 1.1747 - acc: 0.5019 - val_loss: 1.0246 - val_acc: 0.5829\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.1443 - acc: 0.4879 - val_loss: 1.0069 - val_acc: 0.5829\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.1233 - acc: 0.5032 - val_loss: 0.9927 - val_acc: 0.5886\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 1.0965 - acc: 0.5313 - val_loss: 0.9802 - val_acc: 0.5943\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 37us/step - loss: 1.0848 - acc: 0.5179 - val_loss: 0.9693 - val_acc: 0.6000\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0589 - acc: 0.5358 - val_loss: 0.9572 - val_acc: 0.6171\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.0510 - acc: 0.5307 - val_loss: 0.9487 - val_acc: 0.5943\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.0347 - acc: 0.5473 - val_loss: 0.9440 - val_acc: 0.6229\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 1.0507 - acc: 0.5536 - val_loss: 0.9337 - val_acc: 0.6171\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0168 - acc: 0.5651 - val_loss: 0.9320 - val_acc: 0.6286\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.0026 - acc: 0.5728 - val_loss: 0.9281 - val_acc: 0.6229\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9836 - acc: 0.5837 - val_loss: 0.9199 - val_acc: 0.6343\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9867 - acc: 0.5824 - val_loss: 0.9127 - val_acc: 0.6457\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9846 - acc: 0.5920 - val_loss: 0.9100 - val_acc: 0.6400\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9664 - acc: 0.5868 - val_loss: 0.9059 - val_acc: 0.6514\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9679 - acc: 0.5875 - val_loss: 0.9036 - val_acc: 0.6514\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9577 - acc: 0.6028 - val_loss: 0.8991 - val_acc: 0.6514\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9654 - acc: 0.5875 - val_loss: 0.8956 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9373 - acc: 0.6092 - val_loss: 0.8951 - val_acc: 0.6514\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9318 - acc: 0.6143 - val_loss: 0.8933 - val_acc: 0.6343\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9505 - acc: 0.6066 - val_loss: 0.8908 - val_acc: 0.6514\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9399 - acc: 0.5926 - val_loss: 0.8881 - val_acc: 0.6514\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9237 - acc: 0.6213 - val_loss: 0.8891 - val_acc: 0.6686\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9280 - acc: 0.6086 - val_loss: 0.8857 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9319 - acc: 0.5964 - val_loss: 0.8852 - val_acc: 0.6571\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9076 - acc: 0.6232 - val_loss: 0.8813 - val_acc: 0.6514\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9052 - acc: 0.6245 - val_loss: 0.8846 - val_acc: 0.6457\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8947 - acc: 0.6303 - val_loss: 0.8820 - val_acc: 0.6514\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8990 - acc: 0.6252 - val_loss: 0.8819 - val_acc: 0.6571\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8958 - acc: 0.6303 - val_loss: 0.8799 - val_acc: 0.6629\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9045 - acc: 0.6226 - val_loss: 0.8802 - val_acc: 0.6571\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9036 - acc: 0.6028 - val_loss: 0.8803 - val_acc: 0.6457\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8615 - acc: 0.6526 - val_loss: 0.8778 - val_acc: 0.6514\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8752 - acc: 0.6379 - val_loss: 0.8773 - val_acc: 0.6400\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8800 - acc: 0.6405 - val_loss: 0.8771 - val_acc: 0.6400\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8789 - acc: 0.6469 - val_loss: 0.8775 - val_acc: 0.6457\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.8641 - acc: 0.6481 - val_loss: 0.8746 - val_acc: 0.6457\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8638 - acc: 0.6347 - val_loss: 0.8762 - val_acc: 0.6400\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8588 - acc: 0.6520 - val_loss: 0.8783 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8795 - acc: 0.6424 - val_loss: 0.8753 - val_acc: 0.6571\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8610 - acc: 0.6347 - val_loss: 0.8781 - val_acc: 0.6514\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8494 - acc: 0.6456 - val_loss: 0.8785 - val_acc: 0.6514\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8692 - acc: 0.6552 - val_loss: 0.8757 - val_acc: 0.6571\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8593 - acc: 0.6322 - val_loss: 0.8788 - val_acc: 0.6400\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8558 - acc: 0.6577 - val_loss: 0.8786 - val_acc: 0.6629\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8558 - acc: 0.6469 - val_loss: 0.8808 - val_acc: 0.6629\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8336 - acc: 0.6526 - val_loss: 0.8861 - val_acc: 0.6286\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.6552 - val_loss: 0.8857 - val_acc: 0.6286\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8396 - acc: 0.6545 - val_loss: 0.8804 - val_acc: 0.6514\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8352 - acc: 0.6443 - val_loss: 0.8818 - val_acc: 0.6343\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8234 - acc: 0.6571 - val_loss: 0.8855 - val_acc: 0.6514\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8384 - acc: 0.6520 - val_loss: 0.8857 - val_acc: 0.6343\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8108 - acc: 0.6839 - val_loss: 0.8886 - val_acc: 0.6286\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8246 - acc: 0.6692 - val_loss: 0.8870 - val_acc: 0.6343\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8168 - acc: 0.6667 - val_loss: 0.8884 - val_acc: 0.6400\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.8237 - acc: 0.6616 - val_loss: 0.8875 - val_acc: 0.6457\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8195 - acc: 0.6762 - val_loss: 0.8868 - val_acc: 0.6343\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8095 - acc: 0.6667 - val_loss: 0.8848 - val_acc: 0.6514\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8263 - acc: 0.6660 - val_loss: 0.8912 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8021 - acc: 0.6609 - val_loss: 0.8956 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8193 - acc: 0.6750 - val_loss: 0.8930 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8163 - acc: 0.6679 - val_loss: 0.8904 - val_acc: 0.6286\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 36us/step - loss: 0.7973 - acc: 0.6711 - val_loss: 0.8933 - val_acc: 0.6343\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8043 - acc: 0.6673 - val_loss: 0.8947 - val_acc: 0.6343\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7976 - acc: 0.6679 - val_loss: 0.8932 - val_acc: 0.6343\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8006 - acc: 0.6635 - val_loss: 0.8892 - val_acc: 0.6400\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7989 - acc: 0.6782 - val_loss: 0.8967 - val_acc: 0.6286\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7832 - acc: 0.6845 - val_loss: 0.8966 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7968 - acc: 0.6750 - val_loss: 0.8905 - val_acc: 0.6343\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8003 - acc: 0.6731 - val_loss: 0.8883 - val_acc: 0.6343\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7814 - acc: 0.6826 - val_loss: 0.8937 - val_acc: 0.6171\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7847 - acc: 0.6756 - val_loss: 0.8901 - val_acc: 0.6457\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7928 - acc: 0.6788 - val_loss: 0.8920 - val_acc: 0.6343\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7755 - acc: 0.6788 - val_loss: 0.8965 - val_acc: 0.6457\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7720 - acc: 0.6916 - val_loss: 0.9018 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7793 - acc: 0.6807 - val_loss: 0.9043 - val_acc: 0.6457\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7770 - acc: 0.6801 - val_loss: 0.9003 - val_acc: 0.6286\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7584 - acc: 0.6928 - val_loss: 0.9004 - val_acc: 0.6286\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7753 - acc: 0.6922 - val_loss: 0.8983 - val_acc: 0.6343\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7639 - acc: 0.6916 - val_loss: 0.9018 - val_acc: 0.6400\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7576 - acc: 0.6890 - val_loss: 0.9028 - val_acc: 0.6457\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7756 - acc: 0.6928 - val_loss: 0.9006 - val_acc: 0.6229\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7732 - acc: 0.6954 - val_loss: 0.9064 - val_acc: 0.6343\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7657 - acc: 0.6897 - val_loss: 0.9035 - val_acc: 0.6457\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7712 - acc: 0.6782 - val_loss: 0.9013 - val_acc: 0.6457\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7630 - acc: 0.7005 - val_loss: 0.9038 - val_acc: 0.6457\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7721 - acc: 0.6820 - val_loss: 0.9048 - val_acc: 0.6229\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7856 - acc: 0.6877 - val_loss: 0.9042 - val_acc: 0.6343\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7530 - acc: 0.6973 - val_loss: 0.9059 - val_acc: 0.6343\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 37us/step - loss: 0.7573 - acc: 0.6954 - val_loss: 0.9062 - val_acc: 0.6343\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7562 - acc: 0.6954 - val_loss: 0.9072 - val_acc: 0.6400\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 37us/step - loss: 0.7526 - acc: 0.7018 - val_loss: 0.9118 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7614 - acc: 0.6826 - val_loss: 0.9058 - val_acc: 0.6514\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7390 - acc: 0.7043 - val_loss: 0.9059 - val_acc: 0.6514\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7468 - acc: 0.6999 - val_loss: 0.9103 - val_acc: 0.6571\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7474 - acc: 0.6967 - val_loss: 0.9059 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7141 - acc: 0.7056 - val_loss: 0.9117 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7401 - acc: 0.7031 - val_loss: 0.9153 - val_acc: 0.6343\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7347 - acc: 0.6960 - val_loss: 0.9132 - val_acc: 0.6343\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7469 - acc: 0.7043 - val_loss: 0.9142 - val_acc: 0.6286\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7316 - acc: 0.7043 - val_loss: 0.9160 - val_acc: 0.6343\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7070 - acc: 0.7178 - val_loss: 0.9215 - val_acc: 0.6286\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7356 - acc: 0.7056 - val_loss: 0.9231 - val_acc: 0.6286\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7329 - acc: 0.7037 - val_loss: 0.9186 - val_acc: 0.6400\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7310 - acc: 0.7107 - val_loss: 0.9177 - val_acc: 0.6400\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7364 - acc: 0.6999 - val_loss: 0.9198 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7165 - acc: 0.7063 - val_loss: 0.9237 - val_acc: 0.6400\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7292 - acc: 0.7190 - val_loss: 0.9247 - val_acc: 0.6400\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7204 - acc: 0.7088 - val_loss: 0.9231 - val_acc: 0.6286\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7319 - acc: 0.7011 - val_loss: 0.9253 - val_acc: 0.6286\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7342 - acc: 0.7101 - val_loss: 0.9238 - val_acc: 0.6171\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7016 - acc: 0.7024 - val_loss: 0.9246 - val_acc: 0.6171\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7170 - acc: 0.7120 - val_loss: 0.9256 - val_acc: 0.6171\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6995 - acc: 0.7331 - val_loss: 0.9250 - val_acc: 0.6171\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7296 - acc: 0.7095 - val_loss: 0.9263 - val_acc: 0.6400\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7287 - acc: 0.7082 - val_loss: 0.9268 - val_acc: 0.6286\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7062 - acc: 0.7043 - val_loss: 0.9343 - val_acc: 0.6286\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7017 - acc: 0.7095 - val_loss: 0.9356 - val_acc: 0.6229\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6961 - acc: 0.7184 - val_loss: 0.9337 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6959 - acc: 0.7312 - val_loss: 0.9344 - val_acc: 0.6171\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6889 - acc: 0.7222 - val_loss: 0.9421 - val_acc: 0.6114\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7139 - acc: 0.7063 - val_loss: 0.9416 - val_acc: 0.6229\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6984 - acc: 0.7312 - val_loss: 0.9419 - val_acc: 0.6171\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7022 - acc: 0.7280 - val_loss: 0.9479 - val_acc: 0.6229\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7058 - acc: 0.7299 - val_loss: 0.9506 - val_acc: 0.6057\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7039 - acc: 0.7229 - val_loss: 0.9503 - val_acc: 0.6057\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6899 - acc: 0.7158 - val_loss: 0.9458 - val_acc: 0.6171\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6904 - acc: 0.7075 - val_loss: 0.9461 - val_acc: 0.6171\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6787 - acc: 0.7165 - val_loss: 0.9444 - val_acc: 0.6171\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6886 - acc: 0.7248 - val_loss: 0.9465 - val_acc: 0.6171\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6929 - acc: 0.7235 - val_loss: 0.9530 - val_acc: 0.6000\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.6950 - acc: 0.7171 - val_loss: 0.9514 - val_acc: 0.5943\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7170 - acc: 0.7037 - val_loss: 0.9480 - val_acc: 0.6114\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6989 - acc: 0.7126 - val_loss: 0.9504 - val_acc: 0.6057\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6784 - acc: 0.7369 - val_loss: 0.9532 - val_acc: 0.6057\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6708 - acc: 0.7375 - val_loss: 0.9545 - val_acc: 0.6171\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6780 - acc: 0.7356 - val_loss: 0.9555 - val_acc: 0.6171\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6807 - acc: 0.7171 - val_loss: 0.9523 - val_acc: 0.6057\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6844 - acc: 0.7190 - val_loss: 0.9531 - val_acc: 0.6229\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6639 - acc: 0.7299 - val_loss: 0.9537 - val_acc: 0.6343\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6693 - acc: 0.7209 - val_loss: 0.9582 - val_acc: 0.6171\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6806 - acc: 0.7312 - val_loss: 0.9608 - val_acc: 0.6057\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6777 - acc: 0.7267 - val_loss: 0.9579 - val_acc: 0.6057\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6716 - acc: 0.7292 - val_loss: 0.9547 - val_acc: 0.6114\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6660 - acc: 0.7395 - val_loss: 0.9576 - val_acc: 0.6171\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6618 - acc: 0.7229 - val_loss: 0.9642 - val_acc: 0.6171\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6562 - acc: 0.7356 - val_loss: 0.9626 - val_acc: 0.6000\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6604 - acc: 0.7471 - val_loss: 0.9686 - val_acc: 0.6114\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6841 - acc: 0.7280 - val_loss: 0.9671 - val_acc: 0.6000\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6515 - acc: 0.7292 - val_loss: 0.9638 - val_acc: 0.5886\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6592 - acc: 0.7363 - val_loss: 0.9657 - val_acc: 0.6057\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6429 - acc: 0.7356 - val_loss: 0.9684 - val_acc: 0.6000\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6661 - acc: 0.7299 - val_loss: 0.9665 - val_acc: 0.6057\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6643 - acc: 0.7369 - val_loss: 0.9707 - val_acc: 0.6114\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6553 - acc: 0.7478 - val_loss: 0.9739 - val_acc: 0.5943\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6457 - acc: 0.7522 - val_loss: 0.9736 - val_acc: 0.6114\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6587 - acc: 0.7427 - val_loss: 0.9764 - val_acc: 0.6171\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6435 - acc: 0.7427 - val_loss: 0.9737 - val_acc: 0.6057\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6479 - acc: 0.7407 - val_loss: 0.9716 - val_acc: 0.6057\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6577 - acc: 0.7382 - val_loss: 0.9769 - val_acc: 0.6000\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6409 - acc: 0.7312 - val_loss: 0.9783 - val_acc: 0.6000\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6291 - acc: 0.7420 - val_loss: 0.9829 - val_acc: 0.5943\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6473 - acc: 0.7305 - val_loss: 0.9803 - val_acc: 0.6000\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6318 - acc: 0.7433 - val_loss: 0.9765 - val_acc: 0.6114\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6263 - acc: 0.7535 - val_loss: 0.9788 - val_acc: 0.6229\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6251 - acc: 0.7465 - val_loss: 0.9821 - val_acc: 0.6057\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6306 - acc: 0.7503 - val_loss: 0.9855 - val_acc: 0.6000\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6284 - acc: 0.7497 - val_loss: 0.9847 - val_acc: 0.6000\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6222 - acc: 0.7522 - val_loss: 0.9818 - val_acc: 0.6000\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6303 - acc: 0.7407 - val_loss: 0.9809 - val_acc: 0.6000\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6478 - acc: 0.7388 - val_loss: 0.9869 - val_acc: 0.6171\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6211 - acc: 0.7522 - val_loss: 0.9858 - val_acc: 0.6286\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6276 - acc: 0.7497 - val_loss: 0.9900 - val_acc: 0.6114\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6222 - acc: 0.7478 - val_loss: 0.9933 - val_acc: 0.6057\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6389 - acc: 0.7414 - val_loss: 0.9941 - val_acc: 0.6000\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6260 - acc: 0.7535 - val_loss: 1.0030 - val_acc: 0.6000\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6071 - acc: 0.7478 - val_loss: 1.0029 - val_acc: 0.6000\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6268 - acc: 0.7497 - val_loss: 1.0028 - val_acc: 0.6114\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6449 - acc: 0.7305 - val_loss: 0.9992 - val_acc: 0.6057\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6255 - acc: 0.7580 - val_loss: 0.9937 - val_acc: 0.6000\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6129 - acc: 0.7676 - val_loss: 0.9967 - val_acc: 0.6000\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6218 - acc: 0.7618 - val_loss: 0.9999 - val_acc: 0.6114\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6099 - acc: 0.7484 - val_loss: 1.0012 - val_acc: 0.5943\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6165 - acc: 0.7452 - val_loss: 1.0007 - val_acc: 0.6057\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6169 - acc: 0.7612 - val_loss: 1.0027 - val_acc: 0.6057\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6020 - acc: 0.7720 - val_loss: 1.0031 - val_acc: 0.6171\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6183 - acc: 0.7414 - val_loss: 1.0050 - val_acc: 0.6229\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6101 - acc: 0.7599 - val_loss: 1.0045 - val_acc: 0.6229\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6045 - acc: 0.7548 - val_loss: 1.0106 - val_acc: 0.6171\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6206 - acc: 0.7490 - val_loss: 1.0115 - val_acc: 0.6229\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6253 - acc: 0.7484 - val_loss: 1.0098 - val_acc: 0.6171\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6175 - acc: 0.7516 - val_loss: 1.0186 - val_acc: 0.6114\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6104 - acc: 0.7593 - val_loss: 1.0191 - val_acc: 0.6114\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6105 - acc: 0.7490 - val_loss: 1.0145 - val_acc: 0.6171\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6069 - acc: 0.7612 - val_loss: 1.0107 - val_acc: 0.6171\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6052 - acc: 0.7548 - val_loss: 1.0087 - val_acc: 0.6114\n",
      "194/194 [==============================] - 0s 40us/step\n",
      "1741/1741 [==============================] - 0s 26us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 4s 2ms/step - loss: 1.6073 - acc: 0.2727 - val_loss: 1.3657 - val_acc: 0.3829\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.3691 - acc: 0.3563 - val_loss: 1.2051 - val_acc: 0.5314\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.2569 - acc: 0.4240 - val_loss: 1.1221 - val_acc: 0.5600\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1956 - acc: 0.4559 - val_loss: 1.0732 - val_acc: 0.5943\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.1479 - acc: 0.4789 - val_loss: 1.0407 - val_acc: 0.6000\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.1329 - acc: 0.5026 - val_loss: 1.0178 - val_acc: 0.6000\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0934 - acc: 0.5128 - val_loss: 1.0087 - val_acc: 0.5886\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0940 - acc: 0.5192 - val_loss: 0.9962 - val_acc: 0.6114\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0848 - acc: 0.5179 - val_loss: 0.9825 - val_acc: 0.6114\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0616 - acc: 0.5447 - val_loss: 0.9768 - val_acc: 0.6286\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0302 - acc: 0.5466 - val_loss: 0.9701 - val_acc: 0.6286\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0323 - acc: 0.5536 - val_loss: 0.9655 - val_acc: 0.6229\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.0365 - acc: 0.5511 - val_loss: 0.9593 - val_acc: 0.6286\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.0092 - acc: 0.5734 - val_loss: 0.9544 - val_acc: 0.6400\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9869 - acc: 0.5760 - val_loss: 0.9535 - val_acc: 0.6286\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 1.0076 - acc: 0.5600 - val_loss: 0.9464 - val_acc: 0.6457\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9769 - acc: 0.5977 - val_loss: 0.9416 - val_acc: 0.6286\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9761 - acc: 0.5766 - val_loss: 0.9365 - val_acc: 0.6400\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.9683 - acc: 0.5773 - val_loss: 0.9354 - val_acc: 0.6229\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9627 - acc: 0.5868 - val_loss: 0.9321 - val_acc: 0.6229\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 36us/step - loss: 0.9773 - acc: 0.5715 - val_loss: 0.9262 - val_acc: 0.6400\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 37us/step - loss: 0.9445 - acc: 0.6086 - val_loss: 0.9240 - val_acc: 0.6400\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.9553 - acc: 0.5856 - val_loss: 0.9202 - val_acc: 0.6400\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9268 - acc: 0.6015 - val_loss: 0.9191 - val_acc: 0.6400\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9541 - acc: 0.6015 - val_loss: 0.9156 - val_acc: 0.6514\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9294 - acc: 0.6047 - val_loss: 0.9199 - val_acc: 0.6514\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.9281 - acc: 0.6015 - val_loss: 0.9175 - val_acc: 0.6629\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9188 - acc: 0.6137 - val_loss: 0.9166 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 37us/step - loss: 0.9232 - acc: 0.6162 - val_loss: 0.9165 - val_acc: 0.6457\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 35us/step - loss: 0.9248 - acc: 0.6143 - val_loss: 0.9130 - val_acc: 0.6400\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9126 - acc: 0.6213 - val_loss: 0.9129 - val_acc: 0.6457\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.9114 - acc: 0.6079 - val_loss: 0.9119 - val_acc: 0.6457\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9053 - acc: 0.6207 - val_loss: 0.9151 - val_acc: 0.6457\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8778 - acc: 0.6296 - val_loss: 0.9112 - val_acc: 0.6571\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8953 - acc: 0.6264 - val_loss: 0.9090 - val_acc: 0.6400\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8846 - acc: 0.6239 - val_loss: 0.9086 - val_acc: 0.6457\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8974 - acc: 0.6149 - val_loss: 0.9079 - val_acc: 0.6457\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8894 - acc: 0.6239 - val_loss: 0.9113 - val_acc: 0.6629\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8836 - acc: 0.6386 - val_loss: 0.9129 - val_acc: 0.6571\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8612 - acc: 0.6430 - val_loss: 0.9136 - val_acc: 0.6400\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8824 - acc: 0.6245 - val_loss: 0.9093 - val_acc: 0.6457\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8874 - acc: 0.6181 - val_loss: 0.9124 - val_acc: 0.6571\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8801 - acc: 0.6398 - val_loss: 0.9118 - val_acc: 0.6400\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8689 - acc: 0.6475 - val_loss: 0.9041 - val_acc: 0.6400\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8771 - acc: 0.6443 - val_loss: 0.9095 - val_acc: 0.6629\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8627 - acc: 0.6379 - val_loss: 0.9109 - val_acc: 0.6686\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8585 - acc: 0.6475 - val_loss: 0.9116 - val_acc: 0.6514\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8645 - acc: 0.6443 - val_loss: 0.9124 - val_acc: 0.6514\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8592 - acc: 0.6443 - val_loss: 0.9151 - val_acc: 0.6457\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8722 - acc: 0.6290 - val_loss: 0.9159 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8697 - acc: 0.6418 - val_loss: 0.9158 - val_acc: 0.6400\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8511 - acc: 0.6475 - val_loss: 0.9165 - val_acc: 0.6400\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8417 - acc: 0.6341 - val_loss: 0.9138 - val_acc: 0.6343\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8503 - acc: 0.6539 - val_loss: 0.9115 - val_acc: 0.6343\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8425 - acc: 0.6673 - val_loss: 0.9109 - val_acc: 0.6400\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8329 - acc: 0.6596 - val_loss: 0.9062 - val_acc: 0.6343\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8284 - acc: 0.6641 - val_loss: 0.9078 - val_acc: 0.6343\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8267 - acc: 0.6622 - val_loss: 0.9109 - val_acc: 0.6400\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8263 - acc: 0.6603 - val_loss: 0.9163 - val_acc: 0.6400\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8602 - acc: 0.6347 - val_loss: 0.9158 - val_acc: 0.6343\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8270 - acc: 0.6667 - val_loss: 0.9136 - val_acc: 0.6457\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8124 - acc: 0.6679 - val_loss: 0.9164 - val_acc: 0.6343\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8219 - acc: 0.6660 - val_loss: 0.9160 - val_acc: 0.6171\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8242 - acc: 0.6641 - val_loss: 0.9143 - val_acc: 0.6286\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8186 - acc: 0.6513 - val_loss: 0.9121 - val_acc: 0.6286\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8180 - acc: 0.6724 - val_loss: 0.9095 - val_acc: 0.6400\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8091 - acc: 0.6660 - val_loss: 0.9103 - val_acc: 0.6400\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7911 - acc: 0.6782 - val_loss: 0.9110 - val_acc: 0.6514\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7971 - acc: 0.6718 - val_loss: 0.9160 - val_acc: 0.6286\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8148 - acc: 0.6750 - val_loss: 0.9173 - val_acc: 0.6343\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8019 - acc: 0.6609 - val_loss: 0.9153 - val_acc: 0.6286\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8125 - acc: 0.6552 - val_loss: 0.9191 - val_acc: 0.6343\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7987 - acc: 0.6788 - val_loss: 0.9274 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8029 - acc: 0.6667 - val_loss: 0.9250 - val_acc: 0.6171\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8032 - acc: 0.6622 - val_loss: 0.9199 - val_acc: 0.6171\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7978 - acc: 0.6711 - val_loss: 0.9230 - val_acc: 0.6229\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7909 - acc: 0.6788 - val_loss: 0.9244 - val_acc: 0.6171\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7874 - acc: 0.6814 - val_loss: 0.9255 - val_acc: 0.6171\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8075 - acc: 0.6596 - val_loss: 0.9199 - val_acc: 0.6114\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7970 - acc: 0.6858 - val_loss: 0.9225 - val_acc: 0.6171\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7906 - acc: 0.6762 - val_loss: 0.9223 - val_acc: 0.6171\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7839 - acc: 0.6833 - val_loss: 0.9233 - val_acc: 0.6286\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.8003 - acc: 0.6596 - val_loss: 0.9218 - val_acc: 0.6343\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8017 - acc: 0.6756 - val_loss: 0.9173 - val_acc: 0.6457\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7858 - acc: 0.6858 - val_loss: 0.9192 - val_acc: 0.6229\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.7707 - acc: 0.6801 - val_loss: 0.9237 - val_acc: 0.6229\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7731 - acc: 0.6731 - val_loss: 0.9260 - val_acc: 0.6229\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7617 - acc: 0.6897 - val_loss: 0.9266 - val_acc: 0.6171\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7745 - acc: 0.6865 - val_loss: 0.9252 - val_acc: 0.6343\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7688 - acc: 0.6865 - val_loss: 0.9248 - val_acc: 0.6286\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7743 - acc: 0.6667 - val_loss: 0.9220 - val_acc: 0.6343\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7699 - acc: 0.6884 - val_loss: 0.9216 - val_acc: 0.6343\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7619 - acc: 0.6980 - val_loss: 0.9239 - val_acc: 0.6286\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7542 - acc: 0.6877 - val_loss: 0.9246 - val_acc: 0.6229\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7505 - acc: 0.7024 - val_loss: 0.9244 - val_acc: 0.6286\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7754 - acc: 0.6884 - val_loss: 0.9284 - val_acc: 0.6229\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7612 - acc: 0.6845 - val_loss: 0.9229 - val_acc: 0.6571\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7501 - acc: 0.6954 - val_loss: 0.9269 - val_acc: 0.6286\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7555 - acc: 0.6877 - val_loss: 0.9290 - val_acc: 0.6286\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7540 - acc: 0.6801 - val_loss: 0.9291 - val_acc: 0.6229\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7518 - acc: 0.6903 - val_loss: 0.9285 - val_acc: 0.6286\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7687 - acc: 0.6814 - val_loss: 0.9302 - val_acc: 0.6171\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7427 - acc: 0.6986 - val_loss: 0.9417 - val_acc: 0.6057\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7625 - acc: 0.6871 - val_loss: 0.9376 - val_acc: 0.6171\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7549 - acc: 0.6884 - val_loss: 0.9383 - val_acc: 0.6057\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7604 - acc: 0.6960 - val_loss: 0.9365 - val_acc: 0.6171\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7350 - acc: 0.7063 - val_loss: 0.9338 - val_acc: 0.6229\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7388 - acc: 0.6986 - val_loss: 0.9393 - val_acc: 0.6171\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7219 - acc: 0.7075 - val_loss: 0.9407 - val_acc: 0.6057\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7515 - acc: 0.6954 - val_loss: 0.9412 - val_acc: 0.6229\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7407 - acc: 0.7031 - val_loss: 0.9406 - val_acc: 0.6286\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7360 - acc: 0.6973 - val_loss: 0.9415 - val_acc: 0.6286\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7553 - acc: 0.6999 - val_loss: 0.9387 - val_acc: 0.6343\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7234 - acc: 0.7107 - val_loss: 0.9425 - val_acc: 0.6171\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7385 - acc: 0.7031 - val_loss: 0.9417 - val_acc: 0.6400\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7150 - acc: 0.7082 - val_loss: 0.9456 - val_acc: 0.6114\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7266 - acc: 0.7178 - val_loss: 0.9422 - val_acc: 0.6171\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7215 - acc: 0.7139 - val_loss: 0.9474 - val_acc: 0.6057\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7140 - acc: 0.7031 - val_loss: 0.9518 - val_acc: 0.6114\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7182 - acc: 0.6999 - val_loss: 0.9510 - val_acc: 0.6171\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7041 - acc: 0.7139 - val_loss: 0.9521 - val_acc: 0.6171\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7080 - acc: 0.7063 - val_loss: 0.9527 - val_acc: 0.5943\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7221 - acc: 0.7018 - val_loss: 0.9591 - val_acc: 0.6171\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7209 - acc: 0.7114 - val_loss: 0.9535 - val_acc: 0.6114\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7169 - acc: 0.6916 - val_loss: 0.9514 - val_acc: 0.6229\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7177 - acc: 0.7095 - val_loss: 0.9575 - val_acc: 0.6171\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7170 - acc: 0.6897 - val_loss: 0.9633 - val_acc: 0.6057\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7175 - acc: 0.7139 - val_loss: 0.9529 - val_acc: 0.6171\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7237 - acc: 0.7005 - val_loss: 0.9552 - val_acc: 0.6171\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6884 - acc: 0.7139 - val_loss: 0.9633 - val_acc: 0.6171\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6974 - acc: 0.7069 - val_loss: 0.9624 - val_acc: 0.6229\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6882 - acc: 0.7216 - val_loss: 0.9645 - val_acc: 0.6229\n",
      "Epoch 133/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6976 - acc: 0.7165 - val_loss: 0.9640 - val_acc: 0.6114\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7047 - acc: 0.7146 - val_loss: 0.9686 - val_acc: 0.6229\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7005 - acc: 0.7133 - val_loss: 0.9676 - val_acc: 0.6229\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6978 - acc: 0.7126 - val_loss: 0.9633 - val_acc: 0.6286\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7176 - acc: 0.7088 - val_loss: 0.9610 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6946 - acc: 0.6999 - val_loss: 0.9614 - val_acc: 0.6400\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6881 - acc: 0.7222 - val_loss: 0.9578 - val_acc: 0.6229\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7006 - acc: 0.7267 - val_loss: 0.9577 - val_acc: 0.6114\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6862 - acc: 0.7197 - val_loss: 0.9612 - val_acc: 0.6057\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6909 - acc: 0.7126 - val_loss: 0.9655 - val_acc: 0.6114\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6904 - acc: 0.7286 - val_loss: 0.9693 - val_acc: 0.6229\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6796 - acc: 0.7229 - val_loss: 0.9652 - val_acc: 0.6171\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6830 - acc: 0.7267 - val_loss: 0.9641 - val_acc: 0.6229\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6951 - acc: 0.7292 - val_loss: 0.9639 - val_acc: 0.6171\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6864 - acc: 0.7165 - val_loss: 0.9646 - val_acc: 0.6343\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6806 - acc: 0.7229 - val_loss: 0.9599 - val_acc: 0.6286\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6774 - acc: 0.7299 - val_loss: 0.9614 - val_acc: 0.6286\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6720 - acc: 0.7248 - val_loss: 0.9702 - val_acc: 0.6171\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6767 - acc: 0.7369 - val_loss: 0.9703 - val_acc: 0.6229\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6894 - acc: 0.7139 - val_loss: 0.9708 - val_acc: 0.6114\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6794 - acc: 0.7235 - val_loss: 0.9744 - val_acc: 0.6171\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6856 - acc: 0.7261 - val_loss: 0.9736 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6690 - acc: 0.7305 - val_loss: 0.9756 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6584 - acc: 0.7318 - val_loss: 0.9792 - val_acc: 0.6171\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6708 - acc: 0.7286 - val_loss: 0.9777 - val_acc: 0.6229\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6935 - acc: 0.7216 - val_loss: 0.9853 - val_acc: 0.6343\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6629 - acc: 0.7382 - val_loss: 0.9847 - val_acc: 0.6286\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6613 - acc: 0.7452 - val_loss: 0.9852 - val_acc: 0.6286\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6645 - acc: 0.7363 - val_loss: 0.9866 - val_acc: 0.6229\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6643 - acc: 0.7375 - val_loss: 0.9852 - val_acc: 0.6229\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6694 - acc: 0.7292 - val_loss: 0.9863 - val_acc: 0.6286\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6572 - acc: 0.7395 - val_loss: 0.9927 - val_acc: 0.6286\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6450 - acc: 0.7439 - val_loss: 0.9844 - val_acc: 0.6400\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6643 - acc: 0.7178 - val_loss: 0.9781 - val_acc: 0.6343\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6566 - acc: 0.7414 - val_loss: 0.9834 - val_acc: 0.6343\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6472 - acc: 0.7382 - val_loss: 0.9854 - val_acc: 0.6343\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6472 - acc: 0.7395 - val_loss: 0.9915 - val_acc: 0.6343\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6596 - acc: 0.7318 - val_loss: 0.9960 - val_acc: 0.6286\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6498 - acc: 0.7458 - val_loss: 0.9939 - val_acc: 0.6229\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6582 - acc: 0.7312 - val_loss: 0.9915 - val_acc: 0.6229\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6531 - acc: 0.7344 - val_loss: 0.9953 - val_acc: 0.6514\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6445 - acc: 0.7363 - val_loss: 0.9997 - val_acc: 0.6229\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6464 - acc: 0.7420 - val_loss: 1.0002 - val_acc: 0.6514\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6330 - acc: 0.7522 - val_loss: 1.0078 - val_acc: 0.6457\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6465 - acc: 0.7356 - val_loss: 1.0156 - val_acc: 0.6286\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6506 - acc: 0.7350 - val_loss: 1.0116 - val_acc: 0.6171\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6265 - acc: 0.7650 - val_loss: 1.0026 - val_acc: 0.6286\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6578 - acc: 0.7318 - val_loss: 1.0018 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6304 - acc: 0.7497 - val_loss: 1.0046 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6121 - acc: 0.7561 - val_loss: 1.0104 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6432 - acc: 0.7446 - val_loss: 1.0128 - val_acc: 0.6343\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6323 - acc: 0.7452 - val_loss: 1.0158 - val_acc: 0.6343\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6257 - acc: 0.7433 - val_loss: 1.0226 - val_acc: 0.6229\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6471 - acc: 0.7465 - val_loss: 1.0244 - val_acc: 0.6229\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6214 - acc: 0.7478 - val_loss: 1.0152 - val_acc: 0.6457\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6200 - acc: 0.7631 - val_loss: 1.0161 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6432 - acc: 0.7490 - val_loss: 1.0229 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6442 - acc: 0.7407 - val_loss: 1.0307 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6224 - acc: 0.7484 - val_loss: 1.0267 - val_acc: 0.6400\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6270 - acc: 0.7561 - val_loss: 1.0292 - val_acc: 0.6457\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.6219 - acc: 0.7599 - val_loss: 1.0308 - val_acc: 0.6457\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6307 - acc: 0.7503 - val_loss: 1.0317 - val_acc: 0.6571\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6048 - acc: 0.7580 - val_loss: 1.0335 - val_acc: 0.6400\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.5823 - acc: 0.7752 - val_loss: 1.0320 - val_acc: 0.6514\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6097 - acc: 0.7542 - val_loss: 1.0313 - val_acc: 0.6400\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6137 - acc: 0.7586 - val_loss: 1.0286 - val_acc: 0.6514\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6146 - acc: 0.7612 - val_loss: 1.0410 - val_acc: 0.6286\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6101 - acc: 0.7599 - val_loss: 1.0340 - val_acc: 0.6514\n",
      "194/194 [==============================] - 0s 43us/step\n",
      "1741/1741 [==============================] - 0s 29us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 4s 2ms/step - loss: 1.6052 - acc: 0.2656 - val_loss: 1.2983 - val_acc: 0.3714\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.3883 - acc: 0.3429 - val_loss: 1.1729 - val_acc: 0.5314\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.3129 - acc: 0.4036 - val_loss: 1.1236 - val_acc: 0.5771\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.2473 - acc: 0.4432 - val_loss: 1.0797 - val_acc: 0.6057\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1868 - acc: 0.4674 - val_loss: 1.0475 - val_acc: 0.6057\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.1500 - acc: 0.5051 - val_loss: 1.0243 - val_acc: 0.6000\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.1469 - acc: 0.4962 - val_loss: 1.0069 - val_acc: 0.6171\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1169 - acc: 0.5172 - val_loss: 0.9927 - val_acc: 0.6229\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.1077 - acc: 0.5249 - val_loss: 0.9797 - val_acc: 0.6229\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0920 - acc: 0.5255 - val_loss: 0.9727 - val_acc: 0.6171\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.0878 - acc: 0.5307 - val_loss: 0.9606 - val_acc: 0.6400\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 1.0382 - acc: 0.5556 - val_loss: 0.9449 - val_acc: 0.6629\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.0480 - acc: 0.5460 - val_loss: 0.9445 - val_acc: 0.6571\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0335 - acc: 0.5575 - val_loss: 0.9373 - val_acc: 0.6743\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0252 - acc: 0.5453 - val_loss: 0.9279 - val_acc: 0.6857\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0144 - acc: 0.5690 - val_loss: 0.9252 - val_acc: 0.6743\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0123 - acc: 0.5658 - val_loss: 0.9222 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.0172 - acc: 0.5677 - val_loss: 0.9211 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9793 - acc: 0.5907 - val_loss: 0.9146 - val_acc: 0.6629\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9727 - acc: 0.5817 - val_loss: 0.9055 - val_acc: 0.6857\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9808 - acc: 0.5722 - val_loss: 0.9042 - val_acc: 0.6686\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9830 - acc: 0.5747 - val_loss: 0.9107 - val_acc: 0.6914\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9575 - acc: 0.6054 - val_loss: 0.9060 - val_acc: 0.6800\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9569 - acc: 0.5990 - val_loss: 0.9023 - val_acc: 0.6971\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9623 - acc: 0.5971 - val_loss: 0.8979 - val_acc: 0.6971\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9590 - acc: 0.5945 - val_loss: 0.8976 - val_acc: 0.6629\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9311 - acc: 0.6111 - val_loss: 0.8956 - val_acc: 0.6800\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9269 - acc: 0.6367 - val_loss: 0.9000 - val_acc: 0.6800\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9287 - acc: 0.6022 - val_loss: 0.8994 - val_acc: 0.6686\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9362 - acc: 0.6015 - val_loss: 0.8987 - val_acc: 0.6686\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9278 - acc: 0.6130 - val_loss: 0.8973 - val_acc: 0.6629\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9249 - acc: 0.6220 - val_loss: 0.8897 - val_acc: 0.6857\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9108 - acc: 0.6341 - val_loss: 0.8890 - val_acc: 0.6743\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9257 - acc: 0.6245 - val_loss: 0.8893 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9140 - acc: 0.6105 - val_loss: 0.8905 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9169 - acc: 0.6188 - val_loss: 0.8920 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9082 - acc: 0.6111 - val_loss: 0.8938 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8866 - acc: 0.6264 - val_loss: 0.8907 - val_acc: 0.6800\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8816 - acc: 0.6379 - val_loss: 0.8891 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8980 - acc: 0.6143 - val_loss: 0.8880 - val_acc: 0.6743\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8834 - acc: 0.6309 - val_loss: 0.8895 - val_acc: 0.6743\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8711 - acc: 0.6405 - val_loss: 0.8885 - val_acc: 0.6743\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8692 - acc: 0.6290 - val_loss: 0.8894 - val_acc: 0.6629\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8877 - acc: 0.6232 - val_loss: 0.8908 - val_acc: 0.6686\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8612 - acc: 0.6603 - val_loss: 0.8905 - val_acc: 0.6571\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8866 - acc: 0.6347 - val_loss: 0.8880 - val_acc: 0.6629\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8650 - acc: 0.6437 - val_loss: 0.8886 - val_acc: 0.6800\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8764 - acc: 0.6367 - val_loss: 0.8919 - val_acc: 0.6629\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8818 - acc: 0.6322 - val_loss: 0.8895 - val_acc: 0.6857\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8453 - acc: 0.6494 - val_loss: 0.8935 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8360 - acc: 0.6507 - val_loss: 0.8927 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8510 - acc: 0.6577 - val_loss: 0.8911 - val_acc: 0.6800\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8578 - acc: 0.6507 - val_loss: 0.8911 - val_acc: 0.6629\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8390 - acc: 0.6584 - val_loss: 0.8935 - val_acc: 0.6743\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8452 - acc: 0.6539 - val_loss: 0.8925 - val_acc: 0.6629\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8391 - acc: 0.6526 - val_loss: 0.8925 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8237 - acc: 0.6564 - val_loss: 0.8960 - val_acc: 0.6629\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8370 - acc: 0.6520 - val_loss: 0.8953 - val_acc: 0.6686\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8453 - acc: 0.6513 - val_loss: 0.8983 - val_acc: 0.6629\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8393 - acc: 0.6545 - val_loss: 0.9009 - val_acc: 0.6743\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8315 - acc: 0.6622 - val_loss: 0.9034 - val_acc: 0.6743\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8335 - acc: 0.6641 - val_loss: 0.9021 - val_acc: 0.6686\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8378 - acc: 0.6526 - val_loss: 0.9028 - val_acc: 0.6629\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8330 - acc: 0.6660 - val_loss: 0.9024 - val_acc: 0.6800\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8161 - acc: 0.6654 - val_loss: 0.9042 - val_acc: 0.6686\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8148 - acc: 0.6679 - val_loss: 0.9016 - val_acc: 0.6629\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8290 - acc: 0.6654 - val_loss: 0.9032 - val_acc: 0.6743\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8326 - acc: 0.6686 - val_loss: 0.9034 - val_acc: 0.6629\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8230 - acc: 0.6526 - val_loss: 0.9048 - val_acc: 0.6800\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8288 - acc: 0.6648 - val_loss: 0.9019 - val_acc: 0.6971\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8041 - acc: 0.6724 - val_loss: 0.9059 - val_acc: 0.6686\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8142 - acc: 0.6584 - val_loss: 0.9040 - val_acc: 0.6686\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8409 - acc: 0.6545 - val_loss: 0.9054 - val_acc: 0.6800\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7975 - acc: 0.6705 - val_loss: 0.9057 - val_acc: 0.6686\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7859 - acc: 0.6833 - val_loss: 0.9017 - val_acc: 0.6800\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8038 - acc: 0.6648 - val_loss: 0.8994 - val_acc: 0.6743\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7951 - acc: 0.6820 - val_loss: 0.9011 - val_acc: 0.6743\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7971 - acc: 0.6622 - val_loss: 0.9013 - val_acc: 0.6686\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7920 - acc: 0.6794 - val_loss: 0.9066 - val_acc: 0.6743\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7977 - acc: 0.6782 - val_loss: 0.9054 - val_acc: 0.6743\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8014 - acc: 0.6654 - val_loss: 0.9055 - val_acc: 0.6800\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8085 - acc: 0.6788 - val_loss: 0.9026 - val_acc: 0.6800\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7803 - acc: 0.6699 - val_loss: 0.9080 - val_acc: 0.6857\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7705 - acc: 0.6903 - val_loss: 0.9130 - val_acc: 0.6800\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7920 - acc: 0.6756 - val_loss: 0.9140 - val_acc: 0.6686\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7890 - acc: 0.6845 - val_loss: 0.9192 - val_acc: 0.6571\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7972 - acc: 0.6737 - val_loss: 0.9124 - val_acc: 0.6914\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7766 - acc: 0.6858 - val_loss: 0.9117 - val_acc: 0.6857\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7864 - acc: 0.6916 - val_loss: 0.9185 - val_acc: 0.6800\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7790 - acc: 0.6692 - val_loss: 0.9154 - val_acc: 0.6743\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7698 - acc: 0.6871 - val_loss: 0.9164 - val_acc: 0.6686\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7858 - acc: 0.6820 - val_loss: 0.9155 - val_acc: 0.6800\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7884 - acc: 0.6699 - val_loss: 0.9177 - val_acc: 0.6686\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7618 - acc: 0.6871 - val_loss: 0.9183 - val_acc: 0.6857\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7641 - acc: 0.6948 - val_loss: 0.9230 - val_acc: 0.6686\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7666 - acc: 0.6858 - val_loss: 0.9281 - val_acc: 0.6629\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7390 - acc: 0.6922 - val_loss: 0.9253 - val_acc: 0.6800\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7659 - acc: 0.6960 - val_loss: 0.9300 - val_acc: 0.6629\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7778 - acc: 0.6845 - val_loss: 0.9307 - val_acc: 0.6629\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7630 - acc: 0.6858 - val_loss: 0.9285 - val_acc: 0.6857\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7761 - acc: 0.6794 - val_loss: 0.9286 - val_acc: 0.6743\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7475 - acc: 0.6916 - val_loss: 0.9321 - val_acc: 0.6743\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7836 - acc: 0.6935 - val_loss: 0.9355 - val_acc: 0.6686\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7561 - acc: 0.6782 - val_loss: 0.9308 - val_acc: 0.6686\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7471 - acc: 0.6980 - val_loss: 0.9301 - val_acc: 0.6857\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.7438 - acc: 0.7114 - val_loss: 0.9398 - val_acc: 0.6686\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7634 - acc: 0.6833 - val_loss: 0.9389 - val_acc: 0.6857\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7439 - acc: 0.7107 - val_loss: 0.9323 - val_acc: 0.6800\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7454 - acc: 0.6986 - val_loss: 0.9341 - val_acc: 0.6743\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7361 - acc: 0.7043 - val_loss: 0.9365 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7164 - acc: 0.7101 - val_loss: 0.9358 - val_acc: 0.6857\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7443 - acc: 0.6954 - val_loss: 0.9365 - val_acc: 0.6971\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7319 - acc: 0.7024 - val_loss: 0.9414 - val_acc: 0.6800\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7353 - acc: 0.6903 - val_loss: 0.9445 - val_acc: 0.6800\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7409 - acc: 0.6903 - val_loss: 0.9438 - val_acc: 0.6800\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7220 - acc: 0.6960 - val_loss: 0.9411 - val_acc: 0.6914\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7199 - acc: 0.7095 - val_loss: 0.9414 - val_acc: 0.6800\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7296 - acc: 0.6960 - val_loss: 0.9380 - val_acc: 0.6857\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7424 - acc: 0.6916 - val_loss: 0.9449 - val_acc: 0.6743\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7312 - acc: 0.7069 - val_loss: 0.9512 - val_acc: 0.6800\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7171 - acc: 0.7146 - val_loss: 0.9531 - val_acc: 0.6800\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7407 - acc: 0.7005 - val_loss: 0.9546 - val_acc: 0.6686\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7392 - acc: 0.6967 - val_loss: 0.9550 - val_acc: 0.6686\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7354 - acc: 0.7011 - val_loss: 0.9508 - val_acc: 0.6743\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7181 - acc: 0.7139 - val_loss: 0.9489 - val_acc: 0.6629\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7196 - acc: 0.7095 - val_loss: 0.9520 - val_acc: 0.6514\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7310 - acc: 0.6973 - val_loss: 0.9553 - val_acc: 0.6629\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7331 - acc: 0.7120 - val_loss: 0.9564 - val_acc: 0.6686\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7347 - acc: 0.7082 - val_loss: 0.9652 - val_acc: 0.6686\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7067 - acc: 0.7069 - val_loss: 0.9611 - val_acc: 0.6857\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7088 - acc: 0.7101 - val_loss: 0.9600 - val_acc: 0.6857\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7164 - acc: 0.7095 - val_loss: 0.9634 - val_acc: 0.6743\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7183 - acc: 0.7152 - val_loss: 0.9647 - val_acc: 0.6800\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6989 - acc: 0.7152 - val_loss: 0.9648 - val_acc: 0.6914\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7027 - acc: 0.7088 - val_loss: 0.9682 - val_acc: 0.6743\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6855 - acc: 0.7324 - val_loss: 0.9664 - val_acc: 0.6743\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6904 - acc: 0.7273 - val_loss: 0.9634 - val_acc: 0.6800\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6747 - acc: 0.7222 - val_loss: 0.9678 - val_acc: 0.6629\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7151 - acc: 0.7069 - val_loss: 0.9723 - val_acc: 0.6686\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7041 - acc: 0.7216 - val_loss: 0.9798 - val_acc: 0.6457\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6986 - acc: 0.7190 - val_loss: 0.9862 - val_acc: 0.6686\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6831 - acc: 0.7197 - val_loss: 0.9855 - val_acc: 0.6686\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6870 - acc: 0.7254 - val_loss: 0.9843 - val_acc: 0.6743\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6925 - acc: 0.7305 - val_loss: 0.9868 - val_acc: 0.6629\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6846 - acc: 0.7280 - val_loss: 0.9844 - val_acc: 0.6629\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6914 - acc: 0.7273 - val_loss: 0.9798 - val_acc: 0.6743\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6862 - acc: 0.7146 - val_loss: 0.9823 - val_acc: 0.6686\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7000 - acc: 0.7056 - val_loss: 0.9889 - val_acc: 0.6743\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6854 - acc: 0.7261 - val_loss: 0.9870 - val_acc: 0.6629\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6819 - acc: 0.7248 - val_loss: 0.9846 - val_acc: 0.6800\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6931 - acc: 0.7203 - val_loss: 0.9854 - val_acc: 0.6743\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6858 - acc: 0.7197 - val_loss: 0.9898 - val_acc: 0.6514\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6747 - acc: 0.7299 - val_loss: 0.9881 - val_acc: 0.6514\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6902 - acc: 0.7318 - val_loss: 0.9902 - val_acc: 0.6514\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6693 - acc: 0.7318 - val_loss: 0.9907 - val_acc: 0.6857\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6793 - acc: 0.7146 - val_loss: 0.9891 - val_acc: 0.6686\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6649 - acc: 0.7433 - val_loss: 0.9873 - val_acc: 0.6743\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6789 - acc: 0.7331 - val_loss: 0.9931 - val_acc: 0.6629\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6617 - acc: 0.7324 - val_loss: 0.9951 - val_acc: 0.6629\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6668 - acc: 0.7318 - val_loss: 0.9940 - val_acc: 0.6514\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6641 - acc: 0.7350 - val_loss: 0.9895 - val_acc: 0.6686\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6697 - acc: 0.7356 - val_loss: 0.9921 - val_acc: 0.6571\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6543 - acc: 0.7433 - val_loss: 0.9934 - val_acc: 0.6514\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6587 - acc: 0.7280 - val_loss: 0.9918 - val_acc: 0.6571\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6573 - acc: 0.7324 - val_loss: 0.9972 - val_acc: 0.6629\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6692 - acc: 0.7235 - val_loss: 1.0006 - val_acc: 0.6686\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6530 - acc: 0.7331 - val_loss: 1.0091 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6466 - acc: 0.7363 - val_loss: 1.0055 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6524 - acc: 0.7375 - val_loss: 1.0115 - val_acc: 0.6571\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6626 - acc: 0.7286 - val_loss: 1.0122 - val_acc: 0.6514\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6763 - acc: 0.7267 - val_loss: 1.0071 - val_acc: 0.6629\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6636 - acc: 0.7299 - val_loss: 1.0054 - val_acc: 0.6629\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6394 - acc: 0.7420 - val_loss: 1.0040 - val_acc: 0.6457\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6318 - acc: 0.7369 - val_loss: 1.0066 - val_acc: 0.6400\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6518 - acc: 0.7337 - val_loss: 1.0098 - val_acc: 0.6514\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6573 - acc: 0.7382 - val_loss: 1.0070 - val_acc: 0.6457\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6439 - acc: 0.7446 - val_loss: 1.0065 - val_acc: 0.6514\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6410 - acc: 0.7446 - val_loss: 1.0169 - val_acc: 0.6400\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6284 - acc: 0.7497 - val_loss: 1.0162 - val_acc: 0.6400\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6530 - acc: 0.7388 - val_loss: 1.0203 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6167 - acc: 0.7612 - val_loss: 1.0225 - val_acc: 0.6457\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6424 - acc: 0.7401 - val_loss: 1.0243 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6166 - acc: 0.7561 - val_loss: 1.0300 - val_acc: 0.6400\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6366 - acc: 0.7292 - val_loss: 1.0264 - val_acc: 0.6286\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6532 - acc: 0.7439 - val_loss: 1.0245 - val_acc: 0.6343\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6456 - acc: 0.7331 - val_loss: 1.0249 - val_acc: 0.6400\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6344 - acc: 0.7369 - val_loss: 1.0264 - val_acc: 0.6400\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6273 - acc: 0.7529 - val_loss: 1.0313 - val_acc: 0.6400\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6298 - acc: 0.7567 - val_loss: 1.0298 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.6331 - acc: 0.7375 - val_loss: 1.0340 - val_acc: 0.6343\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.6370 - acc: 0.7561 - val_loss: 1.0412 - val_acc: 0.6343\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6309 - acc: 0.7503 - val_loss: 1.0368 - val_acc: 0.6457\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.6300 - acc: 0.7554 - val_loss: 1.0378 - val_acc: 0.6286\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6196 - acc: 0.7599 - val_loss: 1.0388 - val_acc: 0.6457\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6216 - acc: 0.7433 - val_loss: 1.0396 - val_acc: 0.6343\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6201 - acc: 0.7490 - val_loss: 1.0401 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6118 - acc: 0.7490 - val_loss: 1.0409 - val_acc: 0.6286\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.6232 - acc: 0.7542 - val_loss: 1.0402 - val_acc: 0.6400\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6276 - acc: 0.7573 - val_loss: 1.0421 - val_acc: 0.6457\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.6185 - acc: 0.7414 - val_loss: 1.0496 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 41us/step\n",
      "1741/1741 [==============================] - 0s 25us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 4s 3ms/step - loss: 1.6135 - acc: 0.2810 - val_loss: 1.3004 - val_acc: 0.4057\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.3529 - acc: 0.3768 - val_loss: 1.1574 - val_acc: 0.5429\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.2837 - acc: 0.4074 - val_loss: 1.0940 - val_acc: 0.6171\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.2104 - acc: 0.4508 - val_loss: 1.0517 - val_acc: 0.6400\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.1950 - acc: 0.4585 - val_loss: 1.0303 - val_acc: 0.6343\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.1618 - acc: 0.4879 - val_loss: 1.0099 - val_acc: 0.6343\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.1427 - acc: 0.4885 - val_loss: 0.9943 - val_acc: 0.6457\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.1008 - acc: 0.5217 - val_loss: 0.9804 - val_acc: 0.6514\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 1.1104 - acc: 0.5032 - val_loss: 0.9635 - val_acc: 0.6800\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.0607 - acc: 0.5364 - val_loss: 0.9552 - val_acc: 0.6800\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 1.0756 - acc: 0.5332 - val_loss: 0.9470 - val_acc: 0.6686\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0540 - acc: 0.5473 - val_loss: 0.9398 - val_acc: 0.6686\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0424 - acc: 0.5517 - val_loss: 0.9325 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0084 - acc: 0.5594 - val_loss: 0.9280 - val_acc: 0.6857\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9987 - acc: 0.5670 - val_loss: 0.9220 - val_acc: 0.6971\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.0144 - acc: 0.5594 - val_loss: 0.9221 - val_acc: 0.6857\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.0030 - acc: 0.5837 - val_loss: 0.9176 - val_acc: 0.6914\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9827 - acc: 0.5722 - val_loss: 0.9127 - val_acc: 0.7029\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9737 - acc: 0.5677 - val_loss: 0.9105 - val_acc: 0.6914\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9642 - acc: 0.5913 - val_loss: 0.9030 - val_acc: 0.6971\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9672 - acc: 0.5900 - val_loss: 0.9024 - val_acc: 0.7143\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9716 - acc: 0.5734 - val_loss: 0.8987 - val_acc: 0.6971\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.9449 - acc: 0.5843 - val_loss: 0.8968 - val_acc: 0.6971\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9356 - acc: 0.6117 - val_loss: 0.8938 - val_acc: 0.6914\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9551 - acc: 0.5958 - val_loss: 0.8904 - val_acc: 0.6800\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9502 - acc: 0.5862 - val_loss: 0.8907 - val_acc: 0.6914\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9352 - acc: 0.5862 - val_loss: 0.8854 - val_acc: 0.6914\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9302 - acc: 0.6079 - val_loss: 0.8843 - val_acc: 0.6914\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9170 - acc: 0.6137 - val_loss: 0.8870 - val_acc: 0.6857\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.9132 - acc: 0.6245 - val_loss: 0.8871 - val_acc: 0.6800\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9438 - acc: 0.6034 - val_loss: 0.8911 - val_acc: 0.6857\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9093 - acc: 0.6245 - val_loss: 0.8879 - val_acc: 0.6800\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 38us/step - loss: 0.9244 - acc: 0.6111 - val_loss: 0.8892 - val_acc: 0.6743\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.9240 - acc: 0.6060 - val_loss: 0.8885 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.9194 - acc: 0.6169 - val_loss: 0.8845 - val_acc: 0.6800\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9153 - acc: 0.6117 - val_loss: 0.8829 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8863 - acc: 0.6469 - val_loss: 0.8862 - val_acc: 0.6800\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8908 - acc: 0.6098 - val_loss: 0.8869 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8887 - acc: 0.6162 - val_loss: 0.8835 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8910 - acc: 0.6181 - val_loss: 0.8854 - val_acc: 0.6914\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8816 - acc: 0.6430 - val_loss: 0.8856 - val_acc: 0.6857\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8702 - acc: 0.6367 - val_loss: 0.8797 - val_acc: 0.6800\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8840 - acc: 0.6392 - val_loss: 0.8750 - val_acc: 0.6857\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8728 - acc: 0.6392 - val_loss: 0.8793 - val_acc: 0.6857\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8679 - acc: 0.6354 - val_loss: 0.8844 - val_acc: 0.6743\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8593 - acc: 0.6507 - val_loss: 0.8889 - val_acc: 0.6800\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8638 - acc: 0.6360 - val_loss: 0.8864 - val_acc: 0.6629\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8813 - acc: 0.6398 - val_loss: 0.8866 - val_acc: 0.6571\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8577 - acc: 0.6411 - val_loss: 0.8822 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8604 - acc: 0.6494 - val_loss: 0.8886 - val_acc: 0.6629\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8504 - acc: 0.6545 - val_loss: 0.8905 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8519 - acc: 0.6481 - val_loss: 0.8847 - val_acc: 0.6629\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8517 - acc: 0.6507 - val_loss: 0.8843 - val_acc: 0.6571\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8456 - acc: 0.6450 - val_loss: 0.8835 - val_acc: 0.6629\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8379 - acc: 0.6545 - val_loss: 0.8834 - val_acc: 0.6743\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8379 - acc: 0.6603 - val_loss: 0.8889 - val_acc: 0.6686\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8278 - acc: 0.6520 - val_loss: 0.8890 - val_acc: 0.6571\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 39us/step - loss: 0.8268 - acc: 0.6654 - val_loss: 0.8926 - val_acc: 0.6743\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8218 - acc: 0.6558 - val_loss: 0.8912 - val_acc: 0.6686\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8180 - acc: 0.6533 - val_loss: 0.8878 - val_acc: 0.6686\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8149 - acc: 0.6762 - val_loss: 0.8885 - val_acc: 0.6629\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8158 - acc: 0.6622 - val_loss: 0.8960 - val_acc: 0.6686\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8345 - acc: 0.6603 - val_loss: 0.8931 - val_acc: 0.6629\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8095 - acc: 0.6577 - val_loss: 0.8938 - val_acc: 0.6686\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8157 - acc: 0.6609 - val_loss: 0.8937 - val_acc: 0.6857\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8255 - acc: 0.6596 - val_loss: 0.8961 - val_acc: 0.6800\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7965 - acc: 0.6737 - val_loss: 0.8950 - val_acc: 0.6800\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8209 - acc: 0.6667 - val_loss: 0.8931 - val_acc: 0.6800\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8116 - acc: 0.6814 - val_loss: 0.8947 - val_acc: 0.6857\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8005 - acc: 0.6705 - val_loss: 0.9012 - val_acc: 0.6857\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8191 - acc: 0.6596 - val_loss: 0.8989 - val_acc: 0.6800\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8080 - acc: 0.6660 - val_loss: 0.8998 - val_acc: 0.6743\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8143 - acc: 0.6552 - val_loss: 0.8985 - val_acc: 0.6914\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7914 - acc: 0.6788 - val_loss: 0.8954 - val_acc: 0.6857\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7885 - acc: 0.6801 - val_loss: 0.9005 - val_acc: 0.6800\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7968 - acc: 0.6724 - val_loss: 0.8991 - val_acc: 0.6800\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7951 - acc: 0.6673 - val_loss: 0.9018 - val_acc: 0.6800\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7986 - acc: 0.6807 - val_loss: 0.9044 - val_acc: 0.6857\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7980 - acc: 0.6724 - val_loss: 0.9027 - val_acc: 0.6857\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7834 - acc: 0.6807 - val_loss: 0.9055 - val_acc: 0.6686\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7793 - acc: 0.6890 - val_loss: 0.9128 - val_acc: 0.6743\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7897 - acc: 0.6788 - val_loss: 0.9109 - val_acc: 0.6686\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7750 - acc: 0.6877 - val_loss: 0.9054 - val_acc: 0.6743\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7755 - acc: 0.6903 - val_loss: 0.9079 - val_acc: 0.6857\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7960 - acc: 0.6769 - val_loss: 0.9113 - val_acc: 0.6857\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7860 - acc: 0.6852 - val_loss: 0.9068 - val_acc: 0.6629\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7640 - acc: 0.6916 - val_loss: 0.9101 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7628 - acc: 0.6916 - val_loss: 0.9209 - val_acc: 0.6629\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7467 - acc: 0.6967 - val_loss: 0.9139 - val_acc: 0.6686\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7730 - acc: 0.6782 - val_loss: 0.9092 - val_acc: 0.6743\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7661 - acc: 0.6909 - val_loss: 0.9051 - val_acc: 0.6800\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7683 - acc: 0.7018 - val_loss: 0.9060 - val_acc: 0.6914\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7539 - acc: 0.6973 - val_loss: 0.9075 - val_acc: 0.6629\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7780 - acc: 0.6865 - val_loss: 0.9070 - val_acc: 0.6743\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7626 - acc: 0.6769 - val_loss: 0.9072 - val_acc: 0.6857\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7776 - acc: 0.6794 - val_loss: 0.9089 - val_acc: 0.6800\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7397 - acc: 0.7005 - val_loss: 0.9106 - val_acc: 0.6686\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7472 - acc: 0.6992 - val_loss: 0.9113 - val_acc: 0.6629\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7447 - acc: 0.6890 - val_loss: 0.9106 - val_acc: 0.6914\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7581 - acc: 0.6967 - val_loss: 0.9145 - val_acc: 0.6743\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7705 - acc: 0.6877 - val_loss: 0.9166 - val_acc: 0.6800\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7316 - acc: 0.7075 - val_loss: 0.9170 - val_acc: 0.6800\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7545 - acc: 0.6890 - val_loss: 0.9192 - val_acc: 0.6971\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7466 - acc: 0.6909 - val_loss: 0.9194 - val_acc: 0.6800\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7407 - acc: 0.7063 - val_loss: 0.9313 - val_acc: 0.6686\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7332 - acc: 0.7011 - val_loss: 0.9335 - val_acc: 0.6800\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7393 - acc: 0.7082 - val_loss: 0.9238 - val_acc: 0.6800\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7513 - acc: 0.6916 - val_loss: 0.9239 - val_acc: 0.6629\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7425 - acc: 0.6992 - val_loss: 0.9307 - val_acc: 0.6686\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7285 - acc: 0.6948 - val_loss: 0.9318 - val_acc: 0.6571\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7320 - acc: 0.7024 - val_loss: 0.9304 - val_acc: 0.6571\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7290 - acc: 0.6916 - val_loss: 0.9327 - val_acc: 0.6514\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7247 - acc: 0.7037 - val_loss: 0.9297 - val_acc: 0.6629\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7316 - acc: 0.7133 - val_loss: 0.9363 - val_acc: 0.6800\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7148 - acc: 0.7043 - val_loss: 0.9380 - val_acc: 0.6686\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7212 - acc: 0.7024 - val_loss: 0.9389 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7154 - acc: 0.6999 - val_loss: 0.9423 - val_acc: 0.6686\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7159 - acc: 0.7043 - val_loss: 0.9456 - val_acc: 0.6800\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7174 - acc: 0.6999 - val_loss: 0.9448 - val_acc: 0.6743\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7223 - acc: 0.7031 - val_loss: 0.9431 - val_acc: 0.6857\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7216 - acc: 0.6967 - val_loss: 0.9391 - val_acc: 0.6629\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7095 - acc: 0.7082 - val_loss: 0.9422 - val_acc: 0.6743\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6867 - acc: 0.7158 - val_loss: 0.9457 - val_acc: 0.6571\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7077 - acc: 0.7050 - val_loss: 0.9450 - val_acc: 0.6686\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7051 - acc: 0.7299 - val_loss: 0.9397 - val_acc: 0.6629\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7032 - acc: 0.7120 - val_loss: 0.9395 - val_acc: 0.6629\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7291 - acc: 0.6960 - val_loss: 0.9465 - val_acc: 0.6457\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7063 - acc: 0.7133 - val_loss: 0.9474 - val_acc: 0.6457\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6976 - acc: 0.7165 - val_loss: 0.9496 - val_acc: 0.6400\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7036 - acc: 0.7120 - val_loss: 0.9499 - val_acc: 0.6629\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6989 - acc: 0.7222 - val_loss: 0.9428 - val_acc: 0.6571\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6925 - acc: 0.7235 - val_loss: 0.9460 - val_acc: 0.6571\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6974 - acc: 0.7299 - val_loss: 0.9485 - val_acc: 0.6457\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7045 - acc: 0.7075 - val_loss: 0.9509 - val_acc: 0.6571\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7044 - acc: 0.6960 - val_loss: 0.9518 - val_acc: 0.6571\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7011 - acc: 0.7120 - val_loss: 0.9526 - val_acc: 0.6629\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6800 - acc: 0.7292 - val_loss: 0.9516 - val_acc: 0.6629\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7022 - acc: 0.7133 - val_loss: 0.9506 - val_acc: 0.6629\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6932 - acc: 0.7165 - val_loss: 0.9620 - val_acc: 0.6571\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6925 - acc: 0.7203 - val_loss: 0.9522 - val_acc: 0.6514\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6767 - acc: 0.7331 - val_loss: 0.9465 - val_acc: 0.6571\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6856 - acc: 0.7203 - val_loss: 0.9593 - val_acc: 0.6571\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6744 - acc: 0.7305 - val_loss: 0.9618 - val_acc: 0.6629\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6724 - acc: 0.7299 - val_loss: 0.9528 - val_acc: 0.6571\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6845 - acc: 0.7267 - val_loss: 0.9591 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6786 - acc: 0.7312 - val_loss: 0.9684 - val_acc: 0.6457\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6682 - acc: 0.7305 - val_loss: 0.9663 - val_acc: 0.6457\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6689 - acc: 0.7229 - val_loss: 0.9700 - val_acc: 0.6514\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6741 - acc: 0.7261 - val_loss: 0.9704 - val_acc: 0.6457\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6641 - acc: 0.7382 - val_loss: 0.9708 - val_acc: 0.6571\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6881 - acc: 0.7158 - val_loss: 0.9748 - val_acc: 0.6457\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6686 - acc: 0.7216 - val_loss: 0.9691 - val_acc: 0.6629\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6881 - acc: 0.7222 - val_loss: 0.9685 - val_acc: 0.6629\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6592 - acc: 0.7254 - val_loss: 0.9770 - val_acc: 0.6514\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6703 - acc: 0.7216 - val_loss: 0.9706 - val_acc: 0.6514\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6662 - acc: 0.7222 - val_loss: 0.9678 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6662 - acc: 0.7375 - val_loss: 0.9813 - val_acc: 0.6457\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6662 - acc: 0.7344 - val_loss: 0.9821 - val_acc: 0.6457\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6651 - acc: 0.7241 - val_loss: 0.9656 - val_acc: 0.6400\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6538 - acc: 0.7414 - val_loss: 0.9687 - val_acc: 0.6514\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6389 - acc: 0.7401 - val_loss: 0.9688 - val_acc: 0.6457\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6407 - acc: 0.7401 - val_loss: 0.9763 - val_acc: 0.6514\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6756 - acc: 0.7171 - val_loss: 0.9820 - val_acc: 0.6743\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6482 - acc: 0.7395 - val_loss: 0.9766 - val_acc: 0.6571\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6507 - acc: 0.7478 - val_loss: 0.9849 - val_acc: 0.6457\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6734 - acc: 0.7203 - val_loss: 0.9874 - val_acc: 0.6514\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6598 - acc: 0.7375 - val_loss: 0.9931 - val_acc: 0.6514\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6488 - acc: 0.7344 - val_loss: 0.9863 - val_acc: 0.6514\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6443 - acc: 0.7382 - val_loss: 0.9856 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6753 - acc: 0.7203 - val_loss: 0.9887 - val_acc: 0.6514\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6571 - acc: 0.7318 - val_loss: 0.9859 - val_acc: 0.6514\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6359 - acc: 0.7407 - val_loss: 0.9885 - val_acc: 0.6457\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6403 - acc: 0.7312 - val_loss: 0.9843 - val_acc: 0.6629\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6373 - acc: 0.7497 - val_loss: 0.9820 - val_acc: 0.6514\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6467 - acc: 0.7312 - val_loss: 0.9842 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6332 - acc: 0.7401 - val_loss: 0.9814 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6262 - acc: 0.7497 - val_loss: 0.9905 - val_acc: 0.6514\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6113 - acc: 0.7535 - val_loss: 1.0018 - val_acc: 0.6514\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6304 - acc: 0.7529 - val_loss: 0.9984 - val_acc: 0.6571\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6198 - acc: 0.7503 - val_loss: 1.0021 - val_acc: 0.6571\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6234 - acc: 0.7484 - val_loss: 0.9993 - val_acc: 0.6457\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6299 - acc: 0.7497 - val_loss: 1.0036 - val_acc: 0.6514\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.6118 - acc: 0.7484 - val_loss: 1.0004 - val_acc: 0.6629\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6069 - acc: 0.7618 - val_loss: 1.0013 - val_acc: 0.6514\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6273 - acc: 0.7535 - val_loss: 0.9993 - val_acc: 0.6514\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.6177 - acc: 0.7484 - val_loss: 1.0028 - val_acc: 0.6457\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6206 - acc: 0.7465 - val_loss: 1.0096 - val_acc: 0.6514\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6292 - acc: 0.7350 - val_loss: 0.9991 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6199 - acc: 0.7478 - val_loss: 1.0071 - val_acc: 0.6514\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6295 - acc: 0.7452 - val_loss: 1.0180 - val_acc: 0.6457\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6043 - acc: 0.7529 - val_loss: 1.0011 - val_acc: 0.6629\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6216 - acc: 0.7503 - val_loss: 1.0096 - val_acc: 0.6514\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6130 - acc: 0.7497 - val_loss: 1.0224 - val_acc: 0.6514\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6035 - acc: 0.7548 - val_loss: 1.0209 - val_acc: 0.6400\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5985 - acc: 0.7599 - val_loss: 1.0195 - val_acc: 0.6629\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6147 - acc: 0.7465 - val_loss: 1.0216 - val_acc: 0.6571\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6050 - acc: 0.7490 - val_loss: 1.0279 - val_acc: 0.6571\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6188 - acc: 0.7522 - val_loss: 1.0132 - val_acc: 0.6457\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6207 - acc: 0.7490 - val_loss: 1.0142 - val_acc: 0.6514\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6247 - acc: 0.7471 - val_loss: 1.0252 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 60us/step\n",
      "1741/1741 [==============================] - 0s 50us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 4s 3ms/step - loss: 1.7346 - acc: 0.2578 - val_loss: 1.4363 - val_acc: 0.2743\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.4625 - acc: 0.3306 - val_loss: 1.2351 - val_acc: 0.4571\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.3113 - acc: 0.4020 - val_loss: 1.1329 - val_acc: 0.5600\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.2352 - acc: 0.4601 - val_loss: 1.0778 - val_acc: 0.6057\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1632 - acc: 0.4863 - val_loss: 1.0465 - val_acc: 0.6286\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1560 - acc: 0.4863 - val_loss: 1.0206 - val_acc: 0.6229\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1236 - acc: 0.5080 - val_loss: 1.0055 - val_acc: 0.6171\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 1.1032 - acc: 0.5150 - val_loss: 0.9953 - val_acc: 0.6343\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 1.0786 - acc: 0.5380 - val_loss: 0.9851 - val_acc: 0.6514\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 1.0808 - acc: 0.5399 - val_loss: 0.9794 - val_acc: 0.6514\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0594 - acc: 0.5341 - val_loss: 0.9719 - val_acc: 0.6571\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0661 - acc: 0.5310 - val_loss: 0.9634 - val_acc: 0.6571\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 1.0333 - acc: 0.5475 - val_loss: 0.9603 - val_acc: 0.6514\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0459 - acc: 0.5431 - val_loss: 0.9535 - val_acc: 0.6457\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0275 - acc: 0.5539 - val_loss: 0.9444 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0113 - acc: 0.5833 - val_loss: 0.9396 - val_acc: 0.6686\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.9927 - acc: 0.5699 - val_loss: 0.9337 - val_acc: 0.6571\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0095 - acc: 0.5590 - val_loss: 0.9307 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9924 - acc: 0.5769 - val_loss: 0.9286 - val_acc: 0.6571\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9894 - acc: 0.5654 - val_loss: 0.9281 - val_acc: 0.6800\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.9863 - acc: 0.5820 - val_loss: 0.9282 - val_acc: 0.6629\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.9695 - acc: 0.5986 - val_loss: 0.9286 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9457 - acc: 0.5890 - val_loss: 0.9229 - val_acc: 0.6629\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9547 - acc: 0.6056 - val_loss: 0.9222 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9731 - acc: 0.5884 - val_loss: 0.9189 - val_acc: 0.6686\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9503 - acc: 0.6069 - val_loss: 0.9176 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9428 - acc: 0.6031 - val_loss: 0.9141 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9317 - acc: 0.6075 - val_loss: 0.9149 - val_acc: 0.6743\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9252 - acc: 0.6158 - val_loss: 0.9129 - val_acc: 0.6743\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.9302 - acc: 0.6177 - val_loss: 0.9141 - val_acc: 0.6743\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.9356 - acc: 0.5992 - val_loss: 0.9139 - val_acc: 0.6800\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.9109 - acc: 0.6254 - val_loss: 0.9153 - val_acc: 0.6743\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9278 - acc: 0.6088 - val_loss: 0.9135 - val_acc: 0.6686\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.9225 - acc: 0.6082 - val_loss: 0.9072 - val_acc: 0.6686\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8942 - acc: 0.6209 - val_loss: 0.9058 - val_acc: 0.6857\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8940 - acc: 0.6407 - val_loss: 0.9073 - val_acc: 0.6800\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8876 - acc: 0.6228 - val_loss: 0.9098 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.9124 - acc: 0.6094 - val_loss: 0.9090 - val_acc: 0.6743\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9078 - acc: 0.6114 - val_loss: 0.9031 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8845 - acc: 0.6471 - val_loss: 0.9033 - val_acc: 0.6743\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8877 - acc: 0.6350 - val_loss: 0.9009 - val_acc: 0.6800\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8933 - acc: 0.6292 - val_loss: 0.8974 - val_acc: 0.6800\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.8835 - acc: 0.6350 - val_loss: 0.8944 - val_acc: 0.6914\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9000 - acc: 0.6216 - val_loss: 0.8985 - val_acc: 0.6914\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8790 - acc: 0.6331 - val_loss: 0.8986 - val_acc: 0.6857\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8637 - acc: 0.6331 - val_loss: 0.9044 - val_acc: 0.6857\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8642 - acc: 0.6522 - val_loss: 0.9109 - val_acc: 0.6914\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8686 - acc: 0.6350 - val_loss: 0.9075 - val_acc: 0.6971\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8648 - acc: 0.6433 - val_loss: 0.9037 - val_acc: 0.6800\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8691 - acc: 0.6337 - val_loss: 0.9056 - val_acc: 0.6800\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8630 - acc: 0.6445 - val_loss: 0.9094 - val_acc: 0.6914\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8615 - acc: 0.6554 - val_loss: 0.9144 - val_acc: 0.6800\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8627 - acc: 0.6324 - val_loss: 0.9108 - val_acc: 0.6800\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8742 - acc: 0.6433 - val_loss: 0.9041 - val_acc: 0.6914\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8514 - acc: 0.6509 - val_loss: 0.9033 - val_acc: 0.6914\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8622 - acc: 0.6445 - val_loss: 0.9041 - val_acc: 0.6686\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8477 - acc: 0.6331 - val_loss: 0.9078 - val_acc: 0.6800\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8319 - acc: 0.6560 - val_loss: 0.9032 - val_acc: 0.6800\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8343 - acc: 0.6535 - val_loss: 0.9073 - val_acc: 0.6743\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8275 - acc: 0.6560 - val_loss: 0.9009 - val_acc: 0.6914\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8179 - acc: 0.6579 - val_loss: 0.9034 - val_acc: 0.6914\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8396 - acc: 0.6420 - val_loss: 0.9029 - val_acc: 0.6800\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8285 - acc: 0.6611 - val_loss: 0.9064 - val_acc: 0.7029\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8162 - acc: 0.6694 - val_loss: 0.9087 - val_acc: 0.6971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8454 - acc: 0.6605 - val_loss: 0.9098 - val_acc: 0.7029\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8120 - acc: 0.6631 - val_loss: 0.9070 - val_acc: 0.6971\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8323 - acc: 0.6669 - val_loss: 0.9092 - val_acc: 0.6971\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8176 - acc: 0.6701 - val_loss: 0.9170 - val_acc: 0.6914\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8259 - acc: 0.6771 - val_loss: 0.9119 - val_acc: 0.6971\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8130 - acc: 0.6752 - val_loss: 0.9145 - val_acc: 0.6857\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8268 - acc: 0.6656 - val_loss: 0.9106 - val_acc: 0.6971\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8144 - acc: 0.6599 - val_loss: 0.9005 - val_acc: 0.6743\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7918 - acc: 0.6854 - val_loss: 0.9074 - val_acc: 0.6857\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8117 - acc: 0.6669 - val_loss: 0.9120 - val_acc: 0.6914\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7900 - acc: 0.6733 - val_loss: 0.9129 - val_acc: 0.6800\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8070 - acc: 0.6816 - val_loss: 0.9122 - val_acc: 0.6800\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7966 - acc: 0.6765 - val_loss: 0.9126 - val_acc: 0.6857\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8142 - acc: 0.6701 - val_loss: 0.9171 - val_acc: 0.6914\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8141 - acc: 0.6637 - val_loss: 0.9172 - val_acc: 0.6743\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7793 - acc: 0.6809 - val_loss: 0.9129 - val_acc: 0.6743\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7886 - acc: 0.6752 - val_loss: 0.9154 - val_acc: 0.6800\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7773 - acc: 0.6790 - val_loss: 0.9174 - val_acc: 0.6800\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7915 - acc: 0.6784 - val_loss: 0.9230 - val_acc: 0.6571\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7751 - acc: 0.6886 - val_loss: 0.9213 - val_acc: 0.6629\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8002 - acc: 0.6784 - val_loss: 0.9166 - val_acc: 0.6571\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7756 - acc: 0.6765 - val_loss: 0.9162 - val_acc: 0.6686\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7804 - acc: 0.6822 - val_loss: 0.9256 - val_acc: 0.6743\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7697 - acc: 0.6892 - val_loss: 0.9283 - val_acc: 0.6629\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7769 - acc: 0.6841 - val_loss: 0.9268 - val_acc: 0.6800\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7846 - acc: 0.6828 - val_loss: 0.9284 - val_acc: 0.6800\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7574 - acc: 0.6873 - val_loss: 0.9244 - val_acc: 0.6800\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7836 - acc: 0.6688 - val_loss: 0.9250 - val_acc: 0.6629\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7757 - acc: 0.6656 - val_loss: 0.9187 - val_acc: 0.6514\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7632 - acc: 0.6816 - val_loss: 0.9264 - val_acc: 0.6457\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7743 - acc: 0.6879 - val_loss: 0.9342 - val_acc: 0.6514\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7605 - acc: 0.6905 - val_loss: 0.9247 - val_acc: 0.6514\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7556 - acc: 0.6886 - val_loss: 0.9259 - val_acc: 0.6800\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7680 - acc: 0.6950 - val_loss: 0.9348 - val_acc: 0.6629\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7426 - acc: 0.6962 - val_loss: 0.9311 - val_acc: 0.6686\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7558 - acc: 0.6918 - val_loss: 0.9321 - val_acc: 0.6914\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7658 - acc: 0.6918 - val_loss: 0.9353 - val_acc: 0.6686\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7321 - acc: 0.7013 - val_loss: 0.9333 - val_acc: 0.6571\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7599 - acc: 0.6918 - val_loss: 0.9352 - val_acc: 0.6686\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7551 - acc: 0.6937 - val_loss: 0.9450 - val_acc: 0.6629\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7323 - acc: 0.7064 - val_loss: 0.9476 - val_acc: 0.6571\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7561 - acc: 0.6841 - val_loss: 0.9408 - val_acc: 0.6629\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7490 - acc: 0.6816 - val_loss: 0.9400 - val_acc: 0.6686\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7597 - acc: 0.6777 - val_loss: 0.9440 - val_acc: 0.6629\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7340 - acc: 0.7026 - val_loss: 0.9487 - val_acc: 0.6629\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7353 - acc: 0.7109 - val_loss: 0.9390 - val_acc: 0.6514\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7407 - acc: 0.6905 - val_loss: 0.9451 - val_acc: 0.6629\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7345 - acc: 0.7013 - val_loss: 0.9507 - val_acc: 0.6571\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7353 - acc: 0.6956 - val_loss: 0.9487 - val_acc: 0.6743\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7280 - acc: 0.7084 - val_loss: 0.9485 - val_acc: 0.6686\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7351 - acc: 0.6937 - val_loss: 0.9534 - val_acc: 0.6571\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7256 - acc: 0.6969 - val_loss: 0.9531 - val_acc: 0.6629\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7311 - acc: 0.7001 - val_loss: 0.9456 - val_acc: 0.6629\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7420 - acc: 0.7007 - val_loss: 0.9449 - val_acc: 0.6686\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7372 - acc: 0.6956 - val_loss: 0.9556 - val_acc: 0.6686\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7216 - acc: 0.6937 - val_loss: 0.9477 - val_acc: 0.6743\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7102 - acc: 0.7096 - val_loss: 0.9464 - val_acc: 0.6800\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7375 - acc: 0.6905 - val_loss: 0.9538 - val_acc: 0.6686\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7186 - acc: 0.7179 - val_loss: 0.9617 - val_acc: 0.6686\n",
      "Epoch 124/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7278 - acc: 0.6956 - val_loss: 0.9614 - val_acc: 0.6629\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7141 - acc: 0.7103 - val_loss: 0.9510 - val_acc: 0.6743\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.6983 - acc: 0.7173 - val_loss: 0.9577 - val_acc: 0.6686\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7260 - acc: 0.7058 - val_loss: 0.9604 - val_acc: 0.6686\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7194 - acc: 0.7020 - val_loss: 0.9630 - val_acc: 0.6629\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7133 - acc: 0.7135 - val_loss: 0.9594 - val_acc: 0.6743\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6941 - acc: 0.7090 - val_loss: 0.9651 - val_acc: 0.6629\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7100 - acc: 0.7026 - val_loss: 0.9671 - val_acc: 0.6629\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6959 - acc: 0.7192 - val_loss: 0.9710 - val_acc: 0.6629\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7109 - acc: 0.6962 - val_loss: 0.9763 - val_acc: 0.6743\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.6981 - acc: 0.7179 - val_loss: 0.9738 - val_acc: 0.6629\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7213 - acc: 0.6924 - val_loss: 0.9713 - val_acc: 0.6686\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7028 - acc: 0.7135 - val_loss: 0.9642 - val_acc: 0.6629\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6989 - acc: 0.7052 - val_loss: 0.9674 - val_acc: 0.6743\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6944 - acc: 0.7071 - val_loss: 0.9768 - val_acc: 0.6629\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6886 - acc: 0.7109 - val_loss: 0.9769 - val_acc: 0.6743\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6766 - acc: 0.7160 - val_loss: 0.9803 - val_acc: 0.6629\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.6903 - acc: 0.7320 - val_loss: 0.9818 - val_acc: 0.6571\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.6955 - acc: 0.7218 - val_loss: 0.9780 - val_acc: 0.6629\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7024 - acc: 0.7160 - val_loss: 0.9883 - val_acc: 0.6629\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6792 - acc: 0.7352 - val_loss: 0.9921 - val_acc: 0.6686\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6846 - acc: 0.7205 - val_loss: 0.9931 - val_acc: 0.6743\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6760 - acc: 0.7307 - val_loss: 0.9840 - val_acc: 0.6571\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6691 - acc: 0.7211 - val_loss: 0.9875 - val_acc: 0.6571\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6805 - acc: 0.7237 - val_loss: 0.9902 - val_acc: 0.6571\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6761 - acc: 0.7224 - val_loss: 1.0011 - val_acc: 0.6629\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6932 - acc: 0.7262 - val_loss: 0.9937 - val_acc: 0.6686\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6643 - acc: 0.7320 - val_loss: 0.9889 - val_acc: 0.6686\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6714 - acc: 0.7313 - val_loss: 0.9956 - val_acc: 0.6686\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6750 - acc: 0.7262 - val_loss: 0.9994 - val_acc: 0.6571\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6659 - acc: 0.7205 - val_loss: 1.0003 - val_acc: 0.6686\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6718 - acc: 0.7262 - val_loss: 1.0023 - val_acc: 0.6629\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6690 - acc: 0.7250 - val_loss: 1.0045 - val_acc: 0.6629\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6670 - acc: 0.7288 - val_loss: 1.0038 - val_acc: 0.6571\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6670 - acc: 0.7371 - val_loss: 1.0045 - val_acc: 0.6457\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6577 - acc: 0.7364 - val_loss: 1.0063 - val_acc: 0.6514\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6414 - acc: 0.7313 - val_loss: 0.9992 - val_acc: 0.6514\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6441 - acc: 0.7479 - val_loss: 1.0024 - val_acc: 0.6400\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6557 - acc: 0.7224 - val_loss: 1.0109 - val_acc: 0.6343\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6627 - acc: 0.7301 - val_loss: 1.0076 - val_acc: 0.6343\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6709 - acc: 0.7243 - val_loss: 1.0103 - val_acc: 0.6571\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6837 - acc: 0.7116 - val_loss: 1.0192 - val_acc: 0.6571\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6628 - acc: 0.7294 - val_loss: 1.0138 - val_acc: 0.6571\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6436 - acc: 0.7326 - val_loss: 1.0143 - val_acc: 0.6629\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6634 - acc: 0.7326 - val_loss: 1.0104 - val_acc: 0.6629\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6589 - acc: 0.7294 - val_loss: 1.0169 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6471 - acc: 0.7447 - val_loss: 1.0272 - val_acc: 0.6514\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6331 - acc: 0.7447 - val_loss: 1.0151 - val_acc: 0.6571\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6413 - acc: 0.7352 - val_loss: 1.0189 - val_acc: 0.6514\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.6589 - acc: 0.7415 - val_loss: 1.0163 - val_acc: 0.6514\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6444 - acc: 0.7505 - val_loss: 1.0131 - val_acc: 0.6571\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6508 - acc: 0.7390 - val_loss: 1.0195 - val_acc: 0.6514\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6331 - acc: 0.7473 - val_loss: 1.0306 - val_acc: 0.6571\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6362 - acc: 0.7332 - val_loss: 1.0302 - val_acc: 0.6514\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6361 - acc: 0.7403 - val_loss: 1.0134 - val_acc: 0.6514\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6414 - acc: 0.7320 - val_loss: 1.0213 - val_acc: 0.6457\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6479 - acc: 0.7435 - val_loss: 1.0262 - val_acc: 0.6514\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6367 - acc: 0.7441 - val_loss: 1.0226 - val_acc: 0.6514\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6235 - acc: 0.7530 - val_loss: 1.0306 - val_acc: 0.6514\n",
      "Epoch 183/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6123 - acc: 0.7479 - val_loss: 1.0356 - val_acc: 0.6400\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6122 - acc: 0.7473 - val_loss: 1.0357 - val_acc: 0.6514\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6086 - acc: 0.7486 - val_loss: 1.0358 - val_acc: 0.6514\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6248 - acc: 0.7435 - val_loss: 1.0373 - val_acc: 0.6571\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6379 - acc: 0.7428 - val_loss: 1.0223 - val_acc: 0.6457\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6179 - acc: 0.7511 - val_loss: 1.0253 - val_acc: 0.6457\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6243 - acc: 0.7556 - val_loss: 1.0313 - val_acc: 0.6457\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6219 - acc: 0.7473 - val_loss: 1.0381 - val_acc: 0.6514\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6326 - acc: 0.7473 - val_loss: 1.0481 - val_acc: 0.6457\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6294 - acc: 0.7498 - val_loss: 1.0503 - val_acc: 0.6400\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6243 - acc: 0.7473 - val_loss: 1.0427 - val_acc: 0.6457\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5848 - acc: 0.7620 - val_loss: 1.0400 - val_acc: 0.6514\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6232 - acc: 0.7396 - val_loss: 1.0448 - val_acc: 0.6514\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6159 - acc: 0.7530 - val_loss: 1.0468 - val_acc: 0.6457\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6185 - acc: 0.7498 - val_loss: 1.0547 - val_acc: 0.6514\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6063 - acc: 0.7524 - val_loss: 1.0584 - val_acc: 0.6457\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6155 - acc: 0.7537 - val_loss: 1.0627 - val_acc: 0.6400\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6191 - acc: 0.7441 - val_loss: 1.0505 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 40us/step\n",
      "1742/1742 [==============================] - 0s 25us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.5657 - acc: 0.2827 - val_loss: 1.2780 - val_acc: 0.4171\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 1.3731 - acc: 0.3580 - val_loss: 1.1743 - val_acc: 0.5086\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 1.2826 - acc: 0.4110 - val_loss: 1.1155 - val_acc: 0.5771\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.2451 - acc: 0.4327 - val_loss: 1.0804 - val_acc: 0.6114\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 1.1718 - acc: 0.4812 - val_loss: 1.0462 - val_acc: 0.6171\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 1.1792 - acc: 0.4703 - val_loss: 1.0240 - val_acc: 0.6286\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.1521 - acc: 0.4895 - val_loss: 1.0083 - val_acc: 0.6343\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1137 - acc: 0.5156 - val_loss: 0.9944 - val_acc: 0.6343\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.1125 - acc: 0.4997 - val_loss: 0.9836 - val_acc: 0.6286\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 1.0796 - acc: 0.5297 - val_loss: 0.9713 - val_acc: 0.6171\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0600 - acc: 0.5329 - val_loss: 0.9607 - val_acc: 0.6343\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.0480 - acc: 0.5539 - val_loss: 0.9497 - val_acc: 0.6286\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 1.0258 - acc: 0.5782 - val_loss: 0.9429 - val_acc: 0.6171\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.0437 - acc: 0.5418 - val_loss: 0.9385 - val_acc: 0.6286\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.0156 - acc: 0.5597 - val_loss: 0.9329 - val_acc: 0.6171\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0179 - acc: 0.5539 - val_loss: 0.9303 - val_acc: 0.6400\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9988 - acc: 0.5775 - val_loss: 0.9214 - val_acc: 0.6400\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9817 - acc: 0.5820 - val_loss: 0.9152 - val_acc: 0.6400\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9922 - acc: 0.5814 - val_loss: 0.9133 - val_acc: 0.6343\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9644 - acc: 0.5954 - val_loss: 0.9048 - val_acc: 0.6286\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9713 - acc: 0.5820 - val_loss: 0.9026 - val_acc: 0.6286\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.9537 - acc: 0.5941 - val_loss: 0.8957 - val_acc: 0.6457\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9671 - acc: 0.5922 - val_loss: 0.8936 - val_acc: 0.6571\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9621 - acc: 0.5986 - val_loss: 0.8950 - val_acc: 0.6514\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9552 - acc: 0.6018 - val_loss: 0.8951 - val_acc: 0.6629\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9244 - acc: 0.6171 - val_loss: 0.8955 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9427 - acc: 0.6011 - val_loss: 0.8857 - val_acc: 0.6286\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.9176 - acc: 0.6152 - val_loss: 0.8855 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.9186 - acc: 0.6248 - val_loss: 0.8818 - val_acc: 0.6514\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.9051 - acc: 0.6337 - val_loss: 0.8836 - val_acc: 0.6629\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.9096 - acc: 0.6184 - val_loss: 0.8893 - val_acc: 0.6629\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9060 - acc: 0.6158 - val_loss: 0.8872 - val_acc: 0.6514\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8970 - acc: 0.6292 - val_loss: 0.8782 - val_acc: 0.6514\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8994 - acc: 0.6292 - val_loss: 0.8733 - val_acc: 0.6686\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9010 - acc: 0.6216 - val_loss: 0.8724 - val_acc: 0.6571\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8848 - acc: 0.6165 - val_loss: 0.8741 - val_acc: 0.6629\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8877 - acc: 0.6203 - val_loss: 0.8742 - val_acc: 0.6686\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8830 - acc: 0.6292 - val_loss: 0.8776 - val_acc: 0.6629\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8758 - acc: 0.6388 - val_loss: 0.8781 - val_acc: 0.6571\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8858 - acc: 0.6267 - val_loss: 0.8783 - val_acc: 0.6514\n",
      "Epoch 41/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8773 - acc: 0.6458 - val_loss: 0.8826 - val_acc: 0.6629\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8709 - acc: 0.6209 - val_loss: 0.8763 - val_acc: 0.6514\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8587 - acc: 0.6503 - val_loss: 0.8793 - val_acc: 0.6571\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8664 - acc: 0.6420 - val_loss: 0.8786 - val_acc: 0.6571\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8762 - acc: 0.6388 - val_loss: 0.8786 - val_acc: 0.6686\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8498 - acc: 0.6522 - val_loss: 0.8702 - val_acc: 0.6571\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8707 - acc: 0.6343 - val_loss: 0.8683 - val_acc: 0.6571\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8629 - acc: 0.6414 - val_loss: 0.8712 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8670 - acc: 0.6465 - val_loss: 0.8761 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8466 - acc: 0.6528 - val_loss: 0.8740 - val_acc: 0.6571\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8326 - acc: 0.6554 - val_loss: 0.8724 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8303 - acc: 0.6650 - val_loss: 0.8765 - val_acc: 0.6571\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8339 - acc: 0.6414 - val_loss: 0.8736 - val_acc: 0.6571\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8444 - acc: 0.6637 - val_loss: 0.8745 - val_acc: 0.6629\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8328 - acc: 0.6535 - val_loss: 0.8729 - val_acc: 0.6686\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8296 - acc: 0.6637 - val_loss: 0.8740 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8342 - acc: 0.6541 - val_loss: 0.8752 - val_acc: 0.6686\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8191 - acc: 0.6720 - val_loss: 0.8725 - val_acc: 0.6629\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8245 - acc: 0.6643 - val_loss: 0.8711 - val_acc: 0.6571\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8040 - acc: 0.6713 - val_loss: 0.8747 - val_acc: 0.6571\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8275 - acc: 0.6650 - val_loss: 0.8734 - val_acc: 0.6571\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8146 - acc: 0.6490 - val_loss: 0.8720 - val_acc: 0.6571\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8010 - acc: 0.6758 - val_loss: 0.8709 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8230 - acc: 0.6713 - val_loss: 0.8738 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8176 - acc: 0.6669 - val_loss: 0.8695 - val_acc: 0.6571\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7957 - acc: 0.6713 - val_loss: 0.8671 - val_acc: 0.6629\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8172 - acc: 0.6675 - val_loss: 0.8757 - val_acc: 0.6686\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8082 - acc: 0.6631 - val_loss: 0.8745 - val_acc: 0.6571\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7968 - acc: 0.6911 - val_loss: 0.8696 - val_acc: 0.6514\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.8195 - acc: 0.6579 - val_loss: 0.8684 - val_acc: 0.6571\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7946 - acc: 0.6662 - val_loss: 0.8707 - val_acc: 0.6629\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7994 - acc: 0.6694 - val_loss: 0.8724 - val_acc: 0.6686\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7895 - acc: 0.6701 - val_loss: 0.8677 - val_acc: 0.6800\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7954 - acc: 0.6682 - val_loss: 0.8658 - val_acc: 0.6571\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7893 - acc: 0.6765 - val_loss: 0.8679 - val_acc: 0.6686\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7793 - acc: 0.6835 - val_loss: 0.8725 - val_acc: 0.6686\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8032 - acc: 0.6720 - val_loss: 0.8731 - val_acc: 0.6686\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.7900 - acc: 0.6765 - val_loss: 0.8700 - val_acc: 0.6571\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.7725 - acc: 0.6899 - val_loss: 0.8737 - val_acc: 0.6571\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7785 - acc: 0.6816 - val_loss: 0.8706 - val_acc: 0.6629\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.7768 - acc: 0.6835 - val_loss: 0.8751 - val_acc: 0.6514\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.7982 - acc: 0.6618 - val_loss: 0.8805 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7761 - acc: 0.6899 - val_loss: 0.8813 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7846 - acc: 0.6771 - val_loss: 0.8794 - val_acc: 0.6571\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7791 - acc: 0.6816 - val_loss: 0.8799 - val_acc: 0.6571\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7491 - acc: 0.6981 - val_loss: 0.8740 - val_acc: 0.6629\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7412 - acc: 0.6956 - val_loss: 0.8741 - val_acc: 0.6571\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.7822 - acc: 0.6701 - val_loss: 0.8753 - val_acc: 0.6514\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7778 - acc: 0.6771 - val_loss: 0.8719 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7692 - acc: 0.6892 - val_loss: 0.8777 - val_acc: 0.6629\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7523 - acc: 0.6841 - val_loss: 0.8818 - val_acc: 0.6629\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7673 - acc: 0.6892 - val_loss: 0.8760 - val_acc: 0.6457\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7443 - acc: 0.6950 - val_loss: 0.8746 - val_acc: 0.6571\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7569 - acc: 0.6969 - val_loss: 0.8734 - val_acc: 0.6514\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7621 - acc: 0.6873 - val_loss: 0.8730 - val_acc: 0.6514\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7393 - acc: 0.6937 - val_loss: 0.8695 - val_acc: 0.6629\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.7425 - acc: 0.7109 - val_loss: 0.8731 - val_acc: 0.6571\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7607 - acc: 0.6841 - val_loss: 0.8788 - val_acc: 0.6571\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7412 - acc: 0.7122 - val_loss: 0.8845 - val_acc: 0.6514\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7529 - acc: 0.6892 - val_loss: 0.8788 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7456 - acc: 0.6816 - val_loss: 0.8783 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7379 - acc: 0.6994 - val_loss: 0.8789 - val_acc: 0.6571\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7317 - acc: 0.7256 - val_loss: 0.8832 - val_acc: 0.6571\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7403 - acc: 0.7071 - val_loss: 0.8793 - val_acc: 0.6514\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7282 - acc: 0.7039 - val_loss: 0.8816 - val_acc: 0.6571\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7368 - acc: 0.6994 - val_loss: 0.8842 - val_acc: 0.6571\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7415 - acc: 0.7020 - val_loss: 0.8889 - val_acc: 0.6514\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7361 - acc: 0.6930 - val_loss: 0.8884 - val_acc: 0.6514\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7377 - acc: 0.6956 - val_loss: 0.8875 - val_acc: 0.6457\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7142 - acc: 0.7045 - val_loss: 0.8881 - val_acc: 0.6514\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7098 - acc: 0.7198 - val_loss: 0.8898 - val_acc: 0.6514\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7196 - acc: 0.7071 - val_loss: 0.8883 - val_acc: 0.6514\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7221 - acc: 0.7033 - val_loss: 0.8862 - val_acc: 0.6629\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7323 - acc: 0.6950 - val_loss: 0.8906 - val_acc: 0.6457\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.7203 - acc: 0.7052 - val_loss: 0.8879 - val_acc: 0.6514\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6991 - acc: 0.7173 - val_loss: 0.8835 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7057 - acc: 0.7147 - val_loss: 0.8922 - val_acc: 0.6514\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7176 - acc: 0.7122 - val_loss: 0.8983 - val_acc: 0.6514\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7091 - acc: 0.7071 - val_loss: 0.9009 - val_acc: 0.6343\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6841 - acc: 0.7250 - val_loss: 0.8977 - val_acc: 0.6286\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7133 - acc: 0.7084 - val_loss: 0.8976 - val_acc: 0.6571\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7085 - acc: 0.7154 - val_loss: 0.8971 - val_acc: 0.6514\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6990 - acc: 0.7313 - val_loss: 0.8970 - val_acc: 0.6571\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6959 - acc: 0.7135 - val_loss: 0.8985 - val_acc: 0.6514\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7052 - acc: 0.7179 - val_loss: 0.8989 - val_acc: 0.6571\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7229 - acc: 0.6969 - val_loss: 0.8981 - val_acc: 0.6514\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7034 - acc: 0.7116 - val_loss: 0.9042 - val_acc: 0.6514\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7057 - acc: 0.7084 - val_loss: 0.9003 - val_acc: 0.6514\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7068 - acc: 0.7052 - val_loss: 0.8961 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6901 - acc: 0.7250 - val_loss: 0.9004 - val_acc: 0.6571\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6796 - acc: 0.7269 - val_loss: 0.8972 - val_acc: 0.6514\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6955 - acc: 0.7211 - val_loss: 0.8937 - val_acc: 0.6743\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.6962 - acc: 0.7116 - val_loss: 0.8956 - val_acc: 0.6571\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6988 - acc: 0.7205 - val_loss: 0.8953 - val_acc: 0.6457\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6894 - acc: 0.7179 - val_loss: 0.8976 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6833 - acc: 0.7167 - val_loss: 0.8996 - val_acc: 0.6571\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6930 - acc: 0.7154 - val_loss: 0.9040 - val_acc: 0.6514\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7087 - acc: 0.7116 - val_loss: 0.8985 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6874 - acc: 0.7224 - val_loss: 0.9020 - val_acc: 0.6400\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6848 - acc: 0.7281 - val_loss: 0.9058 - val_acc: 0.6514\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6984 - acc: 0.7141 - val_loss: 0.9067 - val_acc: 0.6457\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6790 - acc: 0.7167 - val_loss: 0.9021 - val_acc: 0.6571\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6690 - acc: 0.7313 - val_loss: 0.9069 - val_acc: 0.6457\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6949 - acc: 0.7230 - val_loss: 0.9109 - val_acc: 0.6457\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6656 - acc: 0.7288 - val_loss: 0.9082 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6629 - acc: 0.7339 - val_loss: 0.9040 - val_acc: 0.6571\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6630 - acc: 0.7237 - val_loss: 0.9095 - val_acc: 0.6457\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6808 - acc: 0.7211 - val_loss: 0.9148 - val_acc: 0.6400\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.6785 - acc: 0.7218 - val_loss: 0.9202 - val_acc: 0.6514\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6738 - acc: 0.7230 - val_loss: 0.9136 - val_acc: 0.6457\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6609 - acc: 0.7403 - val_loss: 0.9111 - val_acc: 0.6457\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6576 - acc: 0.7396 - val_loss: 0.9161 - val_acc: 0.6457\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6729 - acc: 0.7281 - val_loss: 0.9146 - val_acc: 0.6514\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.6849 - acc: 0.7198 - val_loss: 0.9054 - val_acc: 0.6629\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6468 - acc: 0.7396 - val_loss: 0.9072 - val_acc: 0.6571\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6382 - acc: 0.7281 - val_loss: 0.9022 - val_acc: 0.6629\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.6542 - acc: 0.7262 - val_loss: 0.9089 - val_acc: 0.6629\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6553 - acc: 0.7262 - val_loss: 0.9090 - val_acc: 0.6629\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6530 - acc: 0.7384 - val_loss: 0.9112 - val_acc: 0.6457\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.6384 - acc: 0.7364 - val_loss: 0.9144 - val_acc: 0.6514\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6495 - acc: 0.7415 - val_loss: 0.9177 - val_acc: 0.6743\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6452 - acc: 0.7473 - val_loss: 0.9196 - val_acc: 0.6571\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6430 - acc: 0.7275 - val_loss: 0.9210 - val_acc: 0.6629\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6610 - acc: 0.7275 - val_loss: 0.9156 - val_acc: 0.6743\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6404 - acc: 0.7403 - val_loss: 0.9211 - val_acc: 0.6629\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6446 - acc: 0.7396 - val_loss: 0.9180 - val_acc: 0.6571\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6311 - acc: 0.7524 - val_loss: 0.9207 - val_acc: 0.6514\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6369 - acc: 0.7601 - val_loss: 0.9203 - val_acc: 0.6629\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6416 - acc: 0.7447 - val_loss: 0.9248 - val_acc: 0.6514\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6357 - acc: 0.7454 - val_loss: 0.9271 - val_acc: 0.6686\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6233 - acc: 0.7537 - val_loss: 0.9244 - val_acc: 0.6686\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6331 - acc: 0.7498 - val_loss: 0.9270 - val_acc: 0.6571\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6300 - acc: 0.7447 - val_loss: 0.9265 - val_acc: 0.6571\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6451 - acc: 0.7339 - val_loss: 0.9246 - val_acc: 0.6629\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6312 - acc: 0.7530 - val_loss: 0.9242 - val_acc: 0.6571\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6234 - acc: 0.7530 - val_loss: 0.9343 - val_acc: 0.6400\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6245 - acc: 0.7384 - val_loss: 0.9363 - val_acc: 0.6514\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6122 - acc: 0.7530 - val_loss: 0.9374 - val_acc: 0.6457\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6269 - acc: 0.7415 - val_loss: 0.9304 - val_acc: 0.6514\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6128 - acc: 0.7409 - val_loss: 0.9276 - val_acc: 0.6686\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6289 - acc: 0.7409 - val_loss: 0.9315 - val_acc: 0.6571\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6193 - acc: 0.7549 - val_loss: 0.9345 - val_acc: 0.6571\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6404 - acc: 0.7358 - val_loss: 0.9374 - val_acc: 0.6571\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6176 - acc: 0.7505 - val_loss: 0.9395 - val_acc: 0.6629\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6102 - acc: 0.7518 - val_loss: 0.9401 - val_acc: 0.6629\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6144 - acc: 0.7530 - val_loss: 0.9340 - val_acc: 0.6686\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6107 - acc: 0.7562 - val_loss: 0.9347 - val_acc: 0.6571\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6265 - acc: 0.7364 - val_loss: 0.9416 - val_acc: 0.6571\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6092 - acc: 0.7620 - val_loss: 0.9457 - val_acc: 0.6743\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.5865 - acc: 0.7543 - val_loss: 0.9514 - val_acc: 0.6629\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5914 - acc: 0.7639 - val_loss: 0.9441 - val_acc: 0.6800\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6045 - acc: 0.7581 - val_loss: 0.9470 - val_acc: 0.6571\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6148 - acc: 0.7505 - val_loss: 0.9412 - val_acc: 0.6743\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6178 - acc: 0.7377 - val_loss: 0.9403 - val_acc: 0.6857\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5854 - acc: 0.7658 - val_loss: 0.9381 - val_acc: 0.6686\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6084 - acc: 0.7543 - val_loss: 0.9404 - val_acc: 0.6800\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5931 - acc: 0.7562 - val_loss: 0.9384 - val_acc: 0.6743\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6168 - acc: 0.7479 - val_loss: 0.9464 - val_acc: 0.6743\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.5976 - acc: 0.7473 - val_loss: 0.9581 - val_acc: 0.6514\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.5962 - acc: 0.7632 - val_loss: 0.9533 - val_acc: 0.6686\n",
      "193/193 [==============================] - 0s 39us/step\n",
      "1742/1742 [==============================] - 0s 23us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 4s 3ms/step - loss: 1.4604 - acc: 0.3114 - val_loss: 1.2243 - val_acc: 0.5029\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 1.3157 - acc: 0.3886 - val_loss: 1.1415 - val_acc: 0.5714\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 1.2436 - acc: 0.4384 - val_loss: 1.0966 - val_acc: 0.5886\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 1.2085 - acc: 0.4563 - val_loss: 1.0580 - val_acc: 0.5886\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 1.1790 - acc: 0.4684 - val_loss: 1.0334 - val_acc: 0.6171\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 1.1306 - acc: 0.5201 - val_loss: 1.0111 - val_acc: 0.6229\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 1.1167 - acc: 0.5080 - val_loss: 0.9931 - val_acc: 0.6171\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 1.1057 - acc: 0.5022 - val_loss: 0.9796 - val_acc: 0.6400\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 1.0704 - acc: 0.5380 - val_loss: 0.9722 - val_acc: 0.6457\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0396 - acc: 0.5507 - val_loss: 0.9580 - val_acc: 0.6457\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 1.0488 - acc: 0.5456 - val_loss: 0.9575 - val_acc: 0.6686\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 1.0220 - acc: 0.5641 - val_loss: 0.9507 - val_acc: 0.6514\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0190 - acc: 0.5437 - val_loss: 0.9385 - val_acc: 0.6457\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 1.0171 - acc: 0.5673 - val_loss: 0.9335 - val_acc: 0.6571\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.0063 - acc: 0.5756 - val_loss: 0.9275 - val_acc: 0.6571\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.9958 - acc: 0.5865 - val_loss: 0.9195 - val_acc: 0.6629\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.9748 - acc: 0.5871 - val_loss: 0.9158 - val_acc: 0.6629\n",
      "Epoch 18/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.9851 - acc: 0.5973 - val_loss: 0.9148 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.9869 - acc: 0.5826 - val_loss: 0.9159 - val_acc: 0.6686\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.9530 - acc: 0.5992 - val_loss: 0.9218 - val_acc: 0.6629\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9620 - acc: 0.5903 - val_loss: 0.9166 - val_acc: 0.6629\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.9623 - acc: 0.6037 - val_loss: 0.9085 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.9453 - acc: 0.6050 - val_loss: 0.9066 - val_acc: 0.6686\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.9592 - acc: 0.5929 - val_loss: 0.9074 - val_acc: 0.6743\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.9464 - acc: 0.6031 - val_loss: 0.9045 - val_acc: 0.6743\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.9091 - acc: 0.6120 - val_loss: 0.9010 - val_acc: 0.6800\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.9179 - acc: 0.6146 - val_loss: 0.8987 - val_acc: 0.6914\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.9158 - acc: 0.6165 - val_loss: 0.9005 - val_acc: 0.6800\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.9383 - acc: 0.5941 - val_loss: 0.8982 - val_acc: 0.6629\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9272 - acc: 0.6222 - val_loss: 0.8957 - val_acc: 0.6857\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9134 - acc: 0.6146 - val_loss: 0.8896 - val_acc: 0.6743\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.9012 - acc: 0.6273 - val_loss: 0.8958 - val_acc: 0.6800\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.9067 - acc: 0.6056 - val_loss: 0.8920 - val_acc: 0.6800\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8907 - acc: 0.6190 - val_loss: 0.8916 - val_acc: 0.6857\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.9122 - acc: 0.6260 - val_loss: 0.8910 - val_acc: 0.6743\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8709 - acc: 0.6503 - val_loss: 0.8906 - val_acc: 0.6800\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.8822 - acc: 0.6382 - val_loss: 0.8920 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8859 - acc: 0.6292 - val_loss: 0.8931 - val_acc: 0.6629\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.8951 - acc: 0.6324 - val_loss: 0.8940 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8791 - acc: 0.6490 - val_loss: 0.8953 - val_acc: 0.6686\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8688 - acc: 0.6394 - val_loss: 0.8974 - val_acc: 0.6800\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8739 - acc: 0.6535 - val_loss: 0.8903 - val_acc: 0.6800\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8657 - acc: 0.6337 - val_loss: 0.8993 - val_acc: 0.6629\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8705 - acc: 0.6445 - val_loss: 0.8975 - val_acc: 0.6800\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8691 - acc: 0.6401 - val_loss: 0.9018 - val_acc: 0.6743\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8541 - acc: 0.6579 - val_loss: 0.9008 - val_acc: 0.6686\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8587 - acc: 0.6560 - val_loss: 0.8962 - val_acc: 0.6686\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.8520 - acc: 0.6414 - val_loss: 0.8990 - val_acc: 0.6629\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8517 - acc: 0.6420 - val_loss: 0.9007 - val_acc: 0.6743\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.8514 - acc: 0.6535 - val_loss: 0.8953 - val_acc: 0.6686\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8470 - acc: 0.6516 - val_loss: 0.8985 - val_acc: 0.6800\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8439 - acc: 0.6458 - val_loss: 0.9030 - val_acc: 0.6743\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8498 - acc: 0.6554 - val_loss: 0.8992 - val_acc: 0.6857\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8342 - acc: 0.6554 - val_loss: 0.8962 - val_acc: 0.6800\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8540 - acc: 0.6509 - val_loss: 0.8991 - val_acc: 0.6743\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8384 - acc: 0.6522 - val_loss: 0.8964 - val_acc: 0.6857\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8367 - acc: 0.6567 - val_loss: 0.8946 - val_acc: 0.6743\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.8466 - acc: 0.6496 - val_loss: 0.9009 - val_acc: 0.6686\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8189 - acc: 0.6631 - val_loss: 0.8941 - val_acc: 0.6743\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8265 - acc: 0.6509 - val_loss: 0.8919 - val_acc: 0.6743\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8225 - acc: 0.6522 - val_loss: 0.9003 - val_acc: 0.6743\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8158 - acc: 0.6745 - val_loss: 0.9002 - val_acc: 0.6686\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8112 - acc: 0.6675 - val_loss: 0.9005 - val_acc: 0.6743\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8123 - acc: 0.6560 - val_loss: 0.9013 - val_acc: 0.6743\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8116 - acc: 0.6586 - val_loss: 0.9007 - val_acc: 0.6686\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7987 - acc: 0.6682 - val_loss: 0.8955 - val_acc: 0.6743\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8121 - acc: 0.6605 - val_loss: 0.9031 - val_acc: 0.6629\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8170 - acc: 0.6656 - val_loss: 0.9088 - val_acc: 0.6800\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8022 - acc: 0.6771 - val_loss: 0.8963 - val_acc: 0.6686\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8054 - acc: 0.6899 - val_loss: 0.9013 - val_acc: 0.6686\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8038 - acc: 0.6726 - val_loss: 0.9021 - val_acc: 0.6743\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8004 - acc: 0.6618 - val_loss: 0.9061 - val_acc: 0.6743\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8007 - acc: 0.6618 - val_loss: 0.9090 - val_acc: 0.6743\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7924 - acc: 0.6841 - val_loss: 0.9062 - val_acc: 0.6743\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8023 - acc: 0.6758 - val_loss: 0.9102 - val_acc: 0.6686\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8022 - acc: 0.6713 - val_loss: 0.9049 - val_acc: 0.6571\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.8021 - acc: 0.6771 - val_loss: 0.9066 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7946 - acc: 0.6835 - val_loss: 0.9060 - val_acc: 0.6686\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7791 - acc: 0.6828 - val_loss: 0.9068 - val_acc: 0.6686\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7728 - acc: 0.6956 - val_loss: 0.9148 - val_acc: 0.6514\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.8019 - acc: 0.6733 - val_loss: 0.9117 - val_acc: 0.6629\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7783 - acc: 0.6847 - val_loss: 0.9117 - val_acc: 0.6571\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.7703 - acc: 0.6765 - val_loss: 0.9166 - val_acc: 0.6571\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7885 - acc: 0.6835 - val_loss: 0.9205 - val_acc: 0.6571\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7686 - acc: 0.6892 - val_loss: 0.9188 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7450 - acc: 0.7045 - val_loss: 0.9223 - val_acc: 0.6514\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.7571 - acc: 0.7013 - val_loss: 0.9187 - val_acc: 0.6514\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7873 - acc: 0.6809 - val_loss: 0.9225 - val_acc: 0.6571\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7585 - acc: 0.6994 - val_loss: 0.9218 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.7627 - acc: 0.7013 - val_loss: 0.9162 - val_acc: 0.6629\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.7447 - acc: 0.7096 - val_loss: 0.9159 - val_acc: 0.6629\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7849 - acc: 0.6713 - val_loss: 0.9171 - val_acc: 0.6457\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7562 - acc: 0.7096 - val_loss: 0.9113 - val_acc: 0.6400\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7718 - acc: 0.6713 - val_loss: 0.9112 - val_acc: 0.6457\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7624 - acc: 0.6847 - val_loss: 0.9177 - val_acc: 0.6514\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7679 - acc: 0.6809 - val_loss: 0.9268 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.7457 - acc: 0.6847 - val_loss: 0.9275 - val_acc: 0.6514\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7501 - acc: 0.7001 - val_loss: 0.9247 - val_acc: 0.6571\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7516 - acc: 0.6956 - val_loss: 0.9182 - val_acc: 0.6571\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7490 - acc: 0.6911 - val_loss: 0.9223 - val_acc: 0.6514\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7424 - acc: 0.7001 - val_loss: 0.9267 - val_acc: 0.6400\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.7455 - acc: 0.6950 - val_loss: 0.9209 - val_acc: 0.6571\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7551 - acc: 0.6835 - val_loss: 0.9216 - val_acc: 0.6629\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.7313 - acc: 0.6981 - val_loss: 0.9276 - val_acc: 0.6629\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7393 - acc: 0.7077 - val_loss: 0.9309 - val_acc: 0.6571\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.7278 - acc: 0.7090 - val_loss: 0.9334 - val_acc: 0.6629\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.7251 - acc: 0.7045 - val_loss: 0.9272 - val_acc: 0.6686\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7277 - acc: 0.6988 - val_loss: 0.9348 - val_acc: 0.6571\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7391 - acc: 0.6924 - val_loss: 0.9383 - val_acc: 0.6571\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7371 - acc: 0.7026 - val_loss: 0.9380 - val_acc: 0.6457\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7399 - acc: 0.7058 - val_loss: 0.9395 - val_acc: 0.6571\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7329 - acc: 0.7026 - val_loss: 0.9363 - val_acc: 0.6514\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.7342 - acc: 0.7001 - val_loss: 0.9391 - val_acc: 0.6514\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7375 - acc: 0.7026 - val_loss: 0.9353 - val_acc: 0.6457\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7114 - acc: 0.7179 - val_loss: 0.9334 - val_acc: 0.6629\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7245 - acc: 0.7052 - val_loss: 0.9341 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7222 - acc: 0.7039 - val_loss: 0.9385 - val_acc: 0.6629\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7267 - acc: 0.7001 - val_loss: 0.9369 - val_acc: 0.6629\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7185 - acc: 0.7045 - val_loss: 0.9442 - val_acc: 0.6514\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7097 - acc: 0.7128 - val_loss: 0.9400 - val_acc: 0.6571\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7168 - acc: 0.7007 - val_loss: 0.9371 - val_acc: 0.6514\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7149 - acc: 0.7084 - val_loss: 0.9335 - val_acc: 0.6629\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7054 - acc: 0.7205 - val_loss: 0.9389 - val_acc: 0.6514\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7047 - acc: 0.7122 - val_loss: 0.9412 - val_acc: 0.6514\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6927 - acc: 0.7205 - val_loss: 0.9436 - val_acc: 0.6400\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7005 - acc: 0.7205 - val_loss: 0.9429 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7142 - acc: 0.7173 - val_loss: 0.9424 - val_acc: 0.6686\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7084 - acc: 0.7109 - val_loss: 0.9501 - val_acc: 0.6629\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.7007 - acc: 0.7154 - val_loss: 0.9537 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.7144 - acc: 0.7039 - val_loss: 0.9546 - val_acc: 0.6343\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6945 - acc: 0.7294 - val_loss: 0.9421 - val_acc: 0.6571\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.7085 - acc: 0.7160 - val_loss: 0.9472 - val_acc: 0.6514\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7115 - acc: 0.7192 - val_loss: 0.9480 - val_acc: 0.6571\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6813 - acc: 0.7160 - val_loss: 0.9505 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6805 - acc: 0.7192 - val_loss: 0.9610 - val_acc: 0.6343\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6795 - acc: 0.7237 - val_loss: 0.9529 - val_acc: 0.6343\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6872 - acc: 0.7237 - val_loss: 0.9502 - val_acc: 0.6514\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6746 - acc: 0.7294 - val_loss: 0.9521 - val_acc: 0.6571\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6890 - acc: 0.7243 - val_loss: 0.9513 - val_acc: 0.6514\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6732 - acc: 0.7326 - val_loss: 0.9583 - val_acc: 0.6629\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.6601 - acc: 0.7396 - val_loss: 0.9579 - val_acc: 0.6571\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.6979 - acc: 0.7135 - val_loss: 0.9588 - val_acc: 0.6457\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6872 - acc: 0.7173 - val_loss: 0.9533 - val_acc: 0.6457\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6899 - acc: 0.7237 - val_loss: 0.9477 - val_acc: 0.6514\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6650 - acc: 0.7332 - val_loss: 0.9489 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6668 - acc: 0.7364 - val_loss: 0.9539 - val_acc: 0.6514\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6677 - acc: 0.7288 - val_loss: 0.9546 - val_acc: 0.6629\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6651 - acc: 0.7294 - val_loss: 0.9574 - val_acc: 0.6400\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6590 - acc: 0.7396 - val_loss: 0.9605 - val_acc: 0.6457\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6705 - acc: 0.7269 - val_loss: 0.9664 - val_acc: 0.6457\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.6679 - acc: 0.7281 - val_loss: 0.9595 - val_acc: 0.6629\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6648 - acc: 0.7415 - val_loss: 0.9618 - val_acc: 0.6629\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6732 - acc: 0.7288 - val_loss: 0.9655 - val_acc: 0.6571\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6859 - acc: 0.7141 - val_loss: 0.9584 - val_acc: 0.6571\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6550 - acc: 0.7428 - val_loss: 0.9625 - val_acc: 0.6571\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6563 - acc: 0.7409 - val_loss: 0.9644 - val_acc: 0.6629\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6576 - acc: 0.7294 - val_loss: 0.9683 - val_acc: 0.6571\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6417 - acc: 0.7505 - val_loss: 0.9623 - val_acc: 0.6686\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6635 - acc: 0.7339 - val_loss: 0.9655 - val_acc: 0.6629\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6647 - acc: 0.7396 - val_loss: 0.9694 - val_acc: 0.6514\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6550 - acc: 0.7384 - val_loss: 0.9667 - val_acc: 0.6457\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.6603 - acc: 0.7307 - val_loss: 0.9668 - val_acc: 0.6514\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6547 - acc: 0.7358 - val_loss: 0.9660 - val_acc: 0.6629\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6612 - acc: 0.7281 - val_loss: 0.9659 - val_acc: 0.6686\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6467 - acc: 0.7428 - val_loss: 0.9674 - val_acc: 0.6514\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6560 - acc: 0.7262 - val_loss: 0.9677 - val_acc: 0.6686\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6621 - acc: 0.7243 - val_loss: 0.9701 - val_acc: 0.6514\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6450 - acc: 0.7403 - val_loss: 0.9658 - val_acc: 0.6571\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6272 - acc: 0.7486 - val_loss: 0.9690 - val_acc: 0.6629\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6255 - acc: 0.7518 - val_loss: 0.9757 - val_acc: 0.6629\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6441 - acc: 0.7377 - val_loss: 0.9732 - val_acc: 0.6457\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6439 - acc: 0.7498 - val_loss: 0.9707 - val_acc: 0.6571\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 33us/step - loss: 0.6277 - acc: 0.7492 - val_loss: 0.9726 - val_acc: 0.6514\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6508 - acc: 0.7460 - val_loss: 0.9771 - val_acc: 0.6629\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6544 - acc: 0.7320 - val_loss: 0.9863 - val_acc: 0.6514\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6329 - acc: 0.7364 - val_loss: 0.9861 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.6258 - acc: 0.7454 - val_loss: 0.9871 - val_acc: 0.6629\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6325 - acc: 0.7447 - val_loss: 0.9890 - val_acc: 0.6514\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6298 - acc: 0.7396 - val_loss: 0.9832 - val_acc: 0.6571\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6274 - acc: 0.7428 - val_loss: 0.9817 - val_acc: 0.6743\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.6300 - acc: 0.7505 - val_loss: 0.9889 - val_acc: 0.6686\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6144 - acc: 0.7492 - val_loss: 0.9838 - val_acc: 0.6800\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6136 - acc: 0.7549 - val_loss: 0.9826 - val_acc: 0.6571\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.6323 - acc: 0.7466 - val_loss: 0.9889 - val_acc: 0.6743\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6273 - acc: 0.7575 - val_loss: 0.9927 - val_acc: 0.6457\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6063 - acc: 0.7479 - val_loss: 0.9908 - val_acc: 0.6457\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6352 - acc: 0.7403 - val_loss: 0.9963 - val_acc: 0.6457\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6104 - acc: 0.7607 - val_loss: 0.9987 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6205 - acc: 0.7403 - val_loss: 0.9895 - val_acc: 0.6571\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6231 - acc: 0.7396 - val_loss: 0.9853 - val_acc: 0.6457\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.6265 - acc: 0.7518 - val_loss: 0.9813 - val_acc: 0.6571\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6207 - acc: 0.7537 - val_loss: 0.9804 - val_acc: 0.6571\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6106 - acc: 0.7492 - val_loss: 0.9885 - val_acc: 0.6514\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6022 - acc: 0.7524 - val_loss: 0.9867 - val_acc: 0.6514\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 36us/step - loss: 0.6070 - acc: 0.7518 - val_loss: 1.0032 - val_acc: 0.6571\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6019 - acc: 0.7556 - val_loss: 1.0022 - val_acc: 0.6571\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6165 - acc: 0.7428 - val_loss: 0.9970 - val_acc: 0.6571\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6076 - acc: 0.7607 - val_loss: 0.9984 - val_acc: 0.6571\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.5966 - acc: 0.7652 - val_loss: 1.0062 - val_acc: 0.6571\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 34us/step - loss: 0.6043 - acc: 0.7549 - val_loss: 1.0019 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 38us/step\n",
      "1742/1742 [==============================] - 0s 21us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.7480 - acc: 0.2731 - val_loss: 1.4281 - val_acc: 0.3714\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 1.4705 - acc: 0.3497 - val_loss: 1.2310 - val_acc: 0.5257\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 1.3006 - acc: 0.4148 - val_loss: 1.1207 - val_acc: 0.5486\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 1.2247 - acc: 0.4435 - val_loss: 1.0620 - val_acc: 0.5943\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.1660 - acc: 0.4793 - val_loss: 1.0322 - val_acc: 0.5714\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1361 - acc: 0.4882 - val_loss: 1.0112 - val_acc: 0.5943\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.1140 - acc: 0.5067 - val_loss: 0.9925 - val_acc: 0.6114\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 1.1116 - acc: 0.5227 - val_loss: 0.9846 - val_acc: 0.6229\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 1.0903 - acc: 0.5080 - val_loss: 0.9729 - val_acc: 0.6229\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0652 - acc: 0.5475 - val_loss: 0.9614 - val_acc: 0.6343\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0728 - acc: 0.5386 - val_loss: 0.9512 - val_acc: 0.6286\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0485 - acc: 0.5450 - val_loss: 0.9447 - val_acc: 0.6229\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0430 - acc: 0.5533 - val_loss: 0.9358 - val_acc: 0.6400\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0526 - acc: 0.5348 - val_loss: 0.9311 - val_acc: 0.6400\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.9973 - acc: 0.5705 - val_loss: 0.9307 - val_acc: 0.6343\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.0135 - acc: 0.5654 - val_loss: 0.9307 - val_acc: 0.6343\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.9900 - acc: 0.5839 - val_loss: 0.9346 - val_acc: 0.6229\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9949 - acc: 0.5686 - val_loss: 0.9306 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.9788 - acc: 0.5814 - val_loss: 0.9203 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9828 - acc: 0.5763 - val_loss: 0.9153 - val_acc: 0.6514\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.9626 - acc: 0.5807 - val_loss: 0.9147 - val_acc: 0.6571\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9577 - acc: 0.5999 - val_loss: 0.9176 - val_acc: 0.6400\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9567 - acc: 0.5820 - val_loss: 0.9166 - val_acc: 0.6171\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9595 - acc: 0.5903 - val_loss: 0.9160 - val_acc: 0.6343\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.9497 - acc: 0.6139 - val_loss: 0.9160 - val_acc: 0.6171\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.9518 - acc: 0.6120 - val_loss: 0.9190 - val_acc: 0.6514\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.9415 - acc: 0.5967 - val_loss: 0.9169 - val_acc: 0.6514\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.9030 - acc: 0.6228 - val_loss: 0.9203 - val_acc: 0.6514\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 37us/step - loss: 0.9343 - acc: 0.6101 - val_loss: 0.9176 - val_acc: 0.6743\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.9369 - acc: 0.6126 - val_loss: 0.9147 - val_acc: 0.6686\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.9152 - acc: 0.6171 - val_loss: 0.9132 - val_acc: 0.6571\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.9193 - acc: 0.6235 - val_loss: 0.9135 - val_acc: 0.6400\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9273 - acc: 0.6120 - val_loss: 0.9100 - val_acc: 0.6514\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8935 - acc: 0.6216 - val_loss: 0.9083 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9144 - acc: 0.6133 - val_loss: 0.9179 - val_acc: 0.6629\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9052 - acc: 0.6177 - val_loss: 0.9143 - val_acc: 0.6629\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9019 - acc: 0.6337 - val_loss: 0.9076 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8845 - acc: 0.6337 - val_loss: 0.9071 - val_acc: 0.6629\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8901 - acc: 0.6292 - val_loss: 0.9041 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8776 - acc: 0.6318 - val_loss: 0.9100 - val_acc: 0.6514\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8853 - acc: 0.6280 - val_loss: 0.9091 - val_acc: 0.6571\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8744 - acc: 0.6356 - val_loss: 0.9125 - val_acc: 0.6571\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8682 - acc: 0.6414 - val_loss: 0.9137 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8596 - acc: 0.6458 - val_loss: 0.9135 - val_acc: 0.6229\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8787 - acc: 0.6311 - val_loss: 0.9084 - val_acc: 0.6629\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8653 - acc: 0.6484 - val_loss: 0.9167 - val_acc: 0.6571\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8550 - acc: 0.6433 - val_loss: 0.9121 - val_acc: 0.6514\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8745 - acc: 0.6465 - val_loss: 0.9081 - val_acc: 0.6571\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8675 - acc: 0.6420 - val_loss: 0.9171 - val_acc: 0.6629\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8711 - acc: 0.6477 - val_loss: 0.9176 - val_acc: 0.6571\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8517 - acc: 0.6382 - val_loss: 0.9158 - val_acc: 0.6571\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8464 - acc: 0.6445 - val_loss: 0.9175 - val_acc: 0.6457\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8500 - acc: 0.6439 - val_loss: 0.9239 - val_acc: 0.6571\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8373 - acc: 0.6509 - val_loss: 0.9177 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8632 - acc: 0.6490 - val_loss: 0.9157 - val_acc: 0.6457\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8436 - acc: 0.6541 - val_loss: 0.9165 - val_acc: 0.6571\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8347 - acc: 0.6522 - val_loss: 0.9212 - val_acc: 0.6514\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8297 - acc: 0.6484 - val_loss: 0.9198 - val_acc: 0.6629\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8340 - acc: 0.6567 - val_loss: 0.9178 - val_acc: 0.6457\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8375 - acc: 0.6733 - val_loss: 0.9167 - val_acc: 0.6629\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8330 - acc: 0.6713 - val_loss: 0.9150 - val_acc: 0.6629\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8287 - acc: 0.6560 - val_loss: 0.9178 - val_acc: 0.6629\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8257 - acc: 0.6694 - val_loss: 0.9169 - val_acc: 0.6629\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 35us/step - loss: 0.8180 - acc: 0.6694 - val_loss: 0.9181 - val_acc: 0.6629\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8217 - acc: 0.6675 - val_loss: 0.9212 - val_acc: 0.6629\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8230 - acc: 0.6656 - val_loss: 0.9248 - val_acc: 0.6629\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.8106 - acc: 0.6631 - val_loss: 0.9279 - val_acc: 0.6457\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8097 - acc: 0.6784 - val_loss: 0.9272 - val_acc: 0.6514\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8090 - acc: 0.6682 - val_loss: 0.9156 - val_acc: 0.6629\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.8129 - acc: 0.6656 - val_loss: 0.9180 - val_acc: 0.6571\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8085 - acc: 0.6752 - val_loss: 0.9246 - val_acc: 0.6571\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8054 - acc: 0.6688 - val_loss: 0.9261 - val_acc: 0.6514\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8069 - acc: 0.6707 - val_loss: 0.9173 - val_acc: 0.6400\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7933 - acc: 0.6713 - val_loss: 0.9217 - val_acc: 0.6400\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7808 - acc: 0.6873 - val_loss: 0.9272 - val_acc: 0.6457\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8019 - acc: 0.6803 - val_loss: 0.9318 - val_acc: 0.6457\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7897 - acc: 0.6847 - val_loss: 0.9303 - val_acc: 0.6400\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7910 - acc: 0.6777 - val_loss: 0.9361 - val_acc: 0.6400\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7812 - acc: 0.6739 - val_loss: 0.9324 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7851 - acc: 0.6777 - val_loss: 0.9388 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7907 - acc: 0.6758 - val_loss: 0.9415 - val_acc: 0.6514\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7808 - acc: 0.6962 - val_loss: 0.9391 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7770 - acc: 0.6816 - val_loss: 0.9358 - val_acc: 0.6400\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8036 - acc: 0.6592 - val_loss: 0.9380 - val_acc: 0.6400\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7707 - acc: 0.6911 - val_loss: 0.9457 - val_acc: 0.6400\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7822 - acc: 0.6905 - val_loss: 0.9446 - val_acc: 0.6514\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7795 - acc: 0.6879 - val_loss: 0.9453 - val_acc: 0.6514\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7812 - acc: 0.6803 - val_loss: 0.9446 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7586 - acc: 0.6943 - val_loss: 0.9408 - val_acc: 0.6457\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7726 - acc: 0.6950 - val_loss: 0.9419 - val_acc: 0.6400\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7679 - acc: 0.6937 - val_loss: 0.9457 - val_acc: 0.6400\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7567 - acc: 0.6937 - val_loss: 0.9463 - val_acc: 0.6286\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7772 - acc: 0.6860 - val_loss: 0.9412 - val_acc: 0.6286\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7583 - acc: 0.6988 - val_loss: 0.9501 - val_acc: 0.6114\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7489 - acc: 0.6988 - val_loss: 0.9580 - val_acc: 0.6229\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7652 - acc: 0.6918 - val_loss: 0.9491 - val_acc: 0.6286\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7495 - acc: 0.7026 - val_loss: 0.9472 - val_acc: 0.6286\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7460 - acc: 0.6930 - val_loss: 0.9446 - val_acc: 0.6229\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7468 - acc: 0.7007 - val_loss: 0.9470 - val_acc: 0.6343\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7616 - acc: 0.6879 - val_loss: 0.9495 - val_acc: 0.6343\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7527 - acc: 0.7026 - val_loss: 0.9494 - val_acc: 0.6229\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7525 - acc: 0.7058 - val_loss: 0.9494 - val_acc: 0.6286\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7435 - acc: 0.6975 - val_loss: 0.9506 - val_acc: 0.6229\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7377 - acc: 0.7013 - val_loss: 0.9537 - val_acc: 0.6286\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7372 - acc: 0.7013 - val_loss: 0.9550 - val_acc: 0.6229\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7413 - acc: 0.6937 - val_loss: 0.9480 - val_acc: 0.6286\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7616 - acc: 0.6911 - val_loss: 0.9514 - val_acc: 0.6229\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7536 - acc: 0.6765 - val_loss: 0.9511 - val_acc: 0.6343\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7252 - acc: 0.7128 - val_loss: 0.9572 - val_acc: 0.6286\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7227 - acc: 0.7122 - val_loss: 0.9589 - val_acc: 0.6286\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7400 - acc: 0.7039 - val_loss: 0.9557 - val_acc: 0.6286\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7274 - acc: 0.7077 - val_loss: 0.9579 - val_acc: 0.6229\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7314 - acc: 0.7026 - val_loss: 0.9625 - val_acc: 0.6343\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7217 - acc: 0.6988 - val_loss: 0.9605 - val_acc: 0.6286\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7145 - acc: 0.7135 - val_loss: 0.9623 - val_acc: 0.6343\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7282 - acc: 0.7218 - val_loss: 0.9594 - val_acc: 0.6400\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7319 - acc: 0.7052 - val_loss: 0.9558 - val_acc: 0.6114\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7012 - acc: 0.7218 - val_loss: 0.9603 - val_acc: 0.6286\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7230 - acc: 0.7064 - val_loss: 0.9671 - val_acc: 0.6286\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7102 - acc: 0.7173 - val_loss: 0.9634 - val_acc: 0.6286\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7202 - acc: 0.7077 - val_loss: 0.9671 - val_acc: 0.6229\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7146 - acc: 0.7077 - val_loss: 0.9682 - val_acc: 0.6343\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7046 - acc: 0.7135 - val_loss: 0.9659 - val_acc: 0.6286\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7153 - acc: 0.6981 - val_loss: 0.9683 - val_acc: 0.6171\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7013 - acc: 0.7128 - val_loss: 0.9675 - val_acc: 0.6171\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7204 - acc: 0.6994 - val_loss: 0.9697 - val_acc: 0.6171\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7081 - acc: 0.7167 - val_loss: 0.9673 - val_acc: 0.6343\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7174 - acc: 0.7198 - val_loss: 0.9686 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7039 - acc: 0.7147 - val_loss: 0.9679 - val_acc: 0.6343\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7132 - acc: 0.7154 - val_loss: 0.9739 - val_acc: 0.6171\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7088 - acc: 0.7147 - val_loss: 0.9753 - val_acc: 0.6229\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6893 - acc: 0.7281 - val_loss: 0.9796 - val_acc: 0.6171\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7015 - acc: 0.7243 - val_loss: 0.9838 - val_acc: 0.6286\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6984 - acc: 0.7275 - val_loss: 0.9821 - val_acc: 0.6286\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7076 - acc: 0.7135 - val_loss: 0.9837 - val_acc: 0.6286\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6981 - acc: 0.7173 - val_loss: 0.9788 - val_acc: 0.6286\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7059 - acc: 0.7116 - val_loss: 0.9866 - val_acc: 0.6171\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6987 - acc: 0.7173 - val_loss: 0.9811 - val_acc: 0.6171\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6964 - acc: 0.7154 - val_loss: 0.9786 - val_acc: 0.6286\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6809 - acc: 0.7377 - val_loss: 0.9797 - val_acc: 0.6286\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6749 - acc: 0.7275 - val_loss: 0.9848 - val_acc: 0.6114\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6905 - acc: 0.7205 - val_loss: 0.9869 - val_acc: 0.6114\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6678 - acc: 0.7237 - val_loss: 0.9857 - val_acc: 0.6286\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6817 - acc: 0.7364 - val_loss: 0.9890 - val_acc: 0.6114\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6994 - acc: 0.7364 - val_loss: 0.9895 - val_acc: 0.6057\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6944 - acc: 0.7224 - val_loss: 0.9902 - val_acc: 0.6171\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6818 - acc: 0.7326 - val_loss: 0.9914 - val_acc: 0.6114\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6775 - acc: 0.7345 - val_loss: 0.9941 - val_acc: 0.6114\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6745 - acc: 0.7364 - val_loss: 0.9963 - val_acc: 0.6057\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6681 - acc: 0.7345 - val_loss: 0.9918 - val_acc: 0.6171\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 38us/step - loss: 0.6744 - acc: 0.7320 - val_loss: 0.9913 - val_acc: 0.6229\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6861 - acc: 0.7173 - val_loss: 0.9989 - val_acc: 0.6229\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6680 - acc: 0.7269 - val_loss: 0.9986 - val_acc: 0.6286\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6636 - acc: 0.7396 - val_loss: 1.0049 - val_acc: 0.6114\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6936 - acc: 0.7237 - val_loss: 0.9994 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6451 - acc: 0.7460 - val_loss: 0.9940 - val_acc: 0.6343\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6722 - acc: 0.7307 - val_loss: 0.9957 - val_acc: 0.6229\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.6669 - acc: 0.7415 - val_loss: 0.9937 - val_acc: 0.6171\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6574 - acc: 0.7435 - val_loss: 0.9862 - val_acc: 0.6171\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6498 - acc: 0.7409 - val_loss: 0.9877 - val_acc: 0.6171\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6800 - acc: 0.7358 - val_loss: 0.9922 - val_acc: 0.6171\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6514 - acc: 0.7435 - val_loss: 0.9906 - val_acc: 0.6229\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6696 - acc: 0.7256 - val_loss: 0.9937 - val_acc: 0.6114\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6531 - acc: 0.7371 - val_loss: 0.9995 - val_acc: 0.6171\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6653 - acc: 0.7352 - val_loss: 0.9983 - val_acc: 0.6000\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6554 - acc: 0.7371 - val_loss: 1.0016 - val_acc: 0.6171\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6587 - acc: 0.7173 - val_loss: 0.9986 - val_acc: 0.6171\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6585 - acc: 0.7498 - val_loss: 1.0038 - val_acc: 0.6114\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6419 - acc: 0.7479 - val_loss: 1.0079 - val_acc: 0.6114\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6623 - acc: 0.7371 - val_loss: 1.0134 - val_acc: 0.6171\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6395 - acc: 0.7505 - val_loss: 1.0196 - val_acc: 0.6114\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6463 - acc: 0.7415 - val_loss: 1.0159 - val_acc: 0.6229\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6293 - acc: 0.7447 - val_loss: 1.0123 - val_acc: 0.6286\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6411 - acc: 0.7518 - val_loss: 1.0132 - val_acc: 0.6229\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.6215 - acc: 0.7569 - val_loss: 1.0110 - val_acc: 0.6171\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6498 - acc: 0.7428 - val_loss: 1.0158 - val_acc: 0.6171\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6331 - acc: 0.7422 - val_loss: 1.0162 - val_acc: 0.6171\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6409 - acc: 0.7492 - val_loss: 1.0113 - val_acc: 0.6171\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6449 - acc: 0.7556 - val_loss: 1.0146 - val_acc: 0.6171\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6505 - acc: 0.7403 - val_loss: 1.0191 - val_acc: 0.6057\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6380 - acc: 0.7460 - val_loss: 1.0217 - val_acc: 0.6114\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6340 - acc: 0.7428 - val_loss: 1.0189 - val_acc: 0.6229\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6342 - acc: 0.7562 - val_loss: 1.0184 - val_acc: 0.6229\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6081 - acc: 0.7645 - val_loss: 1.0248 - val_acc: 0.6057\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6393 - acc: 0.7466 - val_loss: 1.0255 - val_acc: 0.6171\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6358 - acc: 0.7652 - val_loss: 1.0268 - val_acc: 0.6114\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6264 - acc: 0.7632 - val_loss: 1.0275 - val_acc: 0.6057\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6573 - acc: 0.7326 - val_loss: 1.0324 - val_acc: 0.6000\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6121 - acc: 0.7530 - val_loss: 1.0327 - val_acc: 0.6171\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6188 - acc: 0.7479 - val_loss: 1.0317 - val_acc: 0.6171\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6167 - acc: 0.7588 - val_loss: 1.0352 - val_acc: 0.6057\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6159 - acc: 0.7492 - val_loss: 1.0349 - val_acc: 0.6171\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6251 - acc: 0.7626 - val_loss: 1.0333 - val_acc: 0.6229\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6120 - acc: 0.7518 - val_loss: 1.0391 - val_acc: 0.6114\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5949 - acc: 0.7677 - val_loss: 1.0377 - val_acc: 0.6114\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6293 - acc: 0.7530 - val_loss: 1.0403 - val_acc: 0.6000\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5998 - acc: 0.7709 - val_loss: 1.0390 - val_acc: 0.6114\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6065 - acc: 0.7620 - val_loss: 1.0476 - val_acc: 0.6057\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6206 - acc: 0.7588 - val_loss: 1.0499 - val_acc: 0.6057\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5970 - acc: 0.7601 - val_loss: 1.0451 - val_acc: 0.6000\n",
      "193/193 [==============================] - 0s 45us/step\n",
      "1742/1742 [==============================] - 0s 31us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 4s 3ms/step - loss: 1.4783 - acc: 0.3050 - val_loss: 1.2632 - val_acc: 0.4514\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.3471 - acc: 0.3740 - val_loss: 1.1790 - val_acc: 0.4514\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.2579 - acc: 0.4199 - val_loss: 1.1292 - val_acc: 0.4971\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1812 - acc: 0.4697 - val_loss: 1.0933 - val_acc: 0.5200\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1619 - acc: 0.4608 - val_loss: 1.0673 - val_acc: 0.5429\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.1417 - acc: 0.4850 - val_loss: 1.0524 - val_acc: 0.5429\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.1054 - acc: 0.5029 - val_loss: 1.0383 - val_acc: 0.5200\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0931 - acc: 0.5118 - val_loss: 1.0157 - val_acc: 0.5543\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0780 - acc: 0.5348 - val_loss: 1.0085 - val_acc: 0.5257\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0616 - acc: 0.5469 - val_loss: 1.0064 - val_acc: 0.5429\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0589 - acc: 0.5386 - val_loss: 0.9972 - val_acc: 0.5314\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0261 - acc: 0.5616 - val_loss: 0.9942 - val_acc: 0.5543\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.0148 - acc: 0.5724 - val_loss: 0.9917 - val_acc: 0.5543\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0310 - acc: 0.5629 - val_loss: 0.9887 - val_acc: 0.5771\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9916 - acc: 0.5756 - val_loss: 0.9843 - val_acc: 0.5600\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.9983 - acc: 0.5839 - val_loss: 0.9765 - val_acc: 0.5829\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.9745 - acc: 0.5909 - val_loss: 0.9746 - val_acc: 0.5657\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9769 - acc: 0.5756 - val_loss: 0.9772 - val_acc: 0.5543\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9770 - acc: 0.5865 - val_loss: 0.9777 - val_acc: 0.5371\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9480 - acc: 0.6107 - val_loss: 0.9810 - val_acc: 0.5657\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9561 - acc: 0.5954 - val_loss: 0.9741 - val_acc: 0.5543\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9543 - acc: 0.6043 - val_loss: 0.9689 - val_acc: 0.5714\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9418 - acc: 0.6043 - val_loss: 0.9681 - val_acc: 0.5543\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9544 - acc: 0.5980 - val_loss: 0.9699 - val_acc: 0.5714\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9424 - acc: 0.6094 - val_loss: 0.9750 - val_acc: 0.5657\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9429 - acc: 0.6043 - val_loss: 0.9638 - val_acc: 0.5829\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9422 - acc: 0.6088 - val_loss: 0.9632 - val_acc: 0.5714\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9318 - acc: 0.6120 - val_loss: 0.9647 - val_acc: 0.5600\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8990 - acc: 0.6197 - val_loss: 0.9656 - val_acc: 0.5657\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9145 - acc: 0.6133 - val_loss: 0.9619 - val_acc: 0.5829\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9197 - acc: 0.6126 - val_loss: 0.9634 - val_acc: 0.5771\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9166 - acc: 0.6056 - val_loss: 0.9634 - val_acc: 0.5829\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9092 - acc: 0.6177 - val_loss: 0.9586 - val_acc: 0.5771\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8924 - acc: 0.6356 - val_loss: 0.9611 - val_acc: 0.5771\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8942 - acc: 0.6216 - val_loss: 0.9665 - val_acc: 0.5771\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8975 - acc: 0.6158 - val_loss: 0.9679 - val_acc: 0.5657\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8852 - acc: 0.6299 - val_loss: 0.9709 - val_acc: 0.5829\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8737 - acc: 0.6382 - val_loss: 0.9692 - val_acc: 0.5771\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8897 - acc: 0.6299 - val_loss: 0.9618 - val_acc: 0.5714\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8801 - acc: 0.6286 - val_loss: 0.9667 - val_acc: 0.5829\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8833 - acc: 0.6433 - val_loss: 0.9679 - val_acc: 0.5829\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8762 - acc: 0.6356 - val_loss: 0.9686 - val_acc: 0.5829\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8496 - acc: 0.6592 - val_loss: 0.9765 - val_acc: 0.5714\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8782 - acc: 0.6241 - val_loss: 0.9723 - val_acc: 0.5886\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8611 - acc: 0.6522 - val_loss: 0.9660 - val_acc: 0.5943\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8482 - acc: 0.6484 - val_loss: 0.9712 - val_acc: 0.5943\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8697 - acc: 0.6445 - val_loss: 0.9787 - val_acc: 0.6000\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8647 - acc: 0.6477 - val_loss: 0.9783 - val_acc: 0.6000\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8562 - acc: 0.6503 - val_loss: 0.9773 - val_acc: 0.6000\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8467 - acc: 0.6554 - val_loss: 0.9794 - val_acc: 0.6057\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8481 - acc: 0.6382 - val_loss: 0.9767 - val_acc: 0.5943\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8530 - acc: 0.6420 - val_loss: 0.9767 - val_acc: 0.6000\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8390 - acc: 0.6439 - val_loss: 0.9790 - val_acc: 0.6000\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8353 - acc: 0.6490 - val_loss: 0.9752 - val_acc: 0.6000\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8385 - acc: 0.6579 - val_loss: 0.9720 - val_acc: 0.6000\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8325 - acc: 0.6650 - val_loss: 0.9704 - val_acc: 0.5943\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8117 - acc: 0.6733 - val_loss: 0.9761 - val_acc: 0.5943\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8208 - acc: 0.6554 - val_loss: 0.9747 - val_acc: 0.5943\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8080 - acc: 0.6662 - val_loss: 0.9802 - val_acc: 0.6000\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8156 - acc: 0.6707 - val_loss: 0.9723 - val_acc: 0.6000\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8414 - acc: 0.6465 - val_loss: 0.9769 - val_acc: 0.6057\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8038 - acc: 0.6956 - val_loss: 0.9826 - val_acc: 0.6114\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8256 - acc: 0.6567 - val_loss: 0.9875 - val_acc: 0.6114\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8054 - acc: 0.6631 - val_loss: 0.9840 - val_acc: 0.6114\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7999 - acc: 0.6796 - val_loss: 0.9790 - val_acc: 0.6114\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8374 - acc: 0.6567 - val_loss: 0.9850 - val_acc: 0.6114\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8027 - acc: 0.6790 - val_loss: 0.9829 - val_acc: 0.6057\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8182 - acc: 0.6720 - val_loss: 0.9857 - val_acc: 0.6114\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8043 - acc: 0.6733 - val_loss: 0.9919 - val_acc: 0.6114\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8069 - acc: 0.6694 - val_loss: 0.9898 - val_acc: 0.6114\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8168 - acc: 0.6720 - val_loss: 0.9950 - val_acc: 0.6057\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8034 - acc: 0.6745 - val_loss: 0.9950 - val_acc: 0.6057\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7936 - acc: 0.6675 - val_loss: 1.0008 - val_acc: 0.6057\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7900 - acc: 0.6822 - val_loss: 0.9903 - val_acc: 0.6114\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7902 - acc: 0.6688 - val_loss: 0.9895 - val_acc: 0.6000\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7985 - acc: 0.6701 - val_loss: 0.9968 - val_acc: 0.6057\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7874 - acc: 0.6860 - val_loss: 0.9949 - val_acc: 0.6000\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7860 - acc: 0.6790 - val_loss: 0.9936 - val_acc: 0.5943\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7734 - acc: 0.6847 - val_loss: 0.9955 - val_acc: 0.6057\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7817 - acc: 0.6873 - val_loss: 1.0034 - val_acc: 0.6000\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7672 - acc: 0.6873 - val_loss: 1.0085 - val_acc: 0.6000\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7666 - acc: 0.6950 - val_loss: 1.0033 - val_acc: 0.6057\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7849 - acc: 0.6867 - val_loss: 1.0098 - val_acc: 0.6114\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7727 - acc: 0.6873 - val_loss: 1.0147 - val_acc: 0.6114\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7705 - acc: 0.7001 - val_loss: 1.0102 - val_acc: 0.6114\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7615 - acc: 0.6943 - val_loss: 0.9965 - val_acc: 0.6057\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7765 - acc: 0.6860 - val_loss: 1.0023 - val_acc: 0.6057\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7653 - acc: 0.6969 - val_loss: 1.0110 - val_acc: 0.6000\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7597 - acc: 0.6937 - val_loss: 1.0103 - val_acc: 0.6000\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7564 - acc: 0.6879 - val_loss: 1.0100 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7655 - acc: 0.6847 - val_loss: 1.0091 - val_acc: 0.6000\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7529 - acc: 0.7007 - val_loss: 1.0107 - val_acc: 0.6000\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7517 - acc: 0.6860 - val_loss: 1.0064 - val_acc: 0.6114\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7703 - acc: 0.6911 - val_loss: 1.0164 - val_acc: 0.6057\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7663 - acc: 0.6860 - val_loss: 1.0135 - val_acc: 0.6114\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7514 - acc: 0.6956 - val_loss: 1.0093 - val_acc: 0.6057\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7637 - acc: 0.7001 - val_loss: 1.0136 - val_acc: 0.6000\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7598 - acc: 0.6924 - val_loss: 1.0157 - val_acc: 0.6057\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7525 - acc: 0.6981 - val_loss: 1.0187 - val_acc: 0.6000\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7573 - acc: 0.7001 - val_loss: 1.0184 - val_acc: 0.6057\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7527 - acc: 0.6930 - val_loss: 1.0144 - val_acc: 0.5943\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7359 - acc: 0.6994 - val_loss: 1.0211 - val_acc: 0.6000\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7480 - acc: 0.7064 - val_loss: 1.0221 - val_acc: 0.5943\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7442 - acc: 0.7077 - val_loss: 1.0186 - val_acc: 0.5943\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7369 - acc: 0.7090 - val_loss: 1.0118 - val_acc: 0.5886\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7275 - acc: 0.7033 - val_loss: 1.0198 - val_acc: 0.5886\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7380 - acc: 0.7007 - val_loss: 1.0252 - val_acc: 0.5943\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7292 - acc: 0.7141 - val_loss: 1.0200 - val_acc: 0.5829\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7355 - acc: 0.6962 - val_loss: 1.0229 - val_acc: 0.5943\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7287 - acc: 0.7026 - val_loss: 1.0221 - val_acc: 0.6114\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7202 - acc: 0.7090 - val_loss: 1.0230 - val_acc: 0.5771\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7277 - acc: 0.7167 - val_loss: 1.0234 - val_acc: 0.6000\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7146 - acc: 0.7103 - val_loss: 1.0150 - val_acc: 0.5943\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7113 - acc: 0.7109 - val_loss: 1.0218 - val_acc: 0.6000\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7204 - acc: 0.7096 - val_loss: 1.0269 - val_acc: 0.5943\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7194 - acc: 0.7211 - val_loss: 1.0278 - val_acc: 0.5886\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7219 - acc: 0.7116 - val_loss: 1.0274 - val_acc: 0.5886\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7397 - acc: 0.7077 - val_loss: 1.0393 - val_acc: 0.5943\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7221 - acc: 0.7058 - val_loss: 1.0325 - val_acc: 0.6000\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7146 - acc: 0.7122 - val_loss: 1.0315 - val_acc: 0.5943\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7217 - acc: 0.7077 - val_loss: 1.0306 - val_acc: 0.5829\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7220 - acc: 0.7154 - val_loss: 1.0377 - val_acc: 0.5943\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7139 - acc: 0.7211 - val_loss: 1.0428 - val_acc: 0.5771\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7147 - acc: 0.7071 - val_loss: 1.0397 - val_acc: 0.5943\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6878 - acc: 0.7256 - val_loss: 1.0397 - val_acc: 0.5943\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7166 - acc: 0.7237 - val_loss: 1.0421 - val_acc: 0.6000\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7001 - acc: 0.7167 - val_loss: 1.0426 - val_acc: 0.5943\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7075 - acc: 0.7224 - val_loss: 1.0434 - val_acc: 0.5771\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6934 - acc: 0.7256 - val_loss: 1.0422 - val_acc: 0.5829\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6955 - acc: 0.7326 - val_loss: 1.0423 - val_acc: 0.5829\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6907 - acc: 0.7262 - val_loss: 1.0497 - val_acc: 0.5829\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7048 - acc: 0.7205 - val_loss: 1.0523 - val_acc: 0.5771\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.7141 - val_loss: 1.0466 - val_acc: 0.5829\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6999 - acc: 0.7167 - val_loss: 1.0454 - val_acc: 0.5714\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6867 - acc: 0.7313 - val_loss: 1.0466 - val_acc: 0.5886\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7021 - acc: 0.7179 - val_loss: 1.0434 - val_acc: 0.5829\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6861 - acc: 0.7160 - val_loss: 1.0360 - val_acc: 0.5886\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6846 - acc: 0.7262 - val_loss: 1.0407 - val_acc: 0.5771\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6699 - acc: 0.7345 - val_loss: 1.0470 - val_acc: 0.5829\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6811 - acc: 0.7345 - val_loss: 1.0452 - val_acc: 0.5829\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6841 - acc: 0.7377 - val_loss: 1.0522 - val_acc: 0.5886\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6689 - acc: 0.7288 - val_loss: 1.0551 - val_acc: 0.5771\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6947 - acc: 0.7230 - val_loss: 1.0515 - val_acc: 0.5771\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6714 - acc: 0.7250 - val_loss: 1.0531 - val_acc: 0.5771\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6879 - acc: 0.7256 - val_loss: 1.0560 - val_acc: 0.5829\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6782 - acc: 0.7313 - val_loss: 1.0587 - val_acc: 0.5771\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6955 - acc: 0.7211 - val_loss: 1.0569 - val_acc: 0.5771\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6875 - acc: 0.7128 - val_loss: 1.0638 - val_acc: 0.5771\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6778 - acc: 0.7422 - val_loss: 1.0735 - val_acc: 0.5714\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6639 - acc: 0.7428 - val_loss: 1.0680 - val_acc: 0.5771\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6775 - acc: 0.7345 - val_loss: 1.0606 - val_acc: 0.5886\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.6599 - acc: 0.7435 - val_loss: 1.0582 - val_acc: 0.5829\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.6629 - acc: 0.7403 - val_loss: 1.0650 - val_acc: 0.5829\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6861 - acc: 0.7352 - val_loss: 1.0672 - val_acc: 0.5829\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6633 - acc: 0.7384 - val_loss: 1.0705 - val_acc: 0.5771\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6628 - acc: 0.7403 - val_loss: 1.0731 - val_acc: 0.5714\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6638 - acc: 0.7211 - val_loss: 1.0772 - val_acc: 0.5657\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6496 - acc: 0.7428 - val_loss: 1.0787 - val_acc: 0.5714\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6617 - acc: 0.7332 - val_loss: 1.0783 - val_acc: 0.5771\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6472 - acc: 0.7460 - val_loss: 1.0755 - val_acc: 0.5714\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6598 - acc: 0.7281 - val_loss: 1.0815 - val_acc: 0.5771\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6481 - acc: 0.7275 - val_loss: 1.0736 - val_acc: 0.5771\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6605 - acc: 0.7352 - val_loss: 1.0682 - val_acc: 0.5714\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6571 - acc: 0.7384 - val_loss: 1.0721 - val_acc: 0.5657\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6569 - acc: 0.7511 - val_loss: 1.0759 - val_acc: 0.5714\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6449 - acc: 0.7428 - val_loss: 1.0873 - val_acc: 0.5657\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6447 - acc: 0.7473 - val_loss: 1.0871 - val_acc: 0.5543\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6568 - acc: 0.7396 - val_loss: 1.0899 - val_acc: 0.5543\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6418 - acc: 0.7569 - val_loss: 1.0927 - val_acc: 0.5714\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6603 - acc: 0.7339 - val_loss: 1.0958 - val_acc: 0.5714\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6403 - acc: 0.7435 - val_loss: 1.0942 - val_acc: 0.5714\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6211 - acc: 0.7511 - val_loss: 1.0849 - val_acc: 0.5657\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6295 - acc: 0.7345 - val_loss: 1.0876 - val_acc: 0.5714\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6495 - acc: 0.7364 - val_loss: 1.1029 - val_acc: 0.5657\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6348 - acc: 0.7403 - val_loss: 1.1003 - val_acc: 0.5600\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6305 - acc: 0.7581 - val_loss: 1.0961 - val_acc: 0.5714\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6397 - acc: 0.7511 - val_loss: 1.0909 - val_acc: 0.5657\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6322 - acc: 0.7575 - val_loss: 1.0925 - val_acc: 0.5714\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6239 - acc: 0.7588 - val_loss: 1.0969 - val_acc: 0.5657\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6154 - acc: 0.7632 - val_loss: 1.0960 - val_acc: 0.5771\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6256 - acc: 0.7588 - val_loss: 1.0957 - val_acc: 0.5657\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6515 - acc: 0.7409 - val_loss: 1.1006 - val_acc: 0.5657\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6162 - acc: 0.7524 - val_loss: 1.1069 - val_acc: 0.5657\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6129 - acc: 0.7569 - val_loss: 1.0956 - val_acc: 0.5657\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6203 - acc: 0.7403 - val_loss: 1.0957 - val_acc: 0.5657\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6104 - acc: 0.7505 - val_loss: 1.1027 - val_acc: 0.5714\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6091 - acc: 0.7505 - val_loss: 1.1142 - val_acc: 0.5714\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6214 - acc: 0.7537 - val_loss: 1.1116 - val_acc: 0.5714\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6406 - acc: 0.7371 - val_loss: 1.1012 - val_acc: 0.5657\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6142 - acc: 0.7594 - val_loss: 1.1075 - val_acc: 0.5657\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6112 - acc: 0.7658 - val_loss: 1.1063 - val_acc: 0.5714\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6088 - acc: 0.7703 - val_loss: 1.1074 - val_acc: 0.5771\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6109 - acc: 0.7626 - val_loss: 1.1116 - val_acc: 0.5771\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6049 - acc: 0.7613 - val_loss: 1.1072 - val_acc: 0.5714\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6083 - acc: 0.7492 - val_loss: 1.1099 - val_acc: 0.5657\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6172 - acc: 0.7498 - val_loss: 1.1143 - val_acc: 0.5657\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5935 - acc: 0.7645 - val_loss: 1.1052 - val_acc: 0.5600\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6163 - acc: 0.7690 - val_loss: 1.1005 - val_acc: 0.5714\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5983 - acc: 0.7613 - val_loss: 1.1040 - val_acc: 0.5714\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6042 - acc: 0.7575 - val_loss: 1.1152 - val_acc: 0.5657\n",
      "193/193 [==============================] - 0s 52us/step\n",
      "1742/1742 [==============================] - 0s 32us/step\n",
      "Train on 1741 samples, validate on 194 samples\n",
      "Epoch 1/200\n",
      "1741/1741 [==============================] - 4s 2ms/step - loss: 1.4907 - acc: 0.3073 - val_loss: 1.2338 - val_acc: 0.4588\n",
      "Epoch 2/200\n",
      "1741/1741 [==============================] - 0s 53us/step - loss: 1.3179 - acc: 0.3791 - val_loss: 1.1344 - val_acc: 0.5722\n",
      "Epoch 3/200\n",
      "1741/1741 [==============================] - 0s 60us/step - loss: 1.2537 - acc: 0.4291 - val_loss: 1.0847 - val_acc: 0.5773\n",
      "Epoch 4/200\n",
      "1741/1741 [==============================] - 0s 59us/step - loss: 1.1803 - acc: 0.4767 - val_loss: 1.0472 - val_acc: 0.6082\n",
      "Epoch 5/200\n",
      "1741/1741 [==============================] - 0s 51us/step - loss: 1.1448 - acc: 0.4773 - val_loss: 1.0183 - val_acc: 0.6134\n",
      "Epoch 6/200\n",
      "1741/1741 [==============================] - 0s 54us/step - loss: 1.1323 - acc: 0.4917 - val_loss: 0.9966 - val_acc: 0.6134\n",
      "Epoch 7/200\n",
      "1741/1741 [==============================] - 0s 51us/step - loss: 1.1089 - acc: 0.5083 - val_loss: 0.9835 - val_acc: 0.6237\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741/1741 [==============================] - 0s 58us/step - loss: 1.0731 - acc: 0.5244 - val_loss: 0.9699 - val_acc: 0.6186\n",
      "Epoch 9/200\n",
      "1741/1741 [==============================] - 0s 52us/step - loss: 1.0665 - acc: 0.5221 - val_loss: 0.9548 - val_acc: 0.6340\n",
      "Epoch 10/200\n",
      "1741/1741 [==============================] - 0s 52us/step - loss: 1.0675 - acc: 0.5152 - val_loss: 0.9488 - val_acc: 0.6340\n",
      "Epoch 11/200\n",
      "1741/1741 [==============================] - 0s 51us/step - loss: 1.0209 - acc: 0.5635 - val_loss: 0.9382 - val_acc: 0.6443\n",
      "Epoch 12/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 1.0244 - acc: 0.5646 - val_loss: 0.9335 - val_acc: 0.6598\n",
      "Epoch 13/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 1.0139 - acc: 0.5640 - val_loss: 0.9283 - val_acc: 0.6701\n",
      "Epoch 14/200\n",
      "1741/1741 [==============================] - 0s 55us/step - loss: 1.0029 - acc: 0.5750 - val_loss: 0.9187 - val_acc: 0.6546\n",
      "Epoch 15/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 1.0091 - acc: 0.5560 - val_loss: 0.9132 - val_acc: 0.6753\n",
      "Epoch 16/200\n",
      "1741/1741 [==============================] - 0s 60us/step - loss: 0.9926 - acc: 0.5847 - val_loss: 0.9090 - val_acc: 0.6753\n",
      "Epoch 17/200\n",
      "1741/1741 [==============================] - 0s 56us/step - loss: 0.9702 - acc: 0.5841 - val_loss: 0.9079 - val_acc: 0.6546\n",
      "Epoch 18/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.9647 - acc: 0.5847 - val_loss: 0.9021 - val_acc: 0.6701\n",
      "Epoch 19/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.9925 - acc: 0.5692 - val_loss: 0.8945 - val_acc: 0.6804\n",
      "Epoch 20/200\n",
      "1741/1741 [==============================] - 0s 52us/step - loss: 0.9584 - acc: 0.5956 - val_loss: 0.8930 - val_acc: 0.6804\n",
      "Epoch 21/200\n",
      "1741/1741 [==============================] - 0s 57us/step - loss: 0.9500 - acc: 0.5979 - val_loss: 0.8902 - val_acc: 0.6753\n",
      "Epoch 22/200\n",
      "1741/1741 [==============================] - 0s 107us/step - loss: 0.9542 - acc: 0.5991 - val_loss: 0.8890 - val_acc: 0.6753\n",
      "Epoch 23/200\n",
      "1741/1741 [==============================] - 0s 54us/step - loss: 0.9319 - acc: 0.6175 - val_loss: 0.8842 - val_acc: 0.6598\n",
      "Epoch 24/200\n",
      "1741/1741 [==============================] - 0s 69us/step - loss: 0.9459 - acc: 0.5882 - val_loss: 0.8851 - val_acc: 0.6804\n",
      "Epoch 25/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.9312 - acc: 0.6054 - val_loss: 0.8838 - val_acc: 0.6804\n",
      "Epoch 26/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.9387 - acc: 0.6083 - val_loss: 0.8762 - val_acc: 0.6804\n",
      "Epoch 27/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.9329 - acc: 0.6226 - val_loss: 0.8796 - val_acc: 0.6959\n",
      "Epoch 28/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.9128 - acc: 0.6077 - val_loss: 0.8765 - val_acc: 0.6804\n",
      "Epoch 29/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.9087 - acc: 0.6146 - val_loss: 0.8752 - val_acc: 0.6856\n",
      "Epoch 30/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.9142 - acc: 0.6198 - val_loss: 0.8760 - val_acc: 0.6959\n",
      "Epoch 31/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.9203 - acc: 0.6031 - val_loss: 0.8694 - val_acc: 0.6907\n",
      "Epoch 32/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.9014 - acc: 0.6404 - val_loss: 0.8727 - val_acc: 0.6856\n",
      "Epoch 33/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.9065 - acc: 0.6163 - val_loss: 0.8737 - val_acc: 0.6804\n",
      "Epoch 34/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.8897 - acc: 0.6244 - val_loss: 0.8787 - val_acc: 0.6753\n",
      "Epoch 35/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.8942 - acc: 0.6284 - val_loss: 0.8734 - val_acc: 0.6701\n",
      "Epoch 36/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.8839 - acc: 0.6289 - val_loss: 0.8712 - val_acc: 0.6701\n",
      "Epoch 37/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.8990 - acc: 0.6180 - val_loss: 0.8738 - val_acc: 0.6701\n",
      "Epoch 38/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.8926 - acc: 0.6244 - val_loss: 0.8771 - val_acc: 0.6649\n",
      "Epoch 39/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.8821 - acc: 0.6410 - val_loss: 0.8775 - val_acc: 0.6701\n",
      "Epoch 40/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8853 - acc: 0.6335 - val_loss: 0.8755 - val_acc: 0.6649\n",
      "Epoch 41/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8716 - acc: 0.6387 - val_loss: 0.8750 - val_acc: 0.6856\n",
      "Epoch 42/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8902 - acc: 0.6209 - val_loss: 0.8754 - val_acc: 0.6856\n",
      "Epoch 43/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.8500 - acc: 0.6479 - val_loss: 0.8728 - val_acc: 0.6804\n",
      "Epoch 44/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.8690 - acc: 0.6364 - val_loss: 0.8726 - val_acc: 0.6856\n",
      "Epoch 45/200\n",
      "1741/1741 [==============================] - 0s 39us/step - loss: 0.8695 - acc: 0.6450 - val_loss: 0.8757 - val_acc: 0.6649\n",
      "Epoch 46/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.8593 - acc: 0.6468 - val_loss: 0.8788 - val_acc: 0.6598\n",
      "Epoch 47/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8508 - acc: 0.6462 - val_loss: 0.8793 - val_acc: 0.6753\n",
      "Epoch 48/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.8368 - acc: 0.6680 - val_loss: 0.8745 - val_acc: 0.6598\n",
      "Epoch 49/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8521 - acc: 0.6473 - val_loss: 0.8798 - val_acc: 0.6753\n",
      "Epoch 50/200\n",
      "1741/1741 [==============================] - 0s 41us/step - loss: 0.8548 - acc: 0.6491 - val_loss: 0.8852 - val_acc: 0.6701\n",
      "Epoch 51/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.8510 - acc: 0.6577 - val_loss: 0.8826 - val_acc: 0.6701\n",
      "Epoch 52/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.8380 - acc: 0.6680 - val_loss: 0.8795 - val_acc: 0.6701\n",
      "Epoch 53/200\n",
      "1741/1741 [==============================] - 0s 41us/step - loss: 0.8417 - acc: 0.6559 - val_loss: 0.8820 - val_acc: 0.6598\n",
      "Epoch 54/200\n",
      "1741/1741 [==============================] - 0s 39us/step - loss: 0.8594 - acc: 0.6364 - val_loss: 0.8838 - val_acc: 0.6701\n",
      "Epoch 55/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.8466 - acc: 0.6536 - val_loss: 0.8845 - val_acc: 0.6649\n",
      "Epoch 56/200\n",
      "1741/1741 [==============================] - 0s 41us/step - loss: 0.8203 - acc: 0.6703 - val_loss: 0.8851 - val_acc: 0.6495\n",
      "Epoch 57/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.8379 - acc: 0.6617 - val_loss: 0.8872 - val_acc: 0.6546\n",
      "Epoch 58/200\n",
      "1741/1741 [==============================] - 0s 41us/step - loss: 0.8341 - acc: 0.6491 - val_loss: 0.8910 - val_acc: 0.6495\n",
      "Epoch 59/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.8348 - acc: 0.6594 - val_loss: 0.8945 - val_acc: 0.6495\n",
      "Epoch 60/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.8235 - acc: 0.6674 - val_loss: 0.8912 - val_acc: 0.6443\n",
      "Epoch 61/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.8172 - acc: 0.6623 - val_loss: 0.8869 - val_acc: 0.6701\n",
      "Epoch 62/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.8126 - acc: 0.6646 - val_loss: 0.8874 - val_acc: 0.6546\n",
      "Epoch 63/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.8199 - acc: 0.6669 - val_loss: 0.8925 - val_acc: 0.6495\n",
      "Epoch 64/200\n",
      "1741/1741 [==============================] - 0s 41us/step - loss: 0.8048 - acc: 0.6651 - val_loss: 0.8902 - val_acc: 0.6443\n",
      "Epoch 65/200\n",
      "1741/1741 [==============================] - 0s 58us/step - loss: 0.8135 - acc: 0.6640 - val_loss: 0.8910 - val_acc: 0.6546\n",
      "Epoch 66/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8146 - acc: 0.6640 - val_loss: 0.8878 - val_acc: 0.6546\n",
      "Epoch 67/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.8059 - acc: 0.6680 - val_loss: 0.8907 - val_acc: 0.6546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8061 - acc: 0.6841 - val_loss: 0.8948 - val_acc: 0.6546\n",
      "Epoch 69/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.8131 - acc: 0.6651 - val_loss: 0.8957 - val_acc: 0.6598\n",
      "Epoch 70/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8102 - acc: 0.6594 - val_loss: 0.8913 - val_acc: 0.6649\n",
      "Epoch 71/200\n",
      "1741/1741 [==============================] - 0s 40us/step - loss: 0.8088 - acc: 0.6760 - val_loss: 0.8948 - val_acc: 0.6392\n",
      "Epoch 72/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.7909 - acc: 0.6755 - val_loss: 0.8905 - val_acc: 0.6598\n",
      "Epoch 73/200\n",
      "1741/1741 [==============================] - 0s 41us/step - loss: 0.7928 - acc: 0.6657 - val_loss: 0.8906 - val_acc: 0.6495\n",
      "Epoch 74/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.7938 - acc: 0.6680 - val_loss: 0.8933 - val_acc: 0.6495\n",
      "Epoch 75/200\n",
      "1741/1741 [==============================] - 0s 51us/step - loss: 0.8090 - acc: 0.6772 - val_loss: 0.8923 - val_acc: 0.6495\n",
      "Epoch 76/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.7908 - acc: 0.6783 - val_loss: 0.8932 - val_acc: 0.6598\n",
      "Epoch 77/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.8192 - acc: 0.6743 - val_loss: 0.8908 - val_acc: 0.6649\n",
      "Epoch 78/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.7938 - acc: 0.6818 - val_loss: 0.8930 - val_acc: 0.6546\n",
      "Epoch 79/200\n",
      "1741/1741 [==============================] - 0s 51us/step - loss: 0.7943 - acc: 0.6875 - val_loss: 0.8940 - val_acc: 0.6495\n",
      "Epoch 80/200\n",
      "1741/1741 [==============================] - 0s 51us/step - loss: 0.7881 - acc: 0.6875 - val_loss: 0.9036 - val_acc: 0.6495\n",
      "Epoch 81/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.7788 - acc: 0.6651 - val_loss: 0.8970 - val_acc: 0.6495\n",
      "Epoch 82/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.7845 - acc: 0.6910 - val_loss: 0.8950 - val_acc: 0.6495\n",
      "Epoch 83/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.7871 - acc: 0.6743 - val_loss: 0.9001 - val_acc: 0.6495\n",
      "Epoch 84/200\n",
      "1741/1741 [==============================] - 0s 53us/step - loss: 0.7894 - acc: 0.6709 - val_loss: 0.9014 - val_acc: 0.6495\n",
      "Epoch 85/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7658 - acc: 0.6956 - val_loss: 0.8945 - val_acc: 0.6546\n",
      "Epoch 86/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7642 - acc: 0.6703 - val_loss: 0.8952 - val_acc: 0.6546\n",
      "Epoch 87/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.7654 - acc: 0.6858 - val_loss: 0.9017 - val_acc: 0.6598\n",
      "Epoch 88/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.7646 - acc: 0.6812 - val_loss: 0.9046 - val_acc: 0.6495\n",
      "Epoch 89/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.7580 - acc: 0.6881 - val_loss: 0.9019 - val_acc: 0.6495\n",
      "Epoch 90/200\n",
      "1741/1741 [==============================] - 0s 53us/step - loss: 0.7720 - acc: 0.6778 - val_loss: 0.9022 - val_acc: 0.6392\n",
      "Epoch 91/200\n",
      "1741/1741 [==============================] - 0s 54us/step - loss: 0.7699 - acc: 0.7025 - val_loss: 0.9040 - val_acc: 0.6392\n",
      "Epoch 92/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.7394 - acc: 0.7065 - val_loss: 0.9067 - val_acc: 0.6546\n",
      "Epoch 93/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.7636 - acc: 0.6772 - val_loss: 0.9030 - val_acc: 0.6495\n",
      "Epoch 94/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7596 - acc: 0.6864 - val_loss: 0.9010 - val_acc: 0.6546\n",
      "Epoch 95/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.7531 - acc: 0.6962 - val_loss: 0.9037 - val_acc: 0.6495\n",
      "Epoch 96/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.7579 - acc: 0.6893 - val_loss: 0.9115 - val_acc: 0.6495\n",
      "Epoch 97/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.7632 - acc: 0.6760 - val_loss: 0.9097 - val_acc: 0.6443\n",
      "Epoch 98/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.7540 - acc: 0.6984 - val_loss: 0.9093 - val_acc: 0.6392\n",
      "Epoch 99/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.7428 - acc: 0.6973 - val_loss: 0.9089 - val_acc: 0.6392\n",
      "Epoch 100/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.7532 - acc: 0.6921 - val_loss: 0.9150 - val_acc: 0.6443\n",
      "Epoch 101/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.7474 - acc: 0.6990 - val_loss: 0.9101 - val_acc: 0.6443\n",
      "Epoch 102/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.7380 - acc: 0.7082 - val_loss: 0.9114 - val_acc: 0.6443\n",
      "Epoch 103/200\n",
      "1741/1741 [==============================] - 0s 56us/step - loss: 0.7515 - acc: 0.6984 - val_loss: 0.9134 - val_acc: 0.6443\n",
      "Epoch 104/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7232 - acc: 0.7059 - val_loss: 0.9143 - val_acc: 0.6495\n",
      "Epoch 105/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.7448 - acc: 0.6973 - val_loss: 0.9143 - val_acc: 0.6598\n",
      "Epoch 106/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7651 - acc: 0.6944 - val_loss: 0.9082 - val_acc: 0.6546\n",
      "Epoch 107/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7383 - acc: 0.7019 - val_loss: 0.9075 - val_acc: 0.6495\n",
      "Epoch 108/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.7388 - acc: 0.7013 - val_loss: 0.9106 - val_acc: 0.6546\n",
      "Epoch 109/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7348 - acc: 0.6967 - val_loss: 0.9109 - val_acc: 0.6495\n",
      "Epoch 110/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7317 - acc: 0.7002 - val_loss: 0.9131 - val_acc: 0.6443\n",
      "Epoch 111/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.7198 - acc: 0.7007 - val_loss: 0.9174 - val_acc: 0.6443\n",
      "Epoch 112/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.7212 - acc: 0.7048 - val_loss: 0.9218 - val_acc: 0.6443\n",
      "Epoch 113/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.7359 - acc: 0.7019 - val_loss: 0.9145 - val_acc: 0.6546\n",
      "Epoch 114/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.7131 - acc: 0.7099 - val_loss: 0.9176 - val_acc: 0.6392\n",
      "Epoch 115/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.7232 - acc: 0.7071 - val_loss: 0.9218 - val_acc: 0.6443\n",
      "Epoch 116/200\n",
      "1741/1741 [==============================] - 0s 51us/step - loss: 0.7319 - acc: 0.6973 - val_loss: 0.9162 - val_acc: 0.6392\n",
      "Epoch 117/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.7115 - acc: 0.7260 - val_loss: 0.9121 - val_acc: 0.6649\n",
      "Epoch 118/200\n",
      "1741/1741 [==============================] - 0s 42us/step - loss: 0.7222 - acc: 0.7030 - val_loss: 0.9194 - val_acc: 0.6443\n",
      "Epoch 119/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.7313 - acc: 0.7019 - val_loss: 0.9225 - val_acc: 0.6443\n",
      "Epoch 120/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.7138 - acc: 0.7122 - val_loss: 0.9205 - val_acc: 0.6546\n",
      "Epoch 121/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.7028 - acc: 0.7048 - val_loss: 0.9212 - val_acc: 0.6495\n",
      "Epoch 122/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6944 - acc: 0.7249 - val_loss: 0.9245 - val_acc: 0.6495\n",
      "Epoch 123/200\n",
      "1741/1741 [==============================] - 0s 52us/step - loss: 0.7081 - acc: 0.7312 - val_loss: 0.9292 - val_acc: 0.6392\n",
      "Epoch 124/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.7121 - acc: 0.7283 - val_loss: 0.9292 - val_acc: 0.6495\n",
      "Epoch 125/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7079 - acc: 0.7186 - val_loss: 0.9275 - val_acc: 0.6443\n",
      "Epoch 126/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.7143 - acc: 0.7266 - val_loss: 0.9306 - val_acc: 0.6495\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7100 - acc: 0.7099 - val_loss: 0.9338 - val_acc: 0.6495\n",
      "Epoch 128/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.7099 - acc: 0.7071 - val_loss: 0.9264 - val_acc: 0.6649\n",
      "Epoch 129/200\n",
      "1741/1741 [==============================] - 0s 56us/step - loss: 0.7111 - acc: 0.7013 - val_loss: 0.9334 - val_acc: 0.6495\n",
      "Epoch 130/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7155 - acc: 0.7019 - val_loss: 0.9349 - val_acc: 0.6495\n",
      "Epoch 131/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.6934 - acc: 0.7191 - val_loss: 0.9368 - val_acc: 0.6443\n",
      "Epoch 132/200\n",
      "1741/1741 [==============================] - 0s 52us/step - loss: 0.6980 - acc: 0.7076 - val_loss: 0.9351 - val_acc: 0.6495\n",
      "Epoch 133/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.7006 - acc: 0.7157 - val_loss: 0.9320 - val_acc: 0.6598\n",
      "Epoch 134/200\n",
      "1741/1741 [==============================] - 0s 51us/step - loss: 0.7081 - acc: 0.7007 - val_loss: 0.9364 - val_acc: 0.6443\n",
      "Epoch 135/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6915 - acc: 0.7191 - val_loss: 0.9404 - val_acc: 0.6495\n",
      "Epoch 136/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.6882 - acc: 0.7341 - val_loss: 0.9390 - val_acc: 0.6495\n",
      "Epoch 137/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.6994 - acc: 0.7203 - val_loss: 0.9414 - val_acc: 0.6495\n",
      "Epoch 138/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.6995 - acc: 0.7226 - val_loss: 0.9451 - val_acc: 0.6443\n",
      "Epoch 139/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.6918 - acc: 0.7191 - val_loss: 0.9450 - val_acc: 0.6443\n",
      "Epoch 140/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.6849 - acc: 0.7186 - val_loss: 0.9489 - val_acc: 0.6443\n",
      "Epoch 141/200\n",
      "1741/1741 [==============================] - 0s 61us/step - loss: 0.6843 - acc: 0.7346 - val_loss: 0.9471 - val_acc: 0.6495\n",
      "Epoch 142/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6752 - acc: 0.7358 - val_loss: 0.9486 - val_acc: 0.6495\n",
      "Epoch 143/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6899 - acc: 0.7237 - val_loss: 0.9543 - val_acc: 0.6443\n",
      "Epoch 144/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6763 - acc: 0.7168 - val_loss: 0.9499 - val_acc: 0.6495\n",
      "Epoch 145/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.6659 - acc: 0.7312 - val_loss: 0.9514 - val_acc: 0.6495\n",
      "Epoch 146/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.6763 - acc: 0.7295 - val_loss: 0.9613 - val_acc: 0.6495\n",
      "Epoch 147/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.6675 - acc: 0.7306 - val_loss: 0.9550 - val_acc: 0.6546\n",
      "Epoch 148/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6775 - acc: 0.7289 - val_loss: 0.9500 - val_acc: 0.6443\n",
      "Epoch 149/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.6792 - acc: 0.7145 - val_loss: 0.9546 - val_acc: 0.6443\n",
      "Epoch 150/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6805 - acc: 0.7214 - val_loss: 0.9515 - val_acc: 0.6546\n",
      "Epoch 151/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6803 - acc: 0.7289 - val_loss: 0.9505 - val_acc: 0.6598\n",
      "Epoch 152/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6794 - acc: 0.7277 - val_loss: 0.9559 - val_acc: 0.6443\n",
      "Epoch 153/200\n",
      "1741/1741 [==============================] - 0s 55us/step - loss: 0.6947 - acc: 0.7220 - val_loss: 0.9560 - val_acc: 0.6392\n",
      "Epoch 154/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6827 - acc: 0.7272 - val_loss: 0.9540 - val_acc: 0.6598\n",
      "Epoch 155/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.6596 - acc: 0.7323 - val_loss: 0.9538 - val_acc: 0.6598\n",
      "Epoch 156/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6792 - acc: 0.7254 - val_loss: 0.9614 - val_acc: 0.6495\n",
      "Epoch 157/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.6682 - acc: 0.7266 - val_loss: 0.9600 - val_acc: 0.6546\n",
      "Epoch 158/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.6595 - acc: 0.7203 - val_loss: 0.9608 - val_acc: 0.6443\n",
      "Epoch 159/200\n",
      "1741/1741 [==============================] - 0s 54us/step - loss: 0.6538 - acc: 0.7283 - val_loss: 0.9579 - val_acc: 0.6546\n",
      "Epoch 160/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.6571 - acc: 0.7266 - val_loss: 0.9590 - val_acc: 0.6546\n",
      "Epoch 161/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.6652 - acc: 0.7323 - val_loss: 0.9666 - val_acc: 0.6495\n",
      "Epoch 162/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.6657 - acc: 0.7323 - val_loss: 0.9691 - val_acc: 0.6598\n",
      "Epoch 163/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6549 - acc: 0.7312 - val_loss: 0.9742 - val_acc: 0.6649\n",
      "Epoch 164/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.6625 - acc: 0.7346 - val_loss: 0.9752 - val_acc: 0.6546\n",
      "Epoch 165/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.6682 - acc: 0.7300 - val_loss: 0.9757 - val_acc: 0.6495\n",
      "Epoch 166/200\n",
      "1741/1741 [==============================] - 0s 58us/step - loss: 0.6586 - acc: 0.7231 - val_loss: 0.9725 - val_acc: 0.6495\n",
      "Epoch 167/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.6514 - acc: 0.7410 - val_loss: 0.9752 - val_acc: 0.6443\n",
      "Epoch 168/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6428 - acc: 0.7421 - val_loss: 0.9750 - val_acc: 0.6495\n",
      "Epoch 169/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.6533 - acc: 0.7381 - val_loss: 0.9715 - val_acc: 0.6598\n",
      "Epoch 170/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.6373 - acc: 0.7341 - val_loss: 0.9735 - val_acc: 0.6598\n",
      "Epoch 171/200\n",
      "1741/1741 [==============================] - 0s 50us/step - loss: 0.6640 - acc: 0.7266 - val_loss: 0.9802 - val_acc: 0.6495\n",
      "Epoch 172/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.6450 - acc: 0.7507 - val_loss: 0.9737 - val_acc: 0.6546\n",
      "Epoch 173/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6527 - acc: 0.7444 - val_loss: 0.9767 - val_acc: 0.6598\n",
      "Epoch 174/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.6207 - acc: 0.7536 - val_loss: 0.9749 - val_acc: 0.6598\n",
      "Epoch 175/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6386 - acc: 0.7455 - val_loss: 0.9803 - val_acc: 0.6546\n",
      "Epoch 176/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.6325 - acc: 0.7421 - val_loss: 0.9837 - val_acc: 0.6649\n",
      "Epoch 177/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.6210 - acc: 0.7427 - val_loss: 0.9875 - val_acc: 0.6649\n",
      "Epoch 178/200\n",
      "1741/1741 [==============================] - 0s 57us/step - loss: 0.6445 - acc: 0.7346 - val_loss: 0.9832 - val_acc: 0.6598\n",
      "Epoch 179/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.6103 - acc: 0.7519 - val_loss: 0.9809 - val_acc: 0.6598\n",
      "Epoch 180/200\n",
      "1741/1741 [==============================] - 0s 54us/step - loss: 0.6268 - acc: 0.7501 - val_loss: 0.9885 - val_acc: 0.6546\n",
      "Epoch 181/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.6397 - acc: 0.7312 - val_loss: 0.9829 - val_acc: 0.6598\n",
      "Epoch 182/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6218 - acc: 0.7576 - val_loss: 0.9851 - val_acc: 0.6495\n",
      "Epoch 183/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6191 - acc: 0.7507 - val_loss: 0.9893 - val_acc: 0.6495\n",
      "Epoch 184/200\n",
      "1741/1741 [==============================] - 0s 49us/step - loss: 0.6331 - acc: 0.7507 - val_loss: 0.9939 - val_acc: 0.6495\n",
      "Epoch 185/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.6442 - acc: 0.7392 - val_loss: 0.9954 - val_acc: 0.6495\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.6223 - acc: 0.7415 - val_loss: 0.9892 - val_acc: 0.6392\n",
      "Epoch 187/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6306 - acc: 0.7461 - val_loss: 0.9903 - val_acc: 0.6340\n",
      "Epoch 188/200\n",
      "1741/1741 [==============================] - 0s 45us/step - loss: 0.6197 - acc: 0.7593 - val_loss: 0.9919 - val_acc: 0.6392\n",
      "Epoch 189/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.6338 - acc: 0.7433 - val_loss: 0.9923 - val_acc: 0.6546\n",
      "Epoch 190/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6268 - acc: 0.7484 - val_loss: 0.9934 - val_acc: 0.6495\n",
      "Epoch 191/200\n",
      "1741/1741 [==============================] - 0s 55us/step - loss: 0.6352 - acc: 0.7387 - val_loss: 0.9921 - val_acc: 0.6495\n",
      "Epoch 192/200\n",
      "1741/1741 [==============================] - 0s 48us/step - loss: 0.5999 - acc: 0.7593 - val_loss: 0.9945 - val_acc: 0.6443\n",
      "Epoch 193/200\n",
      "1741/1741 [==============================] - 0s 52us/step - loss: 0.6138 - acc: 0.7530 - val_loss: 1.0013 - val_acc: 0.6495\n",
      "Epoch 194/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.6089 - acc: 0.7628 - val_loss: 1.0065 - val_acc: 0.6495\n",
      "Epoch 195/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6305 - acc: 0.7450 - val_loss: 0.9970 - val_acc: 0.6546\n",
      "Epoch 196/200\n",
      "1741/1741 [==============================] - 0s 44us/step - loss: 0.6074 - acc: 0.7490 - val_loss: 1.0033 - val_acc: 0.6443\n",
      "Epoch 197/200\n",
      "1741/1741 [==============================] - 0s 43us/step - loss: 0.5960 - acc: 0.7605 - val_loss: 1.0083 - val_acc: 0.6392\n",
      "Epoch 198/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6138 - acc: 0.7496 - val_loss: 1.0047 - val_acc: 0.6495\n",
      "Epoch 199/200\n",
      "1741/1741 [==============================] - 0s 46us/step - loss: 0.6125 - acc: 0.7576 - val_loss: 1.0187 - val_acc: 0.6443\n",
      "Epoch 200/200\n",
      "1741/1741 [==============================] - 0s 47us/step - loss: 0.6143 - acc: 0.7444 - val_loss: 1.0232 - val_acc: 0.6392\n",
      "{'epochs': 200, 'validation_split': 0.1, 'optimizer': 'adam', 'batch_size': 128}\n",
      "Accuracy: 58.45\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "keras_classifier = KerasClassifier(build_fn=build_ann)\n",
    "parameters = {'batch_size': [128, 256],\n",
    "              'epochs': [100],\n",
    "              'optimizer': ['adam', 'nadam'],\n",
    "              'validation_split': [0.1]}\n",
    "grid_search = GridSearchCV(estimator = keras_classifier,\n",
    "                           param_grid = parameters,\n",
    "                           cv = 10,\n",
    "                           verbose=False)\n",
    "grid_search = grid_search.fit(X_nn, y_nn)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print(best_parameters)\n",
    "print('Accuracy: %0.2f' % (best_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T15:51:55.971371Z",
     "start_time": "2018-06-22T15:05:17.530973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 4s 3ms/step - loss: 1.6561 - acc: 0.2784 - val_loss: 1.2203 - val_acc: 0.4571\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.3317 - acc: 0.3914 - val_loss: 1.0767 - val_acc: 0.5886\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.2202 - acc: 0.4457 - val_loss: 1.0257 - val_acc: 0.6229\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.1588 - acc: 0.4764 - val_loss: 0.9960 - val_acc: 0.6457\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 1.1132 - acc: 0.5057 - val_loss: 0.9683 - val_acc: 0.6629\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.0756 - acc: 0.5236 - val_loss: 0.9525 - val_acc: 0.6514\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 1.0674 - acc: 0.5287 - val_loss: 0.9401 - val_acc: 0.6629\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0437 - acc: 0.5453 - val_loss: 0.9376 - val_acc: 0.6629\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.0327 - acc: 0.5556 - val_loss: 0.9250 - val_acc: 0.6629\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.0145 - acc: 0.5594 - val_loss: 0.9178 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.0080 - acc: 0.5734 - val_loss: 0.9159 - val_acc: 0.6571\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9836 - acc: 0.5754 - val_loss: 0.9135 - val_acc: 0.6743\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9617 - acc: 0.5824 - val_loss: 0.9063 - val_acc: 0.6857\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.9706 - acc: 0.5811 - val_loss: 0.8967 - val_acc: 0.6857\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9806 - acc: 0.5811 - val_loss: 0.9040 - val_acc: 0.6743\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9667 - acc: 0.5843 - val_loss: 0.8909 - val_acc: 0.6914\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9531 - acc: 0.5996 - val_loss: 0.8906 - val_acc: 0.6857\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9559 - acc: 0.5964 - val_loss: 0.8905 - val_acc: 0.6629\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.9300 - acc: 0.5983 - val_loss: 0.8871 - val_acc: 0.6686\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9478 - acc: 0.5939 - val_loss: 0.8892 - val_acc: 0.6800\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9395 - acc: 0.6117 - val_loss: 0.8916 - val_acc: 0.6629\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.9208 - acc: 0.6137 - val_loss: 0.8839 - val_acc: 0.6743\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9232 - acc: 0.6066 - val_loss: 0.8898 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9010 - acc: 0.6309 - val_loss: 0.8859 - val_acc: 0.6743\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9145 - acc: 0.6264 - val_loss: 0.8868 - val_acc: 0.6686\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9151 - acc: 0.6252 - val_loss: 0.8834 - val_acc: 0.6743\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8858 - acc: 0.6290 - val_loss: 0.8879 - val_acc: 0.6743\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.9055 - acc: 0.6271 - val_loss: 0.8859 - val_acc: 0.6857\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.8838 - acc: 0.6379 - val_loss: 0.8891 - val_acc: 0.6571\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.8670 - acc: 0.6411 - val_loss: 0.8913 - val_acc: 0.6629\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.8745 - acc: 0.6456 - val_loss: 0.8870 - val_acc: 0.6743\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.8706 - acc: 0.6379 - val_loss: 0.8931 - val_acc: 0.6800\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8646 - acc: 0.6264 - val_loss: 0.8921 - val_acc: 0.6686\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8634 - acc: 0.6354 - val_loss: 0.8883 - val_acc: 0.6743\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8650 - acc: 0.6437 - val_loss: 0.8955 - val_acc: 0.6686\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8567 - acc: 0.6424 - val_loss: 0.8917 - val_acc: 0.6571\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8544 - acc: 0.6622 - val_loss: 0.8912 - val_acc: 0.6686\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.8513 - acc: 0.6488 - val_loss: 0.9017 - val_acc: 0.6571\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.8347 - acc: 0.6577 - val_loss: 0.9011 - val_acc: 0.6514\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8632 - acc: 0.6507 - val_loss: 0.8961 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8419 - acc: 0.6596 - val_loss: 0.8952 - val_acc: 0.6629\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8148 - acc: 0.6667 - val_loss: 0.8965 - val_acc: 0.6686\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8182 - acc: 0.6533 - val_loss: 0.8971 - val_acc: 0.6686\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.8394 - acc: 0.6513 - val_loss: 0.9027 - val_acc: 0.6514\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 122us/step - loss: 0.8144 - acc: 0.6699 - val_loss: 0.8962 - val_acc: 0.6457\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.8240 - acc: 0.6475 - val_loss: 0.8937 - val_acc: 0.6743\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.8111 - acc: 0.6622 - val_loss: 0.8958 - val_acc: 0.6457\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.8047 - acc: 0.6699 - val_loss: 0.9007 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8369 - acc: 0.6494 - val_loss: 0.9013 - val_acc: 0.6457\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8171 - acc: 0.6654 - val_loss: 0.9001 - val_acc: 0.6571\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.8160 - acc: 0.6782 - val_loss: 0.9012 - val_acc: 0.6457\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.8144 - acc: 0.6679 - val_loss: 0.9055 - val_acc: 0.6457\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7699 - acc: 0.6826 - val_loss: 0.9041 - val_acc: 0.6571\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.8187 - acc: 0.6679 - val_loss: 0.9058 - val_acc: 0.6514\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 124us/step - loss: 0.8008 - acc: 0.6769 - val_loss: 0.9056 - val_acc: 0.6571\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.8030 - acc: 0.6737 - val_loss: 0.9156 - val_acc: 0.6571\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8005 - acc: 0.6775 - val_loss: 0.9089 - val_acc: 0.6571\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7794 - acc: 0.6794 - val_loss: 0.9090 - val_acc: 0.6457\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 124us/step - loss: 0.8014 - acc: 0.6731 - val_loss: 0.9110 - val_acc: 0.6400\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.7807 - acc: 0.6916 - val_loss: 0.9108 - val_acc: 0.6457\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7813 - acc: 0.6884 - val_loss: 0.9075 - val_acc: 0.6457\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7764 - acc: 0.6871 - val_loss: 0.9083 - val_acc: 0.6457\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7754 - acc: 0.6833 - val_loss: 0.9099 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7672 - acc: 0.6890 - val_loss: 0.9113 - val_acc: 0.6514\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7577 - acc: 0.6916 - val_loss: 0.9162 - val_acc: 0.6571\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7539 - acc: 0.7018 - val_loss: 0.9096 - val_acc: 0.6343\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7472 - acc: 0.7005 - val_loss: 0.9132 - val_acc: 0.6400\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7863 - acc: 0.6756 - val_loss: 0.9130 - val_acc: 0.6400\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7578 - acc: 0.6948 - val_loss: 0.9117 - val_acc: 0.6514\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7728 - acc: 0.6826 - val_loss: 0.9235 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7662 - acc: 0.6871 - val_loss: 0.9231 - val_acc: 0.6514\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7621 - acc: 0.6935 - val_loss: 0.9237 - val_acc: 0.6457\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7370 - acc: 0.7043 - val_loss: 0.9330 - val_acc: 0.6343\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7495 - acc: 0.6877 - val_loss: 0.9262 - val_acc: 0.6457\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.7500 - acc: 0.6890 - val_loss: 0.9292 - val_acc: 0.6343\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.7435 - acc: 0.6992 - val_loss: 0.9240 - val_acc: 0.6343\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.7330 - acc: 0.7075 - val_loss: 0.9246 - val_acc: 0.6343\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7346 - acc: 0.6980 - val_loss: 0.9243 - val_acc: 0.6343\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7481 - acc: 0.6909 - val_loss: 0.9314 - val_acc: 0.6343\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7235 - acc: 0.7139 - val_loss: 0.9256 - val_acc: 0.6400\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7254 - acc: 0.7050 - val_loss: 0.9246 - val_acc: 0.6571\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7373 - acc: 0.6935 - val_loss: 0.9350 - val_acc: 0.6400\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7273 - acc: 0.7063 - val_loss: 0.9342 - val_acc: 0.6400\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.7180 - acc: 0.6999 - val_loss: 0.9287 - val_acc: 0.6571\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7220 - acc: 0.7190 - val_loss: 0.9398 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7340 - acc: 0.7063 - val_loss: 0.9366 - val_acc: 0.6571\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6926 - acc: 0.7280 - val_loss: 0.9370 - val_acc: 0.6571\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 116us/step - loss: 0.7082 - acc: 0.7056 - val_loss: 0.9423 - val_acc: 0.6457\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6952 - acc: 0.7203 - val_loss: 0.9463 - val_acc: 0.6457\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7074 - acc: 0.7299 - val_loss: 0.9479 - val_acc: 0.6400\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7006 - acc: 0.7197 - val_loss: 0.9464 - val_acc: 0.6400\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.6873 - acc: 0.7280 - val_loss: 0.9416 - val_acc: 0.6400\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 176us/step - loss: 0.7110 - acc: 0.7209 - val_loss: 0.9445 - val_acc: 0.6343\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 120us/step - loss: 0.6997 - acc: 0.7184 - val_loss: 0.9493 - val_acc: 0.6343\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 112us/step - loss: 0.7104 - acc: 0.7101 - val_loss: 0.9467 - val_acc: 0.6343\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 135us/step - loss: 0.7098 - acc: 0.7184 - val_loss: 0.9445 - val_acc: 0.6343\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 119us/step - loss: 0.6881 - acc: 0.7114 - val_loss: 0.9510 - val_acc: 0.6343\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 139us/step - loss: 0.7066 - acc: 0.7158 - val_loss: 0.9540 - val_acc: 0.6400\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 113us/step - loss: 0.6921 - acc: 0.7241 - val_loss: 0.9518 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.6833 - acc: 0.7318 - val_loss: 0.9516 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 64us/step\n",
      "1741/1741 [==============================] - 0s 85us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4636 - acc: 0.3123 - val_loss: 1.2001 - val_acc: 0.4800\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 1.2843 - acc: 0.3972 - val_loss: 1.1057 - val_acc: 0.5257\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 131us/step - loss: 1.1966 - acc: 0.4604 - val_loss: 1.0520 - val_acc: 0.5829\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 146us/step - loss: 1.1399 - acc: 0.4936 - val_loss: 1.0158 - val_acc: 0.6000\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 152us/step - loss: 1.0985 - acc: 0.5109 - val_loss: 0.9924 - val_acc: 0.6171\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 1.0757 - acc: 0.5498 - val_loss: 0.9672 - val_acc: 0.6400\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 1.0596 - acc: 0.5549 - val_loss: 0.9504 - val_acc: 0.6629\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 1.0322 - acc: 0.5568 - val_loss: 0.9417 - val_acc: 0.6229\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.0190 - acc: 0.5754 - val_loss: 0.9360 - val_acc: 0.6286\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 1.0057 - acc: 0.5658 - val_loss: 0.9289 - val_acc: 0.6514\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.9893 - acc: 0.5824 - val_loss: 0.9244 - val_acc: 0.6743\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9813 - acc: 0.5760 - val_loss: 0.9185 - val_acc: 0.6686\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9658 - acc: 0.5856 - val_loss: 0.9150 - val_acc: 0.6571\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.9397 - acc: 0.6015 - val_loss: 0.9085 - val_acc: 0.6686\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.9328 - acc: 0.6117 - val_loss: 0.9038 - val_acc: 0.6686\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9481 - acc: 0.6015 - val_loss: 0.9000 - val_acc: 0.6571\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9284 - acc: 0.6105 - val_loss: 0.8973 - val_acc: 0.6571\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.9418 - acc: 0.5913 - val_loss: 0.8974 - val_acc: 0.6629\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.9075 - acc: 0.6117 - val_loss: 0.8980 - val_acc: 0.6629\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9115 - acc: 0.6207 - val_loss: 0.8946 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.8919 - acc: 0.6379 - val_loss: 0.8919 - val_acc: 0.6514\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.8943 - acc: 0.6303 - val_loss: 0.8870 - val_acc: 0.6571\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.8928 - acc: 0.6277 - val_loss: 0.8922 - val_acc: 0.6514\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.8947 - acc: 0.6367 - val_loss: 0.8899 - val_acc: 0.6629\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.8962 - acc: 0.6124 - val_loss: 0.8891 - val_acc: 0.6629\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.8798 - acc: 0.6411 - val_loss: 0.8860 - val_acc: 0.6686\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.8928 - acc: 0.6398 - val_loss: 0.8961 - val_acc: 0.6400\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.8767 - acc: 0.6456 - val_loss: 0.8853 - val_acc: 0.6571\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.8398 - acc: 0.6494 - val_loss: 0.8898 - val_acc: 0.6571\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.8728 - acc: 0.6354 - val_loss: 0.8944 - val_acc: 0.6400\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.8668 - acc: 0.6533 - val_loss: 0.8872 - val_acc: 0.6571\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.8401 - acc: 0.6571 - val_loss: 0.8937 - val_acc: 0.6571\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8629 - acc: 0.6450 - val_loss: 0.8848 - val_acc: 0.6457\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.8472 - acc: 0.6596 - val_loss: 0.8921 - val_acc: 0.6571\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.8515 - acc: 0.6558 - val_loss: 0.8931 - val_acc: 0.6629\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8191 - acc: 0.6571 - val_loss: 0.8932 - val_acc: 0.6514\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8330 - acc: 0.6552 - val_loss: 0.8917 - val_acc: 0.6514\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.8314 - acc: 0.6571 - val_loss: 0.8916 - val_acc: 0.6629\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.8391 - acc: 0.6616 - val_loss: 0.8926 - val_acc: 0.6571\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.8334 - acc: 0.6424 - val_loss: 0.8913 - val_acc: 0.6457\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.8129 - acc: 0.6686 - val_loss: 0.8957 - val_acc: 0.6457\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.8211 - acc: 0.6724 - val_loss: 0.8938 - val_acc: 0.6514\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.8041 - acc: 0.6648 - val_loss: 0.8947 - val_acc: 0.6514\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8117 - acc: 0.6718 - val_loss: 0.8936 - val_acc: 0.6343\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8039 - acc: 0.6667 - val_loss: 0.8946 - val_acc: 0.6571\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.8243 - acc: 0.6622 - val_loss: 0.8943 - val_acc: 0.6514\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.8053 - acc: 0.6711 - val_loss: 0.8923 - val_acc: 0.6629\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8053 - acc: 0.6743 - val_loss: 0.8928 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8012 - acc: 0.6679 - val_loss: 0.8960 - val_acc: 0.6571\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 118us/step - loss: 0.7997 - acc: 0.6603 - val_loss: 0.8993 - val_acc: 0.6571\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7960 - acc: 0.6635 - val_loss: 0.9029 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.7999 - acc: 0.6769 - val_loss: 0.8993 - val_acc: 0.6514\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7955 - acc: 0.6826 - val_loss: 0.9013 - val_acc: 0.6514\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7913 - acc: 0.6788 - val_loss: 0.8962 - val_acc: 0.6800\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.7748 - acc: 0.6724 - val_loss: 0.8992 - val_acc: 0.6514\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.7861 - acc: 0.6826 - val_loss: 0.9007 - val_acc: 0.6571\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 115us/step - loss: 0.7737 - acc: 0.6833 - val_loss: 0.8991 - val_acc: 0.6457\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7687 - acc: 0.6986 - val_loss: 0.8961 - val_acc: 0.6629\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7520 - acc: 0.6890 - val_loss: 0.9028 - val_acc: 0.6514\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.7715 - acc: 0.6871 - val_loss: 0.9078 - val_acc: 0.6457\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7346 - acc: 0.7043 - val_loss: 0.9027 - val_acc: 0.6514\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7474 - acc: 0.6992 - val_loss: 0.9061 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7367 - acc: 0.6897 - val_loss: 0.9076 - val_acc: 0.6400\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7640 - acc: 0.6960 - val_loss: 0.9172 - val_acc: 0.6457\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7470 - acc: 0.6941 - val_loss: 0.9135 - val_acc: 0.6457\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7341 - acc: 0.7011 - val_loss: 0.9194 - val_acc: 0.6571\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7491 - acc: 0.7082 - val_loss: 0.9268 - val_acc: 0.6400\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7325 - acc: 0.7050 - val_loss: 0.9220 - val_acc: 0.6571\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7301 - acc: 0.7082 - val_loss: 0.9224 - val_acc: 0.6686\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7303 - acc: 0.7037 - val_loss: 0.9110 - val_acc: 0.6686\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7272 - acc: 0.7139 - val_loss: 0.9164 - val_acc: 0.6514\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7303 - acc: 0.7184 - val_loss: 0.9151 - val_acc: 0.6457\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7246 - acc: 0.7037 - val_loss: 0.9187 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7348 - acc: 0.7069 - val_loss: 0.9232 - val_acc: 0.6400\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7392 - acc: 0.7095 - val_loss: 0.9300 - val_acc: 0.6514\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.7244 - acc: 0.7063 - val_loss: 0.9300 - val_acc: 0.6571\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7289 - acc: 0.7101 - val_loss: 0.9298 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7116 - acc: 0.7203 - val_loss: 0.9313 - val_acc: 0.6457\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7066 - acc: 0.7101 - val_loss: 0.9450 - val_acc: 0.6514\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6954 - acc: 0.7267 - val_loss: 0.9401 - val_acc: 0.6457\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7114 - acc: 0.7133 - val_loss: 0.9376 - val_acc: 0.6629\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7282 - acc: 0.7075 - val_loss: 0.9360 - val_acc: 0.6629\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7083 - acc: 0.7184 - val_loss: 0.9417 - val_acc: 0.6629\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7118 - acc: 0.7158 - val_loss: 0.9372 - val_acc: 0.6514\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.7120 - acc: 0.7050 - val_loss: 0.9402 - val_acc: 0.6457\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7023 - acc: 0.7292 - val_loss: 0.9395 - val_acc: 0.6514\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.6944 - acc: 0.7197 - val_loss: 0.9351 - val_acc: 0.6571\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6943 - acc: 0.7120 - val_loss: 0.9441 - val_acc: 0.6629\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.7068 - acc: 0.7222 - val_loss: 0.9426 - val_acc: 0.6629\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6894 - acc: 0.7152 - val_loss: 0.9408 - val_acc: 0.6629\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6877 - acc: 0.7190 - val_loss: 0.9470 - val_acc: 0.6629\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6680 - acc: 0.7254 - val_loss: 0.9487 - val_acc: 0.6686\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6980 - acc: 0.7222 - val_loss: 0.9526 - val_acc: 0.6686\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6941 - acc: 0.7171 - val_loss: 0.9527 - val_acc: 0.6629\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6700 - acc: 0.7324 - val_loss: 0.9489 - val_acc: 0.6571\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6794 - acc: 0.7152 - val_loss: 0.9515 - val_acc: 0.6571\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6711 - acc: 0.7331 - val_loss: 0.9506 - val_acc: 0.6743\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6896 - acc: 0.7241 - val_loss: 0.9585 - val_acc: 0.6629\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6786 - acc: 0.7344 - val_loss: 0.9584 - val_acc: 0.6514\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6755 - acc: 0.7286 - val_loss: 0.9603 - val_acc: 0.6629\n",
      "194/194 [==============================] - 0s 74us/step\n",
      "1741/1741 [==============================] - 0s 51us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.5857 - acc: 0.2944 - val_loss: 1.1986 - val_acc: 0.4457\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 1.2829 - acc: 0.4157 - val_loss: 1.0752 - val_acc: 0.6000\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 1.1797 - acc: 0.4725 - val_loss: 1.0294 - val_acc: 0.6400\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.1643 - acc: 0.4821 - val_loss: 1.0023 - val_acc: 0.6400\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.0977 - acc: 0.5166 - val_loss: 0.9729 - val_acc: 0.6629\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0766 - acc: 0.5473 - val_loss: 0.9591 - val_acc: 0.6571\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0871 - acc: 0.5166 - val_loss: 0.9520 - val_acc: 0.6514\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.0569 - acc: 0.5377 - val_loss: 0.9405 - val_acc: 0.6514\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 1.0415 - acc: 0.5556 - val_loss: 0.9371 - val_acc: 0.6629\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 112us/step - loss: 1.0423 - acc: 0.5517 - val_loss: 0.9340 - val_acc: 0.6686\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 1.0081 - acc: 0.5543 - val_loss: 0.9221 - val_acc: 0.6571\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9902 - acc: 0.5760 - val_loss: 0.9202 - val_acc: 0.6800\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9896 - acc: 0.5760 - val_loss: 0.9161 - val_acc: 0.6514\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9813 - acc: 0.5862 - val_loss: 0.9153 - val_acc: 0.6686\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9774 - acc: 0.5849 - val_loss: 0.9183 - val_acc: 0.6686\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9601 - acc: 0.5849 - val_loss: 0.9165 - val_acc: 0.6800\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9326 - acc: 0.6162 - val_loss: 0.9107 - val_acc: 0.6629\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9543 - acc: 0.5920 - val_loss: 0.9088 - val_acc: 0.6514\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9273 - acc: 0.6086 - val_loss: 0.9083 - val_acc: 0.6800\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9322 - acc: 0.6124 - val_loss: 0.9046 - val_acc: 0.6971\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9155 - acc: 0.6028 - val_loss: 0.9081 - val_acc: 0.6800\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8988 - acc: 0.6354 - val_loss: 0.9068 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.9146 - acc: 0.6028 - val_loss: 0.9080 - val_acc: 0.6686\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.9187 - acc: 0.6156 - val_loss: 0.9053 - val_acc: 0.6743\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8987 - acc: 0.6258 - val_loss: 0.9051 - val_acc: 0.6743\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.8962 - acc: 0.6315 - val_loss: 0.8997 - val_acc: 0.6914\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.8816 - acc: 0.6315 - val_loss: 0.9018 - val_acc: 0.6800\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8792 - acc: 0.6277 - val_loss: 0.8971 - val_acc: 0.6743\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8760 - acc: 0.6303 - val_loss: 0.8937 - val_acc: 0.6743\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9004 - acc: 0.6143 - val_loss: 0.8951 - val_acc: 0.6857\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8728 - acc: 0.6328 - val_loss: 0.8976 - val_acc: 0.6914\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8794 - acc: 0.6245 - val_loss: 0.8974 - val_acc: 0.6743\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8509 - acc: 0.6456 - val_loss: 0.8957 - val_acc: 0.6686\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8528 - acc: 0.6641 - val_loss: 0.9031 - val_acc: 0.6800\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8599 - acc: 0.6469 - val_loss: 0.9022 - val_acc: 0.6743\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8588 - acc: 0.6501 - val_loss: 0.9038 - val_acc: 0.6686\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8390 - acc: 0.6590 - val_loss: 0.9012 - val_acc: 0.6743\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8507 - acc: 0.6501 - val_loss: 0.9035 - val_acc: 0.6800\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8499 - acc: 0.6679 - val_loss: 0.9056 - val_acc: 0.6857\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8423 - acc: 0.6462 - val_loss: 0.9046 - val_acc: 0.6800\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8413 - acc: 0.6456 - val_loss: 0.9076 - val_acc: 0.6629\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8363 - acc: 0.6533 - val_loss: 0.9046 - val_acc: 0.6571\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8288 - acc: 0.6584 - val_loss: 0.9048 - val_acc: 0.6686\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8334 - acc: 0.6494 - val_loss: 0.9036 - val_acc: 0.6686\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8221 - acc: 0.6545 - val_loss: 0.9034 - val_acc: 0.6743\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8250 - acc: 0.6564 - val_loss: 0.9091 - val_acc: 0.6629\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8215 - acc: 0.6539 - val_loss: 0.9061 - val_acc: 0.6629\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8058 - acc: 0.6794 - val_loss: 0.9101 - val_acc: 0.6743\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8049 - acc: 0.6775 - val_loss: 0.9119 - val_acc: 0.6686\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8022 - acc: 0.6826 - val_loss: 0.9191 - val_acc: 0.6686\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8027 - acc: 0.6660 - val_loss: 0.9227 - val_acc: 0.6514\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7967 - acc: 0.6782 - val_loss: 0.9258 - val_acc: 0.6514\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8148 - acc: 0.6718 - val_loss: 0.9213 - val_acc: 0.6629\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7896 - acc: 0.6775 - val_loss: 0.9233 - val_acc: 0.6571\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7798 - acc: 0.6845 - val_loss: 0.9253 - val_acc: 0.6514\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8213 - acc: 0.6641 - val_loss: 0.9199 - val_acc: 0.6571\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8046 - acc: 0.6692 - val_loss: 0.9245 - val_acc: 0.6629\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7736 - acc: 0.6782 - val_loss: 0.9188 - val_acc: 0.6686\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7963 - acc: 0.6750 - val_loss: 0.9246 - val_acc: 0.6514\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7683 - acc: 0.6769 - val_loss: 0.9246 - val_acc: 0.6571\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7740 - acc: 0.6807 - val_loss: 0.9259 - val_acc: 0.6629\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7661 - acc: 0.6941 - val_loss: 0.9211 - val_acc: 0.6629\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7688 - acc: 0.6743 - val_loss: 0.9219 - val_acc: 0.6400\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7680 - acc: 0.6852 - val_loss: 0.9318 - val_acc: 0.6286\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7662 - acc: 0.6820 - val_loss: 0.9335 - val_acc: 0.6514\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7640 - acc: 0.6839 - val_loss: 0.9366 - val_acc: 0.6457\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7547 - acc: 0.6922 - val_loss: 0.9350 - val_acc: 0.6514\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7810 - acc: 0.6909 - val_loss: 0.9372 - val_acc: 0.6514\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7327 - acc: 0.6973 - val_loss: 0.9335 - val_acc: 0.6229\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7557 - acc: 0.6992 - val_loss: 0.9374 - val_acc: 0.6571\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7350 - acc: 0.7037 - val_loss: 0.9319 - val_acc: 0.6571\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7423 - acc: 0.7037 - val_loss: 0.9372 - val_acc: 0.6629\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7502 - acc: 0.6973 - val_loss: 0.9351 - val_acc: 0.6457\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7410 - acc: 0.6928 - val_loss: 0.9362 - val_acc: 0.6286\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7495 - acc: 0.6820 - val_loss: 0.9328 - val_acc: 0.6343\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7453 - acc: 0.6916 - val_loss: 0.9414 - val_acc: 0.6514\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7363 - acc: 0.7024 - val_loss: 0.9422 - val_acc: 0.6400\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7524 - acc: 0.6935 - val_loss: 0.9467 - val_acc: 0.6343\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7391 - acc: 0.6858 - val_loss: 0.9492 - val_acc: 0.6229\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7337 - acc: 0.6935 - val_loss: 0.9426 - val_acc: 0.6400\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7218 - acc: 0.6954 - val_loss: 0.9385 - val_acc: 0.6343\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7174 - acc: 0.7088 - val_loss: 0.9473 - val_acc: 0.6171\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7201 - acc: 0.7043 - val_loss: 0.9436 - val_acc: 0.6571\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7188 - acc: 0.7082 - val_loss: 0.9487 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7032 - acc: 0.7216 - val_loss: 0.9465 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7093 - acc: 0.7165 - val_loss: 0.9506 - val_acc: 0.6229\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7049 - acc: 0.7139 - val_loss: 0.9543 - val_acc: 0.6171\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6968 - acc: 0.7197 - val_loss: 0.9512 - val_acc: 0.6457\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7206 - acc: 0.7056 - val_loss: 0.9563 - val_acc: 0.6400\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7047 - acc: 0.7107 - val_loss: 0.9593 - val_acc: 0.6343\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7027 - acc: 0.7114 - val_loss: 0.9609 - val_acc: 0.6286\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7123 - acc: 0.6980 - val_loss: 0.9551 - val_acc: 0.6343\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6807 - acc: 0.7235 - val_loss: 0.9514 - val_acc: 0.6457\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6874 - acc: 0.7209 - val_loss: 0.9516 - val_acc: 0.6400\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7131 - acc: 0.7088 - val_loss: 0.9657 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7004 - acc: 0.7229 - val_loss: 0.9660 - val_acc: 0.6400\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6962 - acc: 0.7082 - val_loss: 0.9630 - val_acc: 0.6400\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6863 - acc: 0.7120 - val_loss: 0.9699 - val_acc: 0.6343\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6887 - acc: 0.7197 - val_loss: 0.9720 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7018 - acc: 0.7197 - val_loss: 0.9787 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 66us/step\n",
      "1741/1741 [==============================] - 0s 50us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 4s 3ms/step - loss: 1.4398 - acc: 0.3263 - val_loss: 1.1885 - val_acc: 0.5200\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.2897 - acc: 0.3953 - val_loss: 1.0962 - val_acc: 0.5714\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.2021 - acc: 0.4489 - val_loss: 1.0456 - val_acc: 0.5829\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.1447 - acc: 0.4974 - val_loss: 1.0217 - val_acc: 0.5886\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.1137 - acc: 0.4955 - val_loss: 0.9940 - val_acc: 0.6171\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.0758 - acc: 0.5307 - val_loss: 0.9796 - val_acc: 0.6629\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 1.0691 - acc: 0.5307 - val_loss: 0.9606 - val_acc: 0.6629\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0580 - acc: 0.5364 - val_loss: 0.9457 - val_acc: 0.6514\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.0396 - acc: 0.5536 - val_loss: 0.9360 - val_acc: 0.6686\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0137 - acc: 0.5651 - val_loss: 0.9284 - val_acc: 0.6743\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.0060 - acc: 0.5556 - val_loss: 0.9234 - val_acc: 0.6857\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9746 - acc: 0.5830 - val_loss: 0.9220 - val_acc: 0.6686\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9988 - acc: 0.5734 - val_loss: 0.9196 - val_acc: 0.6686\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.9700 - acc: 0.5830 - val_loss: 0.9116 - val_acc: 0.6686\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9698 - acc: 0.5900 - val_loss: 0.9064 - val_acc: 0.6857\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9547 - acc: 0.5875 - val_loss: 0.9033 - val_acc: 0.6743\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9366 - acc: 0.5958 - val_loss: 0.9088 - val_acc: 0.6800\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9549 - acc: 0.5977 - val_loss: 0.9019 - val_acc: 0.6857\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9421 - acc: 0.6028 - val_loss: 0.8991 - val_acc: 0.7029\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9298 - acc: 0.6111 - val_loss: 0.9023 - val_acc: 0.6800\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9142 - acc: 0.6098 - val_loss: 0.9042 - val_acc: 0.6743\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9279 - acc: 0.6117 - val_loss: 0.9037 - val_acc: 0.6743\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9235 - acc: 0.6207 - val_loss: 0.9027 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9193 - acc: 0.6284 - val_loss: 0.9008 - val_acc: 0.6743\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9194 - acc: 0.6188 - val_loss: 0.9018 - val_acc: 0.6686\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8991 - acc: 0.6130 - val_loss: 0.9006 - val_acc: 0.6686\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9007 - acc: 0.6277 - val_loss: 0.9077 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.8806 - acc: 0.6232 - val_loss: 0.9060 - val_acc: 0.6629\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.8946 - acc: 0.6335 - val_loss: 0.8999 - val_acc: 0.6686\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8811 - acc: 0.6271 - val_loss: 0.8991 - val_acc: 0.6743\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8800 - acc: 0.6252 - val_loss: 0.9025 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8941 - acc: 0.6252 - val_loss: 0.8977 - val_acc: 0.6800\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8710 - acc: 0.6481 - val_loss: 0.8982 - val_acc: 0.6686\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8666 - acc: 0.6501 - val_loss: 0.8989 - val_acc: 0.6914\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8692 - acc: 0.6418 - val_loss: 0.9042 - val_acc: 0.6800\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8715 - acc: 0.6443 - val_loss: 0.9070 - val_acc: 0.6457\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8550 - acc: 0.6443 - val_loss: 0.9032 - val_acc: 0.6629\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8665 - acc: 0.6424 - val_loss: 0.8946 - val_acc: 0.6857\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8465 - acc: 0.6462 - val_loss: 0.8968 - val_acc: 0.6629\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8549 - acc: 0.6367 - val_loss: 0.9051 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8301 - acc: 0.6494 - val_loss: 0.9002 - val_acc: 0.6629\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8435 - acc: 0.6469 - val_loss: 0.9041 - val_acc: 0.6514\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8457 - acc: 0.6494 - val_loss: 0.9098 - val_acc: 0.6457\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8319 - acc: 0.6539 - val_loss: 0.9096 - val_acc: 0.6514\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8247 - acc: 0.6507 - val_loss: 0.9097 - val_acc: 0.6514\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8274 - acc: 0.6609 - val_loss: 0.9115 - val_acc: 0.6571\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8464 - acc: 0.6564 - val_loss: 0.9090 - val_acc: 0.6457\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8323 - acc: 0.6609 - val_loss: 0.9135 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8332 - acc: 0.6462 - val_loss: 0.9115 - val_acc: 0.6571\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8158 - acc: 0.6635 - val_loss: 0.9150 - val_acc: 0.6457\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8027 - acc: 0.6660 - val_loss: 0.9175 - val_acc: 0.6457\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8167 - acc: 0.6596 - val_loss: 0.9194 - val_acc: 0.6457\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8145 - acc: 0.6584 - val_loss: 0.9188 - val_acc: 0.6457\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7959 - acc: 0.6699 - val_loss: 0.9208 - val_acc: 0.6400\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8106 - acc: 0.6584 - val_loss: 0.9190 - val_acc: 0.6514\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7964 - acc: 0.6814 - val_loss: 0.9200 - val_acc: 0.6514\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7921 - acc: 0.6699 - val_loss: 0.9194 - val_acc: 0.6514\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7950 - acc: 0.6673 - val_loss: 0.9277 - val_acc: 0.6629\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7918 - acc: 0.6916 - val_loss: 0.9228 - val_acc: 0.6457\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7786 - acc: 0.6833 - val_loss: 0.9203 - val_acc: 0.6629\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7984 - acc: 0.6699 - val_loss: 0.9260 - val_acc: 0.6571\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7845 - acc: 0.6775 - val_loss: 0.9250 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7722 - acc: 0.6814 - val_loss: 0.9245 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7903 - acc: 0.6826 - val_loss: 0.9276 - val_acc: 0.6514\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7772 - acc: 0.6922 - val_loss: 0.9270 - val_acc: 0.6457\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7583 - acc: 0.6852 - val_loss: 0.9295 - val_acc: 0.6514\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7676 - acc: 0.6852 - val_loss: 0.9317 - val_acc: 0.6571\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7782 - acc: 0.6762 - val_loss: 0.9370 - val_acc: 0.6629\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7729 - acc: 0.6807 - val_loss: 0.9321 - val_acc: 0.6514\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7578 - acc: 0.6980 - val_loss: 0.9448 - val_acc: 0.6629\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7693 - acc: 0.6935 - val_loss: 0.9356 - val_acc: 0.6571\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7628 - acc: 0.6877 - val_loss: 0.9431 - val_acc: 0.6400\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7604 - acc: 0.6814 - val_loss: 0.9424 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7369 - acc: 0.6948 - val_loss: 0.9402 - val_acc: 0.6571\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7552 - acc: 0.7011 - val_loss: 0.9396 - val_acc: 0.6629\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7441 - acc: 0.7011 - val_loss: 0.9408 - val_acc: 0.6457\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7339 - acc: 0.6992 - val_loss: 0.9451 - val_acc: 0.6514\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7683 - acc: 0.6928 - val_loss: 0.9462 - val_acc: 0.6571\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7474 - acc: 0.7037 - val_loss: 0.9446 - val_acc: 0.6686\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7491 - acc: 0.6992 - val_loss: 0.9494 - val_acc: 0.6571\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7285 - acc: 0.6948 - val_loss: 0.9450 - val_acc: 0.6457\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7299 - acc: 0.7126 - val_loss: 0.9512 - val_acc: 0.6400\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7289 - acc: 0.6992 - val_loss: 0.9550 - val_acc: 0.6400\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7518 - acc: 0.6871 - val_loss: 0.9533 - val_acc: 0.6457\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7245 - acc: 0.7011 - val_loss: 0.9576 - val_acc: 0.6457\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7253 - acc: 0.7088 - val_loss: 0.9578 - val_acc: 0.6514\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7282 - acc: 0.7056 - val_loss: 0.9590 - val_acc: 0.6229\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7206 - acc: 0.7031 - val_loss: 0.9660 - val_acc: 0.6400\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7270 - acc: 0.7146 - val_loss: 0.9562 - val_acc: 0.6457\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7177 - acc: 0.7011 - val_loss: 0.9641 - val_acc: 0.6457\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7300 - acc: 0.7024 - val_loss: 0.9654 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6983 - acc: 0.7190 - val_loss: 0.9652 - val_acc: 0.6457\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7187 - acc: 0.7133 - val_loss: 0.9689 - val_acc: 0.6400\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7119 - acc: 0.6909 - val_loss: 0.9666 - val_acc: 0.6629\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7292 - acc: 0.7050 - val_loss: 0.9649 - val_acc: 0.6400\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7045 - acc: 0.7165 - val_loss: 0.9724 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7128 - acc: 0.7197 - val_loss: 0.9693 - val_acc: 0.6514\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6887 - acc: 0.7165 - val_loss: 0.9746 - val_acc: 0.6343\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6931 - acc: 0.7241 - val_loss: 0.9775 - val_acc: 0.6457\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6948 - acc: 0.7133 - val_loss: 0.9828 - val_acc: 0.6229\n",
      "194/194 [==============================] - 0s 63us/step\n",
      "1741/1741 [==============================] - 0s 38us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 4s 3ms/step - loss: 1.4458 - acc: 0.3186 - val_loss: 1.2021 - val_acc: 0.5257\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.2731 - acc: 0.4291 - val_loss: 1.1022 - val_acc: 0.5886\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.2149 - acc: 0.4566 - val_loss: 1.0441 - val_acc: 0.6229\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 1.1427 - acc: 0.4943 - val_loss: 1.0102 - val_acc: 0.6457\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 1.1267 - acc: 0.5115 - val_loss: 0.9896 - val_acc: 0.6629\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.0796 - acc: 0.5319 - val_loss: 0.9676 - val_acc: 0.6457\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.0669 - acc: 0.5434 - val_loss: 0.9524 - val_acc: 0.6286\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0489 - acc: 0.5428 - val_loss: 0.9385 - val_acc: 0.6514\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0278 - acc: 0.5517 - val_loss: 0.9235 - val_acc: 0.6686\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0202 - acc: 0.5734 - val_loss: 0.9280 - val_acc: 0.6514\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9893 - acc: 0.5594 - val_loss: 0.9192 - val_acc: 0.6629\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9808 - acc: 0.5792 - val_loss: 0.9098 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9860 - acc: 0.5888 - val_loss: 0.8990 - val_acc: 0.6800\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9553 - acc: 0.5951 - val_loss: 0.8947 - val_acc: 0.6857\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9758 - acc: 0.5881 - val_loss: 0.8957 - val_acc: 0.6914\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.9574 - acc: 0.5888 - val_loss: 0.8897 - val_acc: 0.6914\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9300 - acc: 0.6175 - val_loss: 0.8902 - val_acc: 0.6857\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9456 - acc: 0.6220 - val_loss: 0.8859 - val_acc: 0.6857\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9241 - acc: 0.5945 - val_loss: 0.8878 - val_acc: 0.6743\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9265 - acc: 0.6201 - val_loss: 0.8859 - val_acc: 0.6686\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9006 - acc: 0.6347 - val_loss: 0.8901 - val_acc: 0.6571\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9115 - acc: 0.6239 - val_loss: 0.8888 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8905 - acc: 0.6188 - val_loss: 0.8885 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9140 - acc: 0.6066 - val_loss: 0.8859 - val_acc: 0.6629\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8910 - acc: 0.6341 - val_loss: 0.8926 - val_acc: 0.6686\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8864 - acc: 0.6405 - val_loss: 0.8837 - val_acc: 0.6629\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.9109 - acc: 0.6111 - val_loss: 0.8767 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8794 - acc: 0.6201 - val_loss: 0.8798 - val_acc: 0.6743\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8898 - acc: 0.6296 - val_loss: 0.8799 - val_acc: 0.6686\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.8734 - acc: 0.6430 - val_loss: 0.8738 - val_acc: 0.6743\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8540 - acc: 0.6450 - val_loss: 0.8773 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8562 - acc: 0.6481 - val_loss: 0.8758 - val_acc: 0.6743\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8505 - acc: 0.6411 - val_loss: 0.8780 - val_acc: 0.6686\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.8641 - acc: 0.6494 - val_loss: 0.8831 - val_acc: 0.6686\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.8537 - acc: 0.6456 - val_loss: 0.8761 - val_acc: 0.6686\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.8529 - acc: 0.6513 - val_loss: 0.8824 - val_acc: 0.6743\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8400 - acc: 0.6424 - val_loss: 0.8863 - val_acc: 0.6686\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8328 - acc: 0.6628 - val_loss: 0.8866 - val_acc: 0.6629\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8299 - acc: 0.6456 - val_loss: 0.8824 - val_acc: 0.6571\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8345 - acc: 0.6641 - val_loss: 0.8843 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8345 - acc: 0.6552 - val_loss: 0.8882 - val_acc: 0.6629\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8310 - acc: 0.6526 - val_loss: 0.8868 - val_acc: 0.6571\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8036 - acc: 0.6648 - val_loss: 0.8854 - val_acc: 0.6514\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8373 - acc: 0.6335 - val_loss: 0.8849 - val_acc: 0.6686\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8168 - acc: 0.6635 - val_loss: 0.8916 - val_acc: 0.6629\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8117 - acc: 0.6686 - val_loss: 0.8897 - val_acc: 0.6743\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8209 - acc: 0.6673 - val_loss: 0.8943 - val_acc: 0.6686\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.8205 - acc: 0.6641 - val_loss: 0.8872 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8215 - acc: 0.6635 - val_loss: 0.8917 - val_acc: 0.6686\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8098 - acc: 0.6622 - val_loss: 0.8839 - val_acc: 0.6686\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8162 - acc: 0.6622 - val_loss: 0.8874 - val_acc: 0.6743\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8070 - acc: 0.6731 - val_loss: 0.8937 - val_acc: 0.6686\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8306 - acc: 0.6475 - val_loss: 0.8901 - val_acc: 0.6743\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7854 - acc: 0.6839 - val_loss: 0.8928 - val_acc: 0.6629\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8102 - acc: 0.6679 - val_loss: 0.8895 - val_acc: 0.6629\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7853 - acc: 0.6775 - val_loss: 0.8917 - val_acc: 0.6914\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7829 - acc: 0.6903 - val_loss: 0.8966 - val_acc: 0.6800\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7888 - acc: 0.6731 - val_loss: 0.8983 - val_acc: 0.6743\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7878 - acc: 0.6635 - val_loss: 0.8893 - val_acc: 0.6629\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7915 - acc: 0.6916 - val_loss: 0.9009 - val_acc: 0.6857\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7841 - acc: 0.6743 - val_loss: 0.8940 - val_acc: 0.6800\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7722 - acc: 0.6750 - val_loss: 0.8993 - val_acc: 0.6743\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7550 - acc: 0.6839 - val_loss: 0.8998 - val_acc: 0.6800\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7676 - acc: 0.6788 - val_loss: 0.9008 - val_acc: 0.6743\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7593 - acc: 0.6967 - val_loss: 0.8971 - val_acc: 0.6571\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7548 - acc: 0.6909 - val_loss: 0.9058 - val_acc: 0.6686\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7589 - acc: 0.6897 - val_loss: 0.9002 - val_acc: 0.6800\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7522 - acc: 0.6845 - val_loss: 0.9061 - val_acc: 0.6686\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7643 - acc: 0.6884 - val_loss: 0.9066 - val_acc: 0.6686\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7564 - acc: 0.6884 - val_loss: 0.9104 - val_acc: 0.6686\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7459 - acc: 0.6992 - val_loss: 0.9095 - val_acc: 0.6457\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7581 - acc: 0.7031 - val_loss: 0.9154 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7402 - acc: 0.7069 - val_loss: 0.9148 - val_acc: 0.6857\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7475 - acc: 0.7018 - val_loss: 0.9138 - val_acc: 0.6800\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7316 - acc: 0.6928 - val_loss: 0.9189 - val_acc: 0.6686\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7454 - acc: 0.6992 - val_loss: 0.9187 - val_acc: 0.6686\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7262 - acc: 0.6992 - val_loss: 0.9162 - val_acc: 0.6743\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7432 - acc: 0.6852 - val_loss: 0.9178 - val_acc: 0.6857\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7500 - acc: 0.6909 - val_loss: 0.9187 - val_acc: 0.6800\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7422 - acc: 0.7056 - val_loss: 0.9175 - val_acc: 0.6800\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.7233 - acc: 0.6980 - val_loss: 0.9204 - val_acc: 0.6743\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.7317 - acc: 0.7031 - val_loss: 0.9229 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.7033 - acc: 0.7056 - val_loss: 0.9267 - val_acc: 0.6629\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7158 - acc: 0.7152 - val_loss: 0.9295 - val_acc: 0.6629\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7237 - acc: 0.7120 - val_loss: 0.9257 - val_acc: 0.6686\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7283 - acc: 0.7114 - val_loss: 0.9271 - val_acc: 0.6571\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7099 - acc: 0.7171 - val_loss: 0.9318 - val_acc: 0.6629\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.7104 - acc: 0.6903 - val_loss: 0.9361 - val_acc: 0.6457\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7169 - acc: 0.7146 - val_loss: 0.9372 - val_acc: 0.6629\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7058 - acc: 0.7139 - val_loss: 0.9340 - val_acc: 0.6514\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6887 - acc: 0.7203 - val_loss: 0.9376 - val_acc: 0.6629\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7007 - acc: 0.7075 - val_loss: 0.9364 - val_acc: 0.6743\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7016 - acc: 0.7101 - val_loss: 0.9443 - val_acc: 0.6571\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6919 - acc: 0.7107 - val_loss: 0.9430 - val_acc: 0.6629\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7044 - acc: 0.7095 - val_loss: 0.9434 - val_acc: 0.6629\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7004 - acc: 0.7216 - val_loss: 0.9453 - val_acc: 0.6629\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7015 - acc: 0.7069 - val_loss: 0.9432 - val_acc: 0.6629\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6938 - acc: 0.7337 - val_loss: 0.9386 - val_acc: 0.6629\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6902 - acc: 0.7165 - val_loss: 0.9467 - val_acc: 0.6629\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6725 - acc: 0.7382 - val_loss: 0.9500 - val_acc: 0.6629\n",
      "194/194 [==============================] - 0s 74us/step\n",
      "1741/1741 [==============================] - 0s 44us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.4924 - acc: 0.3274 - val_loss: 1.1785 - val_acc: 0.5200\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 1.2545 - acc: 0.4314 - val_loss: 1.0719 - val_acc: 0.5771\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.1960 - acc: 0.4595 - val_loss: 1.0282 - val_acc: 0.6343\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 1.1541 - acc: 0.4914 - val_loss: 0.9920 - val_acc: 0.6400\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 1.1114 - acc: 0.5029 - val_loss: 0.9766 - val_acc: 0.6343\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 1.1043 - acc: 0.5175 - val_loss: 0.9518 - val_acc: 0.6571\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.0545 - acc: 0.5399 - val_loss: 0.9410 - val_acc: 0.6743\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 1.0513 - acc: 0.5380 - val_loss: 0.9436 - val_acc: 0.6571\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 1.0243 - acc: 0.5629 - val_loss: 0.9336 - val_acc: 0.6571\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.0024 - acc: 0.5609 - val_loss: 0.9273 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.9977 - acc: 0.5571 - val_loss: 0.9189 - val_acc: 0.6743\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.9722 - acc: 0.5980 - val_loss: 0.9160 - val_acc: 0.6800\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.9778 - acc: 0.5826 - val_loss: 0.9110 - val_acc: 0.6629\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9567 - acc: 0.6063 - val_loss: 0.9132 - val_acc: 0.6914\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9669 - acc: 0.5807 - val_loss: 0.9112 - val_acc: 0.6686\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.9667 - acc: 0.5846 - val_loss: 0.9038 - val_acc: 0.6857\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.9435 - acc: 0.6114 - val_loss: 0.9080 - val_acc: 0.6857\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9422 - acc: 0.6088 - val_loss: 0.8997 - val_acc: 0.6857\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9398 - acc: 0.6177 - val_loss: 0.9014 - val_acc: 0.6971\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9115 - acc: 0.6126 - val_loss: 0.9041 - val_acc: 0.6857\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.9280 - acc: 0.6107 - val_loss: 0.8996 - val_acc: 0.6857\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.9154 - acc: 0.6197 - val_loss: 0.9074 - val_acc: 0.6857\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9030 - acc: 0.6235 - val_loss: 0.8962 - val_acc: 0.6857\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8898 - acc: 0.6171 - val_loss: 0.9007 - val_acc: 0.6914\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8980 - acc: 0.6203 - val_loss: 0.8978 - val_acc: 0.6914\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8786 - acc: 0.6369 - val_loss: 0.9031 - val_acc: 0.6857\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8708 - acc: 0.6420 - val_loss: 0.9001 - val_acc: 0.6857\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8949 - acc: 0.6069 - val_loss: 0.8986 - val_acc: 0.6914\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8860 - acc: 0.6394 - val_loss: 0.8988 - val_acc: 0.6914\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8764 - acc: 0.6369 - val_loss: 0.8947 - val_acc: 0.6857\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8673 - acc: 0.6343 - val_loss: 0.9036 - val_acc: 0.6857\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8612 - acc: 0.6484 - val_loss: 0.9007 - val_acc: 0.6971\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8614 - acc: 0.6420 - val_loss: 0.9036 - val_acc: 0.6971\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8422 - acc: 0.6567 - val_loss: 0.9018 - val_acc: 0.6971\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8506 - acc: 0.6420 - val_loss: 0.9033 - val_acc: 0.6914\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8375 - acc: 0.6528 - val_loss: 0.9032 - val_acc: 0.6914\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8403 - acc: 0.6573 - val_loss: 0.9043 - val_acc: 0.6857\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8465 - acc: 0.6522 - val_loss: 0.9034 - val_acc: 0.6857\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8355 - acc: 0.6592 - val_loss: 0.8989 - val_acc: 0.6914\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8324 - acc: 0.6599 - val_loss: 0.9087 - val_acc: 0.6971\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8420 - acc: 0.6490 - val_loss: 0.9103 - val_acc: 0.6800\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8349 - acc: 0.6567 - val_loss: 0.9069 - val_acc: 0.6914\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8433 - acc: 0.6643 - val_loss: 0.9014 - val_acc: 0.6914\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8325 - acc: 0.6586 - val_loss: 0.9104 - val_acc: 0.6743\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8163 - acc: 0.6573 - val_loss: 0.9172 - val_acc: 0.6857\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8149 - acc: 0.6707 - val_loss: 0.9152 - val_acc: 0.6857\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8090 - acc: 0.6803 - val_loss: 0.9135 - val_acc: 0.6800\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7843 - acc: 0.6771 - val_loss: 0.9095 - val_acc: 0.6857\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8056 - acc: 0.6701 - val_loss: 0.9187 - val_acc: 0.6857\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8099 - acc: 0.6701 - val_loss: 0.9124 - val_acc: 0.6800\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8062 - acc: 0.6669 - val_loss: 0.9096 - val_acc: 0.6800\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7889 - acc: 0.6758 - val_loss: 0.9161 - val_acc: 0.6914\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7994 - acc: 0.6675 - val_loss: 0.9210 - val_acc: 0.6857\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7779 - acc: 0.6879 - val_loss: 0.9219 - val_acc: 0.6686\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7876 - acc: 0.6867 - val_loss: 0.9201 - val_acc: 0.6914\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7804 - acc: 0.6943 - val_loss: 0.9211 - val_acc: 0.6857\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7909 - acc: 0.6854 - val_loss: 0.9189 - val_acc: 0.6800\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7671 - acc: 0.6930 - val_loss: 0.9248 - val_acc: 0.6686\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7555 - acc: 0.6847 - val_loss: 0.9326 - val_acc: 0.6800\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7711 - acc: 0.6854 - val_loss: 0.9249 - val_acc: 0.6686\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7710 - acc: 0.6911 - val_loss: 0.9340 - val_acc: 0.6686\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7569 - acc: 0.6867 - val_loss: 0.9261 - val_acc: 0.6686\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7825 - acc: 0.6765 - val_loss: 0.9315 - val_acc: 0.6800\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7656 - acc: 0.6809 - val_loss: 0.9305 - val_acc: 0.6800\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7618 - acc: 0.7007 - val_loss: 0.9376 - val_acc: 0.6800\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7699 - acc: 0.6752 - val_loss: 0.9349 - val_acc: 0.6686\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7497 - acc: 0.6981 - val_loss: 0.9481 - val_acc: 0.6514\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7359 - acc: 0.6911 - val_loss: 0.9425 - val_acc: 0.6743\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7499 - acc: 0.7013 - val_loss: 0.9447 - val_acc: 0.6629\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7622 - acc: 0.7077 - val_loss: 0.9497 - val_acc: 0.6743\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7515 - acc: 0.7052 - val_loss: 0.9480 - val_acc: 0.6914\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7334 - acc: 0.6905 - val_loss: 0.9431 - val_acc: 0.6629\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7662 - acc: 0.6918 - val_loss: 0.9466 - val_acc: 0.6914\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7273 - acc: 0.6950 - val_loss: 0.9508 - val_acc: 0.6914\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7311 - acc: 0.6905 - val_loss: 0.9499 - val_acc: 0.6571\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7265 - acc: 0.7052 - val_loss: 0.9523 - val_acc: 0.6857\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7194 - acc: 0.6937 - val_loss: 0.9584 - val_acc: 0.6686\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7388 - acc: 0.7090 - val_loss: 0.9520 - val_acc: 0.6629\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7125 - acc: 0.7084 - val_loss: 0.9508 - val_acc: 0.6686\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7088 - acc: 0.7160 - val_loss: 0.9544 - val_acc: 0.6686\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6953 - acc: 0.7058 - val_loss: 0.9605 - val_acc: 0.6800\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7227 - acc: 0.7116 - val_loss: 0.9598 - val_acc: 0.6686\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7217 - acc: 0.6937 - val_loss: 0.9677 - val_acc: 0.6571\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7219 - acc: 0.7205 - val_loss: 0.9662 - val_acc: 0.6514\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7070 - acc: 0.7141 - val_loss: 0.9684 - val_acc: 0.6629\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7162 - acc: 0.7052 - val_loss: 0.9613 - val_acc: 0.6457\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7115 - acc: 0.7077 - val_loss: 0.9695 - val_acc: 0.6514\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7110 - acc: 0.7058 - val_loss: 0.9654 - val_acc: 0.6571\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7178 - acc: 0.7077 - val_loss: 0.9802 - val_acc: 0.6400\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7052 - acc: 0.7052 - val_loss: 0.9658 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.6871 - acc: 0.7147 - val_loss: 0.9733 - val_acc: 0.6400\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7025 - acc: 0.7141 - val_loss: 0.9780 - val_acc: 0.6457\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6991 - acc: 0.7116 - val_loss: 0.9759 - val_acc: 0.6343\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6955 - acc: 0.7147 - val_loss: 0.9800 - val_acc: 0.6343\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6896 - acc: 0.7192 - val_loss: 0.9757 - val_acc: 0.6514\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6900 - acc: 0.7307 - val_loss: 0.9783 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6817 - acc: 0.7275 - val_loss: 0.9882 - val_acc: 0.6400\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6904 - acc: 0.7109 - val_loss: 0.9806 - val_acc: 0.6343\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6818 - acc: 0.7237 - val_loss: 0.9754 - val_acc: 0.6571\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6826 - acc: 0.7250 - val_loss: 0.9832 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 69us/step\n",
      "1742/1742 [==============================] - 0s 40us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 4s 3ms/step - loss: 1.4233 - acc: 0.3274 - val_loss: 1.1904 - val_acc: 0.5143\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 1.2564 - acc: 0.4199 - val_loss: 1.0941 - val_acc: 0.6057\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 1.1709 - acc: 0.4646 - val_loss: 1.0367 - val_acc: 0.6057\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.1163 - acc: 0.5093 - val_loss: 1.0038 - val_acc: 0.6229\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.0976 - acc: 0.5099 - val_loss: 0.9795 - val_acc: 0.6343\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.0532 - acc: 0.5354 - val_loss: 0.9562 - val_acc: 0.6629\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 1.0549 - acc: 0.5399 - val_loss: 0.9415 - val_acc: 0.6686\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 1.0214 - acc: 0.5603 - val_loss: 0.9368 - val_acc: 0.6629\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 1.0089 - acc: 0.5763 - val_loss: 0.9248 - val_acc: 0.6743\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9805 - acc: 0.5699 - val_loss: 0.9162 - val_acc: 0.6514\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9892 - acc: 0.5597 - val_loss: 0.9155 - val_acc: 0.6514\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9805 - acc: 0.5750 - val_loss: 0.9070 - val_acc: 0.6457\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.9633 - acc: 0.5960 - val_loss: 0.9020 - val_acc: 0.6514\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.9484 - acc: 0.5916 - val_loss: 0.9054 - val_acc: 0.6514\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9400 - acc: 0.5922 - val_loss: 0.8976 - val_acc: 0.6457\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9470 - acc: 0.6024 - val_loss: 0.8945 - val_acc: 0.6629\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.9207 - acc: 0.6126 - val_loss: 0.8914 - val_acc: 0.6514\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9202 - acc: 0.6146 - val_loss: 0.8867 - val_acc: 0.6514\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8878 - acc: 0.6267 - val_loss: 0.8851 - val_acc: 0.6571\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9053 - acc: 0.6311 - val_loss: 0.8829 - val_acc: 0.6571\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8957 - acc: 0.6305 - val_loss: 0.8747 - val_acc: 0.6800\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8981 - acc: 0.6299 - val_loss: 0.8787 - val_acc: 0.6457\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8884 - acc: 0.6260 - val_loss: 0.8803 - val_acc: 0.6686\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8946 - acc: 0.6401 - val_loss: 0.8801 - val_acc: 0.6571\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8785 - acc: 0.6216 - val_loss: 0.8798 - val_acc: 0.6571\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8661 - acc: 0.6343 - val_loss: 0.8805 - val_acc: 0.6629\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8769 - acc: 0.6484 - val_loss: 0.8811 - val_acc: 0.6514\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8799 - acc: 0.6280 - val_loss: 0.8752 - val_acc: 0.6743\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8695 - acc: 0.6439 - val_loss: 0.8695 - val_acc: 0.6743\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8507 - acc: 0.6522 - val_loss: 0.8776 - val_acc: 0.6686\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8439 - acc: 0.6650 - val_loss: 0.8731 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8556 - acc: 0.6407 - val_loss: 0.8739 - val_acc: 0.6686\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8482 - acc: 0.6631 - val_loss: 0.8724 - val_acc: 0.6743\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8523 - acc: 0.6394 - val_loss: 0.8777 - val_acc: 0.6629\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8490 - acc: 0.6452 - val_loss: 0.8761 - val_acc: 0.6629\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8481 - acc: 0.6471 - val_loss: 0.8691 - val_acc: 0.6571\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8428 - acc: 0.6592 - val_loss: 0.8781 - val_acc: 0.6686\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8174 - acc: 0.6669 - val_loss: 0.8755 - val_acc: 0.6629\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8226 - acc: 0.6701 - val_loss: 0.8713 - val_acc: 0.6514\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8316 - acc: 0.6592 - val_loss: 0.8706 - val_acc: 0.6629\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8097 - acc: 0.6669 - val_loss: 0.8751 - val_acc: 0.6457\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8221 - acc: 0.6631 - val_loss: 0.8727 - val_acc: 0.6457\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8107 - acc: 0.6605 - val_loss: 0.8753 - val_acc: 0.6457\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8160 - acc: 0.6726 - val_loss: 0.8722 - val_acc: 0.6629\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8118 - acc: 0.6656 - val_loss: 0.8715 - val_acc: 0.6629\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7917 - acc: 0.6713 - val_loss: 0.8802 - val_acc: 0.6629\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7997 - acc: 0.6707 - val_loss: 0.8753 - val_acc: 0.6571\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7914 - acc: 0.6765 - val_loss: 0.8811 - val_acc: 0.6571\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7985 - acc: 0.6765 - val_loss: 0.8746 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7851 - acc: 0.6758 - val_loss: 0.8780 - val_acc: 0.6629\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7891 - acc: 0.6816 - val_loss: 0.8774 - val_acc: 0.6743\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7906 - acc: 0.6688 - val_loss: 0.8780 - val_acc: 0.6571\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7790 - acc: 0.6892 - val_loss: 0.8719 - val_acc: 0.6743\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7724 - acc: 0.6739 - val_loss: 0.8758 - val_acc: 0.6571\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7567 - acc: 0.7045 - val_loss: 0.8769 - val_acc: 0.6629\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7654 - acc: 0.6841 - val_loss: 0.8791 - val_acc: 0.6629\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7627 - acc: 0.6803 - val_loss: 0.8776 - val_acc: 0.6686\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7621 - acc: 0.6835 - val_loss: 0.8804 - val_acc: 0.6571\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7582 - acc: 0.6994 - val_loss: 0.8832 - val_acc: 0.6629\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7546 - acc: 0.6899 - val_loss: 0.8800 - val_acc: 0.6686\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7383 - acc: 0.7064 - val_loss: 0.8798 - val_acc: 0.6686\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7488 - acc: 0.6860 - val_loss: 0.8777 - val_acc: 0.6629\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7364 - acc: 0.7039 - val_loss: 0.8838 - val_acc: 0.6629\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7440 - acc: 0.7007 - val_loss: 0.8833 - val_acc: 0.6571\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7559 - acc: 0.6956 - val_loss: 0.8756 - val_acc: 0.6571\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7411 - acc: 0.6950 - val_loss: 0.8729 - val_acc: 0.6743\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7271 - acc: 0.7007 - val_loss: 0.8807 - val_acc: 0.6629\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7421 - acc: 0.7064 - val_loss: 0.8773 - val_acc: 0.6514\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7330 - acc: 0.7128 - val_loss: 0.8834 - val_acc: 0.6629\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7217 - acc: 0.7039 - val_loss: 0.8929 - val_acc: 0.6629\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7374 - acc: 0.6962 - val_loss: 0.8879 - val_acc: 0.6571\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7140 - acc: 0.7090 - val_loss: 0.8844 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7268 - acc: 0.7026 - val_loss: 0.8899 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7104 - acc: 0.7205 - val_loss: 0.8903 - val_acc: 0.6571\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7053 - acc: 0.7269 - val_loss: 0.9026 - val_acc: 0.6514\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7308 - acc: 0.6911 - val_loss: 0.8965 - val_acc: 0.6514\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7207 - acc: 0.6988 - val_loss: 0.8974 - val_acc: 0.6514\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7063 - acc: 0.7192 - val_loss: 0.8987 - val_acc: 0.6514\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7212 - acc: 0.7064 - val_loss: 0.8957 - val_acc: 0.6686\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7047 - acc: 0.7179 - val_loss: 0.8943 - val_acc: 0.6629\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7144 - acc: 0.7084 - val_loss: 0.8981 - val_acc: 0.6629\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7164 - acc: 0.7096 - val_loss: 0.9024 - val_acc: 0.6629\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6990 - acc: 0.7013 - val_loss: 0.9011 - val_acc: 0.6514\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6906 - acc: 0.7205 - val_loss: 0.9002 - val_acc: 0.6571\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6889 - acc: 0.7269 - val_loss: 0.9009 - val_acc: 0.6514\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7102 - acc: 0.7147 - val_loss: 0.9020 - val_acc: 0.6629\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6942 - acc: 0.7211 - val_loss: 0.8955 - val_acc: 0.6571\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6903 - acc: 0.7147 - val_loss: 0.9025 - val_acc: 0.6629\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6801 - acc: 0.7301 - val_loss: 0.9041 - val_acc: 0.6514\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6758 - acc: 0.7275 - val_loss: 0.9092 - val_acc: 0.6514\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6872 - acc: 0.7218 - val_loss: 0.9101 - val_acc: 0.6571\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6678 - acc: 0.7377 - val_loss: 0.9117 - val_acc: 0.6571\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6891 - acc: 0.7160 - val_loss: 0.9121 - val_acc: 0.6686\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6654 - acc: 0.7294 - val_loss: 0.9149 - val_acc: 0.6514\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6594 - acc: 0.7377 - val_loss: 0.9151 - val_acc: 0.6514\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6775 - acc: 0.7307 - val_loss: 0.9195 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6822 - acc: 0.7135 - val_loss: 0.9097 - val_acc: 0.6571\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6707 - acc: 0.7218 - val_loss: 0.9090 - val_acc: 0.6629\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6582 - acc: 0.7384 - val_loss: 0.9145 - val_acc: 0.6514\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6318 - acc: 0.7466 - val_loss: 0.9120 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 64us/step\n",
      "1742/1742 [==============================] - 0s 45us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 4s 3ms/step - loss: 1.5142 - acc: 0.3095 - val_loss: 1.2033 - val_acc: 0.4514\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.2749 - acc: 0.4161 - val_loss: 1.1044 - val_acc: 0.5714\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 1.1852 - acc: 0.4825 - val_loss: 1.0537 - val_acc: 0.6114\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.1501 - acc: 0.4684 - val_loss: 1.0205 - val_acc: 0.5943\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.1284 - acc: 0.4984 - val_loss: 0.9913 - val_acc: 0.6343\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 1.0799 - acc: 0.5310 - val_loss: 0.9777 - val_acc: 0.6457\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.0570 - acc: 0.5405 - val_loss: 0.9672 - val_acc: 0.6343\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.0187 - acc: 0.5629 - val_loss: 0.9463 - val_acc: 0.6571\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 1.0224 - acc: 0.5501 - val_loss: 0.9392 - val_acc: 0.6457\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.0006 - acc: 0.5737 - val_loss: 0.9334 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9951 - acc: 0.5724 - val_loss: 0.9233 - val_acc: 0.6400\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9911 - acc: 0.5686 - val_loss: 0.9189 - val_acc: 0.6686\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9769 - acc: 0.5807 - val_loss: 0.9155 - val_acc: 0.6514\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.9490 - acc: 0.5967 - val_loss: 0.9121 - val_acc: 0.6571\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9722 - acc: 0.5884 - val_loss: 0.9103 - val_acc: 0.6629\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9714 - acc: 0.5973 - val_loss: 0.9087 - val_acc: 0.6629\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9314 - acc: 0.6082 - val_loss: 0.8969 - val_acc: 0.6571\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9261 - acc: 0.6158 - val_loss: 0.8930 - val_acc: 0.6629\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9109 - acc: 0.6165 - val_loss: 0.8961 - val_acc: 0.6514\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9316 - acc: 0.5986 - val_loss: 0.8908 - val_acc: 0.6800\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9360 - acc: 0.6037 - val_loss: 0.8896 - val_acc: 0.6686\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8932 - acc: 0.6324 - val_loss: 0.8864 - val_acc: 0.6914\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9119 - acc: 0.6063 - val_loss: 0.8936 - val_acc: 0.6857\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9150 - acc: 0.6273 - val_loss: 0.8901 - val_acc: 0.6686\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8979 - acc: 0.6260 - val_loss: 0.8864 - val_acc: 0.6743\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8892 - acc: 0.6324 - val_loss: 0.8929 - val_acc: 0.6743\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8858 - acc: 0.6356 - val_loss: 0.8902 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8626 - acc: 0.6433 - val_loss: 0.8919 - val_acc: 0.6857\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8713 - acc: 0.6311 - val_loss: 0.8926 - val_acc: 0.6743\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8633 - acc: 0.6477 - val_loss: 0.8863 - val_acc: 0.6743\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8739 - acc: 0.6465 - val_loss: 0.8923 - val_acc: 0.6514\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8656 - acc: 0.6401 - val_loss: 0.8936 - val_acc: 0.6457\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8415 - acc: 0.6637 - val_loss: 0.8933 - val_acc: 0.6686\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8627 - acc: 0.6324 - val_loss: 0.8894 - val_acc: 0.6743\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8458 - acc: 0.6414 - val_loss: 0.8915 - val_acc: 0.6571\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8401 - acc: 0.6631 - val_loss: 0.8974 - val_acc: 0.6629\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8470 - acc: 0.6382 - val_loss: 0.8986 - val_acc: 0.6629\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8600 - acc: 0.6516 - val_loss: 0.8964 - val_acc: 0.6629\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8437 - acc: 0.6618 - val_loss: 0.9016 - val_acc: 0.6571\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.8485 - acc: 0.6592 - val_loss: 0.9035 - val_acc: 0.6629\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8320 - acc: 0.6656 - val_loss: 0.8940 - val_acc: 0.6571\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8198 - acc: 0.6503 - val_loss: 0.8942 - val_acc: 0.6743\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8226 - acc: 0.6496 - val_loss: 0.8894 - val_acc: 0.6629\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8192 - acc: 0.6803 - val_loss: 0.9006 - val_acc: 0.6571\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8242 - acc: 0.6688 - val_loss: 0.8907 - val_acc: 0.6571\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8024 - acc: 0.6739 - val_loss: 0.8948 - val_acc: 0.6629\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8215 - acc: 0.6458 - val_loss: 0.8988 - val_acc: 0.6457\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.7980 - acc: 0.6701 - val_loss: 0.8939 - val_acc: 0.6343\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8069 - acc: 0.6803 - val_loss: 0.8942 - val_acc: 0.6514\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8091 - acc: 0.6682 - val_loss: 0.8978 - val_acc: 0.6457\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7973 - acc: 0.6707 - val_loss: 0.9020 - val_acc: 0.6457\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7959 - acc: 0.6777 - val_loss: 0.8997 - val_acc: 0.6457\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7761 - acc: 0.6822 - val_loss: 0.8994 - val_acc: 0.6457\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7815 - acc: 0.6847 - val_loss: 0.9078 - val_acc: 0.6400\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7542 - acc: 0.6981 - val_loss: 0.9030 - val_acc: 0.6400\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7802 - acc: 0.6752 - val_loss: 0.9130 - val_acc: 0.6514\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7825 - acc: 0.6758 - val_loss: 0.9119 - val_acc: 0.6571\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7801 - acc: 0.6860 - val_loss: 0.9029 - val_acc: 0.6571\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7639 - acc: 0.6835 - val_loss: 0.9113 - val_acc: 0.6400\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7940 - acc: 0.6822 - val_loss: 0.9051 - val_acc: 0.6457\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7701 - acc: 0.6835 - val_loss: 0.9053 - val_acc: 0.6514\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7749 - acc: 0.6803 - val_loss: 0.9117 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7662 - acc: 0.6841 - val_loss: 0.9151 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7774 - acc: 0.6828 - val_loss: 0.9075 - val_acc: 0.6514\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7724 - acc: 0.6828 - val_loss: 0.9147 - val_acc: 0.6457\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7725 - acc: 0.6777 - val_loss: 0.9170 - val_acc: 0.6457\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7332 - acc: 0.7090 - val_loss: 0.9197 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7580 - acc: 0.6809 - val_loss: 0.9216 - val_acc: 0.6400\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7625 - acc: 0.6918 - val_loss: 0.9227 - val_acc: 0.6514\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7412 - acc: 0.6969 - val_loss: 0.9225 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7461 - acc: 0.7007 - val_loss: 0.9187 - val_acc: 0.6514\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7410 - acc: 0.6854 - val_loss: 0.9230 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7527 - acc: 0.6822 - val_loss: 0.9146 - val_acc: 0.6457\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7486 - acc: 0.7026 - val_loss: 0.9192 - val_acc: 0.6457\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7326 - acc: 0.7052 - val_loss: 0.9172 - val_acc: 0.6400\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7116 - acc: 0.7052 - val_loss: 0.9200 - val_acc: 0.6457\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7197 - acc: 0.7071 - val_loss: 0.9230 - val_acc: 0.6400\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7287 - acc: 0.7096 - val_loss: 0.9246 - val_acc: 0.6457\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7077 - acc: 0.7154 - val_loss: 0.9294 - val_acc: 0.6400\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7408 - acc: 0.6994 - val_loss: 0.9312 - val_acc: 0.6286\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.7360 - acc: 0.7058 - val_loss: 0.9306 - val_acc: 0.6514\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7170 - acc: 0.7122 - val_loss: 0.9326 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6992 - acc: 0.7135 - val_loss: 0.9292 - val_acc: 0.6400\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7066 - acc: 0.7154 - val_loss: 0.9268 - val_acc: 0.6457\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7030 - acc: 0.7084 - val_loss: 0.9284 - val_acc: 0.6571\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7282 - acc: 0.7084 - val_loss: 0.9343 - val_acc: 0.6343\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6986 - acc: 0.7275 - val_loss: 0.9363 - val_acc: 0.6286\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6982 - acc: 0.7122 - val_loss: 0.9401 - val_acc: 0.6400\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7153 - acc: 0.7160 - val_loss: 0.9341 - val_acc: 0.6229\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7059 - acc: 0.7147 - val_loss: 0.9438 - val_acc: 0.6286\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6928 - acc: 0.7141 - val_loss: 0.9547 - val_acc: 0.6286\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6768 - acc: 0.7371 - val_loss: 0.9453 - val_acc: 0.6343\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6862 - acc: 0.7128 - val_loss: 0.9473 - val_acc: 0.6343\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6875 - acc: 0.7109 - val_loss: 0.9427 - val_acc: 0.6343\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6858 - acc: 0.7332 - val_loss: 0.9428 - val_acc: 0.6343\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6617 - acc: 0.7364 - val_loss: 0.9506 - val_acc: 0.6400\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6913 - acc: 0.7237 - val_loss: 0.9526 - val_acc: 0.6400\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6801 - acc: 0.7205 - val_loss: 0.9533 - val_acc: 0.6457\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6705 - acc: 0.7409 - val_loss: 0.9581 - val_acc: 0.6343\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6802 - acc: 0.7192 - val_loss: 0.9557 - val_acc: 0.6400\n",
      "193/193 [==============================] - 0s 73us/step\n",
      "1742/1742 [==============================] - 0s 49us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.5628 - acc: 0.2872 - val_loss: 1.2371 - val_acc: 0.4629\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 1.2943 - acc: 0.3842 - val_loss: 1.1093 - val_acc: 0.5771\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.2145 - acc: 0.4493 - val_loss: 1.0605 - val_acc: 0.5886\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.1461 - acc: 0.4952 - val_loss: 1.0237 - val_acc: 0.6057\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 1.1210 - acc: 0.5010 - val_loss: 0.9986 - val_acc: 0.6229\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.1005 - acc: 0.5290 - val_loss: 0.9767 - val_acc: 0.6629\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0706 - acc: 0.5444 - val_loss: 0.9650 - val_acc: 0.6629\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.0572 - acc: 0.5392 - val_loss: 0.9512 - val_acc: 0.6629\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.0214 - acc: 0.5629 - val_loss: 0.9362 - val_acc: 0.6514\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.0056 - acc: 0.5718 - val_loss: 0.9300 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9918 - acc: 0.5788 - val_loss: 0.9255 - val_acc: 0.6571\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0035 - acc: 0.5756 - val_loss: 0.9196 - val_acc: 0.6571\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9689 - acc: 0.5877 - val_loss: 0.9158 - val_acc: 0.6629\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9728 - acc: 0.5833 - val_loss: 0.9138 - val_acc: 0.6457\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.9660 - acc: 0.5877 - val_loss: 0.9131 - val_acc: 0.6629\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.9478 - acc: 0.5980 - val_loss: 0.9110 - val_acc: 0.6571\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9451 - acc: 0.6107 - val_loss: 0.9091 - val_acc: 0.6571\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9198 - acc: 0.6222 - val_loss: 0.9032 - val_acc: 0.6629\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9225 - acc: 0.6126 - val_loss: 0.9057 - val_acc: 0.6571\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9133 - acc: 0.6356 - val_loss: 0.9023 - val_acc: 0.6514\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9321 - acc: 0.6088 - val_loss: 0.8979 - val_acc: 0.6571\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9067 - acc: 0.6337 - val_loss: 0.8970 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9095 - acc: 0.6075 - val_loss: 0.8981 - val_acc: 0.6743\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9112 - acc: 0.6120 - val_loss: 0.9037 - val_acc: 0.6457\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8903 - acc: 0.6382 - val_loss: 0.9070 - val_acc: 0.6571\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8994 - acc: 0.6254 - val_loss: 0.9027 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8786 - acc: 0.6388 - val_loss: 0.9072 - val_acc: 0.6629\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8600 - acc: 0.6350 - val_loss: 0.9009 - val_acc: 0.6457\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8694 - acc: 0.6369 - val_loss: 0.9053 - val_acc: 0.6514\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8865 - acc: 0.6197 - val_loss: 0.9058 - val_acc: 0.6457\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8701 - acc: 0.6439 - val_loss: 0.9079 - val_acc: 0.6400\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8571 - acc: 0.6426 - val_loss: 0.9056 - val_acc: 0.6571\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8566 - acc: 0.6503 - val_loss: 0.9111 - val_acc: 0.6514\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8681 - acc: 0.6299 - val_loss: 0.9007 - val_acc: 0.6629\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8579 - acc: 0.6465 - val_loss: 0.9040 - val_acc: 0.6629\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8511 - acc: 0.6522 - val_loss: 0.8991 - val_acc: 0.6629\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8630 - acc: 0.6484 - val_loss: 0.9054 - val_acc: 0.6571\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8249 - acc: 0.6694 - val_loss: 0.9063 - val_acc: 0.6571\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8299 - acc: 0.6739 - val_loss: 0.9154 - val_acc: 0.6629\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8520 - acc: 0.6420 - val_loss: 0.9125 - val_acc: 0.6400\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8332 - acc: 0.6496 - val_loss: 0.9085 - val_acc: 0.6514\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8029 - acc: 0.6694 - val_loss: 0.9065 - val_acc: 0.6514\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8247 - acc: 0.6637 - val_loss: 0.9113 - val_acc: 0.6457\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8330 - acc: 0.6579 - val_loss: 0.9081 - val_acc: 0.6514\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8257 - acc: 0.6560 - val_loss: 0.9086 - val_acc: 0.6514\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8264 - acc: 0.6624 - val_loss: 0.9086 - val_acc: 0.6457\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8047 - acc: 0.6745 - val_loss: 0.9135 - val_acc: 0.6514\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8145 - acc: 0.6618 - val_loss: 0.9063 - val_acc: 0.6743\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8035 - acc: 0.6745 - val_loss: 0.9153 - val_acc: 0.6514\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8128 - acc: 0.6522 - val_loss: 0.9135 - val_acc: 0.6400\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8042 - acc: 0.6713 - val_loss: 0.9092 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7918 - acc: 0.6847 - val_loss: 0.9211 - val_acc: 0.6514\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7820 - acc: 0.6847 - val_loss: 0.9108 - val_acc: 0.6514\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7839 - acc: 0.6796 - val_loss: 0.9121 - val_acc: 0.6514\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7898 - acc: 0.6752 - val_loss: 0.9163 - val_acc: 0.6514\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7832 - acc: 0.6918 - val_loss: 0.9249 - val_acc: 0.6514\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7647 - acc: 0.7020 - val_loss: 0.9214 - val_acc: 0.6286\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7851 - acc: 0.6694 - val_loss: 0.9270 - val_acc: 0.6400\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7663 - acc: 0.6860 - val_loss: 0.9309 - val_acc: 0.6457\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7762 - acc: 0.6847 - val_loss: 0.9325 - val_acc: 0.6457\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7792 - acc: 0.6860 - val_loss: 0.9256 - val_acc: 0.6400\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7772 - acc: 0.6847 - val_loss: 0.9222 - val_acc: 0.6457\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7549 - acc: 0.6796 - val_loss: 0.9290 - val_acc: 0.6400\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7567 - acc: 0.6841 - val_loss: 0.9344 - val_acc: 0.6457\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7402 - acc: 0.7096 - val_loss: 0.9358 - val_acc: 0.6400\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7599 - acc: 0.6937 - val_loss: 0.9357 - val_acc: 0.6457\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7591 - acc: 0.7001 - val_loss: 0.9301 - val_acc: 0.6286\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7378 - acc: 0.7077 - val_loss: 0.9333 - val_acc: 0.6400\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7434 - acc: 0.7064 - val_loss: 0.9417 - val_acc: 0.6286\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7411 - acc: 0.6994 - val_loss: 0.9445 - val_acc: 0.6229\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7393 - acc: 0.7064 - val_loss: 0.9403 - val_acc: 0.6400\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7397 - acc: 0.7007 - val_loss: 0.9429 - val_acc: 0.6400\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7352 - acc: 0.7026 - val_loss: 0.9483 - val_acc: 0.6400\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7230 - acc: 0.7096 - val_loss: 0.9394 - val_acc: 0.6343\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7485 - acc: 0.6911 - val_loss: 0.9465 - val_acc: 0.6171\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7274 - acc: 0.7160 - val_loss: 0.9474 - val_acc: 0.6171\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7466 - acc: 0.7045 - val_loss: 0.9462 - val_acc: 0.6171\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7286 - acc: 0.7045 - val_loss: 0.9479 - val_acc: 0.6171\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7321 - acc: 0.7007 - val_loss: 0.9469 - val_acc: 0.6343\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7271 - acc: 0.7160 - val_loss: 0.9454 - val_acc: 0.6286\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7037 - acc: 0.7186 - val_loss: 0.9489 - val_acc: 0.6457\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7329 - acc: 0.7116 - val_loss: 0.9512 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7353 - acc: 0.6918 - val_loss: 0.9500 - val_acc: 0.6343\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7229 - acc: 0.7013 - val_loss: 0.9525 - val_acc: 0.6343\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7255 - acc: 0.7090 - val_loss: 0.9503 - val_acc: 0.6114\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7182 - acc: 0.7147 - val_loss: 0.9560 - val_acc: 0.6114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6958 - acc: 0.7109 - val_loss: 0.9612 - val_acc: 0.6171\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6953 - acc: 0.7122 - val_loss: 0.9619 - val_acc: 0.6229\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6977 - acc: 0.7160 - val_loss: 0.9611 - val_acc: 0.6229\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6874 - acc: 0.7237 - val_loss: 0.9630 - val_acc: 0.6229\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6897 - acc: 0.7281 - val_loss: 0.9611 - val_acc: 0.6171\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6924 - acc: 0.7147 - val_loss: 0.9608 - val_acc: 0.6229\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6744 - acc: 0.7384 - val_loss: 0.9609 - val_acc: 0.6286\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7052 - acc: 0.7096 - val_loss: 0.9583 - val_acc: 0.6171\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6789 - acc: 0.7250 - val_loss: 0.9679 - val_acc: 0.6229\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6780 - acc: 0.7313 - val_loss: 0.9669 - val_acc: 0.6171\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6976 - acc: 0.7192 - val_loss: 0.9686 - val_acc: 0.6229\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6884 - acc: 0.7211 - val_loss: 0.9643 - val_acc: 0.6229\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6787 - acc: 0.7288 - val_loss: 0.9712 - val_acc: 0.6114\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6613 - acc: 0.7326 - val_loss: 0.9701 - val_acc: 0.6171\n",
      "193/193 [==============================] - 0s 79us/step\n",
      "1742/1742 [==============================] - 0s 50us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 4s 3ms/step - loss: 1.5798 - acc: 0.2942 - val_loss: 1.2420 - val_acc: 0.3943\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.3390 - acc: 0.3829 - val_loss: 1.1484 - val_acc: 0.4686\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.2380 - acc: 0.4378 - val_loss: 1.1005 - val_acc: 0.4971\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.1815 - acc: 0.4818 - val_loss: 1.0648 - val_acc: 0.5200\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.1357 - acc: 0.5067 - val_loss: 1.0470 - val_acc: 0.5257\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.1178 - acc: 0.5188 - val_loss: 1.0255 - val_acc: 0.5429\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.0899 - acc: 0.5105 - val_loss: 1.0096 - val_acc: 0.5486\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.0878 - acc: 0.5188 - val_loss: 1.0012 - val_acc: 0.5657\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.0544 - acc: 0.5463 - val_loss: 0.9984 - val_acc: 0.5657\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.0247 - acc: 0.5507 - val_loss: 0.9903 - val_acc: 0.5657\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 1.0107 - acc: 0.5718 - val_loss: 0.9880 - val_acc: 0.5771\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.0013 - acc: 0.5667 - val_loss: 0.9876 - val_acc: 0.5600\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9809 - acc: 0.5986 - val_loss: 0.9955 - val_acc: 0.5657\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9772 - acc: 0.5954 - val_loss: 0.9811 - val_acc: 0.5771\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9789 - acc: 0.5852 - val_loss: 0.9732 - val_acc: 0.5771\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9586 - acc: 0.5865 - val_loss: 0.9748 - val_acc: 0.5886\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9459 - acc: 0.5903 - val_loss: 0.9836 - val_acc: 0.5829\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9529 - acc: 0.6069 - val_loss: 0.9729 - val_acc: 0.5886\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9605 - acc: 0.5839 - val_loss: 0.9719 - val_acc: 0.5943\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9316 - acc: 0.6082 - val_loss: 0.9782 - val_acc: 0.5886\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9283 - acc: 0.6063 - val_loss: 0.9678 - val_acc: 0.5829\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9364 - acc: 0.6056 - val_loss: 0.9781 - val_acc: 0.5714\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9109 - acc: 0.6133 - val_loss: 0.9705 - val_acc: 0.5771\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9139 - acc: 0.6133 - val_loss: 0.9687 - val_acc: 0.5886\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9125 - acc: 0.6190 - val_loss: 0.9706 - val_acc: 0.5829\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8995 - acc: 0.6407 - val_loss: 0.9699 - val_acc: 0.5829\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8943 - acc: 0.6299 - val_loss: 0.9728 - val_acc: 0.5829\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8960 - acc: 0.6299 - val_loss: 0.9695 - val_acc: 0.5829\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8917 - acc: 0.6299 - val_loss: 0.9793 - val_acc: 0.5829\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8702 - acc: 0.6331 - val_loss: 0.9702 - val_acc: 0.5886\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8717 - acc: 0.6375 - val_loss: 0.9760 - val_acc: 0.5829\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8913 - acc: 0.6228 - val_loss: 0.9658 - val_acc: 0.5886\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8727 - acc: 0.6452 - val_loss: 0.9733 - val_acc: 0.5829\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8669 - acc: 0.6471 - val_loss: 0.9721 - val_acc: 0.5771\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8631 - acc: 0.6452 - val_loss: 0.9692 - val_acc: 0.5829\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8495 - acc: 0.6528 - val_loss: 0.9670 - val_acc: 0.5886\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8531 - acc: 0.6477 - val_loss: 0.9721 - val_acc: 0.5943\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8335 - acc: 0.6579 - val_loss: 0.9729 - val_acc: 0.5943\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8342 - acc: 0.6656 - val_loss: 0.9818 - val_acc: 0.5829\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8339 - acc: 0.6669 - val_loss: 0.9731 - val_acc: 0.5886\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8542 - acc: 0.6382 - val_loss: 0.9733 - val_acc: 0.5886\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8406 - acc: 0.6458 - val_loss: 0.9845 - val_acc: 0.5886\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8500 - acc: 0.6509 - val_loss: 0.9843 - val_acc: 0.5943\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8204 - acc: 0.6707 - val_loss: 0.9786 - val_acc: 0.5943\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8255 - acc: 0.6535 - val_loss: 0.9869 - val_acc: 0.5943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8149 - acc: 0.6694 - val_loss: 0.9889 - val_acc: 0.5886\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8100 - acc: 0.6726 - val_loss: 0.9922 - val_acc: 0.5943\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8242 - acc: 0.6599 - val_loss: 0.9846 - val_acc: 0.5943\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8238 - acc: 0.6624 - val_loss: 0.9892 - val_acc: 0.5886\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8005 - acc: 0.6854 - val_loss: 0.9923 - val_acc: 0.5829\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8083 - acc: 0.6905 - val_loss: 0.9927 - val_acc: 0.5943\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8097 - acc: 0.6637 - val_loss: 0.9887 - val_acc: 0.5886\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8048 - acc: 0.6618 - val_loss: 0.9910 - val_acc: 0.5943\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7905 - acc: 0.6694 - val_loss: 0.9962 - val_acc: 0.5829\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7934 - acc: 0.6867 - val_loss: 0.9956 - val_acc: 0.5943\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7914 - acc: 0.6816 - val_loss: 0.9950 - val_acc: 0.6000\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7897 - acc: 0.6720 - val_loss: 1.0078 - val_acc: 0.5886\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7857 - acc: 0.6765 - val_loss: 1.0078 - val_acc: 0.5886\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7727 - acc: 0.6803 - val_loss: 1.0040 - val_acc: 0.6000\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7771 - acc: 0.6720 - val_loss: 1.0042 - val_acc: 0.5886\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7912 - acc: 0.6739 - val_loss: 1.0075 - val_acc: 0.5886\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7866 - acc: 0.6809 - val_loss: 1.0068 - val_acc: 0.5943\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7562 - acc: 0.7090 - val_loss: 1.0091 - val_acc: 0.5943\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7695 - acc: 0.6867 - val_loss: 1.0114 - val_acc: 0.5943\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7796 - acc: 0.6943 - val_loss: 1.0089 - val_acc: 0.5943\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7587 - acc: 0.6796 - val_loss: 1.0143 - val_acc: 0.5886\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7763 - acc: 0.6816 - val_loss: 1.0103 - val_acc: 0.5886\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7581 - acc: 0.6892 - val_loss: 1.0165 - val_acc: 0.5943\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7634 - acc: 0.6975 - val_loss: 1.0184 - val_acc: 0.5829\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7439 - acc: 0.7001 - val_loss: 1.0147 - val_acc: 0.5829\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7522 - acc: 0.7052 - val_loss: 1.0151 - val_acc: 0.5943\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7389 - acc: 0.7109 - val_loss: 1.0189 - val_acc: 0.5771\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7439 - acc: 0.6950 - val_loss: 1.0296 - val_acc: 0.5829\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7378 - acc: 0.6988 - val_loss: 1.0238 - val_acc: 0.6000\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7450 - acc: 0.6969 - val_loss: 1.0266 - val_acc: 0.5771\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7399 - acc: 0.7058 - val_loss: 1.0313 - val_acc: 0.5771\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7521 - acc: 0.6943 - val_loss: 1.0321 - val_acc: 0.5771\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7478 - acc: 0.6988 - val_loss: 1.0320 - val_acc: 0.5886\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7296 - acc: 0.7077 - val_loss: 1.0395 - val_acc: 0.5714\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7235 - acc: 0.7033 - val_loss: 1.0306 - val_acc: 0.5771\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7286 - acc: 0.7064 - val_loss: 1.0282 - val_acc: 0.5943\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7544 - acc: 0.6905 - val_loss: 1.0354 - val_acc: 0.5886\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7141 - acc: 0.7173 - val_loss: 1.0382 - val_acc: 0.5829\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7292 - acc: 0.7045 - val_loss: 1.0360 - val_acc: 0.5886\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7275 - acc: 0.7109 - val_loss: 1.0314 - val_acc: 0.5943\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7203 - acc: 0.7141 - val_loss: 1.0316 - val_acc: 0.5943\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6995 - acc: 0.7052 - val_loss: 1.0441 - val_acc: 0.5886\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7144 - acc: 0.7167 - val_loss: 1.0368 - val_acc: 0.5714\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7136 - acc: 0.7090 - val_loss: 1.0383 - val_acc: 0.5886\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7038 - acc: 0.7281 - val_loss: 1.0457 - val_acc: 0.5714\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7079 - acc: 0.7154 - val_loss: 1.0381 - val_acc: 0.5943\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7002 - acc: 0.7294 - val_loss: 1.0424 - val_acc: 0.5829\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7088 - acc: 0.7281 - val_loss: 1.0376 - val_acc: 0.5771\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6886 - acc: 0.7218 - val_loss: 1.0466 - val_acc: 0.5829\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6929 - acc: 0.7313 - val_loss: 1.0485 - val_acc: 0.5771\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6738 - acc: 0.7332 - val_loss: 1.0497 - val_acc: 0.5714\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6932 - acc: 0.7326 - val_loss: 1.0520 - val_acc: 0.5943\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7021 - acc: 0.7135 - val_loss: 1.0511 - val_acc: 0.5886\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6842 - acc: 0.7192 - val_loss: 1.0624 - val_acc: 0.5886\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7020 - acc: 0.7179 - val_loss: 1.0606 - val_acc: 0.5943\n",
      "193/193 [==============================] - 0s 73us/step\n",
      "1742/1742 [==============================] - 0s 50us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 4s 3ms/step - loss: 1.4345 - acc: 0.3352 - val_loss: 1.1170 - val_acc: 0.5771\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.1978 - acc: 0.4591 - val_loss: 1.0162 - val_acc: 0.6343\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.1342 - acc: 0.4962 - val_loss: 0.9838 - val_acc: 0.6400\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0769 - acc: 0.5345 - val_loss: 0.9558 - val_acc: 0.6629\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0269 - acc: 0.5619 - val_loss: 0.9234 - val_acc: 0.6514\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.0128 - acc: 0.5722 - val_loss: 0.9193 - val_acc: 0.6914\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9763 - acc: 0.5792 - val_loss: 0.9009 - val_acc: 0.6686\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9570 - acc: 0.5932 - val_loss: 0.9038 - val_acc: 0.6857\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9589 - acc: 0.5881 - val_loss: 0.9027 - val_acc: 0.6686\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9489 - acc: 0.5868 - val_loss: 0.8919 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9363 - acc: 0.5951 - val_loss: 0.8850 - val_acc: 0.6800\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9359 - acc: 0.6137 - val_loss: 0.8958 - val_acc: 0.6800\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8973 - acc: 0.6207 - val_loss: 0.8940 - val_acc: 0.6686\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9006 - acc: 0.6328 - val_loss: 0.8994 - val_acc: 0.6629\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8944 - acc: 0.6392 - val_loss: 0.9052 - val_acc: 0.6457\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9059 - acc: 0.6086 - val_loss: 0.9011 - val_acc: 0.6800\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8712 - acc: 0.6296 - val_loss: 0.8913 - val_acc: 0.6743\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8733 - acc: 0.6456 - val_loss: 0.8925 - val_acc: 0.6743\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8529 - acc: 0.6405 - val_loss: 0.9064 - val_acc: 0.6743\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8521 - acc: 0.6488 - val_loss: 0.8840 - val_acc: 0.6857\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.8440 - acc: 0.6552 - val_loss: 0.9030 - val_acc: 0.6743\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8438 - acc: 0.6379 - val_loss: 0.9001 - val_acc: 0.6571\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8347 - acc: 0.6533 - val_loss: 0.9049 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8296 - acc: 0.6686 - val_loss: 0.9092 - val_acc: 0.6629\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8474 - acc: 0.6577 - val_loss: 0.9018 - val_acc: 0.6629\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8225 - acc: 0.6609 - val_loss: 0.9158 - val_acc: 0.6686\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8094 - acc: 0.6667 - val_loss: 0.9148 - val_acc: 0.6571\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8343 - acc: 0.6520 - val_loss: 0.9079 - val_acc: 0.6686\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8103 - acc: 0.6564 - val_loss: 0.9006 - val_acc: 0.6743\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7965 - acc: 0.6775 - val_loss: 0.9156 - val_acc: 0.6629\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7838 - acc: 0.6756 - val_loss: 0.9224 - val_acc: 0.6514\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7839 - acc: 0.6775 - val_loss: 0.9306 - val_acc: 0.6400\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7759 - acc: 0.6839 - val_loss: 0.9250 - val_acc: 0.6343\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7722 - acc: 0.6960 - val_loss: 0.9220 - val_acc: 0.6457\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7695 - acc: 0.6788 - val_loss: 0.9443 - val_acc: 0.6400\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7573 - acc: 0.7031 - val_loss: 0.9317 - val_acc: 0.6514\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7668 - acc: 0.6877 - val_loss: 0.9317 - val_acc: 0.6571\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7610 - acc: 0.7095 - val_loss: 0.9433 - val_acc: 0.6571\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7299 - acc: 0.7069 - val_loss: 0.9305 - val_acc: 0.6629\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7515 - acc: 0.7063 - val_loss: 0.9382 - val_acc: 0.6514\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7569 - acc: 0.6941 - val_loss: 0.9360 - val_acc: 0.6629\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7376 - acc: 0.7056 - val_loss: 0.9350 - val_acc: 0.6343\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7450 - acc: 0.6935 - val_loss: 0.9456 - val_acc: 0.6400\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7392 - acc: 0.7063 - val_loss: 0.9462 - val_acc: 0.6343\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7378 - acc: 0.7050 - val_loss: 0.9475 - val_acc: 0.6457\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7250 - acc: 0.7133 - val_loss: 0.9495 - val_acc: 0.6229\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7245 - acc: 0.7190 - val_loss: 0.9515 - val_acc: 0.6343\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7299 - acc: 0.7069 - val_loss: 0.9574 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6938 - acc: 0.7184 - val_loss: 0.9530 - val_acc: 0.6514\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7266 - acc: 0.7011 - val_loss: 0.9526 - val_acc: 0.6514\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6973 - acc: 0.7216 - val_loss: 0.9565 - val_acc: 0.6514\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7117 - acc: 0.7171 - val_loss: 0.9529 - val_acc: 0.6400\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7136 - acc: 0.7171 - val_loss: 0.9583 - val_acc: 0.6343\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6963 - acc: 0.7363 - val_loss: 0.9669 - val_acc: 0.6171\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6798 - acc: 0.7114 - val_loss: 0.9714 - val_acc: 0.6457\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6970 - acc: 0.7190 - val_loss: 0.9757 - val_acc: 0.6286\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6674 - acc: 0.7337 - val_loss: 0.9688 - val_acc: 0.6571\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6985 - acc: 0.7261 - val_loss: 0.9756 - val_acc: 0.6514\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6670 - acc: 0.7344 - val_loss: 0.9835 - val_acc: 0.6343\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6974 - acc: 0.7324 - val_loss: 0.9689 - val_acc: 0.6514\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6604 - acc: 0.7395 - val_loss: 0.9685 - val_acc: 0.6686\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6475 - acc: 0.7458 - val_loss: 0.9868 - val_acc: 0.6629\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6684 - acc: 0.7452 - val_loss: 0.9812 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6583 - acc: 0.7554 - val_loss: 1.0000 - val_acc: 0.6571\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6419 - acc: 0.7490 - val_loss: 0.9989 - val_acc: 0.6400\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6388 - acc: 0.7490 - val_loss: 1.0024 - val_acc: 0.6457\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6358 - acc: 0.7484 - val_loss: 0.9984 - val_acc: 0.6514\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6610 - acc: 0.7452 - val_loss: 0.9934 - val_acc: 0.6457\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6368 - acc: 0.7420 - val_loss: 0.9998 - val_acc: 0.6400\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6515 - acc: 0.7414 - val_loss: 1.0015 - val_acc: 0.6400\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6439 - acc: 0.7618 - val_loss: 1.0002 - val_acc: 0.6514\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6364 - acc: 0.7548 - val_loss: 1.0111 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6265 - acc: 0.7548 - val_loss: 1.0240 - val_acc: 0.6457\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6228 - acc: 0.7682 - val_loss: 1.0133 - val_acc: 0.6514\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6145 - acc: 0.7529 - val_loss: 1.0067 - val_acc: 0.6571\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6092 - acc: 0.7733 - val_loss: 1.0108 - val_acc: 0.6457\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6095 - acc: 0.7586 - val_loss: 1.0283 - val_acc: 0.6343\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6006 - acc: 0.7676 - val_loss: 1.0199 - val_acc: 0.6457\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6133 - acc: 0.7650 - val_loss: 1.0287 - val_acc: 0.6629\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6035 - acc: 0.7816 - val_loss: 1.0291 - val_acc: 0.6343\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6003 - acc: 0.7695 - val_loss: 1.0260 - val_acc: 0.6286\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5821 - acc: 0.7765 - val_loss: 1.0350 - val_acc: 0.6343\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5867 - acc: 0.7637 - val_loss: 1.0463 - val_acc: 0.6229\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6037 - acc: 0.7835 - val_loss: 1.0496 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5912 - acc: 0.7829 - val_loss: 1.0454 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5843 - acc: 0.7708 - val_loss: 1.0380 - val_acc: 0.6343\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 118us/step - loss: 0.5915 - acc: 0.7676 - val_loss: 1.0337 - val_acc: 0.6400\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5760 - acc: 0.7784 - val_loss: 1.0341 - val_acc: 0.6343\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5646 - acc: 0.7784 - val_loss: 1.0434 - val_acc: 0.6400\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5884 - acc: 0.7727 - val_loss: 1.0511 - val_acc: 0.6400\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5699 - acc: 0.7701 - val_loss: 1.0479 - val_acc: 0.6343\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5700 - acc: 0.7778 - val_loss: 1.0536 - val_acc: 0.6514\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5823 - acc: 0.7542 - val_loss: 1.0633 - val_acc: 0.6457\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5554 - acc: 0.7867 - val_loss: 1.0757 - val_acc: 0.6229\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5670 - acc: 0.7784 - val_loss: 1.0744 - val_acc: 0.6343\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5445 - acc: 0.7854 - val_loss: 1.0772 - val_acc: 0.6229\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5534 - acc: 0.7893 - val_loss: 1.0696 - val_acc: 0.6400\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5381 - acc: 0.7982 - val_loss: 1.0569 - val_acc: 0.6457\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5590 - acc: 0.7861 - val_loss: 1.0504 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5353 - acc: 0.7982 - val_loss: 1.0692 - val_acc: 0.6571\n",
      "194/194 [==============================] - 0s 69us/step\n",
      "1741/1741 [==============================] - 0s 49us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 4s 3ms/step - loss: 1.4173 - acc: 0.3257 - val_loss: 1.1014 - val_acc: 0.6000\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.1749 - acc: 0.4713 - val_loss: 1.0095 - val_acc: 0.6571\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0989 - acc: 0.5204 - val_loss: 0.9800 - val_acc: 0.6571\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0657 - acc: 0.5421 - val_loss: 0.9458 - val_acc: 0.6629\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.0108 - acc: 0.5626 - val_loss: 0.9344 - val_acc: 0.6743\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9952 - acc: 0.5824 - val_loss: 0.9306 - val_acc: 0.6686\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9755 - acc: 0.5766 - val_loss: 0.9130 - val_acc: 0.6743\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9684 - acc: 0.5894 - val_loss: 0.9121 - val_acc: 0.6800\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9570 - acc: 0.5958 - val_loss: 0.9016 - val_acc: 0.6629\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9229 - acc: 0.6258 - val_loss: 0.8986 - val_acc: 0.6629\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.9249 - acc: 0.6143 - val_loss: 0.8855 - val_acc: 0.6686\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.9099 - acc: 0.6137 - val_loss: 0.8907 - val_acc: 0.6743\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9213 - acc: 0.6105 - val_loss: 0.8889 - val_acc: 0.6857\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8901 - acc: 0.6405 - val_loss: 0.8996 - val_acc: 0.6743\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8693 - acc: 0.6462 - val_loss: 0.8915 - val_acc: 0.6571\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8886 - acc: 0.6373 - val_loss: 0.8922 - val_acc: 0.6514\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8637 - acc: 0.6552 - val_loss: 0.8991 - val_acc: 0.6686\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8545 - acc: 0.6558 - val_loss: 0.8944 - val_acc: 0.6743\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8599 - acc: 0.6430 - val_loss: 0.8948 - val_acc: 0.6857\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8344 - acc: 0.6596 - val_loss: 0.8879 - val_acc: 0.6857\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8306 - acc: 0.6628 - val_loss: 0.8909 - val_acc: 0.6514\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.8200 - acc: 0.6590 - val_loss: 0.9008 - val_acc: 0.6571\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.8221 - acc: 0.6622 - val_loss: 0.9119 - val_acc: 0.6571\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8031 - acc: 0.6673 - val_loss: 0.9084 - val_acc: 0.6571\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8048 - acc: 0.6641 - val_loss: 0.9088 - val_acc: 0.6514\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7866 - acc: 0.6833 - val_loss: 0.9132 - val_acc: 0.6400\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8203 - acc: 0.6654 - val_loss: 0.9081 - val_acc: 0.6400\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8117 - acc: 0.6743 - val_loss: 0.9193 - val_acc: 0.6400\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7990 - acc: 0.6826 - val_loss: 0.9115 - val_acc: 0.6343\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.7880 - acc: 0.6858 - val_loss: 0.9052 - val_acc: 0.6457\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7933 - acc: 0.6820 - val_loss: 0.9086 - val_acc: 0.6343\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7716 - acc: 0.6935 - val_loss: 0.9144 - val_acc: 0.6343\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7724 - acc: 0.6884 - val_loss: 0.9212 - val_acc: 0.6286\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7659 - acc: 0.6762 - val_loss: 0.9166 - val_acc: 0.6343\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7692 - acc: 0.7031 - val_loss: 0.9180 - val_acc: 0.6400\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7704 - acc: 0.6884 - val_loss: 0.9364 - val_acc: 0.6400\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7569 - acc: 0.6954 - val_loss: 0.9320 - val_acc: 0.6400\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7526 - acc: 0.6967 - val_loss: 0.9307 - val_acc: 0.6286\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7315 - acc: 0.7043 - val_loss: 0.9289 - val_acc: 0.6400\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7551 - acc: 0.7069 - val_loss: 0.9340 - val_acc: 0.6629\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7300 - acc: 0.7133 - val_loss: 0.9390 - val_acc: 0.6514\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7066 - acc: 0.7184 - val_loss: 0.9441 - val_acc: 0.6343\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7366 - acc: 0.7120 - val_loss: 0.9475 - val_acc: 0.6457\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7315 - acc: 0.7133 - val_loss: 0.9520 - val_acc: 0.6286\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7101 - acc: 0.7216 - val_loss: 0.9657 - val_acc: 0.6171\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7038 - acc: 0.7165 - val_loss: 0.9593 - val_acc: 0.6229\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.7026 - acc: 0.7241 - val_loss: 0.9555 - val_acc: 0.6286\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6907 - acc: 0.7209 - val_loss: 0.9563 - val_acc: 0.6400\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7112 - acc: 0.7050 - val_loss: 0.9639 - val_acc: 0.6457\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7071 - acc: 0.7190 - val_loss: 0.9598 - val_acc: 0.6286\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6950 - acc: 0.7222 - val_loss: 0.9620 - val_acc: 0.6229\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6797 - acc: 0.7254 - val_loss: 0.9686 - val_acc: 0.6114\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6802 - acc: 0.7203 - val_loss: 0.9677 - val_acc: 0.6114\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.6986 - acc: 0.7229 - val_loss: 0.9788 - val_acc: 0.6343\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.6920 - acc: 0.7312 - val_loss: 0.9763 - val_acc: 0.6286\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6672 - acc: 0.7375 - val_loss: 0.9801 - val_acc: 0.6114\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6699 - acc: 0.7286 - val_loss: 0.9815 - val_acc: 0.6286\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6629 - acc: 0.7350 - val_loss: 0.9845 - val_acc: 0.6343\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6598 - acc: 0.7427 - val_loss: 0.9887 - val_acc: 0.6286\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6535 - acc: 0.7497 - val_loss: 0.9891 - val_acc: 0.6400\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6585 - acc: 0.7458 - val_loss: 0.9866 - val_acc: 0.6229\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6635 - acc: 0.7363 - val_loss: 1.0008 - val_acc: 0.6400\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6557 - acc: 0.7382 - val_loss: 1.0037 - val_acc: 0.6286\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6528 - acc: 0.7439 - val_loss: 1.0068 - val_acc: 0.6343\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6273 - acc: 0.7535 - val_loss: 1.0080 - val_acc: 0.6343\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6299 - acc: 0.7522 - val_loss: 1.0225 - val_acc: 0.6286\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6334 - acc: 0.7356 - val_loss: 1.0258 - val_acc: 0.6171\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6228 - acc: 0.7573 - val_loss: 1.0410 - val_acc: 0.6229\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6291 - acc: 0.7503 - val_loss: 1.0263 - val_acc: 0.6114\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6155 - acc: 0.7484 - val_loss: 1.0443 - val_acc: 0.6343\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6233 - acc: 0.7535 - val_loss: 1.0349 - val_acc: 0.6171\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6124 - acc: 0.7618 - val_loss: 1.0316 - val_acc: 0.6000\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6103 - acc: 0.7535 - val_loss: 1.0524 - val_acc: 0.6000\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6111 - acc: 0.7503 - val_loss: 1.0539 - val_acc: 0.6057\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5943 - acc: 0.7580 - val_loss: 1.0371 - val_acc: 0.6114\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5950 - acc: 0.7669 - val_loss: 1.0443 - val_acc: 0.6229\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5871 - acc: 0.7669 - val_loss: 1.0650 - val_acc: 0.6171\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5698 - acc: 0.7765 - val_loss: 1.0644 - val_acc: 0.6286\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5717 - acc: 0.7867 - val_loss: 1.0680 - val_acc: 0.6343\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5904 - acc: 0.7676 - val_loss: 1.0615 - val_acc: 0.6114\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5731 - acc: 0.7739 - val_loss: 1.0822 - val_acc: 0.6057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.5680 - acc: 0.7688 - val_loss: 1.0823 - val_acc: 0.6286\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5597 - acc: 0.7733 - val_loss: 1.0797 - val_acc: 0.6114\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5759 - acc: 0.7752 - val_loss: 1.0978 - val_acc: 0.6171\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5655 - acc: 0.7784 - val_loss: 1.0961 - val_acc: 0.6171\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5450 - acc: 0.7880 - val_loss: 1.0902 - val_acc: 0.6114\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5773 - acc: 0.7733 - val_loss: 1.0933 - val_acc: 0.6286\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5808 - acc: 0.7682 - val_loss: 1.1032 - val_acc: 0.6171\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5821 - acc: 0.7861 - val_loss: 1.1038 - val_acc: 0.6229\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5444 - acc: 0.7765 - val_loss: 1.1106 - val_acc: 0.6286\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.5789 - acc: 0.7714 - val_loss: 1.0978 - val_acc: 0.6286\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5486 - acc: 0.7925 - val_loss: 1.0979 - val_acc: 0.6343\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5423 - acc: 0.7989 - val_loss: 1.1028 - val_acc: 0.6171\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5381 - acc: 0.7944 - val_loss: 1.1167 - val_acc: 0.6286\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5595 - acc: 0.7854 - val_loss: 1.1065 - val_acc: 0.6229\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5470 - acc: 0.7803 - val_loss: 1.1046 - val_acc: 0.6171\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5373 - acc: 0.7931 - val_loss: 1.1065 - val_acc: 0.6286\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5436 - acc: 0.7880 - val_loss: 1.1335 - val_acc: 0.6114\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5156 - acc: 0.8110 - val_loss: 1.1270 - val_acc: 0.6229\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5231 - acc: 0.7912 - val_loss: 1.1175 - val_acc: 0.6286\n",
      "194/194 [==============================] - 0s 72us/step\n",
      "1741/1741 [==============================] - 0s 43us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4219 - acc: 0.3263 - val_loss: 1.1163 - val_acc: 0.5714\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.2248 - acc: 0.4374 - val_loss: 1.0389 - val_acc: 0.6114\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.1170 - acc: 0.5057 - val_loss: 0.9930 - val_acc: 0.6057\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 1.0791 - acc: 0.5313 - val_loss: 0.9757 - val_acc: 0.6114\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 1.0386 - acc: 0.5581 - val_loss: 0.9585 - val_acc: 0.6571\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.0259 - acc: 0.5626 - val_loss: 0.9505 - val_acc: 0.6400\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9909 - acc: 0.5677 - val_loss: 0.9242 - val_acc: 0.6800\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 107us/step - loss: 0.9716 - acc: 0.5843 - val_loss: 0.9121 - val_acc: 0.6800\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9581 - acc: 0.5990 - val_loss: 0.9121 - val_acc: 0.6800\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9565 - acc: 0.5843 - val_loss: 0.9056 - val_acc: 0.6800\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9343 - acc: 0.6092 - val_loss: 0.9036 - val_acc: 0.6571\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9359 - acc: 0.6086 - val_loss: 0.8965 - val_acc: 0.6857\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9134 - acc: 0.6175 - val_loss: 0.9008 - val_acc: 0.6743\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9130 - acc: 0.6194 - val_loss: 0.9039 - val_acc: 0.6914\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8807 - acc: 0.6245 - val_loss: 0.8977 - val_acc: 0.6800\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8845 - acc: 0.6264 - val_loss: 0.8913 - val_acc: 0.6800\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8979 - acc: 0.6309 - val_loss: 0.8993 - val_acc: 0.6800\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 119us/step - loss: 0.8623 - acc: 0.6456 - val_loss: 0.9067 - val_acc: 0.6629\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8729 - acc: 0.6488 - val_loss: 0.8989 - val_acc: 0.6686\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8439 - acc: 0.6552 - val_loss: 0.9045 - val_acc: 0.6743\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8288 - acc: 0.6507 - val_loss: 0.9052 - val_acc: 0.6686\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.8111 - acc: 0.6692 - val_loss: 0.9135 - val_acc: 0.6514\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8263 - acc: 0.6724 - val_loss: 0.9123 - val_acc: 0.6571\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 126us/step - loss: 0.8328 - acc: 0.6603 - val_loss: 0.9085 - val_acc: 0.6629\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8236 - acc: 0.6596 - val_loss: 0.9094 - val_acc: 0.6629\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8291 - acc: 0.6545 - val_loss: 0.9117 - val_acc: 0.6686\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8269 - acc: 0.6648 - val_loss: 0.9153 - val_acc: 0.6400\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7989 - acc: 0.6801 - val_loss: 0.9185 - val_acc: 0.6629\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8022 - acc: 0.6705 - val_loss: 0.9170 - val_acc: 0.6514\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7850 - acc: 0.6794 - val_loss: 0.9279 - val_acc: 0.6400\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.7942 - acc: 0.6686 - val_loss: 0.9258 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.7962 - acc: 0.6775 - val_loss: 0.9270 - val_acc: 0.6286\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.7670 - acc: 0.6814 - val_loss: 0.9327 - val_acc: 0.6114\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7910 - acc: 0.6769 - val_loss: 0.9346 - val_acc: 0.6229\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7734 - acc: 0.6935 - val_loss: 0.9235 - val_acc: 0.6571\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7823 - acc: 0.6737 - val_loss: 0.9282 - val_acc: 0.6457\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7665 - acc: 0.6845 - val_loss: 0.9437 - val_acc: 0.6400\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7692 - acc: 0.6845 - val_loss: 0.9388 - val_acc: 0.6514\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7617 - acc: 0.6922 - val_loss: 0.9329 - val_acc: 0.6457\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7481 - acc: 0.6992 - val_loss: 0.9279 - val_acc: 0.6571\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7516 - acc: 0.6903 - val_loss: 0.9402 - val_acc: 0.6457\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7487 - acc: 0.7005 - val_loss: 0.9461 - val_acc: 0.6171\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7235 - acc: 0.7165 - val_loss: 0.9508 - val_acc: 0.6400\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7349 - acc: 0.6967 - val_loss: 0.9513 - val_acc: 0.6400\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.7209 - acc: 0.7222 - val_loss: 0.9521 - val_acc: 0.6457\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7293 - acc: 0.7031 - val_loss: 0.9601 - val_acc: 0.6514\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 147us/step - loss: 0.7291 - acc: 0.7037 - val_loss: 0.9605 - val_acc: 0.6571\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 152us/step - loss: 0.7112 - acc: 0.7075 - val_loss: 0.9665 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 123us/step - loss: 0.7332 - acc: 0.7050 - val_loss: 0.9679 - val_acc: 0.6514\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6926 - acc: 0.7318 - val_loss: 0.9753 - val_acc: 0.6400\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6998 - acc: 0.7133 - val_loss: 0.9779 - val_acc: 0.6400\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6860 - acc: 0.7229 - val_loss: 0.9841 - val_acc: 0.6286\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6798 - acc: 0.7190 - val_loss: 0.9721 - val_acc: 0.6514\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6830 - acc: 0.7273 - val_loss: 0.9849 - val_acc: 0.6629\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6949 - acc: 0.7126 - val_loss: 0.9822 - val_acc: 0.6514\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6832 - acc: 0.7229 - val_loss: 0.9781 - val_acc: 0.6286\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6935 - acc: 0.7216 - val_loss: 0.9894 - val_acc: 0.6514\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6865 - acc: 0.7286 - val_loss: 0.9906 - val_acc: 0.6286\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6696 - acc: 0.7222 - val_loss: 0.9976 - val_acc: 0.6286\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6761 - acc: 0.7375 - val_loss: 1.0079 - val_acc: 0.6457\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6652 - acc: 0.7395 - val_loss: 1.0114 - val_acc: 0.6457\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6881 - acc: 0.7190 - val_loss: 1.0078 - val_acc: 0.6343\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6550 - acc: 0.7395 - val_loss: 1.0139 - val_acc: 0.6286\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6392 - acc: 0.7439 - val_loss: 1.0213 - val_acc: 0.6400\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6438 - acc: 0.7458 - val_loss: 1.0190 - val_acc: 0.6571\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6494 - acc: 0.7375 - val_loss: 1.0198 - val_acc: 0.6229\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6299 - acc: 0.7414 - val_loss: 1.0167 - val_acc: 0.6514\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6173 - acc: 0.7427 - val_loss: 1.0275 - val_acc: 0.6400\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6483 - acc: 0.7401 - val_loss: 1.0266 - val_acc: 0.6571\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6327 - acc: 0.7369 - val_loss: 1.0232 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6161 - acc: 0.7586 - val_loss: 1.0262 - val_acc: 0.6457\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6185 - acc: 0.7522 - val_loss: 1.0344 - val_acc: 0.6343\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6281 - acc: 0.7586 - val_loss: 1.0466 - val_acc: 0.6629\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6212 - acc: 0.7446 - val_loss: 1.0562 - val_acc: 0.6457\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6206 - acc: 0.7458 - val_loss: 1.0501 - val_acc: 0.6400\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6028 - acc: 0.7682 - val_loss: 1.0607 - val_acc: 0.6514\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5891 - acc: 0.7586 - val_loss: 1.0647 - val_acc: 0.6286\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6187 - acc: 0.7542 - val_loss: 1.0577 - val_acc: 0.6343\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6053 - acc: 0.7656 - val_loss: 1.0636 - val_acc: 0.6343\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5894 - acc: 0.7701 - val_loss: 1.0727 - val_acc: 0.6343\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5774 - acc: 0.7593 - val_loss: 1.0640 - val_acc: 0.6571\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5731 - acc: 0.7714 - val_loss: 1.0558 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5726 - acc: 0.7765 - val_loss: 1.0659 - val_acc: 0.6571\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5800 - acc: 0.7759 - val_loss: 1.0799 - val_acc: 0.6457\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.5943 - acc: 0.7701 - val_loss: 1.0855 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5614 - acc: 0.7759 - val_loss: 1.0781 - val_acc: 0.6571\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5734 - acc: 0.7810 - val_loss: 1.0903 - val_acc: 0.6343\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5653 - acc: 0.7797 - val_loss: 1.0922 - val_acc: 0.6457\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5630 - acc: 0.7752 - val_loss: 1.1000 - val_acc: 0.6400\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.5654 - acc: 0.7759 - val_loss: 1.0902 - val_acc: 0.6229\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5771 - acc: 0.7580 - val_loss: 1.0947 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5642 - acc: 0.7765 - val_loss: 1.1056 - val_acc: 0.6514\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5565 - acc: 0.7752 - val_loss: 1.1089 - val_acc: 0.6400\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5609 - acc: 0.7720 - val_loss: 1.1140 - val_acc: 0.6343\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5464 - acc: 0.7810 - val_loss: 1.1132 - val_acc: 0.6514\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5407 - acc: 0.7925 - val_loss: 1.1060 - val_acc: 0.6343\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5547 - acc: 0.7861 - val_loss: 1.1260 - val_acc: 0.6400\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5228 - acc: 0.7950 - val_loss: 1.1276 - val_acc: 0.6343\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5355 - acc: 0.7944 - val_loss: 1.1242 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5246 - acc: 0.7969 - val_loss: 1.1428 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 78us/step\n",
      "1741/1741 [==============================] - 0s 48us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4215 - acc: 0.3442 - val_loss: 1.1160 - val_acc: 0.5829\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 1.2184 - acc: 0.4470 - val_loss: 1.0251 - val_acc: 0.6000\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 1.1590 - acc: 0.4994 - val_loss: 0.9723 - val_acc: 0.6229\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 1.0904 - acc: 0.5300 - val_loss: 0.9438 - val_acc: 0.6457\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0451 - acc: 0.5421 - val_loss: 0.9204 - val_acc: 0.6686\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.0254 - acc: 0.5600 - val_loss: 0.9088 - val_acc: 0.6400\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9781 - acc: 0.5900 - val_loss: 0.8993 - val_acc: 0.6571\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.9831 - acc: 0.5754 - val_loss: 0.8924 - val_acc: 0.6571\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9579 - acc: 0.6003 - val_loss: 0.9012 - val_acc: 0.6743\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.9767 - acc: 0.5862 - val_loss: 0.8934 - val_acc: 0.6800\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9572 - acc: 0.6092 - val_loss: 0.8974 - val_acc: 0.6743\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9286 - acc: 0.6143 - val_loss: 0.8954 - val_acc: 0.6743\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9363 - acc: 0.6175 - val_loss: 0.8991 - val_acc: 0.6571\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8974 - acc: 0.6232 - val_loss: 0.8922 - val_acc: 0.6857\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9159 - acc: 0.5996 - val_loss: 0.8943 - val_acc: 0.6629\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8712 - acc: 0.6354 - val_loss: 0.8979 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8883 - acc: 0.6207 - val_loss: 0.8920 - val_acc: 0.6629\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8873 - acc: 0.6277 - val_loss: 0.8888 - val_acc: 0.6800\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8681 - acc: 0.6424 - val_loss: 0.8959 - val_acc: 0.6686\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8548 - acc: 0.6430 - val_loss: 0.8958 - val_acc: 0.6686\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8605 - acc: 0.6488 - val_loss: 0.8988 - val_acc: 0.6743\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8452 - acc: 0.6443 - val_loss: 0.8983 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8431 - acc: 0.6564 - val_loss: 0.9017 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8288 - acc: 0.6603 - val_loss: 0.9050 - val_acc: 0.6800\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8405 - acc: 0.6622 - val_loss: 0.9096 - val_acc: 0.6514\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8131 - acc: 0.6679 - val_loss: 0.9110 - val_acc: 0.6571\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8194 - acc: 0.6494 - val_loss: 0.9119 - val_acc: 0.6457\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8144 - acc: 0.6667 - val_loss: 0.9097 - val_acc: 0.6686\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8346 - acc: 0.6577 - val_loss: 0.9083 - val_acc: 0.6743\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8111 - acc: 0.6526 - val_loss: 0.9113 - val_acc: 0.6743\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8049 - acc: 0.6596 - val_loss: 0.9098 - val_acc: 0.6457\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8037 - acc: 0.6756 - val_loss: 0.9217 - val_acc: 0.6457\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7934 - acc: 0.6743 - val_loss: 0.9202 - val_acc: 0.6571\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7809 - acc: 0.6820 - val_loss: 0.9302 - val_acc: 0.6743\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7564 - acc: 0.6865 - val_loss: 0.9273 - val_acc: 0.6800\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7702 - acc: 0.7031 - val_loss: 0.9271 - val_acc: 0.6914\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7803 - acc: 0.6711 - val_loss: 0.9231 - val_acc: 0.6743\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.7437 - acc: 0.7063 - val_loss: 0.9362 - val_acc: 0.6457\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.7489 - acc: 0.6922 - val_loss: 0.9419 - val_acc: 0.6857\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7695 - acc: 0.6858 - val_loss: 0.9458 - val_acc: 0.6743\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7628 - acc: 0.6992 - val_loss: 0.9481 - val_acc: 0.6686\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7685 - acc: 0.6826 - val_loss: 0.9485 - val_acc: 0.6457\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7544 - acc: 0.7095 - val_loss: 0.9461 - val_acc: 0.6743\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7625 - acc: 0.6948 - val_loss: 0.9514 - val_acc: 0.6914\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7347 - acc: 0.7075 - val_loss: 0.9602 - val_acc: 0.6800\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7432 - acc: 0.6897 - val_loss: 0.9632 - val_acc: 0.6629\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7391 - acc: 0.7101 - val_loss: 0.9653 - val_acc: 0.6686\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7304 - acc: 0.7063 - val_loss: 0.9648 - val_acc: 0.6571\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7117 - acc: 0.7075 - val_loss: 0.9672 - val_acc: 0.6457\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7178 - acc: 0.7152 - val_loss: 0.9683 - val_acc: 0.6629\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7177 - acc: 0.7133 - val_loss: 0.9694 - val_acc: 0.6286\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6971 - acc: 0.7190 - val_loss: 0.9756 - val_acc: 0.6571\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7093 - acc: 0.7056 - val_loss: 0.9704 - val_acc: 0.6686\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7036 - acc: 0.7216 - val_loss: 0.9823 - val_acc: 0.6457\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7058 - acc: 0.7146 - val_loss: 0.9714 - val_acc: 0.6914\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7022 - acc: 0.7126 - val_loss: 0.9700 - val_acc: 0.6629\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6831 - acc: 0.7375 - val_loss: 0.9757 - val_acc: 0.6686\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6977 - acc: 0.7190 - val_loss: 0.9822 - val_acc: 0.6743\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6770 - acc: 0.7209 - val_loss: 0.9813 - val_acc: 0.6457\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6908 - acc: 0.7209 - val_loss: 0.9799 - val_acc: 0.6571\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6757 - acc: 0.7484 - val_loss: 0.9931 - val_acc: 0.6571\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6763 - acc: 0.7190 - val_loss: 1.0053 - val_acc: 0.6457\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6671 - acc: 0.7458 - val_loss: 0.9916 - val_acc: 0.6571\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6499 - acc: 0.7439 - val_loss: 1.0013 - val_acc: 0.6514\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6697 - acc: 0.7344 - val_loss: 1.0087 - val_acc: 0.6457\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6662 - acc: 0.7407 - val_loss: 1.0233 - val_acc: 0.6400\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6542 - acc: 0.7350 - val_loss: 1.0214 - val_acc: 0.6629\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6511 - acc: 0.7305 - val_loss: 1.0087 - val_acc: 0.6629\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6406 - acc: 0.7446 - val_loss: 1.0188 - val_acc: 0.6629\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6330 - acc: 0.7529 - val_loss: 1.0139 - val_acc: 0.6514\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6451 - acc: 0.7427 - val_loss: 1.0266 - val_acc: 0.6514\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6302 - acc: 0.7510 - val_loss: 1.0212 - val_acc: 0.6400\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6247 - acc: 0.7548 - val_loss: 1.0258 - val_acc: 0.6686\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6424 - acc: 0.7529 - val_loss: 1.0336 - val_acc: 0.6457\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6404 - acc: 0.7337 - val_loss: 1.0231 - val_acc: 0.6400\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6187 - acc: 0.7650 - val_loss: 1.0190 - val_acc: 0.6514\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6179 - acc: 0.7567 - val_loss: 1.0223 - val_acc: 0.6571\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6176 - acc: 0.7554 - val_loss: 1.0384 - val_acc: 0.6571\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6299 - acc: 0.7573 - val_loss: 1.0306 - val_acc: 0.6571\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6299 - acc: 0.7388 - val_loss: 1.0412 - val_acc: 0.6571\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6123 - acc: 0.7522 - val_loss: 1.0338 - val_acc: 0.6571\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6062 - acc: 0.7567 - val_loss: 1.0397 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6154 - acc: 0.7586 - val_loss: 1.0641 - val_acc: 0.6457\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6185 - acc: 0.7586 - val_loss: 1.0514 - val_acc: 0.6457\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6067 - acc: 0.7548 - val_loss: 1.0698 - val_acc: 0.6514\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6016 - acc: 0.7554 - val_loss: 1.0694 - val_acc: 0.6457\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5878 - acc: 0.7676 - val_loss: 1.0731 - val_acc: 0.6457\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5868 - acc: 0.7688 - val_loss: 1.0683 - val_acc: 0.6629\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5951 - acc: 0.7593 - val_loss: 1.0719 - val_acc: 0.6514\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5780 - acc: 0.7810 - val_loss: 1.0764 - val_acc: 0.6571\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5926 - acc: 0.7739 - val_loss: 1.0777 - val_acc: 0.6514\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5602 - acc: 0.7835 - val_loss: 1.0845 - val_acc: 0.6457\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5753 - acc: 0.7803 - val_loss: 1.0828 - val_acc: 0.6514\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5909 - acc: 0.7695 - val_loss: 1.0918 - val_acc: 0.6514\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5529 - acc: 0.7822 - val_loss: 1.0878 - val_acc: 0.6343\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.5748 - acc: 0.7765 - val_loss: 1.0987 - val_acc: 0.6286\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5379 - acc: 0.7944 - val_loss: 1.0948 - val_acc: 0.6343\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5714 - acc: 0.7867 - val_loss: 1.0945 - val_acc: 0.6514\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5614 - acc: 0.7899 - val_loss: 1.0957 - val_acc: 0.6743\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5552 - acc: 0.7886 - val_loss: 1.1011 - val_acc: 0.6629\n",
      "194/194 [==============================] - 0s 78us/step\n",
      "1741/1741 [==============================] - 0s 52us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4028 - acc: 0.3448 - val_loss: 1.0981 - val_acc: 0.5486\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 1.2174 - acc: 0.4508 - val_loss: 1.0193 - val_acc: 0.6000\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.1494 - acc: 0.4847 - val_loss: 0.9788 - val_acc: 0.6229\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.0671 - acc: 0.5275 - val_loss: 0.9487 - val_acc: 0.6743\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.0477 - acc: 0.5409 - val_loss: 0.9347 - val_acc: 0.6686\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.0046 - acc: 0.5670 - val_loss: 0.9306 - val_acc: 0.6629\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 1.0018 - acc: 0.5709 - val_loss: 0.9162 - val_acc: 0.6914\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.9760 - acc: 0.5849 - val_loss: 0.9117 - val_acc: 0.6743\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.9582 - acc: 0.5862 - val_loss: 0.8871 - val_acc: 0.6686\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9497 - acc: 0.6003 - val_loss: 0.8995 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.9582 - acc: 0.5932 - val_loss: 0.9003 - val_acc: 0.6629\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9257 - acc: 0.6169 - val_loss: 0.8894 - val_acc: 0.6914\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9203 - acc: 0.6130 - val_loss: 0.8784 - val_acc: 0.6800\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8891 - acc: 0.6169 - val_loss: 0.8797 - val_acc: 0.6914\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9059 - acc: 0.6201 - val_loss: 0.8886 - val_acc: 0.6800\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8984 - acc: 0.6354 - val_loss: 0.8822 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8835 - acc: 0.6194 - val_loss: 0.8928 - val_acc: 0.6686\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8757 - acc: 0.6424 - val_loss: 0.8886 - val_acc: 0.6800\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8740 - acc: 0.6290 - val_loss: 0.8888 - val_acc: 0.6629\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8555 - acc: 0.6526 - val_loss: 0.8887 - val_acc: 0.6743\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.8543 - acc: 0.6462 - val_loss: 0.8880 - val_acc: 0.6743\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8200 - acc: 0.6558 - val_loss: 0.8738 - val_acc: 0.6743\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8540 - acc: 0.6424 - val_loss: 0.8778 - val_acc: 0.6857\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8450 - acc: 0.6628 - val_loss: 0.8825 - val_acc: 0.6800\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8471 - acc: 0.6481 - val_loss: 0.8900 - val_acc: 0.6800\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8246 - acc: 0.6641 - val_loss: 0.8873 - val_acc: 0.6914\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8162 - acc: 0.6584 - val_loss: 0.8910 - val_acc: 0.6857\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8010 - acc: 0.6775 - val_loss: 0.8821 - val_acc: 0.6971\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8241 - acc: 0.6558 - val_loss: 0.8888 - val_acc: 0.6629\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8024 - acc: 0.6724 - val_loss: 0.8952 - val_acc: 0.6914\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.8052 - acc: 0.6801 - val_loss: 0.8984 - val_acc: 0.6629\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7937 - acc: 0.6737 - val_loss: 0.8956 - val_acc: 0.6800\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7713 - acc: 0.6884 - val_loss: 0.9117 - val_acc: 0.6857\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7891 - acc: 0.6756 - val_loss: 0.9007 - val_acc: 0.6457\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7777 - acc: 0.6884 - val_loss: 0.9068 - val_acc: 0.6743\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7777 - acc: 0.6833 - val_loss: 0.9157 - val_acc: 0.6686\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7738 - acc: 0.6833 - val_loss: 0.9296 - val_acc: 0.6629\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7658 - acc: 0.6871 - val_loss: 0.9245 - val_acc: 0.6571\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7426 - acc: 0.6960 - val_loss: 0.9315 - val_acc: 0.6686\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7540 - acc: 0.6909 - val_loss: 0.9389 - val_acc: 0.6457\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7466 - acc: 0.6935 - val_loss: 0.9302 - val_acc: 0.6629\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7381 - acc: 0.6948 - val_loss: 0.9325 - val_acc: 0.6629\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7164 - acc: 0.7203 - val_loss: 0.9349 - val_acc: 0.6629\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7367 - acc: 0.7018 - val_loss: 0.9352 - val_acc: 0.6514\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7276 - acc: 0.7075 - val_loss: 0.9427 - val_acc: 0.6571\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7227 - acc: 0.7075 - val_loss: 0.9369 - val_acc: 0.6514\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7237 - acc: 0.7005 - val_loss: 0.9344 - val_acc: 0.6686\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7130 - acc: 0.7024 - val_loss: 0.9397 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6929 - acc: 0.7203 - val_loss: 0.9431 - val_acc: 0.6686\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6975 - acc: 0.7273 - val_loss: 0.9478 - val_acc: 0.6457\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7076 - acc: 0.7133 - val_loss: 0.9403 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6827 - acc: 0.7350 - val_loss: 0.9486 - val_acc: 0.6686\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6733 - acc: 0.7171 - val_loss: 0.9508 - val_acc: 0.6571\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6925 - acc: 0.7216 - val_loss: 0.9396 - val_acc: 0.6629\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6876 - acc: 0.7248 - val_loss: 0.9502 - val_acc: 0.6571\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.6884 - acc: 0.7184 - val_loss: 0.9628 - val_acc: 0.6629\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6647 - acc: 0.7216 - val_loss: 0.9674 - val_acc: 0.6571\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6946 - acc: 0.7241 - val_loss: 0.9709 - val_acc: 0.6571\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6905 - acc: 0.7120 - val_loss: 0.9589 - val_acc: 0.6629\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6771 - acc: 0.7375 - val_loss: 0.9637 - val_acc: 0.6571\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6680 - acc: 0.7222 - val_loss: 0.9587 - val_acc: 0.6743\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6676 - acc: 0.7267 - val_loss: 0.9631 - val_acc: 0.6686\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6536 - acc: 0.7382 - val_loss: 0.9729 - val_acc: 0.6686\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6644 - acc: 0.7388 - val_loss: 0.9752 - val_acc: 0.6629\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6551 - acc: 0.7344 - val_loss: 0.9947 - val_acc: 0.6686\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6595 - acc: 0.7318 - val_loss: 0.9880 - val_acc: 0.6629\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6292 - acc: 0.7529 - val_loss: 0.9917 - val_acc: 0.6571\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6471 - acc: 0.7388 - val_loss: 0.9952 - val_acc: 0.6571\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6493 - acc: 0.7344 - val_loss: 1.0099 - val_acc: 0.6686\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6256 - acc: 0.7458 - val_loss: 1.0061 - val_acc: 0.6629\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6241 - acc: 0.7561 - val_loss: 1.0192 - val_acc: 0.6629\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6339 - acc: 0.7433 - val_loss: 1.0095 - val_acc: 0.6686\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6440 - acc: 0.7484 - val_loss: 1.0204 - val_acc: 0.6629\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6032 - acc: 0.7663 - val_loss: 0.9933 - val_acc: 0.6743\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6035 - acc: 0.7554 - val_loss: 1.0103 - val_acc: 0.6686\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6311 - acc: 0.7356 - val_loss: 1.0127 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6253 - acc: 0.7484 - val_loss: 1.0219 - val_acc: 0.6743\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6156 - acc: 0.7510 - val_loss: 1.0241 - val_acc: 0.6629\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6062 - acc: 0.7605 - val_loss: 1.0416 - val_acc: 0.6629\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6006 - acc: 0.7542 - val_loss: 1.0405 - val_acc: 0.6629\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5976 - acc: 0.7503 - val_loss: 1.0383 - val_acc: 0.6686\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6213 - acc: 0.7420 - val_loss: 1.0359 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5890 - acc: 0.7625 - val_loss: 1.0457 - val_acc: 0.6571\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5850 - acc: 0.7644 - val_loss: 1.0491 - val_acc: 0.6629\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5944 - acc: 0.7727 - val_loss: 1.0639 - val_acc: 0.6514\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5917 - acc: 0.7688 - val_loss: 1.0630 - val_acc: 0.6686\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.5710 - acc: 0.7739 - val_loss: 1.0659 - val_acc: 0.6629\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5889 - acc: 0.7720 - val_loss: 1.0699 - val_acc: 0.6571\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5684 - acc: 0.7842 - val_loss: 1.0604 - val_acc: 0.6743\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5769 - acc: 0.7803 - val_loss: 1.0699 - val_acc: 0.6457\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5779 - acc: 0.7746 - val_loss: 1.0584 - val_acc: 0.6571\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5515 - acc: 0.7759 - val_loss: 1.0802 - val_acc: 0.6571\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5752 - acc: 0.7644 - val_loss: 1.0729 - val_acc: 0.6629\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5528 - acc: 0.7816 - val_loss: 1.0839 - val_acc: 0.6686\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5483 - acc: 0.7867 - val_loss: 1.0940 - val_acc: 0.6571\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5430 - acc: 0.7957 - val_loss: 1.0904 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5343 - acc: 0.7848 - val_loss: 1.0888 - val_acc: 0.6457\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5408 - acc: 0.7861 - val_loss: 1.1019 - val_acc: 0.6343\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5429 - acc: 0.7893 - val_loss: 1.0790 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5396 - acc: 0.7848 - val_loss: 1.0854 - val_acc: 0.6286\n",
      "194/194 [==============================] - 0s 71us/step\n",
      "1741/1741 [==============================] - 0s 47us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.3793 - acc: 0.3433 - val_loss: 1.1024 - val_acc: 0.5543\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.1784 - acc: 0.4767 - val_loss: 1.0014 - val_acc: 0.6514\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 1.0990 - acc: 0.5175 - val_loss: 0.9611 - val_acc: 0.6571\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0468 - acc: 0.5578 - val_loss: 0.9466 - val_acc: 0.6571\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0289 - acc: 0.5667 - val_loss: 0.9278 - val_acc: 0.6629\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.9998 - acc: 0.5648 - val_loss: 0.9228 - val_acc: 0.6971\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9741 - acc: 0.5916 - val_loss: 0.9086 - val_acc: 0.6971\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9847 - acc: 0.5858 - val_loss: 0.9093 - val_acc: 0.6857\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9491 - acc: 0.5877 - val_loss: 0.9140 - val_acc: 0.6800\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9269 - acc: 0.6056 - val_loss: 0.9020 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9253 - acc: 0.6171 - val_loss: 0.9004 - val_acc: 0.6571\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9047 - acc: 0.6280 - val_loss: 0.9037 - val_acc: 0.6686\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9120 - acc: 0.6184 - val_loss: 0.9041 - val_acc: 0.6800\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.9018 - acc: 0.6190 - val_loss: 0.8985 - val_acc: 0.6800\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8976 - acc: 0.6126 - val_loss: 0.9054 - val_acc: 0.6686\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8725 - acc: 0.6299 - val_loss: 0.8951 - val_acc: 0.6914\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8710 - acc: 0.6426 - val_loss: 0.9028 - val_acc: 0.6857\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8695 - acc: 0.6509 - val_loss: 0.9085 - val_acc: 0.6743\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8770 - acc: 0.6439 - val_loss: 0.8975 - val_acc: 0.6686\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8469 - acc: 0.6528 - val_loss: 0.9063 - val_acc: 0.6743\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8360 - acc: 0.6465 - val_loss: 0.9147 - val_acc: 0.6629\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8386 - acc: 0.6516 - val_loss: 0.9121 - val_acc: 0.6800\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8427 - acc: 0.6439 - val_loss: 0.9078 - val_acc: 0.6743\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8311 - acc: 0.6548 - val_loss: 0.9138 - val_acc: 0.6800\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8097 - acc: 0.6669 - val_loss: 0.9108 - val_acc: 0.6800\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8027 - acc: 0.6624 - val_loss: 0.9103 - val_acc: 0.6857\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8236 - acc: 0.6573 - val_loss: 0.9057 - val_acc: 0.6800\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8118 - acc: 0.6611 - val_loss: 0.9123 - val_acc: 0.6914\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7896 - acc: 0.6771 - val_loss: 0.9157 - val_acc: 0.6914\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8017 - acc: 0.6650 - val_loss: 0.9207 - val_acc: 0.6743\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8002 - acc: 0.6758 - val_loss: 0.9410 - val_acc: 0.6743\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7874 - acc: 0.6835 - val_loss: 0.9314 - val_acc: 0.6800\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7640 - acc: 0.6911 - val_loss: 0.9314 - val_acc: 0.6743\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7646 - acc: 0.6777 - val_loss: 0.9340 - val_acc: 0.6686\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7548 - acc: 0.6950 - val_loss: 0.9401 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7663 - acc: 0.6796 - val_loss: 0.9385 - val_acc: 0.6743\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7618 - acc: 0.6752 - val_loss: 0.9428 - val_acc: 0.6857\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7431 - acc: 0.6924 - val_loss: 0.9469 - val_acc: 0.6857\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7475 - acc: 0.6899 - val_loss: 0.9482 - val_acc: 0.6914\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7361 - acc: 0.7020 - val_loss: 0.9434 - val_acc: 0.6743\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7371 - acc: 0.6950 - val_loss: 0.9576 - val_acc: 0.6800\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7337 - acc: 0.6981 - val_loss: 0.9515 - val_acc: 0.6686\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7180 - acc: 0.7039 - val_loss: 0.9593 - val_acc: 0.6800\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7212 - acc: 0.7058 - val_loss: 0.9731 - val_acc: 0.6743\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7070 - acc: 0.7058 - val_loss: 0.9629 - val_acc: 0.6686\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7101 - acc: 0.7141 - val_loss: 0.9799 - val_acc: 0.6743\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7024 - acc: 0.7243 - val_loss: 0.9857 - val_acc: 0.6686\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6911 - acc: 0.7320 - val_loss: 0.9911 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7053 - acc: 0.7160 - val_loss: 1.0028 - val_acc: 0.6629\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6866 - acc: 0.7262 - val_loss: 0.9979 - val_acc: 0.6743\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6721 - acc: 0.7230 - val_loss: 1.0069 - val_acc: 0.6514\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7050 - acc: 0.7084 - val_loss: 1.0155 - val_acc: 0.6743\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6765 - acc: 0.7320 - val_loss: 1.0108 - val_acc: 0.6800\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6836 - acc: 0.7288 - val_loss: 1.0067 - val_acc: 0.6857\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6732 - acc: 0.7409 - val_loss: 0.9997 - val_acc: 0.6571\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6747 - acc: 0.7307 - val_loss: 1.0155 - val_acc: 0.6686\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6800 - acc: 0.7339 - val_loss: 1.0121 - val_acc: 0.6457\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6705 - acc: 0.7313 - val_loss: 1.0179 - val_acc: 0.6343\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6806 - acc: 0.7301 - val_loss: 1.0315 - val_acc: 0.6800\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6676 - acc: 0.7275 - val_loss: 1.0343 - val_acc: 0.6400\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6404 - acc: 0.7352 - val_loss: 1.0340 - val_acc: 0.6743\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6337 - acc: 0.7396 - val_loss: 1.0382 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6421 - acc: 0.7403 - val_loss: 1.0473 - val_acc: 0.6629\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6510 - acc: 0.7403 - val_loss: 1.0513 - val_acc: 0.6457\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6228 - acc: 0.7505 - val_loss: 1.0509 - val_acc: 0.6514\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6265 - acc: 0.7556 - val_loss: 1.0570 - val_acc: 0.6571\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6326 - acc: 0.7454 - val_loss: 1.0572 - val_acc: 0.6686\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6478 - acc: 0.7352 - val_loss: 1.0584 - val_acc: 0.6514\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6285 - acc: 0.7524 - val_loss: 1.0706 - val_acc: 0.6629\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6206 - acc: 0.7435 - val_loss: 1.0487 - val_acc: 0.6629\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6231 - acc: 0.7530 - val_loss: 1.0530 - val_acc: 0.6457\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6046 - acc: 0.7652 - val_loss: 1.0729 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6033 - acc: 0.7632 - val_loss: 1.0701 - val_acc: 0.6571\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5904 - acc: 0.7703 - val_loss: 1.0727 - val_acc: 0.6571\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6004 - acc: 0.7677 - val_loss: 1.0853 - val_acc: 0.6457\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5936 - acc: 0.7447 - val_loss: 1.1011 - val_acc: 0.6571\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5952 - acc: 0.7703 - val_loss: 1.0943 - val_acc: 0.6457\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5733 - acc: 0.7671 - val_loss: 1.0974 - val_acc: 0.6514\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5758 - acc: 0.7671 - val_loss: 1.1153 - val_acc: 0.6400\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5926 - acc: 0.7754 - val_loss: 1.1231 - val_acc: 0.6457\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5731 - acc: 0.7658 - val_loss: 1.1210 - val_acc: 0.6400\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5833 - acc: 0.7569 - val_loss: 1.1176 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5683 - acc: 0.7696 - val_loss: 1.1313 - val_acc: 0.6400\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5689 - acc: 0.7786 - val_loss: 1.1309 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5830 - acc: 0.7696 - val_loss: 1.1292 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5726 - acc: 0.7786 - val_loss: 1.1394 - val_acc: 0.6457\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5696 - acc: 0.7830 - val_loss: 1.1446 - val_acc: 0.6457\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5667 - acc: 0.7728 - val_loss: 1.1553 - val_acc: 0.6457\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5648 - acc: 0.7881 - val_loss: 1.1548 - val_acc: 0.6400\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5376 - acc: 0.7926 - val_loss: 1.1505 - val_acc: 0.6400\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5539 - acc: 0.7843 - val_loss: 1.1528 - val_acc: 0.6400\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5370 - acc: 0.7964 - val_loss: 1.1458 - val_acc: 0.6400\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5315 - acc: 0.7951 - val_loss: 1.1558 - val_acc: 0.6514\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5368 - acc: 0.7926 - val_loss: 1.1663 - val_acc: 0.6457\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5321 - acc: 0.8009 - val_loss: 1.1821 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5526 - acc: 0.7907 - val_loss: 1.1779 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5355 - acc: 0.7939 - val_loss: 1.1794 - val_acc: 0.6514\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5240 - acc: 0.8034 - val_loss: 1.1776 - val_acc: 0.6514\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5419 - acc: 0.7811 - val_loss: 1.2000 - val_acc: 0.6457\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5244 - acc: 0.8009 - val_loss: 1.1995 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 62us/step\n",
      "1742/1742 [==============================] - 0s 46us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.4553 - acc: 0.3293 - val_loss: 1.1479 - val_acc: 0.5143\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.2252 - acc: 0.4403 - val_loss: 1.0439 - val_acc: 0.6229\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 1.1353 - acc: 0.5048 - val_loss: 0.9858 - val_acc: 0.6457\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.0788 - acc: 0.5424 - val_loss: 0.9624 - val_acc: 0.6286\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 1.0305 - acc: 0.5475 - val_loss: 0.9414 - val_acc: 0.6343\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.9983 - acc: 0.5705 - val_loss: 0.9193 - val_acc: 0.6514\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9980 - acc: 0.5756 - val_loss: 0.9192 - val_acc: 0.6571\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9651 - acc: 0.6043 - val_loss: 0.8938 - val_acc: 0.6571\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9466 - acc: 0.6037 - val_loss: 0.8932 - val_acc: 0.6571\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.9501 - acc: 0.5884 - val_loss: 0.8781 - val_acc: 0.6800\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9463 - acc: 0.6018 - val_loss: 0.8678 - val_acc: 0.6686\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.9126 - acc: 0.6158 - val_loss: 0.8728 - val_acc: 0.6686\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.9067 - acc: 0.6248 - val_loss: 0.8711 - val_acc: 0.6629\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9023 - acc: 0.6324 - val_loss: 0.8756 - val_acc: 0.6629\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8850 - acc: 0.6171 - val_loss: 0.8684 - val_acc: 0.6629\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8787 - acc: 0.6388 - val_loss: 0.8717 - val_acc: 0.6514\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8677 - acc: 0.6611 - val_loss: 0.8788 - val_acc: 0.6743\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8532 - acc: 0.6516 - val_loss: 0.8697 - val_acc: 0.6800\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8602 - acc: 0.6503 - val_loss: 0.8686 - val_acc: 0.6514\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.8475 - acc: 0.6420 - val_loss: 0.8689 - val_acc: 0.6686\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8338 - acc: 0.6560 - val_loss: 0.8675 - val_acc: 0.6400\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8206 - acc: 0.6771 - val_loss: 0.8724 - val_acc: 0.6400\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8193 - acc: 0.6631 - val_loss: 0.8683 - val_acc: 0.6457\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7992 - acc: 0.6662 - val_loss: 0.8785 - val_acc: 0.6400\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8053 - acc: 0.6637 - val_loss: 0.8721 - val_acc: 0.6571\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7950 - acc: 0.6841 - val_loss: 0.8733 - val_acc: 0.6514\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7798 - acc: 0.6765 - val_loss: 0.8683 - val_acc: 0.6571\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7906 - acc: 0.6867 - val_loss: 0.8785 - val_acc: 0.6686\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7982 - acc: 0.6726 - val_loss: 0.8683 - val_acc: 0.6686\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7894 - acc: 0.6816 - val_loss: 0.8717 - val_acc: 0.6686\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7800 - acc: 0.6688 - val_loss: 0.8662 - val_acc: 0.6514\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8013 - acc: 0.6745 - val_loss: 0.8771 - val_acc: 0.6686\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7598 - acc: 0.6892 - val_loss: 0.8733 - val_acc: 0.6743\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7704 - acc: 0.6950 - val_loss: 0.8783 - val_acc: 0.6571\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7524 - acc: 0.6905 - val_loss: 0.8714 - val_acc: 0.6800\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7669 - acc: 0.6911 - val_loss: 0.8710 - val_acc: 0.6571\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7555 - acc: 0.6879 - val_loss: 0.8822 - val_acc: 0.6400\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7610 - acc: 0.6950 - val_loss: 0.8798 - val_acc: 0.6514\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7338 - acc: 0.6943 - val_loss: 0.8795 - val_acc: 0.6571\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7300 - acc: 0.6969 - val_loss: 0.8818 - val_acc: 0.6457\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7357 - acc: 0.7103 - val_loss: 0.8921 - val_acc: 0.6457\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7327 - acc: 0.7013 - val_loss: 0.8977 - val_acc: 0.6686\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7235 - acc: 0.6930 - val_loss: 0.8830 - val_acc: 0.6514\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.7246 - acc: 0.7128 - val_loss: 0.8855 - val_acc: 0.6514\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7104 - acc: 0.7096 - val_loss: 0.8991 - val_acc: 0.6457\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7030 - acc: 0.7173 - val_loss: 0.8944 - val_acc: 0.6571\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7112 - acc: 0.7103 - val_loss: 0.9038 - val_acc: 0.6629\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7251 - acc: 0.7045 - val_loss: 0.8998 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7033 - acc: 0.7198 - val_loss: 0.9037 - val_acc: 0.6457\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6881 - acc: 0.7269 - val_loss: 0.9157 - val_acc: 0.6457\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.6762 - acc: 0.7313 - val_loss: 0.9015 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6796 - acc: 0.7307 - val_loss: 0.9069 - val_acc: 0.6514\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.6769 - acc: 0.7179 - val_loss: 0.9104 - val_acc: 0.6571\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6834 - acc: 0.7198 - val_loss: 0.9018 - val_acc: 0.6571\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6989 - acc: 0.7122 - val_loss: 0.8995 - val_acc: 0.6514\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6691 - acc: 0.7326 - val_loss: 0.9115 - val_acc: 0.6400\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6875 - acc: 0.7071 - val_loss: 0.9004 - val_acc: 0.6629\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6558 - acc: 0.7403 - val_loss: 0.9088 - val_acc: 0.6514\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.6626 - acc: 0.7269 - val_loss: 0.9043 - val_acc: 0.6629\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6457 - acc: 0.7454 - val_loss: 0.9017 - val_acc: 0.6743\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6637 - acc: 0.7332 - val_loss: 0.9137 - val_acc: 0.6571\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6593 - acc: 0.7332 - val_loss: 0.9183 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6169 - acc: 0.7569 - val_loss: 0.9175 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6239 - acc: 0.7556 - val_loss: 0.9301 - val_acc: 0.6400\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6376 - acc: 0.7498 - val_loss: 0.9224 - val_acc: 0.6629\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6412 - acc: 0.7588 - val_loss: 0.9326 - val_acc: 0.6514\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6302 - acc: 0.7556 - val_loss: 0.9379 - val_acc: 0.6400\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6280 - acc: 0.7530 - val_loss: 0.9375 - val_acc: 0.6400\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6196 - acc: 0.7543 - val_loss: 0.9388 - val_acc: 0.6514\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6276 - acc: 0.7530 - val_loss: 0.9420 - val_acc: 0.6571\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6035 - acc: 0.7569 - val_loss: 0.9359 - val_acc: 0.6571\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6085 - acc: 0.7511 - val_loss: 0.9416 - val_acc: 0.6400\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6109 - acc: 0.7447 - val_loss: 0.9365 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6054 - acc: 0.7594 - val_loss: 0.9493 - val_acc: 0.6514\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6050 - acc: 0.7613 - val_loss: 0.9412 - val_acc: 0.6629\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6034 - acc: 0.7549 - val_loss: 0.9567 - val_acc: 0.6629\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6210 - acc: 0.7537 - val_loss: 0.9707 - val_acc: 0.6571\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6085 - acc: 0.7588 - val_loss: 0.9602 - val_acc: 0.6629\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5924 - acc: 0.7671 - val_loss: 0.9534 - val_acc: 0.6571\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5748 - acc: 0.7728 - val_loss: 0.9581 - val_acc: 0.6571\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5977 - acc: 0.7671 - val_loss: 0.9596 - val_acc: 0.6571\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5773 - acc: 0.7735 - val_loss: 0.9585 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.5669 - acc: 0.7728 - val_loss: 0.9629 - val_acc: 0.6571\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5712 - acc: 0.7722 - val_loss: 0.9759 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5870 - acc: 0.7664 - val_loss: 0.9836 - val_acc: 0.6457\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5730 - acc: 0.7798 - val_loss: 0.9734 - val_acc: 0.6629\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5809 - acc: 0.7811 - val_loss: 0.9828 - val_acc: 0.6457\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5485 - acc: 0.7837 - val_loss: 1.0031 - val_acc: 0.6514\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5502 - acc: 0.7843 - val_loss: 0.9856 - val_acc: 0.6514\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5301 - acc: 0.7875 - val_loss: 0.9872 - val_acc: 0.6400\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5532 - acc: 0.7754 - val_loss: 0.9867 - val_acc: 0.6400\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5410 - acc: 0.7913 - val_loss: 0.9950 - val_acc: 0.6286\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5460 - acc: 0.7939 - val_loss: 0.9980 - val_acc: 0.6286\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5575 - acc: 0.7779 - val_loss: 0.9846 - val_acc: 0.6571\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5676 - acc: 0.7754 - val_loss: 1.0097 - val_acc: 0.6571\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5139 - acc: 0.7951 - val_loss: 1.0067 - val_acc: 0.6629\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5343 - acc: 0.7875 - val_loss: 1.0167 - val_acc: 0.6629\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5236 - acc: 0.7932 - val_loss: 1.0194 - val_acc: 0.6571\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5295 - acc: 0.7875 - val_loss: 1.0179 - val_acc: 0.6514\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5235 - acc: 0.7913 - val_loss: 1.0150 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 204us/step\n",
      "1742/1742 [==============================] - 0s 53us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.4254 - acc: 0.3395 - val_loss: 1.0979 - val_acc: 0.5829\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 1.2030 - acc: 0.4582 - val_loss: 1.0244 - val_acc: 0.6057\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.1342 - acc: 0.4933 - val_loss: 0.9791 - val_acc: 0.6171\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.0771 - acc: 0.5348 - val_loss: 0.9529 - val_acc: 0.6171\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0507 - acc: 0.5514 - val_loss: 0.9317 - val_acc: 0.6286\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.0009 - acc: 0.5775 - val_loss: 0.9249 - val_acc: 0.6514\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.0061 - acc: 0.5846 - val_loss: 0.9050 - val_acc: 0.6800\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9639 - acc: 0.5890 - val_loss: 0.9098 - val_acc: 0.6914\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9635 - acc: 0.5973 - val_loss: 0.9087 - val_acc: 0.6686\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9392 - acc: 0.6063 - val_loss: 0.8926 - val_acc: 0.6743\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9110 - acc: 0.6209 - val_loss: 0.8933 - val_acc: 0.6857\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9086 - acc: 0.6165 - val_loss: 0.8946 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9087 - acc: 0.6184 - val_loss: 0.8958 - val_acc: 0.6857\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8915 - acc: 0.6286 - val_loss: 0.8951 - val_acc: 0.7029\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8828 - acc: 0.6267 - val_loss: 0.8883 - val_acc: 0.6857\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8993 - acc: 0.6254 - val_loss: 0.8942 - val_acc: 0.6914\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8638 - acc: 0.6299 - val_loss: 0.8907 - val_acc: 0.6914\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8547 - acc: 0.6503 - val_loss: 0.8862 - val_acc: 0.6857\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8774 - acc: 0.6407 - val_loss: 0.8879 - val_acc: 0.6857\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8559 - acc: 0.6439 - val_loss: 0.8862 - val_acc: 0.6743\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8449 - acc: 0.6465 - val_loss: 0.9031 - val_acc: 0.6571\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8539 - acc: 0.6535 - val_loss: 0.8884 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8132 - acc: 0.6624 - val_loss: 0.9038 - val_acc: 0.6743\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8312 - acc: 0.6554 - val_loss: 0.8965 - val_acc: 0.6686\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8266 - acc: 0.6701 - val_loss: 0.9086 - val_acc: 0.6686\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8266 - acc: 0.6586 - val_loss: 0.9023 - val_acc: 0.6629\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8024 - acc: 0.6669 - val_loss: 0.9132 - val_acc: 0.6743\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8128 - acc: 0.6656 - val_loss: 0.9106 - val_acc: 0.6629\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8113 - acc: 0.6624 - val_loss: 0.9138 - val_acc: 0.6686\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7898 - acc: 0.6694 - val_loss: 0.9159 - val_acc: 0.6800\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8020 - acc: 0.6745 - val_loss: 0.9144 - val_acc: 0.6629\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7694 - acc: 0.6854 - val_loss: 0.9058 - val_acc: 0.6629\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7961 - acc: 0.6707 - val_loss: 0.9037 - val_acc: 0.6514\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7815 - acc: 0.6905 - val_loss: 0.9071 - val_acc: 0.6514\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7582 - acc: 0.6854 - val_loss: 0.9215 - val_acc: 0.6571\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7702 - acc: 0.6892 - val_loss: 0.9071 - val_acc: 0.6514\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7559 - acc: 0.6694 - val_loss: 0.9073 - val_acc: 0.6686\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7507 - acc: 0.6905 - val_loss: 0.9206 - val_acc: 0.6514\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7461 - acc: 0.6943 - val_loss: 0.9214 - val_acc: 0.6457\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7367 - acc: 0.7007 - val_loss: 0.9241 - val_acc: 0.6343\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7420 - acc: 0.6892 - val_loss: 0.9169 - val_acc: 0.6743\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7544 - acc: 0.6918 - val_loss: 0.9229 - val_acc: 0.6571\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7368 - acc: 0.7020 - val_loss: 0.9245 - val_acc: 0.6514\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7357 - acc: 0.6924 - val_loss: 0.9276 - val_acc: 0.6514\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7387 - acc: 0.6981 - val_loss: 0.9339 - val_acc: 0.6229\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7225 - acc: 0.7192 - val_loss: 0.9284 - val_acc: 0.6457\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7143 - acc: 0.7096 - val_loss: 0.9264 - val_acc: 0.6457\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7147 - acc: 0.7128 - val_loss: 0.9305 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6916 - acc: 0.7224 - val_loss: 0.9333 - val_acc: 0.6571\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7106 - acc: 0.7224 - val_loss: 0.9345 - val_acc: 0.6514\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6910 - acc: 0.7167 - val_loss: 0.9437 - val_acc: 0.6343\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6881 - acc: 0.7192 - val_loss: 0.9565 - val_acc: 0.6457\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6951 - acc: 0.7250 - val_loss: 0.9531 - val_acc: 0.6400\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7024 - acc: 0.7211 - val_loss: 0.9543 - val_acc: 0.6343\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6731 - acc: 0.7275 - val_loss: 0.9581 - val_acc: 0.6629\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6872 - acc: 0.7116 - val_loss: 0.9521 - val_acc: 0.6571\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6827 - acc: 0.7205 - val_loss: 0.9620 - val_acc: 0.6571\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6793 - acc: 0.7390 - val_loss: 0.9600 - val_acc: 0.6457\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6531 - acc: 0.7294 - val_loss: 0.9734 - val_acc: 0.6514\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6658 - acc: 0.7352 - val_loss: 0.9714 - val_acc: 0.6514\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6604 - acc: 0.7403 - val_loss: 0.9694 - val_acc: 0.6457\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6525 - acc: 0.7435 - val_loss: 0.9784 - val_acc: 0.6457\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6484 - acc: 0.7486 - val_loss: 0.9688 - val_acc: 0.6571\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6468 - acc: 0.7428 - val_loss: 0.9624 - val_acc: 0.6457\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6503 - acc: 0.7358 - val_loss: 0.9813 - val_acc: 0.6629\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6581 - acc: 0.7409 - val_loss: 0.9920 - val_acc: 0.6514\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6401 - acc: 0.7326 - val_loss: 0.9802 - val_acc: 0.6343\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6300 - acc: 0.7422 - val_loss: 0.9966 - val_acc: 0.6343\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6423 - acc: 0.7364 - val_loss: 0.9969 - val_acc: 0.6286\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6141 - acc: 0.7511 - val_loss: 0.9773 - val_acc: 0.6343\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6378 - acc: 0.7441 - val_loss: 0.9835 - val_acc: 0.6400\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6355 - acc: 0.7364 - val_loss: 0.9946 - val_acc: 0.6286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5990 - acc: 0.7498 - val_loss: 0.9849 - val_acc: 0.6057\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6051 - acc: 0.7511 - val_loss: 0.9854 - val_acc: 0.6457\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6402 - acc: 0.7345 - val_loss: 0.9947 - val_acc: 0.6400\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6133 - acc: 0.7486 - val_loss: 1.0075 - val_acc: 0.6457\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5938 - acc: 0.7632 - val_loss: 1.0134 - val_acc: 0.6400\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6013 - acc: 0.7498 - val_loss: 1.0035 - val_acc: 0.6400\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6035 - acc: 0.7683 - val_loss: 1.0173 - val_acc: 0.6343\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5960 - acc: 0.7652 - val_loss: 1.0081 - val_acc: 0.6457\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6066 - acc: 0.7658 - val_loss: 1.0156 - val_acc: 0.6514\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5933 - acc: 0.7652 - val_loss: 1.0071 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5750 - acc: 0.7677 - val_loss: 1.0297 - val_acc: 0.6343\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5764 - acc: 0.7747 - val_loss: 1.0411 - val_acc: 0.6457\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5737 - acc: 0.7664 - val_loss: 1.0097 - val_acc: 0.6571\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5479 - acc: 0.7849 - val_loss: 1.0162 - val_acc: 0.6686\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5658 - acc: 0.7613 - val_loss: 1.0478 - val_acc: 0.6457\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5636 - acc: 0.7728 - val_loss: 1.0361 - val_acc: 0.6286\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.5602 - acc: 0.7696 - val_loss: 1.0399 - val_acc: 0.6343\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.5527 - acc: 0.7786 - val_loss: 1.0493 - val_acc: 0.6457\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5741 - acc: 0.7658 - val_loss: 1.0488 - val_acc: 0.6229\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5676 - acc: 0.7754 - val_loss: 1.0437 - val_acc: 0.6229\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5561 - acc: 0.7856 - val_loss: 1.0543 - val_acc: 0.6171\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5539 - acc: 0.7773 - val_loss: 1.0572 - val_acc: 0.6114\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5444 - acc: 0.7881 - val_loss: 1.0670 - val_acc: 0.6229\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5589 - acc: 0.7830 - val_loss: 1.0620 - val_acc: 0.6171\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5475 - acc: 0.7862 - val_loss: 1.0706 - val_acc: 0.6171\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5360 - acc: 0.7805 - val_loss: 1.0912 - val_acc: 0.6286\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5306 - acc: 0.7926 - val_loss: 1.0959 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5310 - acc: 0.7964 - val_loss: 1.1012 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 66us/step\n",
      "1742/1742 [==============================] - 0s 51us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.4329 - acc: 0.3223 - val_loss: 1.1216 - val_acc: 0.5714\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.2195 - acc: 0.4518 - val_loss: 1.0248 - val_acc: 0.6057\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.1303 - acc: 0.5048 - val_loss: 0.9840 - val_acc: 0.6457\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.0608 - acc: 0.5392 - val_loss: 0.9503 - val_acc: 0.6629\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0222 - acc: 0.5731 - val_loss: 0.9455 - val_acc: 0.6286\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.0203 - acc: 0.5743 - val_loss: 0.9284 - val_acc: 0.6514\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9782 - acc: 0.5839 - val_loss: 0.9238 - val_acc: 0.6571\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.9779 - acc: 0.5960 - val_loss: 0.9129 - val_acc: 0.6800\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9765 - acc: 0.5903 - val_loss: 0.9115 - val_acc: 0.6629\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9394 - acc: 0.6139 - val_loss: 0.9210 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9362 - acc: 0.6190 - val_loss: 0.9149 - val_acc: 0.6857\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9249 - acc: 0.6075 - val_loss: 0.9128 - val_acc: 0.6857\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9078 - acc: 0.6184 - val_loss: 0.9217 - val_acc: 0.6457\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9149 - acc: 0.6324 - val_loss: 0.9148 - val_acc: 0.6514\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8920 - acc: 0.6343 - val_loss: 0.9052 - val_acc: 0.6571\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8845 - acc: 0.6331 - val_loss: 0.9035 - val_acc: 0.6743\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8563 - acc: 0.6477 - val_loss: 0.9072 - val_acc: 0.6686\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8686 - acc: 0.6286 - val_loss: 0.9184 - val_acc: 0.6629\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8511 - acc: 0.6324 - val_loss: 0.9028 - val_acc: 0.6686\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8544 - acc: 0.6535 - val_loss: 0.9140 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8642 - acc: 0.6554 - val_loss: 0.9158 - val_acc: 0.6457\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8366 - acc: 0.6560 - val_loss: 0.9175 - val_acc: 0.6571\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8361 - acc: 0.6579 - val_loss: 0.9160 - val_acc: 0.6571\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8391 - acc: 0.6611 - val_loss: 0.9133 - val_acc: 0.6457\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.8133 - acc: 0.6669 - val_loss: 0.9054 - val_acc: 0.6571\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8114 - acc: 0.6707 - val_loss: 0.9081 - val_acc: 0.6571\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8233 - acc: 0.6624 - val_loss: 0.9163 - val_acc: 0.6457\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8032 - acc: 0.6713 - val_loss: 0.9142 - val_acc: 0.6571\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8059 - acc: 0.6720 - val_loss: 0.9254 - val_acc: 0.6400\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7905 - acc: 0.6713 - val_loss: 0.9254 - val_acc: 0.6629\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7829 - acc: 0.6892 - val_loss: 0.9229 - val_acc: 0.6629\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7741 - acc: 0.6816 - val_loss: 0.9239 - val_acc: 0.6457\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.7707 - acc: 0.6956 - val_loss: 0.9214 - val_acc: 0.6514\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7618 - acc: 0.6962 - val_loss: 0.9314 - val_acc: 0.6571\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7705 - acc: 0.6873 - val_loss: 0.9316 - val_acc: 0.6400\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7683 - acc: 0.6975 - val_loss: 0.9308 - val_acc: 0.6400\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7622 - acc: 0.6956 - val_loss: 0.9303 - val_acc: 0.6686\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7631 - acc: 0.6809 - val_loss: 0.9524 - val_acc: 0.6343\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7530 - acc: 0.7077 - val_loss: 0.9458 - val_acc: 0.6514\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7545 - acc: 0.6911 - val_loss: 0.9383 - val_acc: 0.6514\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.7448 - acc: 0.6975 - val_loss: 0.9435 - val_acc: 0.6514\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7477 - acc: 0.7039 - val_loss: 0.9400 - val_acc: 0.6629\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7246 - acc: 0.7154 - val_loss: 0.9482 - val_acc: 0.6514\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7260 - acc: 0.7122 - val_loss: 0.9549 - val_acc: 0.6457\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7329 - acc: 0.7103 - val_loss: 0.9588 - val_acc: 0.6400\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7187 - acc: 0.7243 - val_loss: 0.9685 - val_acc: 0.6629\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7193 - acc: 0.7122 - val_loss: 0.9606 - val_acc: 0.6571\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7150 - acc: 0.7122 - val_loss: 0.9604 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.6976 - acc: 0.7384 - val_loss: 0.9679 - val_acc: 0.6571\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7264 - acc: 0.7077 - val_loss: 0.9674 - val_acc: 0.6514\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7027 - acc: 0.7141 - val_loss: 0.9672 - val_acc: 0.6400\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6962 - acc: 0.7256 - val_loss: 0.9820 - val_acc: 0.6343\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6910 - acc: 0.7211 - val_loss: 0.9816 - val_acc: 0.6343\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6820 - acc: 0.7377 - val_loss: 0.9801 - val_acc: 0.6514\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6887 - acc: 0.7243 - val_loss: 0.9780 - val_acc: 0.6400\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6809 - acc: 0.7320 - val_loss: 0.9691 - val_acc: 0.6457\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6710 - acc: 0.7339 - val_loss: 0.9799 - val_acc: 0.6514\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6627 - acc: 0.7409 - val_loss: 0.9747 - val_acc: 0.6514\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6670 - acc: 0.7326 - val_loss: 0.9878 - val_acc: 0.6457\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6621 - acc: 0.7332 - val_loss: 0.9914 - val_acc: 0.6457\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6519 - acc: 0.7530 - val_loss: 0.9869 - val_acc: 0.6571\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6641 - acc: 0.7377 - val_loss: 0.9940 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6569 - acc: 0.7415 - val_loss: 1.0101 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6227 - acc: 0.7626 - val_loss: 1.0065 - val_acc: 0.6514\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6449 - acc: 0.7371 - val_loss: 1.0074 - val_acc: 0.6400\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6304 - acc: 0.7454 - val_loss: 0.9980 - val_acc: 0.6343\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6437 - acc: 0.7358 - val_loss: 1.0043 - val_acc: 0.6400\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6383 - acc: 0.7422 - val_loss: 1.0119 - val_acc: 0.6400\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6426 - acc: 0.7581 - val_loss: 1.0230 - val_acc: 0.6400\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6245 - acc: 0.7511 - val_loss: 1.0145 - val_acc: 0.6514\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6457 - acc: 0.7428 - val_loss: 1.0250 - val_acc: 0.6343\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.6088 - acc: 0.7652 - val_loss: 1.0184 - val_acc: 0.6400\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6146 - acc: 0.7639 - val_loss: 1.0294 - val_acc: 0.6457\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6164 - acc: 0.7473 - val_loss: 1.0330 - val_acc: 0.6343\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6258 - acc: 0.7754 - val_loss: 1.0333 - val_acc: 0.6400\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.5927 - acc: 0.7645 - val_loss: 1.0407 - val_acc: 0.6514\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6061 - acc: 0.7581 - val_loss: 1.0364 - val_acc: 0.6171\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5995 - acc: 0.7715 - val_loss: 1.0183 - val_acc: 0.6400\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6098 - acc: 0.7683 - val_loss: 1.0350 - val_acc: 0.6400\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6017 - acc: 0.7709 - val_loss: 1.0391 - val_acc: 0.6286\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5912 - acc: 0.7837 - val_loss: 1.0448 - val_acc: 0.6400\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.5955 - acc: 0.7588 - val_loss: 1.0433 - val_acc: 0.6343\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.5864 - acc: 0.7703 - val_loss: 1.0405 - val_acc: 0.6400\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5914 - acc: 0.7709 - val_loss: 1.0421 - val_acc: 0.6286\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5745 - acc: 0.7715 - val_loss: 1.0528 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.5561 - acc: 0.7894 - val_loss: 1.0569 - val_acc: 0.6400\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5668 - acc: 0.7690 - val_loss: 1.0519 - val_acc: 0.6286\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.5756 - acc: 0.7811 - val_loss: 1.0447 - val_acc: 0.6343\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.5665 - acc: 0.7735 - val_loss: 1.0507 - val_acc: 0.6229\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5629 - acc: 0.7869 - val_loss: 1.0697 - val_acc: 0.6229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5600 - acc: 0.7824 - val_loss: 1.0747 - val_acc: 0.6286\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5458 - acc: 0.7958 - val_loss: 1.0634 - val_acc: 0.6571\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5432 - acc: 0.7964 - val_loss: 1.0777 - val_acc: 0.6457\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.5733 - acc: 0.7792 - val_loss: 1.0771 - val_acc: 0.6514\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5707 - acc: 0.7798 - val_loss: 1.0750 - val_acc: 0.6571\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.5404 - acc: 0.7862 - val_loss: 1.0969 - val_acc: 0.6400\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.5615 - acc: 0.7856 - val_loss: 1.1017 - val_acc: 0.6457\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.5455 - acc: 0.8003 - val_loss: 1.1084 - val_acc: 0.6400\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5547 - acc: 0.7869 - val_loss: 1.1090 - val_acc: 0.6514\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5360 - acc: 0.7939 - val_loss: 1.1127 - val_acc: 0.6457\n",
      "193/193 [==============================] - 0s 71us/step\n",
      "1742/1742 [==============================] - 0s 53us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.4610 - acc: 0.3395 - val_loss: 1.1464 - val_acc: 0.4971\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 1.2273 - acc: 0.4442 - val_loss: 1.0463 - val_acc: 0.5429\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.1320 - acc: 0.5093 - val_loss: 1.0184 - val_acc: 0.5657\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.0880 - acc: 0.5348 - val_loss: 0.9855 - val_acc: 0.5714\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.0528 - acc: 0.5495 - val_loss: 0.9776 - val_acc: 0.5714\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0069 - acc: 0.5775 - val_loss: 0.9595 - val_acc: 0.5543\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9933 - acc: 0.5699 - val_loss: 0.9640 - val_acc: 0.5486\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9909 - acc: 0.5795 - val_loss: 0.9637 - val_acc: 0.5771\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9658 - acc: 0.5820 - val_loss: 0.9633 - val_acc: 0.5714\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9713 - acc: 0.5890 - val_loss: 0.9659 - val_acc: 0.5886\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9448 - acc: 0.6082 - val_loss: 0.9683 - val_acc: 0.5657\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9282 - acc: 0.6165 - val_loss: 0.9628 - val_acc: 0.5771\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9245 - acc: 0.6043 - val_loss: 0.9613 - val_acc: 0.5657\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9051 - acc: 0.6228 - val_loss: 0.9668 - val_acc: 0.5714\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8907 - acc: 0.6299 - val_loss: 0.9703 - val_acc: 0.5714\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8777 - acc: 0.6369 - val_loss: 0.9600 - val_acc: 0.5771\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8710 - acc: 0.6369 - val_loss: 0.9702 - val_acc: 0.5714\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8777 - acc: 0.6382 - val_loss: 0.9711 - val_acc: 0.5829\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8719 - acc: 0.6273 - val_loss: 0.9648 - val_acc: 0.5886\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8681 - acc: 0.6439 - val_loss: 0.9673 - val_acc: 0.5829\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8389 - acc: 0.6675 - val_loss: 0.9923 - val_acc: 0.5886\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8427 - acc: 0.6509 - val_loss: 0.9813 - val_acc: 0.5943\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8371 - acc: 0.6567 - val_loss: 0.9844 - val_acc: 0.5886\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8291 - acc: 0.6611 - val_loss: 0.9707 - val_acc: 0.6114\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8006 - acc: 0.6586 - val_loss: 0.9847 - val_acc: 0.6000\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8125 - acc: 0.6694 - val_loss: 0.9833 - val_acc: 0.5943\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8175 - acc: 0.6611 - val_loss: 0.9841 - val_acc: 0.6171\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8109 - acc: 0.6579 - val_loss: 0.9863 - val_acc: 0.6114\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8061 - acc: 0.6682 - val_loss: 0.9836 - val_acc: 0.6114\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7939 - acc: 0.6796 - val_loss: 0.9933 - val_acc: 0.5943\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7827 - acc: 0.6809 - val_loss: 0.9978 - val_acc: 0.5886\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7898 - acc: 0.6720 - val_loss: 1.0089 - val_acc: 0.6000\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7765 - acc: 0.6822 - val_loss: 1.0006 - val_acc: 0.6000\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7834 - acc: 0.6969 - val_loss: 1.0044 - val_acc: 0.5943\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7921 - acc: 0.6847 - val_loss: 0.9901 - val_acc: 0.5943\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7509 - acc: 0.6892 - val_loss: 1.0134 - val_acc: 0.6000\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7583 - acc: 0.6969 - val_loss: 1.0110 - val_acc: 0.5943\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7736 - acc: 0.6879 - val_loss: 1.0086 - val_acc: 0.5943\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7592 - acc: 0.6943 - val_loss: 1.0059 - val_acc: 0.6000\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7465 - acc: 0.7103 - val_loss: 1.0015 - val_acc: 0.6114\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7346 - acc: 0.7026 - val_loss: 1.0262 - val_acc: 0.6057\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7344 - acc: 0.7020 - val_loss: 1.0308 - val_acc: 0.5943\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7511 - acc: 0.6816 - val_loss: 1.0285 - val_acc: 0.6000\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7228 - acc: 0.7135 - val_loss: 1.0286 - val_acc: 0.6000\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7295 - acc: 0.7122 - val_loss: 1.0313 - val_acc: 0.6000\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7233 - acc: 0.7135 - val_loss: 1.0319 - val_acc: 0.6057\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7297 - acc: 0.7020 - val_loss: 1.0494 - val_acc: 0.6000\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7236 - acc: 0.7052 - val_loss: 1.0349 - val_acc: 0.6000\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7051 - acc: 0.7211 - val_loss: 1.0436 - val_acc: 0.6057\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7083 - acc: 0.7179 - val_loss: 1.0351 - val_acc: 0.6171\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7126 - acc: 0.7237 - val_loss: 1.0384 - val_acc: 0.6057\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6812 - acc: 0.7256 - val_loss: 1.0396 - val_acc: 0.6000\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6961 - acc: 0.7205 - val_loss: 1.0612 - val_acc: 0.6114\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6936 - acc: 0.7186 - val_loss: 1.0483 - val_acc: 0.6000\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6821 - acc: 0.7307 - val_loss: 1.0581 - val_acc: 0.6171\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6864 - acc: 0.7364 - val_loss: 1.0554 - val_acc: 0.6114\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6754 - acc: 0.7447 - val_loss: 1.0556 - val_acc: 0.6057\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6872 - acc: 0.7262 - val_loss: 1.0615 - val_acc: 0.6000\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6862 - acc: 0.7294 - val_loss: 1.0540 - val_acc: 0.6057\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6627 - acc: 0.7301 - val_loss: 1.0602 - val_acc: 0.5943\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6767 - acc: 0.7230 - val_loss: 1.0580 - val_acc: 0.6000\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6610 - acc: 0.7422 - val_loss: 1.0708 - val_acc: 0.6114\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6466 - acc: 0.7466 - val_loss: 1.0529 - val_acc: 0.6000\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6652 - acc: 0.7262 - val_loss: 1.0435 - val_acc: 0.5943\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6529 - acc: 0.7358 - val_loss: 1.0653 - val_acc: 0.6000\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6535 - acc: 0.7409 - val_loss: 1.0780 - val_acc: 0.5829\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6442 - acc: 0.7466 - val_loss: 1.0929 - val_acc: 0.6057\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6383 - acc: 0.7479 - val_loss: 1.0746 - val_acc: 0.5714\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6510 - acc: 0.7275 - val_loss: 1.0763 - val_acc: 0.5829\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6229 - acc: 0.7543 - val_loss: 1.0960 - val_acc: 0.5829\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6376 - acc: 0.7473 - val_loss: 1.0986 - val_acc: 0.5943\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.6280 - acc: 0.7505 - val_loss: 1.0846 - val_acc: 0.6114\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6117 - acc: 0.7486 - val_loss: 1.1083 - val_acc: 0.5886\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6329 - acc: 0.7543 - val_loss: 1.1152 - val_acc: 0.5829\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6310 - acc: 0.7505 - val_loss: 1.0976 - val_acc: 0.6000\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6139 - acc: 0.7575 - val_loss: 1.1088 - val_acc: 0.5943\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6276 - acc: 0.7511 - val_loss: 1.1040 - val_acc: 0.6000\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6190 - acc: 0.7473 - val_loss: 1.1122 - val_acc: 0.5886\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6202 - acc: 0.7543 - val_loss: 1.1213 - val_acc: 0.5771\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6088 - acc: 0.7601 - val_loss: 1.1151 - val_acc: 0.5943\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6035 - acc: 0.7652 - val_loss: 1.1189 - val_acc: 0.5886\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5721 - acc: 0.7913 - val_loss: 1.1371 - val_acc: 0.5829\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5846 - acc: 0.7639 - val_loss: 1.1272 - val_acc: 0.5829\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5787 - acc: 0.7715 - val_loss: 1.1393 - val_acc: 0.5714\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6137 - acc: 0.7518 - val_loss: 1.1261 - val_acc: 0.5943\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5806 - acc: 0.7728 - val_loss: 1.1205 - val_acc: 0.5771\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5709 - acc: 0.7760 - val_loss: 1.1347 - val_acc: 0.5943\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5723 - acc: 0.7696 - val_loss: 1.1237 - val_acc: 0.5829\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5812 - acc: 0.7811 - val_loss: 1.1141 - val_acc: 0.5886\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5675 - acc: 0.7773 - val_loss: 1.1426 - val_acc: 0.5886\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5586 - acc: 0.7907 - val_loss: 1.1454 - val_acc: 0.5829\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5648 - acc: 0.7907 - val_loss: 1.1381 - val_acc: 0.5829\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5539 - acc: 0.7805 - val_loss: 1.1287 - val_acc: 0.5886\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5580 - acc: 0.7817 - val_loss: 1.1431 - val_acc: 0.5886\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5525 - acc: 0.7881 - val_loss: 1.1577 - val_acc: 0.5886\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5434 - acc: 0.7741 - val_loss: 1.1688 - val_acc: 0.5943\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5478 - acc: 0.7869 - val_loss: 1.1555 - val_acc: 0.5771\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5424 - acc: 0.7894 - val_loss: 1.1702 - val_acc: 0.5829\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5474 - acc: 0.7875 - val_loss: 1.1794 - val_acc: 0.5886\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5429 - acc: 0.7843 - val_loss: 1.1754 - val_acc: 0.5600\n",
      "193/193 [==============================] - 0s 71us/step\n",
      "1742/1742 [==============================] - 0s 52us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4463 - acc: 0.3123 - val_loss: 1.2338 - val_acc: 0.4800\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 1.2794 - acc: 0.4189 - val_loss: 1.1357 - val_acc: 0.5371\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.2163 - acc: 0.4642 - val_loss: 1.0772 - val_acc: 0.5943\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.1693 - acc: 0.4815 - val_loss: 1.0450 - val_acc: 0.5886\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.1204 - acc: 0.4930 - val_loss: 1.0146 - val_acc: 0.6343\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0978 - acc: 0.5287 - val_loss: 0.9904 - val_acc: 0.6286\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.0523 - acc: 0.5460 - val_loss: 0.9670 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0518 - acc: 0.5473 - val_loss: 0.9560 - val_acc: 0.6400\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 1.0275 - acc: 0.5645 - val_loss: 0.9470 - val_acc: 0.6286\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.0305 - acc: 0.5594 - val_loss: 0.9284 - val_acc: 0.6571\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.0093 - acc: 0.5722 - val_loss: 0.9227 - val_acc: 0.6571\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9858 - acc: 0.5811 - val_loss: 0.9182 - val_acc: 0.6457\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9812 - acc: 0.5747 - val_loss: 0.9112 - val_acc: 0.6457\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9706 - acc: 0.5983 - val_loss: 0.9057 - val_acc: 0.6400\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9500 - acc: 0.5913 - val_loss: 0.8985 - val_acc: 0.6629\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9498 - acc: 0.6098 - val_loss: 0.8996 - val_acc: 0.6571\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9371 - acc: 0.6015 - val_loss: 0.8993 - val_acc: 0.6514\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.9359 - acc: 0.6073 - val_loss: 0.8987 - val_acc: 0.6514\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9338 - acc: 0.6034 - val_loss: 0.8928 - val_acc: 0.6629\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.9255 - acc: 0.6092 - val_loss: 0.8975 - val_acc: 0.6571\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9146 - acc: 0.6092 - val_loss: 0.8904 - val_acc: 0.6629\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9087 - acc: 0.6156 - val_loss: 0.8921 - val_acc: 0.6514\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8930 - acc: 0.6296 - val_loss: 0.8856 - val_acc: 0.6629\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8980 - acc: 0.6220 - val_loss: 0.8871 - val_acc: 0.6457\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8890 - acc: 0.6335 - val_loss: 0.8869 - val_acc: 0.6686\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8771 - acc: 0.6360 - val_loss: 0.8879 - val_acc: 0.6514\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8833 - acc: 0.6373 - val_loss: 0.8938 - val_acc: 0.6629\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8781 - acc: 0.6296 - val_loss: 0.8875 - val_acc: 0.6514\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8806 - acc: 0.6335 - val_loss: 0.8938 - val_acc: 0.6629\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8817 - acc: 0.6207 - val_loss: 0.8877 - val_acc: 0.6686\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8595 - acc: 0.6437 - val_loss: 0.8929 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8664 - acc: 0.6360 - val_loss: 0.8945 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8468 - acc: 0.6539 - val_loss: 0.8996 - val_acc: 0.6629\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8497 - acc: 0.6584 - val_loss: 0.8962 - val_acc: 0.6743\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8602 - acc: 0.6545 - val_loss: 0.8979 - val_acc: 0.6629\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8551 - acc: 0.6526 - val_loss: 0.9011 - val_acc: 0.6629\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8371 - acc: 0.6584 - val_loss: 0.8995 - val_acc: 0.6686\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8423 - acc: 0.6552 - val_loss: 0.8954 - val_acc: 0.6629\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8328 - acc: 0.6724 - val_loss: 0.8910 - val_acc: 0.6629\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8277 - acc: 0.6552 - val_loss: 0.8932 - val_acc: 0.6686\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8352 - acc: 0.6577 - val_loss: 0.8884 - val_acc: 0.6686\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8402 - acc: 0.6513 - val_loss: 0.9019 - val_acc: 0.6629\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8301 - acc: 0.6609 - val_loss: 0.8998 - val_acc: 0.6571\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8105 - acc: 0.6820 - val_loss: 0.9030 - val_acc: 0.6629\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8369 - acc: 0.6705 - val_loss: 0.9038 - val_acc: 0.6514\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8110 - acc: 0.6667 - val_loss: 0.8997 - val_acc: 0.6686\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8149 - acc: 0.6692 - val_loss: 0.9005 - val_acc: 0.6514\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7967 - acc: 0.6839 - val_loss: 0.9028 - val_acc: 0.6629\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7945 - acc: 0.6782 - val_loss: 0.9036 - val_acc: 0.6686\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7982 - acc: 0.6756 - val_loss: 0.9032 - val_acc: 0.6571\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8052 - acc: 0.6839 - val_loss: 0.9030 - val_acc: 0.6571\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7870 - acc: 0.6877 - val_loss: 0.9044 - val_acc: 0.6629\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8040 - acc: 0.6539 - val_loss: 0.8980 - val_acc: 0.6686\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8065 - acc: 0.6667 - val_loss: 0.8996 - val_acc: 0.6686\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7736 - acc: 0.6967 - val_loss: 0.9089 - val_acc: 0.6571\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7702 - acc: 0.6871 - val_loss: 0.9008 - val_acc: 0.6629\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7921 - acc: 0.6775 - val_loss: 0.9125 - val_acc: 0.6629\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7762 - acc: 0.6897 - val_loss: 0.9126 - val_acc: 0.6571\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7799 - acc: 0.6718 - val_loss: 0.9164 - val_acc: 0.6571\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7806 - acc: 0.6750 - val_loss: 0.9157 - val_acc: 0.6514\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7656 - acc: 0.7082 - val_loss: 0.9150 - val_acc: 0.6514\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7741 - acc: 0.6922 - val_loss: 0.9213 - val_acc: 0.6457\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7765 - acc: 0.6871 - val_loss: 0.9133 - val_acc: 0.6629\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7520 - acc: 0.6954 - val_loss: 0.9189 - val_acc: 0.6457\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7811 - acc: 0.6935 - val_loss: 0.9223 - val_acc: 0.6514\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7552 - acc: 0.7050 - val_loss: 0.9140 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7554 - acc: 0.6897 - val_loss: 0.9284 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7501 - acc: 0.7024 - val_loss: 0.9328 - val_acc: 0.6457\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7708 - acc: 0.6903 - val_loss: 0.9211 - val_acc: 0.6571\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7305 - acc: 0.6954 - val_loss: 0.9266 - val_acc: 0.6457\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7446 - acc: 0.6922 - val_loss: 0.9293 - val_acc: 0.6400\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7384 - acc: 0.6973 - val_loss: 0.9262 - val_acc: 0.6571\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7416 - acc: 0.7082 - val_loss: 0.9302 - val_acc: 0.6629\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7449 - acc: 0.6903 - val_loss: 0.9303 - val_acc: 0.6514\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7358 - acc: 0.7178 - val_loss: 0.9351 - val_acc: 0.6400\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7278 - acc: 0.7107 - val_loss: 0.9400 - val_acc: 0.6514\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7194 - acc: 0.7158 - val_loss: 0.9356 - val_acc: 0.6629\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7300 - acc: 0.7082 - val_loss: 0.9369 - val_acc: 0.6571\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7308 - acc: 0.7101 - val_loss: 0.9397 - val_acc: 0.6457\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7211 - acc: 0.7114 - val_loss: 0.9437 - val_acc: 0.6457\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7207 - acc: 0.7095 - val_loss: 0.9360 - val_acc: 0.6514\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7291 - acc: 0.7056 - val_loss: 0.9428 - val_acc: 0.6571\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7213 - acc: 0.7101 - val_loss: 0.9522 - val_acc: 0.6400\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7027 - acc: 0.7184 - val_loss: 0.9536 - val_acc: 0.6400\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7232 - acc: 0.7120 - val_loss: 0.9469 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7089 - acc: 0.7216 - val_loss: 0.9478 - val_acc: 0.6571\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7090 - acc: 0.7075 - val_loss: 0.9597 - val_acc: 0.6514\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7160 - acc: 0.7171 - val_loss: 0.9576 - val_acc: 0.6457\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7007 - acc: 0.7229 - val_loss: 0.9572 - val_acc: 0.6457\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7032 - acc: 0.7152 - val_loss: 0.9464 - val_acc: 0.6457\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7140 - acc: 0.7011 - val_loss: 0.9495 - val_acc: 0.6571\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7039 - acc: 0.7190 - val_loss: 0.9453 - val_acc: 0.6457\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7003 - acc: 0.7299 - val_loss: 0.9496 - val_acc: 0.6571\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6746 - acc: 0.7261 - val_loss: 0.9556 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6838 - acc: 0.7241 - val_loss: 0.9604 - val_acc: 0.6514\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6869 - acc: 0.7280 - val_loss: 0.9577 - val_acc: 0.6514\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6796 - acc: 0.7401 - val_loss: 0.9542 - val_acc: 0.6514\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7000 - acc: 0.7241 - val_loss: 0.9576 - val_acc: 0.6571\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6921 - acc: 0.7146 - val_loss: 0.9688 - val_acc: 0.6400\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7015 - acc: 0.7209 - val_loss: 0.9645 - val_acc: 0.6514\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6950 - acc: 0.7197 - val_loss: 0.9609 - val_acc: 0.6514\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6686 - acc: 0.7299 - val_loss: 0.9593 - val_acc: 0.6571\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6749 - acc: 0.7209 - val_loss: 0.9627 - val_acc: 0.6571\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6874 - acc: 0.7350 - val_loss: 0.9699 - val_acc: 0.6457\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6673 - acc: 0.7395 - val_loss: 0.9651 - val_acc: 0.6571\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6696 - acc: 0.7369 - val_loss: 0.9715 - val_acc: 0.6400\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6512 - acc: 0.7388 - val_loss: 0.9674 - val_acc: 0.6629\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6691 - acc: 0.7254 - val_loss: 0.9807 - val_acc: 0.6514\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6636 - acc: 0.7286 - val_loss: 0.9750 - val_acc: 0.6571\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6606 - acc: 0.7382 - val_loss: 0.9883 - val_acc: 0.6571\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6578 - acc: 0.7420 - val_loss: 0.9905 - val_acc: 0.6514\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6644 - acc: 0.7235 - val_loss: 0.9850 - val_acc: 0.6514\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6692 - acc: 0.7286 - val_loss: 0.9817 - val_acc: 0.6571\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6755 - acc: 0.7363 - val_loss: 0.9799 - val_acc: 0.6686\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6596 - acc: 0.7388 - val_loss: 0.9723 - val_acc: 0.6743\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6654 - acc: 0.7420 - val_loss: 0.9848 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6448 - acc: 0.7465 - val_loss: 0.9897 - val_acc: 0.6571\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6518 - acc: 0.7356 - val_loss: 0.9964 - val_acc: 0.6514\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6672 - acc: 0.7344 - val_loss: 0.9911 - val_acc: 0.6571\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6304 - acc: 0.7554 - val_loss: 0.9913 - val_acc: 0.6686\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6514 - acc: 0.7420 - val_loss: 1.0028 - val_acc: 0.6571\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6429 - acc: 0.7439 - val_loss: 0.9995 - val_acc: 0.6629\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6471 - acc: 0.7382 - val_loss: 0.9943 - val_acc: 0.6629\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6486 - acc: 0.7395 - val_loss: 1.0030 - val_acc: 0.6457\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6294 - acc: 0.7452 - val_loss: 0.9932 - val_acc: 0.6629\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6353 - acc: 0.7420 - val_loss: 0.9974 - val_acc: 0.6514\n",
      "Epoch 127/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6398 - acc: 0.7599 - val_loss: 1.0006 - val_acc: 0.6571\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6243 - acc: 0.7433 - val_loss: 1.0046 - val_acc: 0.6743\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6163 - acc: 0.7535 - val_loss: 1.0046 - val_acc: 0.6686\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6302 - acc: 0.7407 - val_loss: 1.0131 - val_acc: 0.6571\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6383 - acc: 0.7446 - val_loss: 1.0176 - val_acc: 0.6629\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6182 - acc: 0.7465 - val_loss: 1.0181 - val_acc: 0.6686\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6332 - acc: 0.7522 - val_loss: 1.0172 - val_acc: 0.6686\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6189 - acc: 0.7593 - val_loss: 1.0230 - val_acc: 0.6629\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6243 - acc: 0.7542 - val_loss: 1.0208 - val_acc: 0.6686\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6208 - acc: 0.7471 - val_loss: 1.0246 - val_acc: 0.6629\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6184 - acc: 0.7586 - val_loss: 1.0176 - val_acc: 0.6800\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6005 - acc: 0.7656 - val_loss: 1.0250 - val_acc: 0.6686\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6002 - acc: 0.7605 - val_loss: 1.0312 - val_acc: 0.6571\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6133 - acc: 0.7599 - val_loss: 1.0222 - val_acc: 0.6800\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5933 - acc: 0.7548 - val_loss: 1.0257 - val_acc: 0.6514\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6185 - acc: 0.7586 - val_loss: 1.0170 - val_acc: 0.6743\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6153 - acc: 0.7618 - val_loss: 1.0297 - val_acc: 0.6629\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6175 - acc: 0.7625 - val_loss: 1.0410 - val_acc: 0.6629\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6317 - acc: 0.7561 - val_loss: 1.0400 - val_acc: 0.6629\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5778 - acc: 0.7739 - val_loss: 1.0340 - val_acc: 0.6629\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6129 - acc: 0.7567 - val_loss: 1.0312 - val_acc: 0.6743\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6087 - acc: 0.7395 - val_loss: 1.0382 - val_acc: 0.6629\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5842 - acc: 0.7752 - val_loss: 1.0425 - val_acc: 0.6571\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5901 - acc: 0.7599 - val_loss: 1.0454 - val_acc: 0.6571\n",
      "194/194 [==============================] - 0s 72us/step\n",
      "1741/1741 [==============================] - 0s 48us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4672 - acc: 0.3276 - val_loss: 1.2011 - val_acc: 0.5257\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.2748 - acc: 0.4144 - val_loss: 1.1159 - val_acc: 0.5543\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 1.1971 - acc: 0.4604 - val_loss: 1.0624 - val_acc: 0.5829\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.1574 - acc: 0.4955 - val_loss: 1.0196 - val_acc: 0.6171\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 1.1198 - acc: 0.5115 - val_loss: 0.9917 - val_acc: 0.6571\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 1.0734 - acc: 0.5415 - val_loss: 0.9710 - val_acc: 0.6571\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0551 - acc: 0.5402 - val_loss: 0.9566 - val_acc: 0.6343\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 1.0562 - acc: 0.5358 - val_loss: 0.9409 - val_acc: 0.6571\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0143 - acc: 0.5734 - val_loss: 0.9335 - val_acc: 0.6686\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 1.0208 - acc: 0.5639 - val_loss: 0.9249 - val_acc: 0.6629\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.0018 - acc: 0.5715 - val_loss: 0.9197 - val_acc: 0.6743\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9908 - acc: 0.5824 - val_loss: 0.9163 - val_acc: 0.6514\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9797 - acc: 0.6022 - val_loss: 0.9051 - val_acc: 0.6457\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9560 - acc: 0.5920 - val_loss: 0.9080 - val_acc: 0.6514\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9562 - acc: 0.5964 - val_loss: 0.9014 - val_acc: 0.6629\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9458 - acc: 0.5888 - val_loss: 0.8973 - val_acc: 0.6571\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.9349 - acc: 0.6041 - val_loss: 0.9015 - val_acc: 0.6686\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9323 - acc: 0.6111 - val_loss: 0.8933 - val_acc: 0.6743\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9263 - acc: 0.6213 - val_loss: 0.8918 - val_acc: 0.6514\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9179 - acc: 0.6156 - val_loss: 0.8939 - val_acc: 0.6686\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8984 - acc: 0.6347 - val_loss: 0.8904 - val_acc: 0.6743\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.9166 - acc: 0.6226 - val_loss: 0.8934 - val_acc: 0.6686\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9015 - acc: 0.6201 - val_loss: 0.8852 - val_acc: 0.6629\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8982 - acc: 0.6347 - val_loss: 0.8887 - val_acc: 0.6686\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8978 - acc: 0.6258 - val_loss: 0.8871 - val_acc: 0.6514\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8953 - acc: 0.6245 - val_loss: 0.8945 - val_acc: 0.6629\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8787 - acc: 0.6258 - val_loss: 0.8878 - val_acc: 0.6571\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8525 - acc: 0.6552 - val_loss: 0.8886 - val_acc: 0.6514\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8608 - acc: 0.6513 - val_loss: 0.8864 - val_acc: 0.6686\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8697 - acc: 0.6488 - val_loss: 0.8883 - val_acc: 0.6514\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8615 - acc: 0.6507 - val_loss: 0.8900 - val_acc: 0.6571\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8420 - acc: 0.6539 - val_loss: 0.8873 - val_acc: 0.6629\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8543 - acc: 0.6379 - val_loss: 0.8877 - val_acc: 0.6514\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8422 - acc: 0.6513 - val_loss: 0.8886 - val_acc: 0.6571\n",
      "Epoch 35/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8436 - acc: 0.6405 - val_loss: 0.8925 - val_acc: 0.6457\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8326 - acc: 0.6424 - val_loss: 0.8876 - val_acc: 0.6457\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8404 - acc: 0.6609 - val_loss: 0.8863 - val_acc: 0.6343\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8284 - acc: 0.6590 - val_loss: 0.8889 - val_acc: 0.6457\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8245 - acc: 0.6724 - val_loss: 0.8876 - val_acc: 0.6457\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8082 - acc: 0.6762 - val_loss: 0.8897 - val_acc: 0.6457\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8136 - acc: 0.6692 - val_loss: 0.8866 - val_acc: 0.6629\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8061 - acc: 0.6699 - val_loss: 0.8947 - val_acc: 0.6400\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8144 - acc: 0.6660 - val_loss: 0.8936 - val_acc: 0.6514\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8181 - acc: 0.6762 - val_loss: 0.8938 - val_acc: 0.6514\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7997 - acc: 0.6679 - val_loss: 0.8949 - val_acc: 0.6571\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7996 - acc: 0.6897 - val_loss: 0.9042 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7891 - acc: 0.6814 - val_loss: 0.9005 - val_acc: 0.6286\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7833 - acc: 0.6801 - val_loss: 0.9013 - val_acc: 0.6400\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8185 - acc: 0.6648 - val_loss: 0.9031 - val_acc: 0.6457\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8138 - acc: 0.6769 - val_loss: 0.9032 - val_acc: 0.6400\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7942 - acc: 0.6731 - val_loss: 0.9029 - val_acc: 0.6514\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8039 - acc: 0.6839 - val_loss: 0.8974 - val_acc: 0.6571\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7769 - acc: 0.6916 - val_loss: 0.9001 - val_acc: 0.6457\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7789 - acc: 0.6845 - val_loss: 0.8990 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7743 - acc: 0.6871 - val_loss: 0.9005 - val_acc: 0.6457\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7648 - acc: 0.6935 - val_loss: 0.9052 - val_acc: 0.6400\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7829 - acc: 0.6890 - val_loss: 0.9089 - val_acc: 0.6571\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7743 - acc: 0.6871 - val_loss: 0.9057 - val_acc: 0.6457\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.7800 - acc: 0.6833 - val_loss: 0.9023 - val_acc: 0.6629\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7626 - acc: 0.6954 - val_loss: 0.9167 - val_acc: 0.6514\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7679 - acc: 0.6897 - val_loss: 0.9101 - val_acc: 0.6571\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7822 - acc: 0.6884 - val_loss: 0.9149 - val_acc: 0.6571\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7560 - acc: 0.6986 - val_loss: 0.9131 - val_acc: 0.6457\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7313 - acc: 0.7095 - val_loss: 0.9158 - val_acc: 0.6343\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7429 - acc: 0.7005 - val_loss: 0.9194 - val_acc: 0.6343\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7485 - acc: 0.6935 - val_loss: 0.9179 - val_acc: 0.6457\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7561 - acc: 0.6820 - val_loss: 0.9150 - val_acc: 0.6400\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7442 - acc: 0.6909 - val_loss: 0.9173 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7478 - acc: 0.7011 - val_loss: 0.9162 - val_acc: 0.6343\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7274 - acc: 0.7101 - val_loss: 0.9205 - val_acc: 0.6343\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7281 - acc: 0.7126 - val_loss: 0.9238 - val_acc: 0.6457\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7473 - acc: 0.6948 - val_loss: 0.9239 - val_acc: 0.6400\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7370 - acc: 0.7005 - val_loss: 0.9196 - val_acc: 0.6343\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7244 - acc: 0.7216 - val_loss: 0.9292 - val_acc: 0.6400\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7079 - acc: 0.7158 - val_loss: 0.9346 - val_acc: 0.6400\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7274 - acc: 0.6960 - val_loss: 0.9392 - val_acc: 0.6286\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7382 - acc: 0.7018 - val_loss: 0.9411 - val_acc: 0.6171\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7242 - acc: 0.7095 - val_loss: 0.9384 - val_acc: 0.6343\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7180 - acc: 0.7126 - val_loss: 0.9411 - val_acc: 0.6343\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7092 - acc: 0.7101 - val_loss: 0.9315 - val_acc: 0.6286\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7039 - acc: 0.7190 - val_loss: 0.9301 - val_acc: 0.6514\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7006 - acc: 0.7184 - val_loss: 0.9276 - val_acc: 0.6514\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6849 - acc: 0.7305 - val_loss: 0.9313 - val_acc: 0.6400\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6905 - acc: 0.7273 - val_loss: 0.9335 - val_acc: 0.6514\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7020 - acc: 0.7267 - val_loss: 0.9456 - val_acc: 0.6400\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7010 - acc: 0.7254 - val_loss: 0.9425 - val_acc: 0.6400\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7017 - acc: 0.7222 - val_loss: 0.9379 - val_acc: 0.6400\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6943 - acc: 0.7273 - val_loss: 0.9442 - val_acc: 0.6457\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7008 - acc: 0.7075 - val_loss: 0.9401 - val_acc: 0.6400\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6771 - acc: 0.7407 - val_loss: 0.9396 - val_acc: 0.6571\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6874 - acc: 0.7273 - val_loss: 0.9430 - val_acc: 0.6457\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6927 - acc: 0.7324 - val_loss: 0.9508 - val_acc: 0.6286\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6894 - acc: 0.7273 - val_loss: 0.9555 - val_acc: 0.6400\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6759 - acc: 0.7261 - val_loss: 0.9480 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6808 - acc: 0.7273 - val_loss: 0.9478 - val_acc: 0.6457\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6645 - acc: 0.7286 - val_loss: 0.9587 - val_acc: 0.6400\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6645 - acc: 0.7427 - val_loss: 0.9525 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6638 - acc: 0.7375 - val_loss: 0.9477 - val_acc: 0.6457\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6702 - acc: 0.7356 - val_loss: 0.9498 - val_acc: 0.6571\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6600 - acc: 0.7427 - val_loss: 0.9568 - val_acc: 0.6400\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6579 - acc: 0.7433 - val_loss: 0.9621 - val_acc: 0.6400\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.6743 - acc: 0.7318 - val_loss: 0.9569 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6474 - acc: 0.7471 - val_loss: 0.9648 - val_acc: 0.6400\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6628 - acc: 0.7401 - val_loss: 0.9595 - val_acc: 0.6343\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6712 - acc: 0.7401 - val_loss: 0.9641 - val_acc: 0.6514\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6519 - acc: 0.7312 - val_loss: 0.9676 - val_acc: 0.6343\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6398 - acc: 0.7465 - val_loss: 0.9698 - val_acc: 0.6343\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6447 - acc: 0.7439 - val_loss: 0.9627 - val_acc: 0.6514\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6491 - acc: 0.7427 - val_loss: 0.9604 - val_acc: 0.6457\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6448 - acc: 0.7420 - val_loss: 0.9709 - val_acc: 0.6343\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6426 - acc: 0.7452 - val_loss: 0.9733 - val_acc: 0.6229\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6418 - acc: 0.7458 - val_loss: 0.9753 - val_acc: 0.6457\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6535 - acc: 0.7395 - val_loss: 0.9755 - val_acc: 0.6400\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6607 - acc: 0.7363 - val_loss: 0.9696 - val_acc: 0.6514\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6171 - acc: 0.7708 - val_loss: 0.9688 - val_acc: 0.6514\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6388 - acc: 0.7452 - val_loss: 0.9764 - val_acc: 0.6457\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6494 - acc: 0.7369 - val_loss: 0.9754 - val_acc: 0.6457\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6439 - acc: 0.7369 - val_loss: 0.9819 - val_acc: 0.6400\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6342 - acc: 0.7542 - val_loss: 0.9871 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6225 - acc: 0.7580 - val_loss: 0.9981 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6399 - acc: 0.7605 - val_loss: 0.9902 - val_acc: 0.6286\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6040 - acc: 0.7669 - val_loss: 0.9873 - val_acc: 0.6400\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6091 - acc: 0.7676 - val_loss: 0.9930 - val_acc: 0.6343\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6243 - acc: 0.7478 - val_loss: 0.9939 - val_acc: 0.6343\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6095 - acc: 0.7593 - val_loss: 1.0007 - val_acc: 0.6400\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6234 - acc: 0.7503 - val_loss: 0.9996 - val_acc: 0.6400\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6033 - acc: 0.7510 - val_loss: 0.9955 - val_acc: 0.6343\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6042 - acc: 0.7593 - val_loss: 1.0024 - val_acc: 0.6171\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6022 - acc: 0.7612 - val_loss: 0.9989 - val_acc: 0.6229\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6097 - acc: 0.7503 - val_loss: 1.0014 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6273 - acc: 0.7548 - val_loss: 1.0065 - val_acc: 0.6400\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6298 - acc: 0.7484 - val_loss: 1.0040 - val_acc: 0.6229\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6073 - acc: 0.7612 - val_loss: 1.0068 - val_acc: 0.6400\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6247 - acc: 0.7446 - val_loss: 1.0030 - val_acc: 0.6343\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6025 - acc: 0.7650 - val_loss: 1.0012 - val_acc: 0.6457\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5994 - acc: 0.7663 - val_loss: 1.0015 - val_acc: 0.6457\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5793 - acc: 0.7631 - val_loss: 1.0097 - val_acc: 0.6286\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5765 - acc: 0.7778 - val_loss: 1.0124 - val_acc: 0.6343\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6060 - acc: 0.7471 - val_loss: 1.0114 - val_acc: 0.6286\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5988 - acc: 0.7656 - val_loss: 1.0218 - val_acc: 0.6343\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5813 - acc: 0.7778 - val_loss: 1.0209 - val_acc: 0.6343\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5710 - acc: 0.7784 - val_loss: 1.0199 - val_acc: 0.6343\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5925 - acc: 0.7695 - val_loss: 1.0137 - val_acc: 0.6514\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5624 - acc: 0.7791 - val_loss: 1.0149 - val_acc: 0.6400\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.5690 - acc: 0.7778 - val_loss: 1.0299 - val_acc: 0.6286\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5684 - acc: 0.7746 - val_loss: 1.0279 - val_acc: 0.6286\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5852 - acc: 0.7733 - val_loss: 1.0315 - val_acc: 0.6286\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5628 - acc: 0.7854 - val_loss: 1.0307 - val_acc: 0.6457\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5543 - acc: 0.7899 - val_loss: 1.0227 - val_acc: 0.6286\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5597 - acc: 0.7771 - val_loss: 1.0377 - val_acc: 0.6286\n",
      "194/194 [==============================] - 0s 70us/step\n",
      "1741/1741 [==============================] - 0s 52us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.6910 - acc: 0.2835 - val_loss: 1.3153 - val_acc: 0.3314\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.3378 - acc: 0.3889 - val_loss: 1.1299 - val_acc: 0.5314\n",
      "Epoch 3/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.2461 - acc: 0.4393 - val_loss: 1.0689 - val_acc: 0.6114\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.1562 - acc: 0.4962 - val_loss: 1.0303 - val_acc: 0.6000\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.1201 - acc: 0.5115 - val_loss: 0.9969 - val_acc: 0.6229\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.1187 - acc: 0.5147 - val_loss: 0.9896 - val_acc: 0.6171\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0846 - acc: 0.5249 - val_loss: 0.9773 - val_acc: 0.5829\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.0566 - acc: 0.5287 - val_loss: 0.9635 - val_acc: 0.5829\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0366 - acc: 0.5568 - val_loss: 0.9535 - val_acc: 0.6114\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.0199 - acc: 0.5683 - val_loss: 0.9548 - val_acc: 0.6171\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0170 - acc: 0.5690 - val_loss: 0.9368 - val_acc: 0.6286\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0097 - acc: 0.5664 - val_loss: 0.9333 - val_acc: 0.6343\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.9958 - acc: 0.5792 - val_loss: 0.9278 - val_acc: 0.6571\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9746 - acc: 0.5971 - val_loss: 0.9216 - val_acc: 0.6629\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9641 - acc: 0.5888 - val_loss: 0.9143 - val_acc: 0.6514\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9769 - acc: 0.5888 - val_loss: 0.9133 - val_acc: 0.6514\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9447 - acc: 0.6022 - val_loss: 0.9156 - val_acc: 0.6400\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9495 - acc: 0.6028 - val_loss: 0.9089 - val_acc: 0.6457\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9559 - acc: 0.6015 - val_loss: 0.9080 - val_acc: 0.6514\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9274 - acc: 0.6130 - val_loss: 0.8991 - val_acc: 0.6457\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9249 - acc: 0.6156 - val_loss: 0.8997 - val_acc: 0.6514\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9291 - acc: 0.6015 - val_loss: 0.9025 - val_acc: 0.6629\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8939 - acc: 0.6277 - val_loss: 0.8996 - val_acc: 0.6686\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9115 - acc: 0.6092 - val_loss: 0.8953 - val_acc: 0.6629\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8819 - acc: 0.6284 - val_loss: 0.9020 - val_acc: 0.6457\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8886 - acc: 0.6284 - val_loss: 0.9009 - val_acc: 0.6800\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8919 - acc: 0.6226 - val_loss: 0.8944 - val_acc: 0.6514\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8680 - acc: 0.6443 - val_loss: 0.8941 - val_acc: 0.6571\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8651 - acc: 0.6296 - val_loss: 0.8994 - val_acc: 0.6571\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.8964 - acc: 0.6201 - val_loss: 0.8964 - val_acc: 0.6514\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8951 - acc: 0.6194 - val_loss: 0.9057 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8680 - acc: 0.6290 - val_loss: 0.9013 - val_acc: 0.6514\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8639 - acc: 0.6424 - val_loss: 0.8989 - val_acc: 0.6400\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8566 - acc: 0.6501 - val_loss: 0.9024 - val_acc: 0.6457\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8576 - acc: 0.6552 - val_loss: 0.8923 - val_acc: 0.6514\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8647 - acc: 0.6341 - val_loss: 0.8930 - val_acc: 0.6457\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8361 - acc: 0.6430 - val_loss: 0.9003 - val_acc: 0.6571\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8443 - acc: 0.6545 - val_loss: 0.8954 - val_acc: 0.6457\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8407 - acc: 0.6450 - val_loss: 0.8961 - val_acc: 0.6571\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8352 - acc: 0.6558 - val_loss: 0.8996 - val_acc: 0.6457\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8307 - acc: 0.6520 - val_loss: 0.8993 - val_acc: 0.6629\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8154 - acc: 0.6622 - val_loss: 0.9057 - val_acc: 0.6514\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8205 - acc: 0.6635 - val_loss: 0.9006 - val_acc: 0.6514\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8249 - acc: 0.6616 - val_loss: 0.9072 - val_acc: 0.6571\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8278 - acc: 0.6731 - val_loss: 0.9044 - val_acc: 0.6743\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8073 - acc: 0.6667 - val_loss: 0.9014 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8129 - acc: 0.6584 - val_loss: 0.9057 - val_acc: 0.6400\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7956 - acc: 0.6737 - val_loss: 0.9175 - val_acc: 0.6343\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8080 - acc: 0.6718 - val_loss: 0.9119 - val_acc: 0.6343\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7969 - acc: 0.6635 - val_loss: 0.9113 - val_acc: 0.6286\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7883 - acc: 0.6845 - val_loss: 0.9122 - val_acc: 0.6400\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8031 - acc: 0.6794 - val_loss: 0.9096 - val_acc: 0.6457\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8089 - acc: 0.6756 - val_loss: 0.9200 - val_acc: 0.6400\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7749 - acc: 0.6820 - val_loss: 0.9094 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7938 - acc: 0.6782 - val_loss: 0.9166 - val_acc: 0.6400\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7988 - acc: 0.6660 - val_loss: 0.9165 - val_acc: 0.6457\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7965 - acc: 0.6865 - val_loss: 0.9147 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7800 - acc: 0.6750 - val_loss: 0.9215 - val_acc: 0.6343\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7962 - acc: 0.6679 - val_loss: 0.9146 - val_acc: 0.6629\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7724 - acc: 0.6871 - val_loss: 0.9267 - val_acc: 0.6457\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7707 - acc: 0.6743 - val_loss: 0.9174 - val_acc: 0.6514\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7714 - acc: 0.6775 - val_loss: 0.9227 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7919 - acc: 0.6731 - val_loss: 0.9227 - val_acc: 0.6457\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7662 - acc: 0.6699 - val_loss: 0.9232 - val_acc: 0.6229\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7650 - acc: 0.6871 - val_loss: 0.9204 - val_acc: 0.6229\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7420 - acc: 0.6992 - val_loss: 0.9214 - val_acc: 0.6286\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7527 - acc: 0.6833 - val_loss: 0.9204 - val_acc: 0.6514\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7477 - acc: 0.6839 - val_loss: 0.9259 - val_acc: 0.6229\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7487 - acc: 0.6814 - val_loss: 0.9303 - val_acc: 0.6514\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7440 - acc: 0.6833 - val_loss: 0.9347 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7577 - acc: 0.6833 - val_loss: 0.9355 - val_acc: 0.6171\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7324 - acc: 0.6967 - val_loss: 0.9395 - val_acc: 0.6400\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7375 - acc: 0.7152 - val_loss: 0.9346 - val_acc: 0.6229\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7565 - acc: 0.6941 - val_loss: 0.9362 - val_acc: 0.6457\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7221 - acc: 0.7133 - val_loss: 0.9289 - val_acc: 0.6514\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7310 - acc: 0.7069 - val_loss: 0.9357 - val_acc: 0.6343\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7234 - acc: 0.6999 - val_loss: 0.9371 - val_acc: 0.6514\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7272 - acc: 0.6967 - val_loss: 0.9375 - val_acc: 0.6400\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7141 - acc: 0.7031 - val_loss: 0.9479 - val_acc: 0.6343\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7171 - acc: 0.7178 - val_loss: 0.9449 - val_acc: 0.6343\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7365 - acc: 0.6967 - val_loss: 0.9474 - val_acc: 0.6171\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7116 - acc: 0.7056 - val_loss: 0.9452 - val_acc: 0.6571\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7240 - acc: 0.6967 - val_loss: 0.9511 - val_acc: 0.6400\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7254 - acc: 0.7050 - val_loss: 0.9508 - val_acc: 0.6286\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7109 - acc: 0.7063 - val_loss: 0.9517 - val_acc: 0.6343\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6951 - acc: 0.7088 - val_loss: 0.9556 - val_acc: 0.6343\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7061 - acc: 0.7043 - val_loss: 0.9576 - val_acc: 0.6343\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7120 - acc: 0.7216 - val_loss: 0.9631 - val_acc: 0.6171\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7041 - acc: 0.7120 - val_loss: 0.9682 - val_acc: 0.6171\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7065 - acc: 0.7063 - val_loss: 0.9656 - val_acc: 0.6286\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6886 - acc: 0.7139 - val_loss: 0.9590 - val_acc: 0.6286\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7095 - acc: 0.7241 - val_loss: 0.9643 - val_acc: 0.6400\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7072 - acc: 0.7037 - val_loss: 0.9570 - val_acc: 0.6343\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6894 - acc: 0.7107 - val_loss: 0.9629 - val_acc: 0.6514\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6911 - acc: 0.7146 - val_loss: 0.9676 - val_acc: 0.6400\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6963 - acc: 0.7158 - val_loss: 0.9641 - val_acc: 0.6343\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6813 - acc: 0.7356 - val_loss: 0.9668 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6880 - acc: 0.7126 - val_loss: 0.9638 - val_acc: 0.6400\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6709 - acc: 0.7280 - val_loss: 0.9691 - val_acc: 0.6457\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6719 - acc: 0.7190 - val_loss: 0.9690 - val_acc: 0.6400\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6755 - acc: 0.7171 - val_loss: 0.9699 - val_acc: 0.6400\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6700 - acc: 0.7254 - val_loss: 0.9780 - val_acc: 0.6286\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6872 - acc: 0.7235 - val_loss: 0.9758 - val_acc: 0.6400\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6821 - acc: 0.7197 - val_loss: 0.9759 - val_acc: 0.6286\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6608 - acc: 0.7286 - val_loss: 0.9834 - val_acc: 0.6343\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6639 - acc: 0.7420 - val_loss: 0.9977 - val_acc: 0.6286\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6659 - acc: 0.7248 - val_loss: 0.9838 - val_acc: 0.6400\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6719 - acc: 0.7216 - val_loss: 0.9942 - val_acc: 0.6229\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6680 - acc: 0.7292 - val_loss: 0.9931 - val_acc: 0.6514\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6405 - acc: 0.7516 - val_loss: 0.9924 - val_acc: 0.6400\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6681 - acc: 0.7305 - val_loss: 0.9977 - val_acc: 0.6286\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6728 - acc: 0.7331 - val_loss: 0.9964 - val_acc: 0.6229\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6687 - acc: 0.7280 - val_loss: 0.9922 - val_acc: 0.6457\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6574 - acc: 0.7369 - val_loss: 0.9981 - val_acc: 0.6343\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.6518 - acc: 0.7369 - val_loss: 0.9962 - val_acc: 0.6229\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6514 - acc: 0.7382 - val_loss: 0.9986 - val_acc: 0.6286\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6461 - acc: 0.7350 - val_loss: 0.9998 - val_acc: 0.6286\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6430 - acc: 0.7337 - val_loss: 0.9988 - val_acc: 0.6286\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6294 - acc: 0.7503 - val_loss: 0.9955 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6299 - acc: 0.7567 - val_loss: 1.0012 - val_acc: 0.6400\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6206 - acc: 0.7471 - val_loss: 1.0125 - val_acc: 0.6229\n",
      "Epoch 122/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6219 - acc: 0.7503 - val_loss: 1.0136 - val_acc: 0.6114\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6351 - acc: 0.7420 - val_loss: 1.0199 - val_acc: 0.6343\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6146 - acc: 0.7580 - val_loss: 1.0264 - val_acc: 0.6286\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6133 - acc: 0.7593 - val_loss: 1.0262 - val_acc: 0.6114\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6395 - acc: 0.7337 - val_loss: 1.0251 - val_acc: 0.6171\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6061 - acc: 0.7618 - val_loss: 1.0332 - val_acc: 0.6171\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6364 - acc: 0.7465 - val_loss: 1.0346 - val_acc: 0.6229\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6200 - acc: 0.7516 - val_loss: 1.0358 - val_acc: 0.6229\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6243 - acc: 0.7465 - val_loss: 1.0333 - val_acc: 0.6229\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6159 - acc: 0.7522 - val_loss: 1.0393 - val_acc: 0.6286\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6149 - acc: 0.7567 - val_loss: 1.0439 - val_acc: 0.6114\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6417 - acc: 0.7433 - val_loss: 1.0302 - val_acc: 0.6286\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5903 - acc: 0.7701 - val_loss: 1.0374 - val_acc: 0.6457\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6030 - acc: 0.7548 - val_loss: 1.0513 - val_acc: 0.6229\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6102 - acc: 0.7605 - val_loss: 1.0416 - val_acc: 0.6286\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6045 - acc: 0.7599 - val_loss: 1.0394 - val_acc: 0.6343\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6169 - acc: 0.7510 - val_loss: 1.0476 - val_acc: 0.6229\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6134 - acc: 0.7497 - val_loss: 1.0474 - val_acc: 0.6286\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6114 - acc: 0.7522 - val_loss: 1.0501 - val_acc: 0.6400\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6098 - acc: 0.7573 - val_loss: 1.0576 - val_acc: 0.6286\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5832 - acc: 0.7656 - val_loss: 1.0630 - val_acc: 0.6343\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5955 - acc: 0.7522 - val_loss: 1.0647 - val_acc: 0.6057\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5845 - acc: 0.7599 - val_loss: 1.0645 - val_acc: 0.6229\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5908 - acc: 0.7593 - val_loss: 1.0653 - val_acc: 0.6286\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5932 - acc: 0.7580 - val_loss: 1.0658 - val_acc: 0.6171\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5893 - acc: 0.7739 - val_loss: 1.0753 - val_acc: 0.6229\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5931 - acc: 0.7708 - val_loss: 1.0809 - val_acc: 0.6229\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5938 - acc: 0.7739 - val_loss: 1.0857 - val_acc: 0.6286\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5842 - acc: 0.7739 - val_loss: 1.0886 - val_acc: 0.6286\n",
      "194/194 [==============================] - 0s 72us/step\n",
      "1741/1741 [==============================] - 0s 51us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.5041 - acc: 0.2995 - val_loss: 1.2100 - val_acc: 0.4743\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.3031 - acc: 0.4087 - val_loss: 1.1106 - val_acc: 0.5600\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.2343 - acc: 0.4457 - val_loss: 1.0636 - val_acc: 0.5886\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.1792 - acc: 0.4764 - val_loss: 1.0243 - val_acc: 0.6171\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.1376 - acc: 0.5019 - val_loss: 1.0043 - val_acc: 0.5886\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 1.1094 - acc: 0.4968 - val_loss: 0.9831 - val_acc: 0.5886\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.0979 - acc: 0.5243 - val_loss: 0.9683 - val_acc: 0.6114\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 1.0789 - acc: 0.5262 - val_loss: 0.9578 - val_acc: 0.6057\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0420 - acc: 0.5434 - val_loss: 0.9468 - val_acc: 0.6229\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.0544 - acc: 0.5402 - val_loss: 0.9369 - val_acc: 0.6457\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0213 - acc: 0.5594 - val_loss: 0.9278 - val_acc: 0.6343\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0234 - acc: 0.5549 - val_loss: 0.9255 - val_acc: 0.6514\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9916 - acc: 0.5843 - val_loss: 0.9213 - val_acc: 0.6400\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0026 - acc: 0.5811 - val_loss: 0.9235 - val_acc: 0.6457\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9833 - acc: 0.5983 - val_loss: 0.9185 - val_acc: 0.6457\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9739 - acc: 0.5945 - val_loss: 0.9185 - val_acc: 0.6514\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.9787 - acc: 0.5805 - val_loss: 0.9095 - val_acc: 0.6629\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9679 - acc: 0.5913 - val_loss: 0.9079 - val_acc: 0.6629\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9457 - acc: 0.5990 - val_loss: 0.9049 - val_acc: 0.6686\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9497 - acc: 0.5964 - val_loss: 0.9030 - val_acc: 0.6686\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9480 - acc: 0.5958 - val_loss: 0.9049 - val_acc: 0.6571\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8990 - acc: 0.6296 - val_loss: 0.9072 - val_acc: 0.6629\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9126 - acc: 0.6194 - val_loss: 0.9065 - val_acc: 0.6571\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9103 - acc: 0.6156 - val_loss: 0.9032 - val_acc: 0.6571\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8979 - acc: 0.6271 - val_loss: 0.8945 - val_acc: 0.6571\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.9284 - acc: 0.6130 - val_loss: 0.8934 - val_acc: 0.6686\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9016 - acc: 0.6105 - val_loss: 0.8960 - val_acc: 0.6686\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9078 - acc: 0.6149 - val_loss: 0.8945 - val_acc: 0.6514\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8835 - acc: 0.6277 - val_loss: 0.8929 - val_acc: 0.6571\n",
      "Epoch 30/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8657 - acc: 0.6481 - val_loss: 0.8968 - val_acc: 0.6571\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8893 - acc: 0.6220 - val_loss: 0.8941 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8784 - acc: 0.6296 - val_loss: 0.8923 - val_acc: 0.6629\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8874 - acc: 0.6347 - val_loss: 0.8964 - val_acc: 0.6571\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8704 - acc: 0.6405 - val_loss: 0.8914 - val_acc: 0.6743\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8726 - acc: 0.6367 - val_loss: 0.8896 - val_acc: 0.6629\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8565 - acc: 0.6398 - val_loss: 0.8936 - val_acc: 0.6571\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8689 - acc: 0.6443 - val_loss: 0.8966 - val_acc: 0.6629\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8557 - acc: 0.6501 - val_loss: 0.9004 - val_acc: 0.6686\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8420 - acc: 0.6533 - val_loss: 0.8921 - val_acc: 0.6743\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8575 - acc: 0.6552 - val_loss: 0.8914 - val_acc: 0.6800\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8422 - acc: 0.6539 - val_loss: 0.8941 - val_acc: 0.6686\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8518 - acc: 0.6462 - val_loss: 0.8892 - val_acc: 0.6857\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8398 - acc: 0.6564 - val_loss: 0.8934 - val_acc: 0.6800\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8454 - acc: 0.6507 - val_loss: 0.8896 - val_acc: 0.6686\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8431 - acc: 0.6456 - val_loss: 0.8916 - val_acc: 0.6629\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8310 - acc: 0.6609 - val_loss: 0.8916 - val_acc: 0.6686\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8491 - acc: 0.6488 - val_loss: 0.8944 - val_acc: 0.6686\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8358 - acc: 0.6481 - val_loss: 0.8917 - val_acc: 0.6629\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8170 - acc: 0.6616 - val_loss: 0.8957 - val_acc: 0.6686\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8139 - acc: 0.6590 - val_loss: 0.8929 - val_acc: 0.6686\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8042 - acc: 0.6622 - val_loss: 0.8993 - val_acc: 0.6629\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8431 - acc: 0.6539 - val_loss: 0.9037 - val_acc: 0.6571\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7874 - acc: 0.6635 - val_loss: 0.9009 - val_acc: 0.6686\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8025 - acc: 0.6897 - val_loss: 0.9039 - val_acc: 0.6629\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7942 - acc: 0.6686 - val_loss: 0.9077 - val_acc: 0.6686\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7941 - acc: 0.6711 - val_loss: 0.9036 - val_acc: 0.6800\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7919 - acc: 0.6750 - val_loss: 0.9074 - val_acc: 0.6743\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7854 - acc: 0.6750 - val_loss: 0.9106 - val_acc: 0.6629\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7908 - acc: 0.6750 - val_loss: 0.9157 - val_acc: 0.6629\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7772 - acc: 0.6973 - val_loss: 0.9127 - val_acc: 0.6686\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7740 - acc: 0.6794 - val_loss: 0.9140 - val_acc: 0.6686\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7873 - acc: 0.6845 - val_loss: 0.9103 - val_acc: 0.6514\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7836 - acc: 0.6782 - val_loss: 0.9163 - val_acc: 0.6629\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7634 - acc: 0.6916 - val_loss: 0.9138 - val_acc: 0.6686\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7701 - acc: 0.6871 - val_loss: 0.9175 - val_acc: 0.6514\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7919 - acc: 0.6762 - val_loss: 0.9205 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7670 - acc: 0.6756 - val_loss: 0.9223 - val_acc: 0.6629\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7764 - acc: 0.6782 - val_loss: 0.9230 - val_acc: 0.6457\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7528 - acc: 0.7050 - val_loss: 0.9177 - val_acc: 0.6457\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7562 - acc: 0.6794 - val_loss: 0.9242 - val_acc: 0.6571\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7717 - acc: 0.6845 - val_loss: 0.9276 - val_acc: 0.6743\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7592 - acc: 0.6890 - val_loss: 0.9290 - val_acc: 0.6514\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7550 - acc: 0.7043 - val_loss: 0.9245 - val_acc: 0.6400\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7659 - acc: 0.6890 - val_loss: 0.9282 - val_acc: 0.6286\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.7470 - acc: 0.7011 - val_loss: 0.9348 - val_acc: 0.6343\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7673 - acc: 0.6897 - val_loss: 0.9396 - val_acc: 0.6286\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7523 - acc: 0.6833 - val_loss: 0.9375 - val_acc: 0.6229\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7339 - acc: 0.7043 - val_loss: 0.9470 - val_acc: 0.6343\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7411 - acc: 0.7133 - val_loss: 0.9424 - val_acc: 0.6343\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7431 - acc: 0.6992 - val_loss: 0.9427 - val_acc: 0.6343\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7368 - acc: 0.7063 - val_loss: 0.9426 - val_acc: 0.6400\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7410 - acc: 0.6941 - val_loss: 0.9391 - val_acc: 0.6400\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7253 - acc: 0.7063 - val_loss: 0.9460 - val_acc: 0.6457\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7139 - acc: 0.7241 - val_loss: 0.9463 - val_acc: 0.6457\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.7344 - acc: 0.7069 - val_loss: 0.9514 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 116us/step - loss: 0.7417 - acc: 0.7037 - val_loss: 0.9484 - val_acc: 0.6571\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7356 - acc: 0.7011 - val_loss: 0.9470 - val_acc: 0.6400\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7160 - acc: 0.7139 - val_loss: 0.9480 - val_acc: 0.6571\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7252 - acc: 0.7011 - val_loss: 0.9452 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7142 - acc: 0.7126 - val_loss: 0.9507 - val_acc: 0.6286\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7319 - acc: 0.6980 - val_loss: 0.9500 - val_acc: 0.6343\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7150 - acc: 0.7184 - val_loss: 0.9564 - val_acc: 0.6457\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6914 - acc: 0.7209 - val_loss: 0.9565 - val_acc: 0.6343\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7049 - acc: 0.7178 - val_loss: 0.9596 - val_acc: 0.6400\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7037 - acc: 0.7126 - val_loss: 0.9617 - val_acc: 0.6457\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7025 - acc: 0.7165 - val_loss: 0.9576 - val_acc: 0.6514\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7038 - acc: 0.7165 - val_loss: 0.9674 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7118 - acc: 0.7152 - val_loss: 0.9677 - val_acc: 0.6229\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6953 - acc: 0.7146 - val_loss: 0.9638 - val_acc: 0.6514\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6865 - acc: 0.7312 - val_loss: 0.9667 - val_acc: 0.6400\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6656 - acc: 0.7350 - val_loss: 0.9683 - val_acc: 0.6286\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6744 - acc: 0.7248 - val_loss: 0.9722 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6838 - acc: 0.7235 - val_loss: 0.9711 - val_acc: 0.6343\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6919 - acc: 0.7216 - val_loss: 0.9789 - val_acc: 0.6171\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7071 - acc: 0.7056 - val_loss: 0.9787 - val_acc: 0.6286\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7021 - acc: 0.7082 - val_loss: 0.9794 - val_acc: 0.6400\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6875 - acc: 0.7158 - val_loss: 0.9786 - val_acc: 0.6400\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6880 - acc: 0.7158 - val_loss: 0.9785 - val_acc: 0.6343\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6907 - acc: 0.7229 - val_loss: 0.9745 - val_acc: 0.6286\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6599 - acc: 0.7312 - val_loss: 0.9845 - val_acc: 0.6343\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6895 - acc: 0.7203 - val_loss: 0.9831 - val_acc: 0.6229\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6806 - acc: 0.7395 - val_loss: 0.9876 - val_acc: 0.6343\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6741 - acc: 0.7152 - val_loss: 0.9869 - val_acc: 0.6343\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6586 - acc: 0.7369 - val_loss: 0.9890 - val_acc: 0.6457\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.6845 - acc: 0.7369 - val_loss: 0.9915 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6706 - acc: 0.7152 - val_loss: 1.0059 - val_acc: 0.6343\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6609 - acc: 0.7414 - val_loss: 1.0033 - val_acc: 0.6457\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6656 - acc: 0.7337 - val_loss: 1.0029 - val_acc: 0.6400\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6675 - acc: 0.7318 - val_loss: 0.9911 - val_acc: 0.6514\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6554 - acc: 0.7356 - val_loss: 0.9991 - val_acc: 0.6400\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6687 - acc: 0.7209 - val_loss: 1.0098 - val_acc: 0.6400\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6294 - acc: 0.7324 - val_loss: 1.0063 - val_acc: 0.6286\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6413 - acc: 0.7356 - val_loss: 1.0039 - val_acc: 0.6343\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6590 - acc: 0.7439 - val_loss: 1.0052 - val_acc: 0.6343\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6369 - acc: 0.7471 - val_loss: 1.0114 - val_acc: 0.6400\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6578 - acc: 0.7324 - val_loss: 1.0154 - val_acc: 0.6229\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6293 - acc: 0.7522 - val_loss: 1.0163 - val_acc: 0.6286\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6316 - acc: 0.7497 - val_loss: 1.0153 - val_acc: 0.6343\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6455 - acc: 0.7369 - val_loss: 1.0206 - val_acc: 0.6343\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6612 - acc: 0.7458 - val_loss: 1.0297 - val_acc: 0.6400\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6426 - acc: 0.7458 - val_loss: 1.0351 - val_acc: 0.6171\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6264 - acc: 0.7510 - val_loss: 1.0413 - val_acc: 0.6057\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6430 - acc: 0.7503 - val_loss: 1.0372 - val_acc: 0.6343\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6376 - acc: 0.7414 - val_loss: 1.0379 - val_acc: 0.6286\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6184 - acc: 0.7484 - val_loss: 1.0416 - val_acc: 0.6286\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6226 - acc: 0.7465 - val_loss: 1.0317 - val_acc: 0.6114\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6332 - acc: 0.7503 - val_loss: 1.0406 - val_acc: 0.6229\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6138 - acc: 0.7529 - val_loss: 1.0430 - val_acc: 0.6286\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.6201 - acc: 0.7465 - val_loss: 1.0477 - val_acc: 0.6057\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6147 - acc: 0.7465 - val_loss: 1.0501 - val_acc: 0.6286\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6116 - acc: 0.7522 - val_loss: 1.0422 - val_acc: 0.6171\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6077 - acc: 0.7516 - val_loss: 1.0496 - val_acc: 0.6286\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6098 - acc: 0.7586 - val_loss: 1.0464 - val_acc: 0.6171\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6148 - acc: 0.7605 - val_loss: 1.0553 - val_acc: 0.6171\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5960 - acc: 0.7650 - val_loss: 1.0549 - val_acc: 0.6114\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6031 - acc: 0.7663 - val_loss: 1.0576 - val_acc: 0.6343\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6032 - acc: 0.7688 - val_loss: 1.0500 - val_acc: 0.6229\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.6120 - acc: 0.7573 - val_loss: 1.0560 - val_acc: 0.6400\n",
      "Epoch 149/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5899 - acc: 0.7656 - val_loss: 1.0644 - val_acc: 0.6057\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5879 - acc: 0.7669 - val_loss: 1.0686 - val_acc: 0.6114\n",
      "194/194 [==============================] - 0s 83us/step\n",
      "1741/1741 [==============================] - 0s 53us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.4397 - acc: 0.3161 - val_loss: 1.1758 - val_acc: 0.5543\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 1.2707 - acc: 0.4195 - val_loss: 1.0821 - val_acc: 0.6057\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.2058 - acc: 0.4464 - val_loss: 1.0322 - val_acc: 0.6457\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 1.1595 - acc: 0.4764 - val_loss: 0.9980 - val_acc: 0.6343\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 1.1028 - acc: 0.5102 - val_loss: 0.9735 - val_acc: 0.6571\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 1.0839 - acc: 0.5307 - val_loss: 0.9579 - val_acc: 0.6514\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.0857 - acc: 0.5153 - val_loss: 0.9433 - val_acc: 0.6686\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0567 - acc: 0.5370 - val_loss: 0.9347 - val_acc: 0.6457\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.0183 - acc: 0.5683 - val_loss: 0.9246 - val_acc: 0.6971\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 1.0336 - acc: 0.5651 - val_loss: 0.9173 - val_acc: 0.6857\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 168us/step - loss: 1.0097 - acc: 0.5766 - val_loss: 0.9149 - val_acc: 0.6629\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 113us/step - loss: 0.9801 - acc: 0.5830 - val_loss: 0.9084 - val_acc: 0.6686\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 1.0096 - acc: 0.5632 - val_loss: 0.9059 - val_acc: 0.6800\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9657 - acc: 0.5939 - val_loss: 0.8988 - val_acc: 0.6743\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9627 - acc: 0.6003 - val_loss: 0.9047 - val_acc: 0.6800\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9525 - acc: 0.6009 - val_loss: 0.8989 - val_acc: 0.6571\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9510 - acc: 0.6015 - val_loss: 0.8961 - val_acc: 0.6914\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9316 - acc: 0.6028 - val_loss: 0.8890 - val_acc: 0.6571\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9327 - acc: 0.6105 - val_loss: 0.8886 - val_acc: 0.6686\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9251 - acc: 0.6009 - val_loss: 0.8882 - val_acc: 0.6743\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.9103 - acc: 0.6137 - val_loss: 0.8854 - val_acc: 0.6743\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.9245 - acc: 0.6092 - val_loss: 0.8851 - val_acc: 0.6800\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9120 - acc: 0.6258 - val_loss: 0.8802 - val_acc: 0.6800\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9077 - acc: 0.6124 - val_loss: 0.8801 - val_acc: 0.6686\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9256 - acc: 0.5996 - val_loss: 0.8854 - val_acc: 0.6857\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8978 - acc: 0.6315 - val_loss: 0.8819 - val_acc: 0.6800\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8953 - acc: 0.6290 - val_loss: 0.8775 - val_acc: 0.6743\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 114us/step - loss: 0.8784 - acc: 0.6315 - val_loss: 0.8807 - val_acc: 0.6743\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.8710 - acc: 0.6437 - val_loss: 0.8785 - val_acc: 0.6800\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8646 - acc: 0.6367 - val_loss: 0.8767 - val_acc: 0.6743\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8837 - acc: 0.6335 - val_loss: 0.8775 - val_acc: 0.6800\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 127us/step - loss: 0.8762 - acc: 0.6456 - val_loss: 0.8735 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 129us/step - loss: 0.8687 - acc: 0.6456 - val_loss: 0.8748 - val_acc: 0.6629\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 114us/step - loss: 0.8590 - acc: 0.6392 - val_loss: 0.8788 - val_acc: 0.6800\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.8745 - acc: 0.6481 - val_loss: 0.8801 - val_acc: 0.6800\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8360 - acc: 0.6577 - val_loss: 0.8856 - val_acc: 0.6800\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.8477 - acc: 0.6731 - val_loss: 0.8852 - val_acc: 0.6914\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8344 - acc: 0.6679 - val_loss: 0.8784 - val_acc: 0.6857\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8310 - acc: 0.6660 - val_loss: 0.8899 - val_acc: 0.6857\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.8474 - acc: 0.6430 - val_loss: 0.8824 - val_acc: 0.6857\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8220 - acc: 0.6711 - val_loss: 0.8857 - val_acc: 0.6857\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8207 - acc: 0.6743 - val_loss: 0.8840 - val_acc: 0.6914\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8335 - acc: 0.6513 - val_loss: 0.8900 - val_acc: 0.6800\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8151 - acc: 0.6654 - val_loss: 0.8861 - val_acc: 0.6686\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8219 - acc: 0.6596 - val_loss: 0.8890 - val_acc: 0.6857\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8168 - acc: 0.6577 - val_loss: 0.8861 - val_acc: 0.6857\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7929 - acc: 0.6699 - val_loss: 0.8870 - val_acc: 0.6914\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8134 - acc: 0.6660 - val_loss: 0.8872 - val_acc: 0.6857\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8094 - acc: 0.6667 - val_loss: 0.8867 - val_acc: 0.6629\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.8063 - acc: 0.6718 - val_loss: 0.8953 - val_acc: 0.6686\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7917 - acc: 0.6801 - val_loss: 0.8963 - val_acc: 0.6686\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7997 - acc: 0.6654 - val_loss: 0.8954 - val_acc: 0.6800\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7938 - acc: 0.6788 - val_loss: 0.8876 - val_acc: 0.6743\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7938 - acc: 0.6807 - val_loss: 0.8916 - val_acc: 0.6743\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7876 - acc: 0.6794 - val_loss: 0.8993 - val_acc: 0.6743\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7878 - acc: 0.6794 - val_loss: 0.8967 - val_acc: 0.6800\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7625 - acc: 0.6960 - val_loss: 0.8927 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7761 - acc: 0.6897 - val_loss: 0.8950 - val_acc: 0.6743\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7781 - acc: 0.6807 - val_loss: 0.8945 - val_acc: 0.6800\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7905 - acc: 0.6794 - val_loss: 0.8914 - val_acc: 0.6743\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8067 - acc: 0.6686 - val_loss: 0.8970 - val_acc: 0.6629\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7723 - acc: 0.6871 - val_loss: 0.8970 - val_acc: 0.6629\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7717 - acc: 0.6903 - val_loss: 0.9030 - val_acc: 0.6686\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7606 - acc: 0.6724 - val_loss: 0.8964 - val_acc: 0.6571\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7826 - acc: 0.6737 - val_loss: 0.9010 - val_acc: 0.6571\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7647 - acc: 0.6877 - val_loss: 0.9121 - val_acc: 0.6686\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7772 - acc: 0.6877 - val_loss: 0.9034 - val_acc: 0.6571\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.7629 - acc: 0.6775 - val_loss: 0.9044 - val_acc: 0.6571\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7629 - acc: 0.6833 - val_loss: 0.9026 - val_acc: 0.6686\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7677 - acc: 0.6903 - val_loss: 0.9076 - val_acc: 0.6686\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7503 - acc: 0.6992 - val_loss: 0.9126 - val_acc: 0.6686\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7564 - acc: 0.6916 - val_loss: 0.9058 - val_acc: 0.6629\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7585 - acc: 0.6941 - val_loss: 0.9056 - val_acc: 0.6629\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7628 - acc: 0.7018 - val_loss: 0.9088 - val_acc: 0.6571\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7227 - acc: 0.7114 - val_loss: 0.9097 - val_acc: 0.6629\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7197 - acc: 0.7082 - val_loss: 0.9066 - val_acc: 0.6686\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7278 - acc: 0.7024 - val_loss: 0.9200 - val_acc: 0.6571\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7593 - acc: 0.6724 - val_loss: 0.9177 - val_acc: 0.6686\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7192 - acc: 0.7011 - val_loss: 0.9164 - val_acc: 0.6686\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7477 - acc: 0.6897 - val_loss: 0.9239 - val_acc: 0.6571\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7302 - acc: 0.6973 - val_loss: 0.9191 - val_acc: 0.6629\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7130 - acc: 0.7190 - val_loss: 0.9249 - val_acc: 0.6514\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7258 - acc: 0.7203 - val_loss: 0.9173 - val_acc: 0.6514\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7253 - acc: 0.7069 - val_loss: 0.9163 - val_acc: 0.6743\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7104 - acc: 0.7101 - val_loss: 0.9253 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7189 - acc: 0.7101 - val_loss: 0.9294 - val_acc: 0.6514\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7215 - acc: 0.7095 - val_loss: 0.9268 - val_acc: 0.6629\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7036 - acc: 0.7043 - val_loss: 0.9276 - val_acc: 0.6514\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6913 - acc: 0.7184 - val_loss: 0.9370 - val_acc: 0.6457\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7006 - acc: 0.7190 - val_loss: 0.9363 - val_acc: 0.6400\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7070 - acc: 0.7050 - val_loss: 0.9361 - val_acc: 0.6514\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6964 - acc: 0.7197 - val_loss: 0.9323 - val_acc: 0.6686\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7072 - acc: 0.7063 - val_loss: 0.9365 - val_acc: 0.6571\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6988 - acc: 0.7158 - val_loss: 0.9498 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6653 - acc: 0.7375 - val_loss: 0.9381 - val_acc: 0.6457\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6825 - acc: 0.7273 - val_loss: 0.9446 - val_acc: 0.6571\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6953 - acc: 0.7165 - val_loss: 0.9438 - val_acc: 0.6457\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6839 - acc: 0.7318 - val_loss: 0.9473 - val_acc: 0.6514\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6831 - acc: 0.7222 - val_loss: 0.9364 - val_acc: 0.6629\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7004 - acc: 0.7190 - val_loss: 0.9458 - val_acc: 0.6571\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6810 - acc: 0.7222 - val_loss: 0.9586 - val_acc: 0.6571\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.6737 - acc: 0.7222 - val_loss: 0.9464 - val_acc: 0.6686\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.6746 - acc: 0.7350 - val_loss: 0.9510 - val_acc: 0.6514\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.6844 - acc: 0.7209 - val_loss: 0.9479 - val_acc: 0.6571\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6669 - acc: 0.7286 - val_loss: 0.9580 - val_acc: 0.6400\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.6740 - acc: 0.7126 - val_loss: 0.9468 - val_acc: 0.6686\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6791 - acc: 0.7229 - val_loss: 0.9608 - val_acc: 0.6571\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6590 - acc: 0.7382 - val_loss: 0.9614 - val_acc: 0.6629\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6851 - acc: 0.7146 - val_loss: 0.9573 - val_acc: 0.6400\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6617 - acc: 0.7241 - val_loss: 0.9709 - val_acc: 0.6514\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.6738 - acc: 0.7197 - val_loss: 0.9730 - val_acc: 0.6514\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6725 - acc: 0.7280 - val_loss: 0.9691 - val_acc: 0.6514\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.6490 - acc: 0.7478 - val_loss: 0.9670 - val_acc: 0.6629\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6478 - acc: 0.7458 - val_loss: 0.9717 - val_acc: 0.6514\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6562 - acc: 0.7241 - val_loss: 0.9742 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6567 - acc: 0.7356 - val_loss: 0.9784 - val_acc: 0.6514\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.6454 - acc: 0.7465 - val_loss: 0.9814 - val_acc: 0.6514\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6372 - acc: 0.7465 - val_loss: 0.9815 - val_acc: 0.6400\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.6287 - acc: 0.7427 - val_loss: 0.9749 - val_acc: 0.6514\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.6442 - acc: 0.7420 - val_loss: 0.9810 - val_acc: 0.6400\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.6437 - acc: 0.7471 - val_loss: 0.9751 - val_acc: 0.6343\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6440 - acc: 0.7388 - val_loss: 0.9804 - val_acc: 0.6457\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6351 - acc: 0.7286 - val_loss: 0.9861 - val_acc: 0.6457\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6387 - acc: 0.7414 - val_loss: 0.9929 - val_acc: 0.6457\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6446 - acc: 0.7350 - val_loss: 0.9897 - val_acc: 0.6629\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6399 - acc: 0.7292 - val_loss: 0.9898 - val_acc: 0.6400\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.6319 - acc: 0.7433 - val_loss: 0.9898 - val_acc: 0.6514\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6337 - acc: 0.7420 - val_loss: 0.9881 - val_acc: 0.6514\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6288 - acc: 0.7433 - val_loss: 0.9873 - val_acc: 0.6457\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6302 - acc: 0.7356 - val_loss: 0.9955 - val_acc: 0.6400\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6145 - acc: 0.7529 - val_loss: 1.0024 - val_acc: 0.6343\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6212 - acc: 0.7554 - val_loss: 0.9996 - val_acc: 0.6343\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6115 - acc: 0.7625 - val_loss: 0.9935 - val_acc: 0.6400\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6175 - acc: 0.7542 - val_loss: 0.9930 - val_acc: 0.6457\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6017 - acc: 0.7516 - val_loss: 0.9999 - val_acc: 0.6457\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6297 - acc: 0.7401 - val_loss: 1.0032 - val_acc: 0.6457\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6057 - acc: 0.7599 - val_loss: 0.9993 - val_acc: 0.6400\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6232 - acc: 0.7529 - val_loss: 1.0040 - val_acc: 0.6400\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6109 - acc: 0.7542 - val_loss: 1.0033 - val_acc: 0.6514\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6213 - acc: 0.7548 - val_loss: 0.9978 - val_acc: 0.6457\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5956 - acc: 0.7427 - val_loss: 1.0027 - val_acc: 0.6514\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6092 - acc: 0.7548 - val_loss: 1.0191 - val_acc: 0.6400\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5979 - acc: 0.7625 - val_loss: 1.0198 - val_acc: 0.6400\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6070 - acc: 0.7561 - val_loss: 1.0142 - val_acc: 0.6571\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5952 - acc: 0.7637 - val_loss: 1.0138 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5771 - acc: 0.7682 - val_loss: 1.0220 - val_acc: 0.6400\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5776 - acc: 0.7656 - val_loss: 1.0183 - val_acc: 0.6457\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5903 - acc: 0.7637 - val_loss: 1.0159 - val_acc: 0.6400\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5737 - acc: 0.7746 - val_loss: 1.0192 - val_acc: 0.6457\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5814 - acc: 0.7669 - val_loss: 1.0201 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 68us/step\n",
      "1741/1741 [==============================] - 0s 49us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.5812 - acc: 0.3076 - val_loss: 1.2660 - val_acc: 0.4800\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 1.3158 - acc: 0.3752 - val_loss: 1.1098 - val_acc: 0.5771\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 1.1771 - acc: 0.4729 - val_loss: 1.0422 - val_acc: 0.6229\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 1.1276 - acc: 0.4888 - val_loss: 1.0057 - val_acc: 0.6457\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.1012 - acc: 0.5233 - val_loss: 0.9857 - val_acc: 0.6343\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.0751 - acc: 0.5348 - val_loss: 0.9674 - val_acc: 0.6514\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 1.0611 - acc: 0.5475 - val_loss: 0.9555 - val_acc: 0.6514\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 1.0383 - acc: 0.5463 - val_loss: 0.9452 - val_acc: 0.6571\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 1.0383 - acc: 0.5482 - val_loss: 0.9334 - val_acc: 0.6457\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 1.0258 - acc: 0.5654 - val_loss: 0.9287 - val_acc: 0.6514\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 1.0185 - acc: 0.5737 - val_loss: 0.9241 - val_acc: 0.6743\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9918 - acc: 0.5763 - val_loss: 0.9160 - val_acc: 0.6686\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.9821 - acc: 0.5769 - val_loss: 0.9134 - val_acc: 0.6686\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.9662 - acc: 0.5814 - val_loss: 0.9103 - val_acc: 0.6914\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.9653 - acc: 0.5986 - val_loss: 0.9073 - val_acc: 0.6800\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9645 - acc: 0.5935 - val_loss: 0.9042 - val_acc: 0.6971\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9433 - acc: 0.6011 - val_loss: 0.9055 - val_acc: 0.6971\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.9406 - acc: 0.6011 - val_loss: 0.9054 - val_acc: 0.6914\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9329 - acc: 0.6075 - val_loss: 0.8987 - val_acc: 0.6971\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.9380 - acc: 0.5973 - val_loss: 0.9003 - val_acc: 0.6914\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9244 - acc: 0.6063 - val_loss: 0.9013 - val_acc: 0.6800\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9006 - acc: 0.6197 - val_loss: 0.8953 - val_acc: 0.6857\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9061 - acc: 0.6190 - val_loss: 0.8918 - val_acc: 0.6971\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9145 - acc: 0.6203 - val_loss: 0.8938 - val_acc: 0.6914\n",
      "Epoch 25/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9231 - acc: 0.6133 - val_loss: 0.8977 - val_acc: 0.6914\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9042 - acc: 0.6184 - val_loss: 0.8984 - val_acc: 0.6857\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8894 - acc: 0.6292 - val_loss: 0.9011 - val_acc: 0.6914\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8776 - acc: 0.6299 - val_loss: 0.8942 - val_acc: 0.6857\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8913 - acc: 0.6216 - val_loss: 0.8897 - val_acc: 0.6914\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9042 - acc: 0.6241 - val_loss: 0.8941 - val_acc: 0.6857\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8757 - acc: 0.6305 - val_loss: 0.8934 - val_acc: 0.6857\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8931 - acc: 0.6388 - val_loss: 0.8963 - val_acc: 0.6743\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8534 - acc: 0.6439 - val_loss: 0.8958 - val_acc: 0.6857\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8749 - acc: 0.6496 - val_loss: 0.8973 - val_acc: 0.6914\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8578 - acc: 0.6471 - val_loss: 0.8930 - val_acc: 0.7029\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8593 - acc: 0.6414 - val_loss: 0.8981 - val_acc: 0.6914\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.8488 - acc: 0.6522 - val_loss: 0.8973 - val_acc: 0.6971\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8454 - acc: 0.6605 - val_loss: 0.8931 - val_acc: 0.6914\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.8492 - acc: 0.6490 - val_loss: 0.9009 - val_acc: 0.6743\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8460 - acc: 0.6516 - val_loss: 0.8982 - val_acc: 0.6914\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8466 - acc: 0.6414 - val_loss: 0.8961 - val_acc: 0.6914\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.8186 - acc: 0.6707 - val_loss: 0.8959 - val_acc: 0.6857\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8400 - acc: 0.6586 - val_loss: 0.9044 - val_acc: 0.6914\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8243 - acc: 0.6579 - val_loss: 0.9042 - val_acc: 0.6914\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8170 - acc: 0.6656 - val_loss: 0.9055 - val_acc: 0.6971\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8163 - acc: 0.6579 - val_loss: 0.9042 - val_acc: 0.6857\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8336 - acc: 0.6611 - val_loss: 0.9059 - val_acc: 0.6914\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8077 - acc: 0.6701 - val_loss: 0.9041 - val_acc: 0.6914\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.8046 - acc: 0.6879 - val_loss: 0.9087 - val_acc: 0.6857\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.8102 - acc: 0.6611 - val_loss: 0.9072 - val_acc: 0.6914\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8098 - acc: 0.6637 - val_loss: 0.9012 - val_acc: 0.7029\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8185 - acc: 0.6637 - val_loss: 0.9046 - val_acc: 0.6914\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8155 - acc: 0.6701 - val_loss: 0.9058 - val_acc: 0.6971\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7970 - acc: 0.6682 - val_loss: 0.9036 - val_acc: 0.6971\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.7766 - acc: 0.6879 - val_loss: 0.9056 - val_acc: 0.6857\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7890 - acc: 0.6765 - val_loss: 0.9159 - val_acc: 0.6743\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7996 - acc: 0.6694 - val_loss: 0.9133 - val_acc: 0.6743\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7946 - acc: 0.6720 - val_loss: 0.9106 - val_acc: 0.6914\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7748 - acc: 0.6969 - val_loss: 0.9192 - val_acc: 0.6743\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7771 - acc: 0.6771 - val_loss: 0.9199 - val_acc: 0.7029\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.7668 - acc: 0.6752 - val_loss: 0.9212 - val_acc: 0.6800\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7667 - acc: 0.6886 - val_loss: 0.9182 - val_acc: 0.6800\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7663 - acc: 0.6790 - val_loss: 0.9157 - val_acc: 0.6743\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 113us/step - loss: 0.7712 - acc: 0.6892 - val_loss: 0.9246 - val_acc: 0.6914\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7731 - acc: 0.6892 - val_loss: 0.9317 - val_acc: 0.6857\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7530 - acc: 0.6969 - val_loss: 0.9333 - val_acc: 0.6743\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7565 - acc: 0.6841 - val_loss: 0.9261 - val_acc: 0.6971\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7598 - acc: 0.6809 - val_loss: 0.9355 - val_acc: 0.6800\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7722 - acc: 0.6745 - val_loss: 0.9415 - val_acc: 0.6800\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7515 - acc: 0.6879 - val_loss: 0.9367 - val_acc: 0.6857\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7404 - acc: 0.7052 - val_loss: 0.9357 - val_acc: 0.6743\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7504 - acc: 0.6847 - val_loss: 0.9384 - val_acc: 0.6857\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.7391 - acc: 0.7039 - val_loss: 0.9429 - val_acc: 0.6914\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7494 - acc: 0.6988 - val_loss: 0.9431 - val_acc: 0.6857\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7276 - acc: 0.6937 - val_loss: 0.9436 - val_acc: 0.6629\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.7359 - acc: 0.6969 - val_loss: 0.9497 - val_acc: 0.6743\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7238 - acc: 0.7033 - val_loss: 0.9485 - val_acc: 0.6857\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7456 - acc: 0.6930 - val_loss: 0.9545 - val_acc: 0.6800\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7346 - acc: 0.6969 - val_loss: 0.9566 - val_acc: 0.6857\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7301 - acc: 0.7013 - val_loss: 0.9520 - val_acc: 0.6743\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7299 - acc: 0.7084 - val_loss: 0.9497 - val_acc: 0.6914\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7225 - acc: 0.6975 - val_loss: 0.9564 - val_acc: 0.6914\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7160 - acc: 0.7045 - val_loss: 0.9584 - val_acc: 0.6800\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7070 - acc: 0.7179 - val_loss: 0.9569 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7052 - acc: 0.7128 - val_loss: 0.9549 - val_acc: 0.6914\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6952 - acc: 0.7103 - val_loss: 0.9552 - val_acc: 0.6971\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.7106 - acc: 0.7154 - val_loss: 0.9634 - val_acc: 0.6800\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7199 - acc: 0.7045 - val_loss: 0.9625 - val_acc: 0.6857\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6940 - acc: 0.7262 - val_loss: 0.9582 - val_acc: 0.6800\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7077 - acc: 0.7116 - val_loss: 0.9641 - val_acc: 0.6800\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7190 - acc: 0.7013 - val_loss: 0.9674 - val_acc: 0.6800\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7082 - acc: 0.7230 - val_loss: 0.9644 - val_acc: 0.6800\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6805 - acc: 0.7167 - val_loss: 0.9646 - val_acc: 0.6571\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.6854 - acc: 0.7160 - val_loss: 0.9626 - val_acc: 0.6629\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6912 - acc: 0.7269 - val_loss: 0.9723 - val_acc: 0.6686\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7076 - acc: 0.7160 - val_loss: 0.9646 - val_acc: 0.6629\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7009 - acc: 0.7224 - val_loss: 0.9686 - val_acc: 0.6629\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6849 - acc: 0.7230 - val_loss: 0.9650 - val_acc: 0.6571\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6936 - acc: 0.7269 - val_loss: 0.9755 - val_acc: 0.6686\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6824 - acc: 0.7262 - val_loss: 0.9780 - val_acc: 0.6686\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6854 - acc: 0.7250 - val_loss: 0.9690 - val_acc: 0.6686\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6650 - acc: 0.7320 - val_loss: 0.9783 - val_acc: 0.6514\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6703 - acc: 0.7243 - val_loss: 0.9732 - val_acc: 0.6571\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6802 - acc: 0.7237 - val_loss: 0.9844 - val_acc: 0.6629\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6796 - acc: 0.7345 - val_loss: 0.9890 - val_acc: 0.6571\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6623 - acc: 0.7320 - val_loss: 0.9864 - val_acc: 0.6629\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6746 - acc: 0.7205 - val_loss: 1.0007 - val_acc: 0.6514\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.6718 - acc: 0.7294 - val_loss: 0.9928 - val_acc: 0.6686\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6787 - acc: 0.7198 - val_loss: 0.9954 - val_acc: 0.6743\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.6564 - acc: 0.7371 - val_loss: 0.9972 - val_acc: 0.6571\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6676 - acc: 0.7307 - val_loss: 0.9933 - val_acc: 0.6686\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6513 - acc: 0.7396 - val_loss: 0.9905 - val_acc: 0.6571\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6512 - acc: 0.7428 - val_loss: 0.9965 - val_acc: 0.6686\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6510 - acc: 0.7313 - val_loss: 0.9970 - val_acc: 0.6743\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6507 - acc: 0.7288 - val_loss: 1.0104 - val_acc: 0.6743\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6454 - acc: 0.7473 - val_loss: 1.0136 - val_acc: 0.6571\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6453 - acc: 0.7428 - val_loss: 1.0166 - val_acc: 0.6457\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6425 - acc: 0.7479 - val_loss: 1.0111 - val_acc: 0.6571\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6405 - acc: 0.7492 - val_loss: 1.0195 - val_acc: 0.6629\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6504 - acc: 0.7390 - val_loss: 1.0122 - val_acc: 0.6514\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6203 - acc: 0.7492 - val_loss: 1.0246 - val_acc: 0.6686\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.6243 - acc: 0.7498 - val_loss: 1.0183 - val_acc: 0.6571\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.6371 - acc: 0.7358 - val_loss: 1.0223 - val_acc: 0.6457\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6287 - acc: 0.7403 - val_loss: 1.0235 - val_acc: 0.6514\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6234 - acc: 0.7384 - val_loss: 1.0363 - val_acc: 0.6514\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6168 - acc: 0.7575 - val_loss: 1.0322 - val_acc: 0.6457\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6189 - acc: 0.7537 - val_loss: 1.0366 - val_acc: 0.6514\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6206 - acc: 0.7498 - val_loss: 1.0354 - val_acc: 0.6457\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6271 - acc: 0.7556 - val_loss: 1.0303 - val_acc: 0.6571\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5997 - acc: 0.7588 - val_loss: 1.0335 - val_acc: 0.6629\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6298 - acc: 0.7460 - val_loss: 1.0367 - val_acc: 0.6457\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6016 - acc: 0.7581 - val_loss: 1.0486 - val_acc: 0.6457\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6306 - acc: 0.7422 - val_loss: 1.0643 - val_acc: 0.6514\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6130 - acc: 0.7594 - val_loss: 1.0627 - val_acc: 0.6571\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5992 - acc: 0.7524 - val_loss: 1.0567 - val_acc: 0.6400\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6221 - acc: 0.7428 - val_loss: 1.0583 - val_acc: 0.6514\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6169 - acc: 0.7422 - val_loss: 1.0590 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6095 - acc: 0.7607 - val_loss: 1.0545 - val_acc: 0.6400\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.5919 - acc: 0.7645 - val_loss: 1.0668 - val_acc: 0.6514\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5947 - acc: 0.7671 - val_loss: 1.0660 - val_acc: 0.6400\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6018 - acc: 0.7613 - val_loss: 1.0711 - val_acc: 0.6343\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6011 - acc: 0.7594 - val_loss: 1.0791 - val_acc: 0.6457\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5943 - acc: 0.7479 - val_loss: 1.0702 - val_acc: 0.6457\n",
      "Epoch 144/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6065 - acc: 0.7671 - val_loss: 1.0720 - val_acc: 0.6457\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5863 - acc: 0.7601 - val_loss: 1.0789 - val_acc: 0.6514\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6020 - acc: 0.7683 - val_loss: 1.0750 - val_acc: 0.6343\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5758 - acc: 0.7658 - val_loss: 1.0767 - val_acc: 0.6514\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5957 - acc: 0.7581 - val_loss: 1.0744 - val_acc: 0.6457\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5685 - acc: 0.7862 - val_loss: 1.0920 - val_acc: 0.6400\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5777 - acc: 0.7722 - val_loss: 1.0810 - val_acc: 0.6400\n",
      "193/193 [==============================] - 0s 71us/step\n",
      "1742/1742 [==============================] - 0s 54us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.4497 - acc: 0.3318 - val_loss: 1.1888 - val_acc: 0.4971\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 1.2577 - acc: 0.4148 - val_loss: 1.0950 - val_acc: 0.5600\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 1.1953 - acc: 0.4620 - val_loss: 1.0365 - val_acc: 0.6000\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 1.1548 - acc: 0.4939 - val_loss: 1.0012 - val_acc: 0.6229\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 1.1166 - acc: 0.5124 - val_loss: 0.9706 - val_acc: 0.6400\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0720 - acc: 0.5386 - val_loss: 0.9594 - val_acc: 0.6457\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.0662 - acc: 0.5424 - val_loss: 0.9398 - val_acc: 0.6571\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.0352 - acc: 0.5533 - val_loss: 0.9334 - val_acc: 0.6686\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 1.0125 - acc: 0.5609 - val_loss: 0.9235 - val_acc: 0.6686\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.9863 - acc: 0.5929 - val_loss: 0.9164 - val_acc: 0.6686\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.9825 - acc: 0.5884 - val_loss: 0.9046 - val_acc: 0.6571\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9894 - acc: 0.5724 - val_loss: 0.9021 - val_acc: 0.6571\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.9813 - acc: 0.5890 - val_loss: 0.8898 - val_acc: 0.6686\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.9414 - acc: 0.6158 - val_loss: 0.8924 - val_acc: 0.6571\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.9338 - acc: 0.6152 - val_loss: 0.8833 - val_acc: 0.6571\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.9208 - acc: 0.6158 - val_loss: 0.8886 - val_acc: 0.6514\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.9363 - acc: 0.6075 - val_loss: 0.8834 - val_acc: 0.6686\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.9234 - acc: 0.6088 - val_loss: 0.8841 - val_acc: 0.6571\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.9014 - acc: 0.6273 - val_loss: 0.8871 - val_acc: 0.6571\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.9214 - acc: 0.6203 - val_loss: 0.8788 - val_acc: 0.6743\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.9151 - acc: 0.6050 - val_loss: 0.8750 - val_acc: 0.6686\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8963 - acc: 0.6254 - val_loss: 0.8737 - val_acc: 0.6743\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.8854 - acc: 0.6292 - val_loss: 0.8760 - val_acc: 0.6686\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8955 - acc: 0.6216 - val_loss: 0.8719 - val_acc: 0.6629\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8798 - acc: 0.6292 - val_loss: 0.8692 - val_acc: 0.6800\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8707 - acc: 0.6369 - val_loss: 0.8723 - val_acc: 0.6743\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8827 - acc: 0.6490 - val_loss: 0.8632 - val_acc: 0.6686\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8610 - acc: 0.6471 - val_loss: 0.8653 - val_acc: 0.6686\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8644 - acc: 0.6420 - val_loss: 0.8616 - val_acc: 0.6800\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8578 - acc: 0.6477 - val_loss: 0.8660 - val_acc: 0.6743\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8801 - acc: 0.6369 - val_loss: 0.8625 - val_acc: 0.6743\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8633 - acc: 0.6388 - val_loss: 0.8587 - val_acc: 0.6914\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8384 - acc: 0.6586 - val_loss: 0.8611 - val_acc: 0.6800\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.8444 - acc: 0.6592 - val_loss: 0.8590 - val_acc: 0.6743\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8511 - acc: 0.6401 - val_loss: 0.8612 - val_acc: 0.6629\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8279 - acc: 0.6662 - val_loss: 0.8618 - val_acc: 0.6743\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8320 - acc: 0.6643 - val_loss: 0.8595 - val_acc: 0.6629\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8425 - acc: 0.6535 - val_loss: 0.8629 - val_acc: 0.6629\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8251 - acc: 0.6854 - val_loss: 0.8715 - val_acc: 0.6571\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7969 - acc: 0.6752 - val_loss: 0.8739 - val_acc: 0.6686\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 122us/step - loss: 0.8188 - acc: 0.6739 - val_loss: 0.8682 - val_acc: 0.6629\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.8215 - acc: 0.6599 - val_loss: 0.8671 - val_acc: 0.6571\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8183 - acc: 0.6528 - val_loss: 0.8601 - val_acc: 0.6629\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8231 - acc: 0.6656 - val_loss: 0.8600 - val_acc: 0.6629\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.8245 - acc: 0.6650 - val_loss: 0.8637 - val_acc: 0.6629\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7978 - acc: 0.6707 - val_loss: 0.8654 - val_acc: 0.6686\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8009 - acc: 0.6701 - val_loss: 0.8629 - val_acc: 0.6743\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7892 - acc: 0.6720 - val_loss: 0.8688 - val_acc: 0.6571\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.7783 - acc: 0.6873 - val_loss: 0.8665 - val_acc: 0.6629\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8138 - acc: 0.6637 - val_loss: 0.8698 - val_acc: 0.6571\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7911 - acc: 0.6816 - val_loss: 0.8678 - val_acc: 0.6686\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 109us/step - loss: 0.7736 - acc: 0.6943 - val_loss: 0.8734 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.7782 - acc: 0.6847 - val_loss: 0.8717 - val_acc: 0.6571\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7806 - acc: 0.6816 - val_loss: 0.8677 - val_acc: 0.6686\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7611 - acc: 0.6784 - val_loss: 0.8682 - val_acc: 0.6629\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7590 - acc: 0.6943 - val_loss: 0.8603 - val_acc: 0.6629\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7611 - acc: 0.6860 - val_loss: 0.8660 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.7827 - acc: 0.6777 - val_loss: 0.8690 - val_acc: 0.6629\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.7558 - acc: 0.6867 - val_loss: 0.8735 - val_acc: 0.6743\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.7522 - acc: 0.6962 - val_loss: 0.8634 - val_acc: 0.6686\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7660 - acc: 0.6879 - val_loss: 0.8595 - val_acc: 0.6743\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7576 - acc: 0.6956 - val_loss: 0.8600 - val_acc: 0.6743\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7471 - acc: 0.6905 - val_loss: 0.8639 - val_acc: 0.6571\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7500 - acc: 0.7064 - val_loss: 0.8696 - val_acc: 0.6629\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7403 - acc: 0.7147 - val_loss: 0.8787 - val_acc: 0.6514\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7541 - acc: 0.6905 - val_loss: 0.8756 - val_acc: 0.6743\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.7441 - acc: 0.7039 - val_loss: 0.8766 - val_acc: 0.6686\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7393 - acc: 0.6988 - val_loss: 0.8729 - val_acc: 0.6629\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7448 - acc: 0.6950 - val_loss: 0.8729 - val_acc: 0.6514\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7285 - acc: 0.7020 - val_loss: 0.8756 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7288 - acc: 0.7052 - val_loss: 0.8724 - val_acc: 0.6457\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.7433 - acc: 0.7064 - val_loss: 0.8774 - val_acc: 0.6686\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.7141 - acc: 0.7084 - val_loss: 0.8791 - val_acc: 0.6686\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.7115 - acc: 0.7090 - val_loss: 0.8858 - val_acc: 0.6514\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.7446 - acc: 0.7058 - val_loss: 0.8824 - val_acc: 0.6514\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.7317 - acc: 0.7064 - val_loss: 0.8870 - val_acc: 0.6457\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 126us/step - loss: 0.7152 - acc: 0.7096 - val_loss: 0.8839 - val_acc: 0.6457\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 125us/step - loss: 0.7286 - acc: 0.6994 - val_loss: 0.8885 - val_acc: 0.6400\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7243 - acc: 0.7198 - val_loss: 0.8848 - val_acc: 0.6457\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 110us/step - loss: 0.6924 - acc: 0.7045 - val_loss: 0.8786 - val_acc: 0.6571\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7000 - acc: 0.7116 - val_loss: 0.8889 - val_acc: 0.6343\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7239 - acc: 0.7103 - val_loss: 0.8834 - val_acc: 0.6514\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7177 - acc: 0.7154 - val_loss: 0.8874 - val_acc: 0.6400\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.7133 - acc: 0.7077 - val_loss: 0.8870 - val_acc: 0.6514\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.6901 - acc: 0.7147 - val_loss: 0.8882 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 107us/step - loss: 0.6900 - acc: 0.7090 - val_loss: 0.8899 - val_acc: 0.6286\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.6839 - acc: 0.7147 - val_loss: 0.8908 - val_acc: 0.6514\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.7018 - acc: 0.7218 - val_loss: 0.8978 - val_acc: 0.6343\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.6804 - acc: 0.7167 - val_loss: 0.8932 - val_acc: 0.6343\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6840 - acc: 0.7301 - val_loss: 0.8880 - val_acc: 0.6400\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6708 - acc: 0.7237 - val_loss: 0.8989 - val_acc: 0.6286\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6958 - acc: 0.7173 - val_loss: 0.8943 - val_acc: 0.6457\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 107us/step - loss: 0.6810 - acc: 0.7205 - val_loss: 0.8939 - val_acc: 0.6343\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6837 - acc: 0.7205 - val_loss: 0.8998 - val_acc: 0.6571\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6806 - acc: 0.7275 - val_loss: 0.8946 - val_acc: 0.6514\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.6657 - acc: 0.7428 - val_loss: 0.9033 - val_acc: 0.6400\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 112us/step - loss: 0.6855 - acc: 0.7262 - val_loss: 0.9033 - val_acc: 0.6457\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6739 - acc: 0.7224 - val_loss: 0.9094 - val_acc: 0.6343\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6610 - acc: 0.7269 - val_loss: 0.9115 - val_acc: 0.6400\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6504 - acc: 0.7390 - val_loss: 0.9169 - val_acc: 0.6286\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6727 - acc: 0.7243 - val_loss: 0.9084 - val_acc: 0.6457\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6697 - acc: 0.7243 - val_loss: 0.9072 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6635 - acc: 0.7390 - val_loss: 0.9108 - val_acc: 0.6400\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.6564 - acc: 0.7307 - val_loss: 0.9192 - val_acc: 0.6286\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6578 - acc: 0.7256 - val_loss: 0.9129 - val_acc: 0.6400\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6464 - acc: 0.7460 - val_loss: 0.9058 - val_acc: 0.6343\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.6625 - acc: 0.7288 - val_loss: 0.9071 - val_acc: 0.6457\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.6665 - acc: 0.7339 - val_loss: 0.9095 - val_acc: 0.6457\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.6602 - acc: 0.7326 - val_loss: 0.9082 - val_acc: 0.6286\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6633 - acc: 0.7339 - val_loss: 0.9022 - val_acc: 0.6514\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6486 - acc: 0.7384 - val_loss: 0.9051 - val_acc: 0.6286\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6225 - acc: 0.7486 - val_loss: 0.9045 - val_acc: 0.6629\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6365 - acc: 0.7281 - val_loss: 0.9144 - val_acc: 0.6229\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6418 - acc: 0.7364 - val_loss: 0.9071 - val_acc: 0.6400\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6336 - acc: 0.7377 - val_loss: 0.9173 - val_acc: 0.6457\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6566 - acc: 0.7326 - val_loss: 0.9210 - val_acc: 0.6400\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6416 - acc: 0.7435 - val_loss: 0.9197 - val_acc: 0.6400\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6345 - acc: 0.7415 - val_loss: 0.9195 - val_acc: 0.6457\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6330 - acc: 0.7384 - val_loss: 0.9226 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6439 - acc: 0.7428 - val_loss: 0.9237 - val_acc: 0.6629\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6421 - acc: 0.7256 - val_loss: 0.9306 - val_acc: 0.6400\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6314 - acc: 0.7415 - val_loss: 0.9289 - val_acc: 0.6457\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6255 - acc: 0.7422 - val_loss: 0.9251 - val_acc: 0.6457\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6383 - acc: 0.7473 - val_loss: 0.9296 - val_acc: 0.6457\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6222 - acc: 0.7486 - val_loss: 0.9326 - val_acc: 0.6400\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6135 - acc: 0.7594 - val_loss: 0.9326 - val_acc: 0.6400\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6092 - acc: 0.7518 - val_loss: 0.9293 - val_acc: 0.6457\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6114 - acc: 0.7505 - val_loss: 0.9237 - val_acc: 0.6514\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6063 - acc: 0.7632 - val_loss: 0.9385 - val_acc: 0.6400\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6128 - acc: 0.7479 - val_loss: 0.9396 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6157 - acc: 0.7473 - val_loss: 0.9337 - val_acc: 0.6629\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6042 - acc: 0.7549 - val_loss: 0.9405 - val_acc: 0.6286\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6153 - acc: 0.7505 - val_loss: 0.9434 - val_acc: 0.6400\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5972 - acc: 0.7696 - val_loss: 0.9443 - val_acc: 0.6343\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6160 - acc: 0.7422 - val_loss: 0.9474 - val_acc: 0.6400\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6075 - acc: 0.7601 - val_loss: 0.9386 - val_acc: 0.6629\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5906 - acc: 0.7671 - val_loss: 0.9426 - val_acc: 0.6514\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5752 - acc: 0.7690 - val_loss: 0.9439 - val_acc: 0.6400\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5779 - acc: 0.7696 - val_loss: 0.9429 - val_acc: 0.6343\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5790 - acc: 0.7696 - val_loss: 0.9432 - val_acc: 0.6457\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6000 - acc: 0.7620 - val_loss: 0.9363 - val_acc: 0.6571\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5711 - acc: 0.7696 - val_loss: 0.9457 - val_acc: 0.6400\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5925 - acc: 0.7671 - val_loss: 0.9454 - val_acc: 0.6629\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5764 - acc: 0.7683 - val_loss: 0.9463 - val_acc: 0.6457\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5800 - acc: 0.7722 - val_loss: 0.9528 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5674 - acc: 0.7715 - val_loss: 0.9569 - val_acc: 0.6457\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5774 - acc: 0.7594 - val_loss: 0.9502 - val_acc: 0.6571\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5820 - acc: 0.7613 - val_loss: 0.9563 - val_acc: 0.6571\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5718 - acc: 0.7709 - val_loss: 0.9557 - val_acc: 0.6571\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5933 - acc: 0.7601 - val_loss: 0.9577 - val_acc: 0.6457\n",
      "193/193 [==============================] - 0s 67us/step\n",
      "1742/1742 [==============================] - 0s 51us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.4558 - acc: 0.3063 - val_loss: 1.2195 - val_acc: 0.5143\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 1.2760 - acc: 0.4078 - val_loss: 1.1241 - val_acc: 0.5429\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.2272 - acc: 0.4301 - val_loss: 1.0650 - val_acc: 0.5943\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 1.1466 - acc: 0.4850 - val_loss: 1.0337 - val_acc: 0.5943\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 1.1049 - acc: 0.5188 - val_loss: 1.0061 - val_acc: 0.6400\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 1.0903 - acc: 0.5303 - val_loss: 0.9846 - val_acc: 0.6400\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.0776 - acc: 0.5316 - val_loss: 0.9615 - val_acc: 0.6286\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 1.0512 - acc: 0.5380 - val_loss: 0.9545 - val_acc: 0.6457\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 1.0343 - acc: 0.5418 - val_loss: 0.9426 - val_acc: 0.6457\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 1.0131 - acc: 0.5782 - val_loss: 0.9366 - val_acc: 0.6629\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9975 - acc: 0.5578 - val_loss: 0.9303 - val_acc: 0.6457\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9893 - acc: 0.5680 - val_loss: 0.9220 - val_acc: 0.6743\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9738 - acc: 0.5999 - val_loss: 0.9178 - val_acc: 0.6743\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9742 - acc: 0.5948 - val_loss: 0.9129 - val_acc: 0.6743\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9700 - acc: 0.5756 - val_loss: 0.9004 - val_acc: 0.6686\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.9492 - acc: 0.6114 - val_loss: 0.9060 - val_acc: 0.6686\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.9323 - acc: 0.5954 - val_loss: 0.9010 - val_acc: 0.6800\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.9330 - acc: 0.5954 - val_loss: 0.8998 - val_acc: 0.6743\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.9289 - acc: 0.6094 - val_loss: 0.8974 - val_acc: 0.6686\n",
      "Epoch 20/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.9118 - acc: 0.6248 - val_loss: 0.8991 - val_acc: 0.6857\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9160 - acc: 0.6209 - val_loss: 0.8996 - val_acc: 0.6971\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9101 - acc: 0.6139 - val_loss: 0.8977 - val_acc: 0.6800\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9047 - acc: 0.6197 - val_loss: 0.8920 - val_acc: 0.6914\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8905 - acc: 0.6343 - val_loss: 0.8964 - val_acc: 0.6857\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8918 - acc: 0.6165 - val_loss: 0.8973 - val_acc: 0.6800\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8925 - acc: 0.6362 - val_loss: 0.8926 - val_acc: 0.6857\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8853 - acc: 0.6216 - val_loss: 0.8906 - val_acc: 0.6800\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8726 - acc: 0.6388 - val_loss: 0.8892 - val_acc: 0.6800\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8942 - acc: 0.6235 - val_loss: 0.8962 - val_acc: 0.6800\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8501 - acc: 0.6458 - val_loss: 0.8913 - val_acc: 0.6686\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.8540 - acc: 0.6560 - val_loss: 0.8955 - val_acc: 0.6686\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8838 - acc: 0.6458 - val_loss: 0.8964 - val_acc: 0.6629\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8601 - acc: 0.6299 - val_loss: 0.8988 - val_acc: 0.6743\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8484 - acc: 0.6624 - val_loss: 0.8930 - val_acc: 0.6686\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8587 - acc: 0.6394 - val_loss: 0.9017 - val_acc: 0.6686\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8526 - acc: 0.6509 - val_loss: 0.9037 - val_acc: 0.6743\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8311 - acc: 0.6586 - val_loss: 0.9003 - val_acc: 0.6686\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8427 - acc: 0.6516 - val_loss: 0.9038 - val_acc: 0.6514\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8123 - acc: 0.6713 - val_loss: 0.9094 - val_acc: 0.6743\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8404 - acc: 0.6541 - val_loss: 0.9074 - val_acc: 0.6629\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8208 - acc: 0.6599 - val_loss: 0.9056 - val_acc: 0.6571\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8260 - acc: 0.6586 - val_loss: 0.9081 - val_acc: 0.6457\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8437 - acc: 0.6599 - val_loss: 0.9078 - val_acc: 0.6514\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8151 - acc: 0.6752 - val_loss: 0.9121 - val_acc: 0.6686\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8261 - acc: 0.6586 - val_loss: 0.9136 - val_acc: 0.6457\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8163 - acc: 0.6573 - val_loss: 0.9063 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7953 - acc: 0.6860 - val_loss: 0.9087 - val_acc: 0.6514\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8142 - acc: 0.6784 - val_loss: 0.9161 - val_acc: 0.6629\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8103 - acc: 0.6643 - val_loss: 0.9086 - val_acc: 0.6514\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8061 - acc: 0.6694 - val_loss: 0.9173 - val_acc: 0.6571\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8131 - acc: 0.6669 - val_loss: 0.9121 - val_acc: 0.6514\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7851 - acc: 0.6835 - val_loss: 0.9089 - val_acc: 0.6514\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7992 - acc: 0.6745 - val_loss: 0.9132 - val_acc: 0.6514\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7856 - acc: 0.6682 - val_loss: 0.9204 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7914 - acc: 0.6828 - val_loss: 0.9145 - val_acc: 0.6457\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7764 - acc: 0.6777 - val_loss: 0.9255 - val_acc: 0.6343\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7762 - acc: 0.6886 - val_loss: 0.9208 - val_acc: 0.6571\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7799 - acc: 0.6822 - val_loss: 0.9168 - val_acc: 0.6457\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7788 - acc: 0.6860 - val_loss: 0.9170 - val_acc: 0.6514\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7956 - acc: 0.6816 - val_loss: 0.9158 - val_acc: 0.6629\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7806 - acc: 0.6860 - val_loss: 0.9110 - val_acc: 0.6514\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7646 - acc: 0.6886 - val_loss: 0.9143 - val_acc: 0.6686\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7575 - acc: 0.6873 - val_loss: 0.9226 - val_acc: 0.6514\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7754 - acc: 0.6924 - val_loss: 0.9153 - val_acc: 0.6514\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7510 - acc: 0.6905 - val_loss: 0.9140 - val_acc: 0.6457\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7368 - acc: 0.6937 - val_loss: 0.9217 - val_acc: 0.6400\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7492 - acc: 0.6981 - val_loss: 0.9280 - val_acc: 0.6286\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7495 - acc: 0.6924 - val_loss: 0.9252 - val_acc: 0.6343\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7217 - acc: 0.7026 - val_loss: 0.9317 - val_acc: 0.6400\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7738 - acc: 0.6796 - val_loss: 0.9257 - val_acc: 0.6343\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7454 - acc: 0.6835 - val_loss: 0.9271 - val_acc: 0.6343\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7669 - acc: 0.6886 - val_loss: 0.9314 - val_acc: 0.6400\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7397 - acc: 0.7007 - val_loss: 0.9372 - val_acc: 0.6400\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7498 - acc: 0.7033 - val_loss: 0.9301 - val_acc: 0.6514\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7494 - acc: 0.6956 - val_loss: 0.9323 - val_acc: 0.6514\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7283 - acc: 0.6988 - val_loss: 0.9396 - val_acc: 0.6343\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7275 - acc: 0.7001 - val_loss: 0.9421 - val_acc: 0.6343\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7311 - acc: 0.7077 - val_loss: 0.9355 - val_acc: 0.6514\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7350 - acc: 0.6969 - val_loss: 0.9364 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7254 - acc: 0.7128 - val_loss: 0.9376 - val_acc: 0.6343\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7387 - acc: 0.7013 - val_loss: 0.9401 - val_acc: 0.6686\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7251 - acc: 0.7052 - val_loss: 0.9404 - val_acc: 0.6400\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.7164 - acc: 0.7026 - val_loss: 0.9457 - val_acc: 0.6400\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7106 - acc: 0.7052 - val_loss: 0.9447 - val_acc: 0.6343\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7068 - acc: 0.7205 - val_loss: 0.9463 - val_acc: 0.6343\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7122 - acc: 0.7020 - val_loss: 0.9520 - val_acc: 0.6343\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7136 - acc: 0.7026 - val_loss: 0.9558 - val_acc: 0.6343\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7124 - acc: 0.7135 - val_loss: 0.9558 - val_acc: 0.6514\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6922 - acc: 0.7198 - val_loss: 0.9604 - val_acc: 0.6343\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7156 - acc: 0.7167 - val_loss: 0.9610 - val_acc: 0.6229\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7189 - acc: 0.7052 - val_loss: 0.9575 - val_acc: 0.6286\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6977 - acc: 0.7173 - val_loss: 0.9669 - val_acc: 0.6400\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7041 - acc: 0.7103 - val_loss: 0.9545 - val_acc: 0.6400\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6996 - acc: 0.7237 - val_loss: 0.9513 - val_acc: 0.6400\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7005 - acc: 0.7192 - val_loss: 0.9619 - val_acc: 0.6514\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6820 - acc: 0.7218 - val_loss: 0.9837 - val_acc: 0.6057\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6967 - acc: 0.7179 - val_loss: 0.9794 - val_acc: 0.6286\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6817 - acc: 0.7122 - val_loss: 0.9765 - val_acc: 0.6229\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7068 - acc: 0.7167 - val_loss: 0.9763 - val_acc: 0.6229\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6721 - acc: 0.7262 - val_loss: 0.9721 - val_acc: 0.6229\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6651 - acc: 0.7262 - val_loss: 0.9758 - val_acc: 0.6286\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6751 - acc: 0.7384 - val_loss: 0.9702 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6763 - acc: 0.7205 - val_loss: 0.9756 - val_acc: 0.6286\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6697 - acc: 0.7409 - val_loss: 0.9646 - val_acc: 0.6343\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6948 - acc: 0.7198 - val_loss: 0.9690 - val_acc: 0.6286\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6592 - acc: 0.7307 - val_loss: 0.9865 - val_acc: 0.6171\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6600 - acc: 0.7326 - val_loss: 0.9713 - val_acc: 0.6343\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6616 - acc: 0.7352 - val_loss: 0.9745 - val_acc: 0.6457\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6681 - acc: 0.7301 - val_loss: 0.9810 - val_acc: 0.6571\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6669 - acc: 0.7352 - val_loss: 0.9855 - val_acc: 0.6457\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6567 - acc: 0.7396 - val_loss: 0.9876 - val_acc: 0.6343\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6698 - acc: 0.7332 - val_loss: 0.9944 - val_acc: 0.6286\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6435 - acc: 0.7384 - val_loss: 0.9761 - val_acc: 0.6286\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6622 - acc: 0.7345 - val_loss: 0.9877 - val_acc: 0.6343\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6588 - acc: 0.7352 - val_loss: 0.9979 - val_acc: 0.6057\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6492 - acc: 0.7403 - val_loss: 0.9980 - val_acc: 0.6171\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6712 - acc: 0.7358 - val_loss: 0.9982 - val_acc: 0.6400\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6589 - acc: 0.7288 - val_loss: 0.9951 - val_acc: 0.6286\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6168 - acc: 0.7435 - val_loss: 1.0019 - val_acc: 0.6286\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6229 - acc: 0.7581 - val_loss: 1.0070 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6403 - acc: 0.7441 - val_loss: 1.0165 - val_acc: 0.6400\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6383 - acc: 0.7403 - val_loss: 1.0081 - val_acc: 0.6343\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6353 - acc: 0.7358 - val_loss: 1.0126 - val_acc: 0.6171\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6256 - acc: 0.7518 - val_loss: 1.0182 - val_acc: 0.6286\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6309 - acc: 0.7396 - val_loss: 1.0151 - val_acc: 0.6286\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6238 - acc: 0.7409 - val_loss: 1.0230 - val_acc: 0.6286\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6199 - acc: 0.7511 - val_loss: 1.0188 - val_acc: 0.6343\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6244 - acc: 0.7466 - val_loss: 1.0113 - val_acc: 0.6229\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6432 - acc: 0.7473 - val_loss: 1.0109 - val_acc: 0.6286\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6224 - acc: 0.7511 - val_loss: 1.0131 - val_acc: 0.6114\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6209 - acc: 0.7581 - val_loss: 1.0211 - val_acc: 0.6171\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6125 - acc: 0.7575 - val_loss: 1.0321 - val_acc: 0.6400\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6243 - acc: 0.7505 - val_loss: 1.0380 - val_acc: 0.6229\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6111 - acc: 0.7530 - val_loss: 1.0341 - val_acc: 0.6400\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6219 - acc: 0.7511 - val_loss: 1.0280 - val_acc: 0.6114\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6167 - acc: 0.7543 - val_loss: 1.0291 - val_acc: 0.6343\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6231 - acc: 0.7575 - val_loss: 1.0255 - val_acc: 0.6400\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6209 - acc: 0.7492 - val_loss: 1.0373 - val_acc: 0.6343\n",
      "Epoch 139/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6107 - acc: 0.7492 - val_loss: 1.0393 - val_acc: 0.6400\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6003 - acc: 0.7715 - val_loss: 1.0412 - val_acc: 0.6343\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5955 - acc: 0.7556 - val_loss: 1.0387 - val_acc: 0.6400\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6044 - acc: 0.7626 - val_loss: 1.0362 - val_acc: 0.6343\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6047 - acc: 0.7575 - val_loss: 1.0326 - val_acc: 0.6400\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5872 - acc: 0.7594 - val_loss: 1.0397 - val_acc: 0.6229\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5810 - acc: 0.7632 - val_loss: 1.0478 - val_acc: 0.6171\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5972 - acc: 0.7677 - val_loss: 1.0562 - val_acc: 0.6286\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6084 - acc: 0.7505 - val_loss: 1.0495 - val_acc: 0.6229\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6043 - acc: 0.7537 - val_loss: 1.0618 - val_acc: 0.6400\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5807 - acc: 0.7626 - val_loss: 1.0668 - val_acc: 0.6171\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.5920 - acc: 0.7569 - val_loss: 1.0567 - val_acc: 0.6286\n",
      "193/193 [==============================] - 0s 81us/step\n",
      "1742/1742 [==============================] - 0s 48us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 5s 3ms/step - loss: 1.5189 - acc: 0.2999 - val_loss: 1.1689 - val_acc: 0.4914\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.3024 - acc: 0.4135 - val_loss: 1.0600 - val_acc: 0.5657\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.1989 - acc: 0.4576 - val_loss: 1.0165 - val_acc: 0.5829\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.1421 - acc: 0.4914 - val_loss: 0.9815 - val_acc: 0.6171\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 1.1317 - acc: 0.5105 - val_loss: 0.9667 - val_acc: 0.6229\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.0962 - acc: 0.5188 - val_loss: 0.9496 - val_acc: 0.6229\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0515 - acc: 0.5392 - val_loss: 0.9403 - val_acc: 0.6286\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.0429 - acc: 0.5597 - val_loss: 0.9264 - val_acc: 0.6514\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 1.0389 - acc: 0.5501 - val_loss: 0.9248 - val_acc: 0.6571\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.0183 - acc: 0.5482 - val_loss: 0.9136 - val_acc: 0.6514\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.0119 - acc: 0.5731 - val_loss: 0.9127 - val_acc: 0.6343\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9836 - acc: 0.5795 - val_loss: 0.9152 - val_acc: 0.6400\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9855 - acc: 0.5814 - val_loss: 0.9068 - val_acc: 0.6629\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9801 - acc: 0.5826 - val_loss: 0.9085 - val_acc: 0.6400\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9695 - acc: 0.5922 - val_loss: 0.9098 - val_acc: 0.6514\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.9561 - acc: 0.5967 - val_loss: 0.8966 - val_acc: 0.6686\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9590 - acc: 0.5820 - val_loss: 0.8978 - val_acc: 0.6743\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9495 - acc: 0.6005 - val_loss: 0.8985 - val_acc: 0.6514\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9504 - acc: 0.5941 - val_loss: 0.9009 - val_acc: 0.6743\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9428 - acc: 0.5967 - val_loss: 0.9097 - val_acc: 0.6629\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9103 - acc: 0.6152 - val_loss: 0.9007 - val_acc: 0.6629\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9091 - acc: 0.6184 - val_loss: 0.8984 - val_acc: 0.6743\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9191 - acc: 0.6069 - val_loss: 0.9021 - val_acc: 0.6457\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8985 - acc: 0.6350 - val_loss: 0.9002 - val_acc: 0.6571\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9106 - acc: 0.6082 - val_loss: 0.8947 - val_acc: 0.6686\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8898 - acc: 0.6216 - val_loss: 0.8975 - val_acc: 0.6514\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8925 - acc: 0.6311 - val_loss: 0.8929 - val_acc: 0.6686\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8949 - acc: 0.6222 - val_loss: 0.8935 - val_acc: 0.6743\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8829 - acc: 0.6254 - val_loss: 0.8989 - val_acc: 0.6629\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8727 - acc: 0.6362 - val_loss: 0.8973 - val_acc: 0.6800\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8772 - acc: 0.6439 - val_loss: 0.8934 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8805 - acc: 0.6280 - val_loss: 0.8946 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8518 - acc: 0.6382 - val_loss: 0.8968 - val_acc: 0.6629\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8671 - acc: 0.6420 - val_loss: 0.8940 - val_acc: 0.6686\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8619 - acc: 0.6350 - val_loss: 0.9033 - val_acc: 0.6457\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8526 - acc: 0.6496 - val_loss: 0.9027 - val_acc: 0.6686\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8646 - acc: 0.6337 - val_loss: 0.9039 - val_acc: 0.6457\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8449 - acc: 0.6509 - val_loss: 0.9028 - val_acc: 0.6629\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8460 - acc: 0.6426 - val_loss: 0.9067 - val_acc: 0.6514\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8315 - acc: 0.6522 - val_loss: 0.9091 - val_acc: 0.6571\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8559 - acc: 0.6369 - val_loss: 0.9051 - val_acc: 0.6571\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8268 - acc: 0.6496 - val_loss: 0.9090 - val_acc: 0.6457\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8332 - acc: 0.6560 - val_loss: 0.9089 - val_acc: 0.6571\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8326 - acc: 0.6509 - val_loss: 0.9106 - val_acc: 0.6400\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8477 - acc: 0.6484 - val_loss: 0.9163 - val_acc: 0.6343\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8264 - acc: 0.6567 - val_loss: 0.9201 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8387 - acc: 0.6726 - val_loss: 0.9124 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8163 - acc: 0.6675 - val_loss: 0.9118 - val_acc: 0.6571\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.8051 - acc: 0.6733 - val_loss: 0.9157 - val_acc: 0.6571\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8155 - acc: 0.6631 - val_loss: 0.9146 - val_acc: 0.6571\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8112 - acc: 0.6739 - val_loss: 0.9111 - val_acc: 0.6629\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8190 - acc: 0.6662 - val_loss: 0.9165 - val_acc: 0.6629\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7905 - acc: 0.6713 - val_loss: 0.9158 - val_acc: 0.6571\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8026 - acc: 0.6701 - val_loss: 0.9205 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7875 - acc: 0.6720 - val_loss: 0.9177 - val_acc: 0.6514\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7961 - acc: 0.6669 - val_loss: 0.9221 - val_acc: 0.6457\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8002 - acc: 0.6771 - val_loss: 0.9246 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7767 - acc: 0.6816 - val_loss: 0.9187 - val_acc: 0.6457\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7778 - acc: 0.6835 - val_loss: 0.9214 - val_acc: 0.6514\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7661 - acc: 0.6841 - val_loss: 0.9270 - val_acc: 0.6343\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7856 - acc: 0.6892 - val_loss: 0.9235 - val_acc: 0.6457\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7797 - acc: 0.6911 - val_loss: 0.9221 - val_acc: 0.6400\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7674 - acc: 0.6962 - val_loss: 0.9273 - val_acc: 0.6286\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7659 - acc: 0.6924 - val_loss: 0.9312 - val_acc: 0.6400\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7749 - acc: 0.6873 - val_loss: 0.9397 - val_acc: 0.6286\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7726 - acc: 0.6899 - val_loss: 0.9402 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7682 - acc: 0.6886 - val_loss: 0.9368 - val_acc: 0.6457\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7732 - acc: 0.6841 - val_loss: 0.9333 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.7460 - acc: 0.7026 - val_loss: 0.9364 - val_acc: 0.6400\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7388 - acc: 0.6981 - val_loss: 0.9347 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7532 - acc: 0.7052 - val_loss: 0.9368 - val_acc: 0.6571\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7612 - acc: 0.6994 - val_loss: 0.9371 - val_acc: 0.6514\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7288 - acc: 0.7122 - val_loss: 0.9363 - val_acc: 0.6343\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7444 - acc: 0.7077 - val_loss: 0.9353 - val_acc: 0.6400\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7285 - acc: 0.7141 - val_loss: 0.9426 - val_acc: 0.6514\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7388 - acc: 0.6969 - val_loss: 0.9483 - val_acc: 0.6514\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7301 - acc: 0.7096 - val_loss: 0.9504 - val_acc: 0.6457\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7362 - acc: 0.7071 - val_loss: 0.9404 - val_acc: 0.6457\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7181 - acc: 0.7192 - val_loss: 0.9434 - val_acc: 0.6400\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7312 - acc: 0.6975 - val_loss: 0.9501 - val_acc: 0.6400\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7431 - acc: 0.6943 - val_loss: 0.9518 - val_acc: 0.6457\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7249 - acc: 0.7109 - val_loss: 0.9571 - val_acc: 0.6457\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7328 - acc: 0.7026 - val_loss: 0.9544 - val_acc: 0.6514\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7388 - acc: 0.7007 - val_loss: 0.9543 - val_acc: 0.6343\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7355 - acc: 0.7026 - val_loss: 0.9571 - val_acc: 0.6400\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7074 - acc: 0.7064 - val_loss: 0.9612 - val_acc: 0.6400\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7126 - acc: 0.7224 - val_loss: 0.9573 - val_acc: 0.6286\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7027 - acc: 0.7109 - val_loss: 0.9601 - val_acc: 0.6286\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6978 - acc: 0.7243 - val_loss: 0.9577 - val_acc: 0.6400\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7056 - acc: 0.7147 - val_loss: 0.9668 - val_acc: 0.6343\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6932 - acc: 0.7192 - val_loss: 0.9654 - val_acc: 0.6343\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6863 - acc: 0.7186 - val_loss: 0.9710 - val_acc: 0.6286\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6854 - acc: 0.7243 - val_loss: 0.9642 - val_acc: 0.6400\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6900 - acc: 0.7173 - val_loss: 0.9696 - val_acc: 0.6286\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6758 - acc: 0.7275 - val_loss: 0.9655 - val_acc: 0.6343\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6676 - acc: 0.7250 - val_loss: 0.9662 - val_acc: 0.6343\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6885 - acc: 0.7186 - val_loss: 0.9667 - val_acc: 0.6286\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.6817 - acc: 0.7403 - val_loss: 0.9633 - val_acc: 0.6343\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.6758 - acc: 0.7250 - val_loss: 0.9719 - val_acc: 0.6229\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6600 - acc: 0.7288 - val_loss: 0.9737 - val_acc: 0.6286\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6992 - acc: 0.7071 - val_loss: 0.9769 - val_acc: 0.6286\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.6815 - acc: 0.7186 - val_loss: 0.9768 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.6630 - acc: 0.7415 - val_loss: 0.9850 - val_acc: 0.6286\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6701 - acc: 0.7243 - val_loss: 0.9888 - val_acc: 0.6286\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6711 - acc: 0.7396 - val_loss: 0.9840 - val_acc: 0.6229\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6796 - acc: 0.7269 - val_loss: 0.9736 - val_acc: 0.6400\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.6696 - acc: 0.7332 - val_loss: 0.9764 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6704 - acc: 0.7332 - val_loss: 0.9868 - val_acc: 0.6229\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6701 - acc: 0.7364 - val_loss: 0.9858 - val_acc: 0.6229\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6548 - acc: 0.7409 - val_loss: 0.9877 - val_acc: 0.6343\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6744 - acc: 0.7288 - val_loss: 0.9835 - val_acc: 0.6286\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6721 - acc: 0.7288 - val_loss: 0.9882 - val_acc: 0.6286\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6427 - acc: 0.7569 - val_loss: 0.9933 - val_acc: 0.6343\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6494 - acc: 0.7454 - val_loss: 0.9988 - val_acc: 0.6286\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6477 - acc: 0.7377 - val_loss: 0.9978 - val_acc: 0.6286\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6676 - acc: 0.7301 - val_loss: 0.9945 - val_acc: 0.6286\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6336 - acc: 0.7377 - val_loss: 0.9906 - val_acc: 0.6343\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6488 - acc: 0.7454 - val_loss: 0.9979 - val_acc: 0.6400\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6598 - acc: 0.7345 - val_loss: 0.9938 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6539 - acc: 0.7364 - val_loss: 0.9945 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6585 - acc: 0.7460 - val_loss: 1.0149 - val_acc: 0.6343\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6583 - acc: 0.7415 - val_loss: 1.0050 - val_acc: 0.6343\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6348 - acc: 0.7466 - val_loss: 1.0040 - val_acc: 0.6343\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6355 - acc: 0.7466 - val_loss: 1.0136 - val_acc: 0.6400\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6239 - acc: 0.7543 - val_loss: 1.0105 - val_acc: 0.6286\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6196 - acc: 0.7703 - val_loss: 1.0127 - val_acc: 0.6286\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6255 - acc: 0.7479 - val_loss: 1.0207 - val_acc: 0.6343\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6277 - acc: 0.7537 - val_loss: 1.0086 - val_acc: 0.6343\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6314 - acc: 0.7601 - val_loss: 1.0141 - val_acc: 0.6343\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 112us/step - loss: 0.6194 - acc: 0.7479 - val_loss: 1.0091 - val_acc: 0.6343\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.6133 - acc: 0.7581 - val_loss: 1.0134 - val_acc: 0.6343\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6085 - acc: 0.7645 - val_loss: 1.0114 - val_acc: 0.6114\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6214 - acc: 0.7652 - val_loss: 1.0243 - val_acc: 0.6114\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6353 - acc: 0.7518 - val_loss: 1.0249 - val_acc: 0.6286\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6221 - acc: 0.7562 - val_loss: 1.0229 - val_acc: 0.6229\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6063 - acc: 0.7632 - val_loss: 1.0188 - val_acc: 0.6114\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6095 - acc: 0.7479 - val_loss: 1.0229 - val_acc: 0.6114\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6396 - acc: 0.7396 - val_loss: 1.0315 - val_acc: 0.6114\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5921 - acc: 0.7747 - val_loss: 1.0310 - val_acc: 0.6171\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6154 - acc: 0.7645 - val_loss: 1.0316 - val_acc: 0.6114\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6127 - acc: 0.7492 - val_loss: 1.0434 - val_acc: 0.6057\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5985 - acc: 0.7607 - val_loss: 1.0294 - val_acc: 0.6171\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6131 - acc: 0.7658 - val_loss: 1.0287 - val_acc: 0.6229\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5930 - acc: 0.7735 - val_loss: 1.0310 - val_acc: 0.6286\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5844 - acc: 0.7543 - val_loss: 1.0355 - val_acc: 0.6229\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5846 - acc: 0.7652 - val_loss: 1.0333 - val_acc: 0.6229\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6009 - acc: 0.7569 - val_loss: 1.0439 - val_acc: 0.6229\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5961 - acc: 0.7645 - val_loss: 1.0548 - val_acc: 0.6286\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5834 - acc: 0.7709 - val_loss: 1.0391 - val_acc: 0.6171\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5805 - acc: 0.7683 - val_loss: 1.0504 - val_acc: 0.6229\n",
      "193/193 [==============================] - 0s 74us/step\n",
      "1742/1742 [==============================] - 0s 54us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.4495 - acc: 0.3031 - val_loss: 1.2295 - val_acc: 0.4229\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 1.2605 - acc: 0.4250 - val_loss: 1.1402 - val_acc: 0.4629\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 1.1972 - acc: 0.4563 - val_loss: 1.0927 - val_acc: 0.5200\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 1.1510 - acc: 0.4812 - val_loss: 1.0614 - val_acc: 0.5371\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 1.1103 - acc: 0.5195 - val_loss: 1.0428 - val_acc: 0.5429\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 1.0760 - acc: 0.5418 - val_loss: 1.0230 - val_acc: 0.5371\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 1.0740 - acc: 0.5239 - val_loss: 1.0090 - val_acc: 0.5486\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 1.0504 - acc: 0.5290 - val_loss: 0.9963 - val_acc: 0.5486\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 1.0135 - acc: 0.5731 - val_loss: 0.9860 - val_acc: 0.5657\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 1.0197 - acc: 0.5807 - val_loss: 0.9812 - val_acc: 0.5714\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 113us/step - loss: 1.0016 - acc: 0.5705 - val_loss: 0.9811 - val_acc: 0.5600\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.9918 - acc: 0.5712 - val_loss: 0.9765 - val_acc: 0.5657\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.9843 - acc: 0.5909 - val_loss: 0.9795 - val_acc: 0.5714\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.9799 - acc: 0.5795 - val_loss: 0.9683 - val_acc: 0.5714\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.9576 - acc: 0.5992 - val_loss: 0.9664 - val_acc: 0.5714\n",
      "Epoch 16/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.9422 - acc: 0.5929 - val_loss: 0.9689 - val_acc: 0.5543\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.9250 - acc: 0.6146 - val_loss: 0.9716 - val_acc: 0.5657\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.9311 - acc: 0.6101 - val_loss: 0.9656 - val_acc: 0.5657\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.9260 - acc: 0.6107 - val_loss: 0.9656 - val_acc: 0.5714\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.9153 - acc: 0.6184 - val_loss: 0.9598 - val_acc: 0.5714\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 107us/step - loss: 0.9153 - acc: 0.6228 - val_loss: 0.9674 - val_acc: 0.5714\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.9154 - acc: 0.6152 - val_loss: 0.9691 - val_acc: 0.5829\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.9087 - acc: 0.6222 - val_loss: 0.9692 - val_acc: 0.5714\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.8962 - acc: 0.6254 - val_loss: 0.9605 - val_acc: 0.5771\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 234us/step - loss: 0.8939 - acc: 0.6216 - val_loss: 0.9655 - val_acc: 0.5657\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 109us/step - loss: 0.8827 - acc: 0.6382 - val_loss: 0.9578 - val_acc: 0.5771\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.9018 - acc: 0.6165 - val_loss: 0.9669 - val_acc: 0.5714\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.8873 - acc: 0.6375 - val_loss: 0.9666 - val_acc: 0.5714\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.8719 - acc: 0.6362 - val_loss: 0.9641 - val_acc: 0.5829\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8649 - acc: 0.6356 - val_loss: 0.9625 - val_acc: 0.5829\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8732 - acc: 0.6445 - val_loss: 0.9628 - val_acc: 0.5943\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8534 - acc: 0.6477 - val_loss: 0.9634 - val_acc: 0.5886\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.8426 - acc: 0.6592 - val_loss: 0.9720 - val_acc: 0.5829\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8571 - acc: 0.6458 - val_loss: 0.9650 - val_acc: 0.5829\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8619 - acc: 0.6452 - val_loss: 0.9701 - val_acc: 0.5886\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8321 - acc: 0.6643 - val_loss: 0.9739 - val_acc: 0.5886\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8402 - acc: 0.6548 - val_loss: 0.9678 - val_acc: 0.5771\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.8377 - acc: 0.6669 - val_loss: 0.9724 - val_acc: 0.5943\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8438 - acc: 0.6579 - val_loss: 0.9782 - val_acc: 0.6057\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8309 - acc: 0.6586 - val_loss: 0.9800 - val_acc: 0.5943\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8007 - acc: 0.6745 - val_loss: 0.9877 - val_acc: 0.5886\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8177 - acc: 0.6784 - val_loss: 0.9821 - val_acc: 0.5886\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.8195 - acc: 0.6567 - val_loss: 0.9835 - val_acc: 0.5886\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.8206 - acc: 0.6592 - val_loss: 0.9838 - val_acc: 0.5886\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8352 - acc: 0.6573 - val_loss: 0.9866 - val_acc: 0.5886\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.8261 - acc: 0.6631 - val_loss: 0.9804 - val_acc: 0.6000\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8079 - acc: 0.6656 - val_loss: 0.9896 - val_acc: 0.6057\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.8050 - acc: 0.6720 - val_loss: 0.9856 - val_acc: 0.5943\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.8200 - acc: 0.6618 - val_loss: 0.9833 - val_acc: 0.5943\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7787 - acc: 0.6765 - val_loss: 0.9920 - val_acc: 0.6000\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7976 - acc: 0.6720 - val_loss: 0.9857 - val_acc: 0.6057\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8039 - acc: 0.6752 - val_loss: 0.9880 - val_acc: 0.6057\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7974 - acc: 0.6803 - val_loss: 0.9872 - val_acc: 0.5886\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7981 - acc: 0.6860 - val_loss: 0.9955 - val_acc: 0.6000\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8035 - acc: 0.6688 - val_loss: 0.9889 - val_acc: 0.6000\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7807 - acc: 0.6924 - val_loss: 0.9911 - val_acc: 0.6057\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7777 - acc: 0.7033 - val_loss: 0.9986 - val_acc: 0.5943\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7782 - acc: 0.6713 - val_loss: 0.9988 - val_acc: 0.6000\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.7595 - acc: 0.6758 - val_loss: 1.0035 - val_acc: 0.6000\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7741 - acc: 0.6854 - val_loss: 0.9983 - val_acc: 0.6057\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7649 - acc: 0.6822 - val_loss: 0.9971 - val_acc: 0.6057\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7735 - acc: 0.6847 - val_loss: 0.9987 - val_acc: 0.6114\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7776 - acc: 0.6771 - val_loss: 1.0017 - val_acc: 0.6114\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7576 - acc: 0.6969 - val_loss: 1.0026 - val_acc: 0.6114\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7754 - acc: 0.6962 - val_loss: 1.0086 - val_acc: 0.6000\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7736 - acc: 0.6918 - val_loss: 1.0124 - val_acc: 0.6000\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7531 - acc: 0.6962 - val_loss: 1.0095 - val_acc: 0.6057\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7676 - acc: 0.6828 - val_loss: 1.0062 - val_acc: 0.6057\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.7481 - acc: 0.6905 - val_loss: 0.9942 - val_acc: 0.6000\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7614 - acc: 0.6899 - val_loss: 1.0028 - val_acc: 0.5943\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7419 - acc: 0.6988 - val_loss: 1.0134 - val_acc: 0.5886\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7569 - acc: 0.6918 - val_loss: 1.0094 - val_acc: 0.5886\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7408 - acc: 0.6937 - val_loss: 1.0040 - val_acc: 0.5886\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7232 - acc: 0.7033 - val_loss: 1.0181 - val_acc: 0.5886\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7290 - acc: 0.7179 - val_loss: 1.0214 - val_acc: 0.5886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7312 - acc: 0.7109 - val_loss: 1.0202 - val_acc: 0.5943\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7346 - acc: 0.7084 - val_loss: 1.0171 - val_acc: 0.5829\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7468 - acc: 0.6943 - val_loss: 1.0187 - val_acc: 0.6000\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7365 - acc: 0.7039 - val_loss: 1.0227 - val_acc: 0.5886\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7423 - acc: 0.7026 - val_loss: 1.0212 - val_acc: 0.5886\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7220 - acc: 0.7077 - val_loss: 1.0185 - val_acc: 0.5886\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7103 - acc: 0.7122 - val_loss: 1.0170 - val_acc: 0.6000\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7023 - acc: 0.7262 - val_loss: 1.0312 - val_acc: 0.5943\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7374 - acc: 0.7141 - val_loss: 1.0289 - val_acc: 0.5943\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.7071 - acc: 0.7109 - val_loss: 1.0274 - val_acc: 0.6000\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.7099 - acc: 0.7128 - val_loss: 1.0333 - val_acc: 0.5886\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.7062 - acc: 0.7288 - val_loss: 1.0278 - val_acc: 0.5829\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7183 - acc: 0.7154 - val_loss: 1.0365 - val_acc: 0.5829\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7176 - acc: 0.7147 - val_loss: 1.0346 - val_acc: 0.5829\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7043 - acc: 0.7256 - val_loss: 1.0331 - val_acc: 0.5829\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7042 - acc: 0.7262 - val_loss: 1.0263 - val_acc: 0.6000\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6872 - acc: 0.7339 - val_loss: 1.0278 - val_acc: 0.5943\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6815 - acc: 0.7352 - val_loss: 1.0302 - val_acc: 0.5829\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6965 - acc: 0.7205 - val_loss: 1.0457 - val_acc: 0.5771\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7088 - acc: 0.7058 - val_loss: 1.0422 - val_acc: 0.5886\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6875 - acc: 0.7390 - val_loss: 1.0435 - val_acc: 0.5886\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6895 - acc: 0.7345 - val_loss: 1.0413 - val_acc: 0.5771\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6892 - acc: 0.7301 - val_loss: 1.0453 - val_acc: 0.5943\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6833 - acc: 0.7269 - val_loss: 1.0496 - val_acc: 0.5829\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.6742 - acc: 0.7332 - val_loss: 1.0475 - val_acc: 0.5886\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6905 - acc: 0.7211 - val_loss: 1.0538 - val_acc: 0.5714\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6836 - acc: 0.7147 - val_loss: 1.0617 - val_acc: 0.5829\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6757 - acc: 0.7313 - val_loss: 1.0586 - val_acc: 0.5714\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6537 - acc: 0.7460 - val_loss: 1.0529 - val_acc: 0.5829\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6882 - acc: 0.7179 - val_loss: 1.0619 - val_acc: 0.5714\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6448 - acc: 0.7409 - val_loss: 1.0632 - val_acc: 0.5771\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6731 - acc: 0.7364 - val_loss: 1.0660 - val_acc: 0.5771\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6761 - acc: 0.7275 - val_loss: 1.0610 - val_acc: 0.5714\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6557 - acc: 0.7403 - val_loss: 1.0682 - val_acc: 0.5771\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6746 - acc: 0.7364 - val_loss: 1.0634 - val_acc: 0.5943\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6424 - acc: 0.7505 - val_loss: 1.0581 - val_acc: 0.5886\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6528 - acc: 0.7384 - val_loss: 1.0686 - val_acc: 0.5886\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6583 - acc: 0.7428 - val_loss: 1.0768 - val_acc: 0.5600\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6418 - acc: 0.7575 - val_loss: 1.0704 - val_acc: 0.5829\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6464 - acc: 0.7326 - val_loss: 1.0790 - val_acc: 0.5714\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6655 - acc: 0.7345 - val_loss: 1.0706 - val_acc: 0.5657\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6437 - acc: 0.7486 - val_loss: 1.0759 - val_acc: 0.5771\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6445 - acc: 0.7415 - val_loss: 1.0777 - val_acc: 0.5771\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6648 - acc: 0.7301 - val_loss: 1.0755 - val_acc: 0.5771\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6546 - acc: 0.7454 - val_loss: 1.0802 - val_acc: 0.5829\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6552 - acc: 0.7377 - val_loss: 1.0854 - val_acc: 0.5600\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6291 - acc: 0.7460 - val_loss: 1.0900 - val_acc: 0.5771\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6340 - acc: 0.7498 - val_loss: 1.0825 - val_acc: 0.5829\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6235 - acc: 0.7588 - val_loss: 1.0855 - val_acc: 0.5829\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6363 - acc: 0.7428 - val_loss: 1.0882 - val_acc: 0.5771\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6082 - acc: 0.7460 - val_loss: 1.1015 - val_acc: 0.5600\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6489 - acc: 0.7441 - val_loss: 1.0892 - val_acc: 0.5829\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6241 - acc: 0.7543 - val_loss: 1.1011 - val_acc: 0.5829\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6239 - acc: 0.7626 - val_loss: 1.0918 - val_acc: 0.5714\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6370 - acc: 0.7447 - val_loss: 1.0962 - val_acc: 0.5714\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6125 - acc: 0.7594 - val_loss: 1.0976 - val_acc: 0.5829\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6265 - acc: 0.7588 - val_loss: 1.0913 - val_acc: 0.5886\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6085 - acc: 0.7766 - val_loss: 1.0920 - val_acc: 0.5771\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6206 - acc: 0.7530 - val_loss: 1.0944 - val_acc: 0.5829\n",
      "Epoch 135/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6242 - acc: 0.7371 - val_loss: 1.1013 - val_acc: 0.5771\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6015 - acc: 0.7556 - val_loss: 1.1015 - val_acc: 0.5714\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6151 - acc: 0.7588 - val_loss: 1.0975 - val_acc: 0.5771\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5985 - acc: 0.7817 - val_loss: 1.0980 - val_acc: 0.5829\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6021 - acc: 0.7722 - val_loss: 1.0907 - val_acc: 0.5829\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5957 - acc: 0.7741 - val_loss: 1.0987 - val_acc: 0.5771\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6216 - acc: 0.7581 - val_loss: 1.1000 - val_acc: 0.5829\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5935 - acc: 0.7575 - val_loss: 1.1109 - val_acc: 0.5771\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6079 - acc: 0.7518 - val_loss: 1.1091 - val_acc: 0.5829\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6025 - acc: 0.7588 - val_loss: 1.1025 - val_acc: 0.5771\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5916 - acc: 0.7537 - val_loss: 1.1145 - val_acc: 0.5886\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5922 - acc: 0.7779 - val_loss: 1.1161 - val_acc: 0.5771\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5875 - acc: 0.7786 - val_loss: 1.1127 - val_acc: 0.5771\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5936 - acc: 0.7588 - val_loss: 1.1150 - val_acc: 0.5886\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5969 - acc: 0.7613 - val_loss: 1.1203 - val_acc: 0.5771\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5926 - acc: 0.7683 - val_loss: 1.1201 - val_acc: 0.5829\n",
      "193/193 [==============================] - 0s 73us/step\n",
      "1742/1742 [==============================] - 0s 53us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.5279 - acc: 0.3282 - val_loss: 1.1176 - val_acc: 0.5829\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.2303 - acc: 0.4349 - val_loss: 1.0213 - val_acc: 0.6229\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.1223 - acc: 0.5019 - val_loss: 0.9857 - val_acc: 0.6400\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 115us/step - loss: 1.0651 - acc: 0.5358 - val_loss: 0.9476 - val_acc: 0.6457\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.0467 - acc: 0.5345 - val_loss: 0.9236 - val_acc: 0.6457\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.9931 - acc: 0.5754 - val_loss: 0.9207 - val_acc: 0.6457\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 1.0158 - acc: 0.5632 - val_loss: 0.9137 - val_acc: 0.6571\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.9682 - acc: 0.5862 - val_loss: 0.8979 - val_acc: 0.6514\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 123us/step - loss: 0.9714 - acc: 0.5766 - val_loss: 0.9040 - val_acc: 0.6686\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.9530 - acc: 0.5932 - val_loss: 0.8999 - val_acc: 0.6571\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 125us/step - loss: 0.9559 - acc: 0.6015 - val_loss: 0.9053 - val_acc: 0.6571\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.9077 - acc: 0.6258 - val_loss: 0.8927 - val_acc: 0.6629\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9202 - acc: 0.6290 - val_loss: 0.8885 - val_acc: 0.6686\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9093 - acc: 0.6149 - val_loss: 0.8976 - val_acc: 0.6571\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8978 - acc: 0.6347 - val_loss: 0.9001 - val_acc: 0.6743\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9033 - acc: 0.6175 - val_loss: 0.8949 - val_acc: 0.6743\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8742 - acc: 0.6418 - val_loss: 0.8972 - val_acc: 0.6743\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8810 - acc: 0.6315 - val_loss: 0.8928 - val_acc: 0.6457\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8969 - acc: 0.6143 - val_loss: 0.8887 - val_acc: 0.6571\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8659 - acc: 0.6322 - val_loss: 0.8903 - val_acc: 0.6514\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8601 - acc: 0.6284 - val_loss: 0.8821 - val_acc: 0.6743\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8477 - acc: 0.6501 - val_loss: 0.8874 - val_acc: 0.6686\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8331 - acc: 0.6494 - val_loss: 0.8923 - val_acc: 0.6686\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8221 - acc: 0.6520 - val_loss: 0.8960 - val_acc: 0.6743\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8278 - acc: 0.6596 - val_loss: 0.8909 - val_acc: 0.6571\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.8204 - acc: 0.6660 - val_loss: 0.9032 - val_acc: 0.6629\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.8115 - acc: 0.6609 - val_loss: 0.8975 - val_acc: 0.6457\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 150us/step - loss: 0.7951 - acc: 0.6794 - val_loss: 0.9007 - val_acc: 0.6514\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 124us/step - loss: 0.8043 - acc: 0.6641 - val_loss: 0.9071 - val_acc: 0.6743\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 164us/step - loss: 0.7918 - acc: 0.6826 - val_loss: 0.9145 - val_acc: 0.6686\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 132us/step - loss: 0.7969 - acc: 0.6769 - val_loss: 0.9208 - val_acc: 0.6514\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.7968 - acc: 0.6737 - val_loss: 0.9136 - val_acc: 0.6343\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7828 - acc: 0.6820 - val_loss: 0.9174 - val_acc: 0.6629\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.7896 - acc: 0.6673 - val_loss: 0.9120 - val_acc: 0.6514\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7711 - acc: 0.6890 - val_loss: 0.9202 - val_acc: 0.6571\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7733 - acc: 0.6980 - val_loss: 0.9226 - val_acc: 0.6571\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7658 - acc: 0.6884 - val_loss: 0.9268 - val_acc: 0.6514\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7638 - acc: 0.6960 - val_loss: 0.9249 - val_acc: 0.6571\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7517 - acc: 0.7005 - val_loss: 0.9239 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7589 - acc: 0.6999 - val_loss: 0.9198 - val_acc: 0.6571\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.7654 - acc: 0.6871 - val_loss: 0.9297 - val_acc: 0.6343\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 117us/step - loss: 0.7342 - acc: 0.7018 - val_loss: 0.9335 - val_acc: 0.6571\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.7657 - acc: 0.6928 - val_loss: 0.9265 - val_acc: 0.6400\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7464 - acc: 0.6922 - val_loss: 0.9295 - val_acc: 0.6686\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7397 - acc: 0.7107 - val_loss: 0.9340 - val_acc: 0.6400\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7443 - acc: 0.7011 - val_loss: 0.9292 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7404 - acc: 0.7114 - val_loss: 0.9378 - val_acc: 0.6571\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7195 - acc: 0.7139 - val_loss: 0.9439 - val_acc: 0.6457\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7063 - acc: 0.7139 - val_loss: 0.9481 - val_acc: 0.6343\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7051 - acc: 0.7171 - val_loss: 0.9547 - val_acc: 0.6286\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.7262 - acc: 0.7133 - val_loss: 0.9495 - val_acc: 0.6343\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7188 - acc: 0.7158 - val_loss: 0.9508 - val_acc: 0.6571\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6944 - acc: 0.7107 - val_loss: 0.9472 - val_acc: 0.6343\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6984 - acc: 0.7126 - val_loss: 0.9557 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6983 - acc: 0.7261 - val_loss: 0.9673 - val_acc: 0.6400\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6934 - acc: 0.7184 - val_loss: 0.9710 - val_acc: 0.6343\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7015 - acc: 0.7312 - val_loss: 0.9640 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6569 - acc: 0.7407 - val_loss: 0.9632 - val_acc: 0.6457\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6729 - acc: 0.7299 - val_loss: 0.9712 - val_acc: 0.6343\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6766 - acc: 0.7292 - val_loss: 0.9669 - val_acc: 0.6571\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6737 - acc: 0.7344 - val_loss: 0.9728 - val_acc: 0.6457\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6522 - acc: 0.7395 - val_loss: 0.9756 - val_acc: 0.6400\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6657 - acc: 0.7490 - val_loss: 0.9854 - val_acc: 0.6343\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6491 - acc: 0.7401 - val_loss: 0.9870 - val_acc: 0.6171\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6398 - acc: 0.7490 - val_loss: 0.9744 - val_acc: 0.6400\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6324 - acc: 0.7458 - val_loss: 0.9808 - val_acc: 0.6286\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6469 - acc: 0.7414 - val_loss: 0.9875 - val_acc: 0.6514\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6580 - acc: 0.7414 - val_loss: 0.9991 - val_acc: 0.6286\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6419 - acc: 0.7439 - val_loss: 0.9946 - val_acc: 0.6514\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6506 - acc: 0.7401 - val_loss: 0.9896 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6534 - acc: 0.7388 - val_loss: 0.9934 - val_acc: 0.6514\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6404 - acc: 0.7548 - val_loss: 0.9936 - val_acc: 0.6400\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6337 - acc: 0.7529 - val_loss: 0.9908 - val_acc: 0.6514\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6211 - acc: 0.7567 - val_loss: 1.0015 - val_acc: 0.6514\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6270 - acc: 0.7388 - val_loss: 1.0063 - val_acc: 0.6571\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6262 - acc: 0.7644 - val_loss: 1.0144 - val_acc: 0.6343\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6017 - acc: 0.7580 - val_loss: 1.0136 - val_acc: 0.6343\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6121 - acc: 0.7548 - val_loss: 1.0274 - val_acc: 0.6286\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6136 - acc: 0.7612 - val_loss: 1.0270 - val_acc: 0.6343\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6017 - acc: 0.7663 - val_loss: 1.0217 - val_acc: 0.6457\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6085 - acc: 0.7599 - val_loss: 1.0316 - val_acc: 0.6400\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6093 - acc: 0.7612 - val_loss: 1.0255 - val_acc: 0.6457\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5803 - acc: 0.7861 - val_loss: 1.0221 - val_acc: 0.6457\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5967 - acc: 0.7644 - val_loss: 1.0272 - val_acc: 0.6343\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6041 - acc: 0.7554 - val_loss: 1.0271 - val_acc: 0.6400\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6171 - acc: 0.7676 - val_loss: 1.0403 - val_acc: 0.6400\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5818 - acc: 0.7810 - val_loss: 1.0396 - val_acc: 0.6343\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5718 - acc: 0.7791 - val_loss: 1.0409 - val_acc: 0.6514\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5853 - acc: 0.7739 - val_loss: 1.0527 - val_acc: 0.6457\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5674 - acc: 0.7822 - val_loss: 1.0525 - val_acc: 0.6629\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5658 - acc: 0.7880 - val_loss: 1.0547 - val_acc: 0.6629\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5715 - acc: 0.7867 - val_loss: 1.0531 - val_acc: 0.6514\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5866 - acc: 0.7771 - val_loss: 1.0499 - val_acc: 0.6457\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5674 - acc: 0.7771 - val_loss: 1.0653 - val_acc: 0.6514\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5648 - acc: 0.7816 - val_loss: 1.0644 - val_acc: 0.6457\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5625 - acc: 0.7925 - val_loss: 1.0804 - val_acc: 0.6457\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5556 - acc: 0.7874 - val_loss: 1.0799 - val_acc: 0.6457\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5512 - acc: 0.7867 - val_loss: 1.0621 - val_acc: 0.6514\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5551 - acc: 0.7816 - val_loss: 1.0760 - val_acc: 0.6457\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5403 - acc: 0.7937 - val_loss: 1.0706 - val_acc: 0.6571\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5676 - acc: 0.7816 - val_loss: 1.0735 - val_acc: 0.6400\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5467 - acc: 0.7963 - val_loss: 1.0770 - val_acc: 0.6286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5432 - acc: 0.7957 - val_loss: 1.0729 - val_acc: 0.6457\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5655 - acc: 0.7861 - val_loss: 1.0799 - val_acc: 0.6286\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5404 - acc: 0.7912 - val_loss: 1.0813 - val_acc: 0.6286\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5399 - acc: 0.7893 - val_loss: 1.0846 - val_acc: 0.6400\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5442 - acc: 0.7893 - val_loss: 1.1018 - val_acc: 0.6229\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5442 - acc: 0.7880 - val_loss: 1.1096 - val_acc: 0.6457\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5281 - acc: 0.8059 - val_loss: 1.1086 - val_acc: 0.6457\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5379 - acc: 0.8123 - val_loss: 1.1120 - val_acc: 0.6286\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5060 - acc: 0.8110 - val_loss: 1.1099 - val_acc: 0.6343\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5204 - acc: 0.7950 - val_loss: 1.1045 - val_acc: 0.6286\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5208 - acc: 0.8091 - val_loss: 1.1173 - val_acc: 0.6457\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5033 - acc: 0.8065 - val_loss: 1.1222 - val_acc: 0.6400\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5292 - acc: 0.7982 - val_loss: 1.1134 - val_acc: 0.6286\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5133 - acc: 0.8193 - val_loss: 1.1250 - val_acc: 0.6400\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4830 - acc: 0.8218 - val_loss: 1.1291 - val_acc: 0.6343\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5059 - acc: 0.8123 - val_loss: 1.1195 - val_acc: 0.6457\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5066 - acc: 0.8014 - val_loss: 1.1150 - val_acc: 0.6171\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4799 - acc: 0.8199 - val_loss: 1.1333 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5241 - acc: 0.8052 - val_loss: 1.1289 - val_acc: 0.6343\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5013 - acc: 0.8244 - val_loss: 1.1364 - val_acc: 0.6229\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4999 - acc: 0.8110 - val_loss: 1.1384 - val_acc: 0.6229\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4878 - acc: 0.8180 - val_loss: 1.1406 - val_acc: 0.6229\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4966 - acc: 0.8167 - val_loss: 1.1294 - val_acc: 0.6229\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.4897 - acc: 0.8238 - val_loss: 1.1327 - val_acc: 0.6286\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4929 - acc: 0.8097 - val_loss: 1.1551 - val_acc: 0.6286\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4535 - acc: 0.8365 - val_loss: 1.1662 - val_acc: 0.6343\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4781 - acc: 0.8263 - val_loss: 1.1709 - val_acc: 0.6286\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4786 - acc: 0.8033 - val_loss: 1.1508 - val_acc: 0.6229\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5001 - acc: 0.8123 - val_loss: 1.1584 - val_acc: 0.6229\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4870 - acc: 0.8193 - val_loss: 1.1626 - val_acc: 0.6286\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4844 - acc: 0.8231 - val_loss: 1.1655 - val_acc: 0.6229\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4640 - acc: 0.8174 - val_loss: 1.1692 - val_acc: 0.6171\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4784 - acc: 0.8212 - val_loss: 1.1698 - val_acc: 0.6229\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4675 - acc: 0.8238 - val_loss: 1.1875 - val_acc: 0.6229\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4386 - acc: 0.8282 - val_loss: 1.1865 - val_acc: 0.6114\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4330 - acc: 0.8474 - val_loss: 1.1918 - val_acc: 0.6057\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4580 - acc: 0.8289 - val_loss: 1.1888 - val_acc: 0.6114\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4492 - acc: 0.8321 - val_loss: 1.1864 - val_acc: 0.6229\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4503 - acc: 0.8410 - val_loss: 1.1843 - val_acc: 0.6057\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4504 - acc: 0.8391 - val_loss: 1.1965 - val_acc: 0.6057\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4757 - acc: 0.8167 - val_loss: 1.2075 - val_acc: 0.6229\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4520 - acc: 0.8436 - val_loss: 1.2094 - val_acc: 0.6114\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4529 - acc: 0.8314 - val_loss: 1.2104 - val_acc: 0.6114\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4354 - acc: 0.8352 - val_loss: 1.2072 - val_acc: 0.6114\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.4502 - acc: 0.8340 - val_loss: 1.2017 - val_acc: 0.6171\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4328 - acc: 0.8359 - val_loss: 1.2153 - val_acc: 0.6000\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4369 - acc: 0.8404 - val_loss: 1.2211 - val_acc: 0.6057\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4512 - acc: 0.8333 - val_loss: 1.2264 - val_acc: 0.6000\n",
      "194/194 [==============================] - 0s 67us/step\n",
      "1741/1741 [==============================] - 0s 39us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4199 - acc: 0.3308 - val_loss: 1.1144 - val_acc: 0.5771\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 1.2097 - acc: 0.4432 - val_loss: 1.0157 - val_acc: 0.6171\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 1.1283 - acc: 0.5019 - val_loss: 0.9747 - val_acc: 0.6057\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0472 - acc: 0.5307 - val_loss: 0.9520 - val_acc: 0.6171\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 1.0118 - acc: 0.5702 - val_loss: 0.9292 - val_acc: 0.6571\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9997 - acc: 0.5805 - val_loss: 0.9158 - val_acc: 0.6514\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9912 - acc: 0.5773 - val_loss: 0.9045 - val_acc: 0.6629\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9558 - acc: 0.6003 - val_loss: 0.9017 - val_acc: 0.6114\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9462 - acc: 0.6086 - val_loss: 0.8925 - val_acc: 0.6343\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9328 - acc: 0.6169 - val_loss: 0.8997 - val_acc: 0.6457\n",
      "Epoch 11/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9353 - acc: 0.6009 - val_loss: 0.8921 - val_acc: 0.6514\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9026 - acc: 0.6149 - val_loss: 0.8837 - val_acc: 0.6400\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8952 - acc: 0.6354 - val_loss: 0.8863 - val_acc: 0.6229\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9023 - acc: 0.6271 - val_loss: 0.8916 - val_acc: 0.6457\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8885 - acc: 0.6284 - val_loss: 0.8802 - val_acc: 0.6514\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8961 - acc: 0.6360 - val_loss: 0.8746 - val_acc: 0.6514\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8690 - acc: 0.6456 - val_loss: 0.8831 - val_acc: 0.6571\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8424 - acc: 0.6558 - val_loss: 0.8806 - val_acc: 0.6514\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8597 - acc: 0.6494 - val_loss: 0.8762 - val_acc: 0.6686\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8412 - acc: 0.6539 - val_loss: 0.8813 - val_acc: 0.6571\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8435 - acc: 0.6290 - val_loss: 0.8812 - val_acc: 0.6229\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8147 - acc: 0.6622 - val_loss: 0.8940 - val_acc: 0.6286\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7986 - acc: 0.6769 - val_loss: 0.8877 - val_acc: 0.6629\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8182 - acc: 0.6596 - val_loss: 0.8929 - val_acc: 0.6400\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7998 - acc: 0.6596 - val_loss: 0.8845 - val_acc: 0.6457\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8124 - acc: 0.6769 - val_loss: 0.8970 - val_acc: 0.6400\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8086 - acc: 0.6724 - val_loss: 0.8915 - val_acc: 0.6571\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7945 - acc: 0.6743 - val_loss: 0.8871 - val_acc: 0.6686\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7876 - acc: 0.6788 - val_loss: 0.8929 - val_acc: 0.6514\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7912 - acc: 0.6756 - val_loss: 0.8892 - val_acc: 0.6457\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7954 - acc: 0.6750 - val_loss: 0.9110 - val_acc: 0.6286\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7584 - acc: 0.7050 - val_loss: 0.9036 - val_acc: 0.6457\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7584 - acc: 0.7056 - val_loss: 0.9115 - val_acc: 0.6400\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7367 - acc: 0.7011 - val_loss: 0.9105 - val_acc: 0.6286\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7655 - acc: 0.6884 - val_loss: 0.9093 - val_acc: 0.6229\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7528 - acc: 0.6935 - val_loss: 0.9097 - val_acc: 0.6457\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7391 - acc: 0.7133 - val_loss: 0.9097 - val_acc: 0.6400\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7502 - acc: 0.6954 - val_loss: 0.9117 - val_acc: 0.6457\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7565 - acc: 0.7133 - val_loss: 0.9271 - val_acc: 0.6286\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7391 - acc: 0.7031 - val_loss: 0.9228 - val_acc: 0.6400\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7302 - acc: 0.7209 - val_loss: 0.9264 - val_acc: 0.6286\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7321 - acc: 0.7031 - val_loss: 0.9336 - val_acc: 0.6343\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7066 - acc: 0.7171 - val_loss: 0.9313 - val_acc: 0.6343\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7350 - acc: 0.7011 - val_loss: 0.9322 - val_acc: 0.6343\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7047 - acc: 0.7114 - val_loss: 0.9263 - val_acc: 0.6343\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7080 - acc: 0.7216 - val_loss: 0.9311 - val_acc: 0.6343\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6982 - acc: 0.7356 - val_loss: 0.9355 - val_acc: 0.6400\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6940 - acc: 0.7312 - val_loss: 0.9338 - val_acc: 0.6286\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7035 - acc: 0.7126 - val_loss: 0.9413 - val_acc: 0.6400\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6962 - acc: 0.7190 - val_loss: 0.9468 - val_acc: 0.6343\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7063 - acc: 0.7318 - val_loss: 0.9465 - val_acc: 0.6229\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6788 - acc: 0.7261 - val_loss: 0.9517 - val_acc: 0.6400\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6740 - acc: 0.7292 - val_loss: 0.9606 - val_acc: 0.6286\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6820 - acc: 0.7478 - val_loss: 0.9557 - val_acc: 0.6400\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6758 - acc: 0.7280 - val_loss: 0.9562 - val_acc: 0.6286\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6772 - acc: 0.7401 - val_loss: 0.9764 - val_acc: 0.6286\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6626 - acc: 0.7254 - val_loss: 0.9800 - val_acc: 0.6343\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6570 - acc: 0.7363 - val_loss: 0.9745 - val_acc: 0.6400\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6802 - acc: 0.7312 - val_loss: 0.9737 - val_acc: 0.6400\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6533 - acc: 0.7350 - val_loss: 0.9849 - val_acc: 0.6286\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6584 - acc: 0.7324 - val_loss: 0.9790 - val_acc: 0.6400\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6434 - acc: 0.7331 - val_loss: 0.9834 - val_acc: 0.6171\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6582 - acc: 0.7427 - val_loss: 0.9769 - val_acc: 0.6229\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6457 - acc: 0.7535 - val_loss: 0.9817 - val_acc: 0.6171\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6344 - acc: 0.7484 - val_loss: 0.9920 - val_acc: 0.6171\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6423 - acc: 0.7407 - val_loss: 0.9932 - val_acc: 0.6286\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6706 - acc: 0.7375 - val_loss: 0.9985 - val_acc: 0.6286\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6406 - acc: 0.7427 - val_loss: 0.9985 - val_acc: 0.6343\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6156 - acc: 0.7650 - val_loss: 0.9940 - val_acc: 0.6286\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6404 - acc: 0.7644 - val_loss: 0.9973 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6312 - acc: 0.7554 - val_loss: 1.0073 - val_acc: 0.6171\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6393 - acc: 0.7516 - val_loss: 1.0143 - val_acc: 0.6400\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6215 - acc: 0.7586 - val_loss: 1.0211 - val_acc: 0.6400\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6071 - acc: 0.7650 - val_loss: 1.0040 - val_acc: 0.6400\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5991 - acc: 0.7605 - val_loss: 1.0088 - val_acc: 0.6514\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6245 - acc: 0.7542 - val_loss: 1.0205 - val_acc: 0.6343\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5954 - acc: 0.7669 - val_loss: 1.0342 - val_acc: 0.6286\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6003 - acc: 0.7727 - val_loss: 1.0315 - val_acc: 0.6343\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5940 - acc: 0.7791 - val_loss: 1.0281 - val_acc: 0.6286\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5807 - acc: 0.7733 - val_loss: 1.0325 - val_acc: 0.6571\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5978 - acc: 0.7637 - val_loss: 1.0339 - val_acc: 0.6343\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5606 - acc: 0.7880 - val_loss: 1.0448 - val_acc: 0.6114\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5932 - acc: 0.7733 - val_loss: 1.0406 - val_acc: 0.6400\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5783 - acc: 0.7701 - val_loss: 1.0484 - val_acc: 0.6286\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5708 - acc: 0.7810 - val_loss: 1.0440 - val_acc: 0.6343\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5476 - acc: 0.7905 - val_loss: 1.0598 - val_acc: 0.6171\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5767 - acc: 0.7727 - val_loss: 1.0725 - val_acc: 0.6171\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5736 - acc: 0.7784 - val_loss: 1.0755 - val_acc: 0.6114\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5788 - acc: 0.7810 - val_loss: 1.0766 - val_acc: 0.6229\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5701 - acc: 0.7867 - val_loss: 1.0824 - val_acc: 0.6171\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5597 - acc: 0.7899 - val_loss: 1.0640 - val_acc: 0.6286\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5435 - acc: 0.7835 - val_loss: 1.0855 - val_acc: 0.6171\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5503 - acc: 0.7784 - val_loss: 1.0759 - val_acc: 0.6171\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5437 - acc: 0.7829 - val_loss: 1.0724 - val_acc: 0.6171\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5386 - acc: 0.7912 - val_loss: 1.0900 - val_acc: 0.6229\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5497 - acc: 0.7893 - val_loss: 1.1190 - val_acc: 0.6114\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5403 - acc: 0.7950 - val_loss: 1.0981 - val_acc: 0.6286\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5113 - acc: 0.8097 - val_loss: 1.1128 - val_acc: 0.6114\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5571 - acc: 0.7727 - val_loss: 1.1202 - val_acc: 0.6114\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5441 - acc: 0.7880 - val_loss: 1.1304 - val_acc: 0.6171\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5280 - acc: 0.7950 - val_loss: 1.1359 - val_acc: 0.6114\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5257 - acc: 0.7982 - val_loss: 1.1255 - val_acc: 0.6114\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5169 - acc: 0.8001 - val_loss: 1.1210 - val_acc: 0.6229\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5371 - acc: 0.7925 - val_loss: 1.1226 - val_acc: 0.6229\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5091 - acc: 0.8059 - val_loss: 1.1340 - val_acc: 0.6000\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5065 - acc: 0.8084 - val_loss: 1.1267 - val_acc: 0.6229\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.4912 - acc: 0.8161 - val_loss: 1.1285 - val_acc: 0.6114\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4900 - acc: 0.8091 - val_loss: 1.1215 - val_acc: 0.5943\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4923 - acc: 0.8161 - val_loss: 1.1268 - val_acc: 0.6229\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4784 - acc: 0.8212 - val_loss: 1.1342 - val_acc: 0.6057\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5225 - acc: 0.7957 - val_loss: 1.1382 - val_acc: 0.6229\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5050 - acc: 0.8052 - val_loss: 1.1400 - val_acc: 0.6343\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4885 - acc: 0.8072 - val_loss: 1.1535 - val_acc: 0.6229\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5191 - acc: 0.8027 - val_loss: 1.1483 - val_acc: 0.6229\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4653 - acc: 0.8231 - val_loss: 1.1621 - val_acc: 0.6286\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4719 - acc: 0.8206 - val_loss: 1.1536 - val_acc: 0.6286\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4862 - acc: 0.8091 - val_loss: 1.1714 - val_acc: 0.6229\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.4759 - acc: 0.8231 - val_loss: 1.1831 - val_acc: 0.6171\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4851 - acc: 0.8199 - val_loss: 1.1707 - val_acc: 0.6114\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4593 - acc: 0.8257 - val_loss: 1.1679 - val_acc: 0.6171\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4920 - acc: 0.8123 - val_loss: 1.1800 - val_acc: 0.6343\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4776 - acc: 0.8135 - val_loss: 1.1818 - val_acc: 0.6229\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4853 - acc: 0.8142 - val_loss: 1.1914 - val_acc: 0.6229\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4821 - acc: 0.8257 - val_loss: 1.1813 - val_acc: 0.6286\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4767 - acc: 0.8206 - val_loss: 1.1917 - val_acc: 0.6229\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4837 - acc: 0.8078 - val_loss: 1.1985 - val_acc: 0.6057\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.4787 - acc: 0.8155 - val_loss: 1.1823 - val_acc: 0.6171\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4837 - acc: 0.8199 - val_loss: 1.1849 - val_acc: 0.6286\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4615 - acc: 0.8244 - val_loss: 1.2051 - val_acc: 0.6343\n",
      "Epoch 130/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4464 - acc: 0.8423 - val_loss: 1.2000 - val_acc: 0.6114\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4456 - acc: 0.8308 - val_loss: 1.2200 - val_acc: 0.6171\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4657 - acc: 0.8206 - val_loss: 1.2106 - val_acc: 0.6343\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4690 - acc: 0.8308 - val_loss: 1.2254 - val_acc: 0.6229\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4503 - acc: 0.8321 - val_loss: 1.2233 - val_acc: 0.6229\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.4413 - acc: 0.8359 - val_loss: 1.2313 - val_acc: 0.6171\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.4552 - acc: 0.8282 - val_loss: 1.2259 - val_acc: 0.6114\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4334 - acc: 0.8499 - val_loss: 1.2387 - val_acc: 0.6114\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4267 - acc: 0.8365 - val_loss: 1.2386 - val_acc: 0.6171\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4493 - acc: 0.8423 - val_loss: 1.2456 - val_acc: 0.6286\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4470 - acc: 0.8327 - val_loss: 1.2424 - val_acc: 0.6286\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4325 - acc: 0.8378 - val_loss: 1.2481 - val_acc: 0.6343\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4412 - acc: 0.8321 - val_loss: 1.2620 - val_acc: 0.6343\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4255 - acc: 0.8467 - val_loss: 1.2734 - val_acc: 0.6343\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4354 - acc: 0.8346 - val_loss: 1.2667 - val_acc: 0.6229\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4367 - acc: 0.8410 - val_loss: 1.2572 - val_acc: 0.6229\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4095 - acc: 0.8531 - val_loss: 1.2559 - val_acc: 0.6286\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4308 - acc: 0.8327 - val_loss: 1.2592 - val_acc: 0.6229\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4214 - acc: 0.8506 - val_loss: 1.2519 - val_acc: 0.6229\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.4383 - acc: 0.8410 - val_loss: 1.2463 - val_acc: 0.6229\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4137 - acc: 0.8448 - val_loss: 1.2519 - val_acc: 0.6286\n",
      "194/194 [==============================] - 0s 67us/step\n",
      "1741/1741 [==============================] - 0s 48us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4272 - acc: 0.3525 - val_loss: 1.0856 - val_acc: 0.6057\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.2144 - acc: 0.4547 - val_loss: 1.0083 - val_acc: 0.6114\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 1.1175 - acc: 0.5134 - val_loss: 0.9765 - val_acc: 0.6286\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0658 - acc: 0.5370 - val_loss: 0.9341 - val_acc: 0.6514\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.0306 - acc: 0.5421 - val_loss: 0.9210 - val_acc: 0.6629\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 1.0061 - acc: 0.5760 - val_loss: 0.9144 - val_acc: 0.6686\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9984 - acc: 0.5690 - val_loss: 0.9041 - val_acc: 0.6743\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9544 - acc: 0.5977 - val_loss: 0.9059 - val_acc: 0.6514\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.9719 - acc: 0.5900 - val_loss: 0.8924 - val_acc: 0.6686\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.9530 - acc: 0.5964 - val_loss: 0.8954 - val_acc: 0.6571\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9425 - acc: 0.6066 - val_loss: 0.8982 - val_acc: 0.6514\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9414 - acc: 0.5888 - val_loss: 0.9004 - val_acc: 0.6571\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9076 - acc: 0.6252 - val_loss: 0.8923 - val_acc: 0.6629\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8862 - acc: 0.6347 - val_loss: 0.8922 - val_acc: 0.6629\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8964 - acc: 0.6213 - val_loss: 0.8967 - val_acc: 0.6629\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8961 - acc: 0.6194 - val_loss: 0.8944 - val_acc: 0.6629\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8717 - acc: 0.6481 - val_loss: 0.8822 - val_acc: 0.6914\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8912 - acc: 0.6284 - val_loss: 0.8843 - val_acc: 0.6686\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8695 - acc: 0.6271 - val_loss: 0.8852 - val_acc: 0.6800\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8580 - acc: 0.6533 - val_loss: 0.8920 - val_acc: 0.6629\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8450 - acc: 0.6564 - val_loss: 0.8884 - val_acc: 0.6629\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8498 - acc: 0.6469 - val_loss: 0.8960 - val_acc: 0.6629\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8312 - acc: 0.6622 - val_loss: 0.8913 - val_acc: 0.6686\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8358 - acc: 0.6526 - val_loss: 0.8901 - val_acc: 0.6629\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8323 - acc: 0.6526 - val_loss: 0.8974 - val_acc: 0.6629\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8182 - acc: 0.6673 - val_loss: 0.9042 - val_acc: 0.6571\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8136 - acc: 0.6622 - val_loss: 0.8988 - val_acc: 0.6571\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8083 - acc: 0.6762 - val_loss: 0.9025 - val_acc: 0.6400\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8220 - acc: 0.6628 - val_loss: 0.8993 - val_acc: 0.6343\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8071 - acc: 0.6737 - val_loss: 0.8907 - val_acc: 0.6629\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7884 - acc: 0.6705 - val_loss: 0.8921 - val_acc: 0.6514\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7856 - acc: 0.6750 - val_loss: 0.8956 - val_acc: 0.6571\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7736 - acc: 0.6814 - val_loss: 0.9052 - val_acc: 0.6514\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7813 - acc: 0.6686 - val_loss: 0.9130 - val_acc: 0.6571\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7619 - acc: 0.6801 - val_loss: 0.9150 - val_acc: 0.6571\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7658 - acc: 0.6909 - val_loss: 0.9197 - val_acc: 0.6629\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7805 - acc: 0.6941 - val_loss: 0.9213 - val_acc: 0.6457\n",
      "Epoch 38/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7611 - acc: 0.6941 - val_loss: 0.9126 - val_acc: 0.6514\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7618 - acc: 0.6858 - val_loss: 0.9248 - val_acc: 0.6514\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7741 - acc: 0.6833 - val_loss: 0.9180 - val_acc: 0.6571\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7357 - acc: 0.7031 - val_loss: 0.9300 - val_acc: 0.6400\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7402 - acc: 0.7107 - val_loss: 0.9323 - val_acc: 0.6400\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7563 - acc: 0.7024 - val_loss: 0.9344 - val_acc: 0.6400\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7468 - acc: 0.7063 - val_loss: 0.9184 - val_acc: 0.6514\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7308 - acc: 0.6986 - val_loss: 0.9258 - val_acc: 0.6571\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7293 - acc: 0.7056 - val_loss: 0.9404 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7282 - acc: 0.6986 - val_loss: 0.9427 - val_acc: 0.6686\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7098 - acc: 0.7133 - val_loss: 0.9421 - val_acc: 0.6457\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6864 - acc: 0.7216 - val_loss: 0.9455 - val_acc: 0.6514\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7158 - acc: 0.7114 - val_loss: 0.9465 - val_acc: 0.6457\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6954 - acc: 0.7203 - val_loss: 0.9508 - val_acc: 0.6514\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7034 - acc: 0.7139 - val_loss: 0.9543 - val_acc: 0.6629\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6819 - acc: 0.7229 - val_loss: 0.9504 - val_acc: 0.6514\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6953 - acc: 0.7229 - val_loss: 0.9598 - val_acc: 0.6343\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6797 - acc: 0.7273 - val_loss: 0.9584 - val_acc: 0.6571\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6938 - acc: 0.7069 - val_loss: 0.9685 - val_acc: 0.6343\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6960 - acc: 0.7203 - val_loss: 0.9574 - val_acc: 0.6286\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6979 - acc: 0.7139 - val_loss: 0.9639 - val_acc: 0.6571\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6535 - acc: 0.7414 - val_loss: 0.9553 - val_acc: 0.6686\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6597 - acc: 0.7299 - val_loss: 0.9644 - val_acc: 0.6514\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6662 - acc: 0.7241 - val_loss: 0.9641 - val_acc: 0.6457\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6666 - acc: 0.7324 - val_loss: 0.9646 - val_acc: 0.6343\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6669 - acc: 0.7267 - val_loss: 0.9760 - val_acc: 0.6457\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6526 - acc: 0.7414 - val_loss: 0.9856 - val_acc: 0.6400\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6637 - acc: 0.7331 - val_loss: 0.9765 - val_acc: 0.6457\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6625 - acc: 0.7344 - val_loss: 0.9810 - val_acc: 0.6400\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6325 - acc: 0.7497 - val_loss: 0.9931 - val_acc: 0.6457\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6281 - acc: 0.7407 - val_loss: 0.9987 - val_acc: 0.6343\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6289 - acc: 0.7567 - val_loss: 0.9979 - val_acc: 0.6514\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6288 - acc: 0.7535 - val_loss: 0.9940 - val_acc: 0.6343\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6307 - acc: 0.7465 - val_loss: 0.9971 - val_acc: 0.6229\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6334 - acc: 0.7414 - val_loss: 1.0096 - val_acc: 0.6457\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6289 - acc: 0.7439 - val_loss: 1.0103 - val_acc: 0.6400\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6225 - acc: 0.7561 - val_loss: 1.0063 - val_acc: 0.6343\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6039 - acc: 0.7605 - val_loss: 1.0132 - val_acc: 0.6286\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6318 - acc: 0.7465 - val_loss: 1.0262 - val_acc: 0.6229\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5810 - acc: 0.7720 - val_loss: 1.0196 - val_acc: 0.6514\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5873 - acc: 0.7688 - val_loss: 1.0193 - val_acc: 0.6514\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6239 - acc: 0.7478 - val_loss: 1.0276 - val_acc: 0.6171\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5837 - acc: 0.7650 - val_loss: 1.0223 - val_acc: 0.6343\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5848 - acc: 0.7605 - val_loss: 1.0356 - val_acc: 0.6457\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5815 - acc: 0.7701 - val_loss: 1.0226 - val_acc: 0.6229\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5818 - acc: 0.7803 - val_loss: 1.0528 - val_acc: 0.6286\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6007 - acc: 0.7695 - val_loss: 1.0667 - val_acc: 0.6229\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5706 - acc: 0.7669 - val_loss: 1.0644 - val_acc: 0.6171\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5824 - acc: 0.7688 - val_loss: 1.0434 - val_acc: 0.6400\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5892 - acc: 0.7759 - val_loss: 1.0518 - val_acc: 0.6343\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5417 - acc: 0.7861 - val_loss: 1.0520 - val_acc: 0.6229\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5593 - acc: 0.7739 - val_loss: 1.0525 - val_acc: 0.6343\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5607 - acc: 0.7676 - val_loss: 1.0710 - val_acc: 0.6114\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5449 - acc: 0.8014 - val_loss: 1.0748 - val_acc: 0.6229\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5578 - acc: 0.7656 - val_loss: 1.0817 - val_acc: 0.6171\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5666 - acc: 0.7829 - val_loss: 1.0819 - val_acc: 0.6286\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5674 - acc: 0.7746 - val_loss: 1.0808 - val_acc: 0.6286\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5203 - acc: 0.8040 - val_loss: 1.0877 - val_acc: 0.6286\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5470 - acc: 0.7867 - val_loss: 1.1120 - val_acc: 0.6229\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5453 - acc: 0.7899 - val_loss: 1.1003 - val_acc: 0.6114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5319 - acc: 0.7925 - val_loss: 1.1123 - val_acc: 0.6057\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5483 - acc: 0.7918 - val_loss: 1.1068 - val_acc: 0.5943\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5411 - acc: 0.7976 - val_loss: 1.1072 - val_acc: 0.6000\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5333 - acc: 0.7918 - val_loss: 1.1197 - val_acc: 0.6000\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5381 - acc: 0.7854 - val_loss: 1.1285 - val_acc: 0.6114\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5243 - acc: 0.7912 - val_loss: 1.1306 - val_acc: 0.6057\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5419 - acc: 0.7893 - val_loss: 1.1425 - val_acc: 0.6000\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5309 - acc: 0.7925 - val_loss: 1.1571 - val_acc: 0.6057\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5366 - acc: 0.7982 - val_loss: 1.1521 - val_acc: 0.6057\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5319 - acc: 0.7912 - val_loss: 1.1441 - val_acc: 0.6171\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5215 - acc: 0.8020 - val_loss: 1.1421 - val_acc: 0.6229\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4984 - acc: 0.7944 - val_loss: 1.1553 - val_acc: 0.6000\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5136 - acc: 0.7963 - val_loss: 1.1515 - val_acc: 0.6000\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4907 - acc: 0.8282 - val_loss: 1.1514 - val_acc: 0.6114\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5000 - acc: 0.8135 - val_loss: 1.1570 - val_acc: 0.5886\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4751 - acc: 0.8110 - val_loss: 1.1505 - val_acc: 0.6057\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4903 - acc: 0.8040 - val_loss: 1.1450 - val_acc: 0.6286\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5080 - acc: 0.8046 - val_loss: 1.1580 - val_acc: 0.6057\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4855 - acc: 0.8078 - val_loss: 1.1690 - val_acc: 0.5943\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5138 - acc: 0.8072 - val_loss: 1.1650 - val_acc: 0.6057\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4861 - acc: 0.8186 - val_loss: 1.1759 - val_acc: 0.5829\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5044 - acc: 0.8059 - val_loss: 1.1730 - val_acc: 0.5886\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5023 - acc: 0.8065 - val_loss: 1.1743 - val_acc: 0.5943\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4981 - acc: 0.8033 - val_loss: 1.1943 - val_acc: 0.6000\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4660 - acc: 0.8148 - val_loss: 1.1890 - val_acc: 0.6057\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4694 - acc: 0.8263 - val_loss: 1.2022 - val_acc: 0.6171\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4882 - acc: 0.8084 - val_loss: 1.1908 - val_acc: 0.6114\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.4745 - acc: 0.8186 - val_loss: 1.2088 - val_acc: 0.6000\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4791 - acc: 0.8110 - val_loss: 1.2180 - val_acc: 0.6057\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4543 - acc: 0.8276 - val_loss: 1.2001 - val_acc: 0.6114\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4691 - acc: 0.8174 - val_loss: 1.2135 - val_acc: 0.6114\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4486 - acc: 0.8148 - val_loss: 1.2027 - val_acc: 0.6114\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4762 - acc: 0.8244 - val_loss: 1.2269 - val_acc: 0.6057\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4464 - acc: 0.8289 - val_loss: 1.2324 - val_acc: 0.6057\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4521 - acc: 0.8308 - val_loss: 1.2384 - val_acc: 0.5886\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4364 - acc: 0.8436 - val_loss: 1.2472 - val_acc: 0.5943\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4538 - acc: 0.8321 - val_loss: 1.2423 - val_acc: 0.5943\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4608 - acc: 0.8391 - val_loss: 1.2517 - val_acc: 0.6000\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4628 - acc: 0.8193 - val_loss: 1.2487 - val_acc: 0.6000\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.4502 - acc: 0.8346 - val_loss: 1.2532 - val_acc: 0.5886\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4422 - acc: 0.8346 - val_loss: 1.2776 - val_acc: 0.6000\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4651 - acc: 0.8174 - val_loss: 1.2829 - val_acc: 0.5886\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4800 - acc: 0.8097 - val_loss: 1.2731 - val_acc: 0.5829\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.4356 - acc: 0.8429 - val_loss: 1.2949 - val_acc: 0.5829\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.4368 - acc: 0.8365 - val_loss: 1.2747 - val_acc: 0.5943\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.4336 - acc: 0.8372 - val_loss: 1.2661 - val_acc: 0.5943\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4609 - acc: 0.8282 - val_loss: 1.2741 - val_acc: 0.6000\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4061 - acc: 0.8557 - val_loss: 1.2974 - val_acc: 0.6057\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4232 - acc: 0.8340 - val_loss: 1.3034 - val_acc: 0.5886\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4130 - acc: 0.8493 - val_loss: 1.3181 - val_acc: 0.5943\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4235 - acc: 0.8301 - val_loss: 1.3039 - val_acc: 0.5886\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4225 - acc: 0.8429 - val_loss: 1.3030 - val_acc: 0.5943\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.3980 - acc: 0.8442 - val_loss: 1.3116 - val_acc: 0.5943\n",
      "194/194 [==============================] - 0s 74us/step\n",
      "1741/1741 [==============================] - 0s 51us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.4564 - acc: 0.3135 - val_loss: 1.1345 - val_acc: 0.5371\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.2011 - acc: 0.4553 - val_loss: 1.0297 - val_acc: 0.6114\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.1442 - acc: 0.4981 - val_loss: 0.9826 - val_acc: 0.6057\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0862 - acc: 0.5179 - val_loss: 0.9544 - val_acc: 0.6457\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 1.0646 - acc: 0.5466 - val_loss: 0.9449 - val_acc: 0.6514\n",
      "Epoch 6/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.0296 - acc: 0.5556 - val_loss: 0.9379 - val_acc: 0.6743\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9767 - acc: 0.5875 - val_loss: 0.9259 - val_acc: 0.6686\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9934 - acc: 0.5811 - val_loss: 0.9161 - val_acc: 0.6629\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9599 - acc: 0.5907 - val_loss: 0.9153 - val_acc: 0.6400\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9420 - acc: 0.6041 - val_loss: 0.9039 - val_acc: 0.6800\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9407 - acc: 0.5996 - val_loss: 0.9017 - val_acc: 0.6629\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9360 - acc: 0.5939 - val_loss: 0.9046 - val_acc: 0.6686\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9372 - acc: 0.6220 - val_loss: 0.8965 - val_acc: 0.6686\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.9127 - acc: 0.6117 - val_loss: 0.8884 - val_acc: 0.6686\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8948 - acc: 0.6309 - val_loss: 0.8947 - val_acc: 0.6686\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8935 - acc: 0.6322 - val_loss: 0.8938 - val_acc: 0.6743\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8832 - acc: 0.6373 - val_loss: 0.8998 - val_acc: 0.6571\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8727 - acc: 0.6373 - val_loss: 0.9056 - val_acc: 0.6514\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8745 - acc: 0.6303 - val_loss: 0.9027 - val_acc: 0.6571\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8751 - acc: 0.6341 - val_loss: 0.8998 - val_acc: 0.6514\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8565 - acc: 0.6367 - val_loss: 0.9096 - val_acc: 0.6686\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8580 - acc: 0.6424 - val_loss: 0.9032 - val_acc: 0.6457\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8463 - acc: 0.6596 - val_loss: 0.9069 - val_acc: 0.6743\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8335 - acc: 0.6507 - val_loss: 0.9035 - val_acc: 0.6686\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8308 - acc: 0.6635 - val_loss: 0.9048 - val_acc: 0.6400\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8230 - acc: 0.6564 - val_loss: 0.9101 - val_acc: 0.6743\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8252 - acc: 0.6622 - val_loss: 0.9110 - val_acc: 0.6743\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8207 - acc: 0.6731 - val_loss: 0.9084 - val_acc: 0.6857\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8250 - acc: 0.6654 - val_loss: 0.9101 - val_acc: 0.6743\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8077 - acc: 0.6616 - val_loss: 0.9128 - val_acc: 0.6743\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8234 - acc: 0.6641 - val_loss: 0.9098 - val_acc: 0.6686\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8097 - acc: 0.6648 - val_loss: 0.9272 - val_acc: 0.6571\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7857 - acc: 0.6692 - val_loss: 0.9143 - val_acc: 0.6629\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7782 - acc: 0.6750 - val_loss: 0.9294 - val_acc: 0.6514\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7738 - acc: 0.6865 - val_loss: 0.9288 - val_acc: 0.6800\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7924 - acc: 0.6686 - val_loss: 0.9356 - val_acc: 0.6571\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7756 - acc: 0.6852 - val_loss: 0.9282 - val_acc: 0.6743\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7446 - acc: 0.6948 - val_loss: 0.9298 - val_acc: 0.6571\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7810 - acc: 0.6852 - val_loss: 0.9313 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7722 - acc: 0.6788 - val_loss: 0.9255 - val_acc: 0.6514\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7580 - acc: 0.6954 - val_loss: 0.9405 - val_acc: 0.6686\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7376 - acc: 0.7018 - val_loss: 0.9400 - val_acc: 0.6686\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7386 - acc: 0.7152 - val_loss: 0.9459 - val_acc: 0.6800\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7380 - acc: 0.7031 - val_loss: 0.9493 - val_acc: 0.6629\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7299 - acc: 0.7075 - val_loss: 0.9583 - val_acc: 0.6571\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7475 - acc: 0.7139 - val_loss: 0.9491 - val_acc: 0.6800\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7309 - acc: 0.6858 - val_loss: 0.9465 - val_acc: 0.6743\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7235 - acc: 0.7031 - val_loss: 0.9529 - val_acc: 0.6857\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7115 - acc: 0.7222 - val_loss: 0.9564 - val_acc: 0.6857\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7177 - acc: 0.7114 - val_loss: 0.9554 - val_acc: 0.6971\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7097 - acc: 0.7178 - val_loss: 0.9507 - val_acc: 0.6971\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7037 - acc: 0.7203 - val_loss: 0.9627 - val_acc: 0.6743\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.6995 - acc: 0.7235 - val_loss: 0.9674 - val_acc: 0.6686\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6949 - acc: 0.7178 - val_loss: 0.9795 - val_acc: 0.6629\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7153 - acc: 0.7229 - val_loss: 0.9654 - val_acc: 0.6629\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7169 - acc: 0.7082 - val_loss: 0.9683 - val_acc: 0.6914\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7020 - acc: 0.7254 - val_loss: 0.9722 - val_acc: 0.6743\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6746 - acc: 0.7318 - val_loss: 0.9746 - val_acc: 0.6743\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6736 - acc: 0.7286 - val_loss: 0.9659 - val_acc: 0.6857\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6783 - acc: 0.7331 - val_loss: 0.9706 - val_acc: 0.6800\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6648 - acc: 0.7350 - val_loss: 0.9766 - val_acc: 0.6914\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6732 - acc: 0.7280 - val_loss: 0.9783 - val_acc: 0.6800\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6631 - acc: 0.7318 - val_loss: 0.9995 - val_acc: 0.6686\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6345 - acc: 0.7663 - val_loss: 0.9899 - val_acc: 0.6686\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6671 - acc: 0.7337 - val_loss: 0.9982 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6581 - acc: 0.7369 - val_loss: 0.9929 - val_acc: 0.6743\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6579 - acc: 0.7395 - val_loss: 1.0010 - val_acc: 0.6629\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6462 - acc: 0.7350 - val_loss: 1.0018 - val_acc: 0.6800\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6433 - acc: 0.7414 - val_loss: 1.0081 - val_acc: 0.6857\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6276 - acc: 0.7593 - val_loss: 1.0087 - val_acc: 0.6857\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6327 - acc: 0.7599 - val_loss: 1.0210 - val_acc: 0.6800\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6449 - acc: 0.7452 - val_loss: 1.0245 - val_acc: 0.6629\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6302 - acc: 0.7503 - val_loss: 1.0219 - val_acc: 0.6800\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6236 - acc: 0.7593 - val_loss: 1.0199 - val_acc: 0.6914\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6416 - acc: 0.7465 - val_loss: 1.0208 - val_acc: 0.6629\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6354 - acc: 0.7484 - val_loss: 1.0315 - val_acc: 0.6743\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6201 - acc: 0.7656 - val_loss: 1.0365 - val_acc: 0.6629\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6103 - acc: 0.7567 - val_loss: 1.0314 - val_acc: 0.6629\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5975 - acc: 0.7497 - val_loss: 1.0418 - val_acc: 0.6514\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6229 - acc: 0.7586 - val_loss: 1.0488 - val_acc: 0.6571\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6093 - acc: 0.7516 - val_loss: 1.0532 - val_acc: 0.6629\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5768 - acc: 0.7784 - val_loss: 1.0444 - val_acc: 0.6857\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5885 - acc: 0.7625 - val_loss: 1.0441 - val_acc: 0.6571\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5799 - acc: 0.7644 - val_loss: 1.0563 - val_acc: 0.6514\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5880 - acc: 0.7669 - val_loss: 1.0556 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5907 - acc: 0.7644 - val_loss: 1.0658 - val_acc: 0.6571\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6006 - acc: 0.7471 - val_loss: 1.0799 - val_acc: 0.6400\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5847 - acc: 0.7618 - val_loss: 1.0778 - val_acc: 0.6457\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5793 - acc: 0.7618 - val_loss: 1.0700 - val_acc: 0.6343\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5748 - acc: 0.7720 - val_loss: 1.1007 - val_acc: 0.6400\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5658 - acc: 0.7733 - val_loss: 1.0846 - val_acc: 0.6343\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5879 - acc: 0.7822 - val_loss: 1.0896 - val_acc: 0.6400\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5543 - acc: 0.7880 - val_loss: 1.0971 - val_acc: 0.6457\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5579 - acc: 0.7784 - val_loss: 1.0949 - val_acc: 0.6571\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5635 - acc: 0.7912 - val_loss: 1.0979 - val_acc: 0.6629\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5246 - acc: 0.7944 - val_loss: 1.0961 - val_acc: 0.6286\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5332 - acc: 0.7810 - val_loss: 1.0960 - val_acc: 0.6457\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5613 - acc: 0.7778 - val_loss: 1.1124 - val_acc: 0.6400\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.5370 - acc: 0.7886 - val_loss: 1.1045 - val_acc: 0.6400\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5477 - acc: 0.7918 - val_loss: 1.1132 - val_acc: 0.6400\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5587 - acc: 0.7771 - val_loss: 1.1159 - val_acc: 0.6457\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5435 - acc: 0.7918 - val_loss: 1.1386 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5552 - acc: 0.7765 - val_loss: 1.1183 - val_acc: 0.6571\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5441 - acc: 0.7905 - val_loss: 1.1382 - val_acc: 0.6514\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5149 - acc: 0.8040 - val_loss: 1.1362 - val_acc: 0.6171\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5455 - acc: 0.7861 - val_loss: 1.1410 - val_acc: 0.6343\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5579 - acc: 0.7969 - val_loss: 1.1315 - val_acc: 0.6400\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5368 - acc: 0.7880 - val_loss: 1.1350 - val_acc: 0.6343\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5371 - acc: 0.7912 - val_loss: 1.1298 - val_acc: 0.6286\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.5173 - acc: 0.8110 - val_loss: 1.1452 - val_acc: 0.6400\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5042 - acc: 0.7976 - val_loss: 1.1440 - val_acc: 0.6286\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5236 - acc: 0.7861 - val_loss: 1.1482 - val_acc: 0.6400\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5271 - acc: 0.7950 - val_loss: 1.1391 - val_acc: 0.6514\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5004 - acc: 0.8072 - val_loss: 1.1253 - val_acc: 0.6571\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.4950 - acc: 0.8212 - val_loss: 1.1466 - val_acc: 0.6514\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5223 - acc: 0.7848 - val_loss: 1.1721 - val_acc: 0.6400\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5252 - acc: 0.8027 - val_loss: 1.1538 - val_acc: 0.6571\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5138 - acc: 0.8097 - val_loss: 1.1565 - val_acc: 0.6571\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4919 - acc: 0.8078 - val_loss: 1.1622 - val_acc: 0.6400\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4971 - acc: 0.8052 - val_loss: 1.1644 - val_acc: 0.6457\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5006 - acc: 0.8078 - val_loss: 1.1751 - val_acc: 0.6400\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5009 - acc: 0.8020 - val_loss: 1.1769 - val_acc: 0.6400\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4930 - acc: 0.8123 - val_loss: 1.1819 - val_acc: 0.6171\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4641 - acc: 0.8257 - val_loss: 1.1898 - val_acc: 0.6343\n",
      "Epoch 125/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4740 - acc: 0.8059 - val_loss: 1.1912 - val_acc: 0.6286\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4831 - acc: 0.8206 - val_loss: 1.2101 - val_acc: 0.6057\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4713 - acc: 0.8212 - val_loss: 1.2040 - val_acc: 0.6229\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4713 - acc: 0.8097 - val_loss: 1.2224 - val_acc: 0.6171\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.5051 - acc: 0.8142 - val_loss: 1.2252 - val_acc: 0.6343\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4812 - acc: 0.8065 - val_loss: 1.2253 - val_acc: 0.6229\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.4759 - acc: 0.8295 - val_loss: 1.2190 - val_acc: 0.6057\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4593 - acc: 0.8340 - val_loss: 1.2134 - val_acc: 0.6229\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4560 - acc: 0.8225 - val_loss: 1.2234 - val_acc: 0.6171\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4580 - acc: 0.8269 - val_loss: 1.2323 - val_acc: 0.6229\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4702 - acc: 0.8193 - val_loss: 1.2330 - val_acc: 0.6114\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4544 - acc: 0.8391 - val_loss: 1.2418 - val_acc: 0.6171\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4595 - acc: 0.8250 - val_loss: 1.2441 - val_acc: 0.6057\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4462 - acc: 0.8372 - val_loss: 1.2518 - val_acc: 0.6000\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.4599 - acc: 0.8244 - val_loss: 1.2342 - val_acc: 0.6000\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.4640 - acc: 0.8308 - val_loss: 1.2346 - val_acc: 0.6057\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4393 - acc: 0.8359 - val_loss: 1.2340 - val_acc: 0.6400\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4687 - acc: 0.8238 - val_loss: 1.2346 - val_acc: 0.6171\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.4313 - acc: 0.8442 - val_loss: 1.2314 - val_acc: 0.6171\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4664 - acc: 0.8180 - val_loss: 1.2355 - val_acc: 0.6229\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4518 - acc: 0.8442 - val_loss: 1.2428 - val_acc: 0.6171\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4497 - acc: 0.8276 - val_loss: 1.2557 - val_acc: 0.5829\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.4630 - acc: 0.8269 - val_loss: 1.2568 - val_acc: 0.6000\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4494 - acc: 0.8340 - val_loss: 1.2678 - val_acc: 0.6000\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4609 - acc: 0.8212 - val_loss: 1.2605 - val_acc: 0.6286\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4345 - acc: 0.8365 - val_loss: 1.2667 - val_acc: 0.6171\n",
      "194/194 [==============================] - 0s 80us/step\n",
      "1741/1741 [==============================] - 0s 56us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 5s 3ms/step - loss: 1.3993 - acc: 0.3340 - val_loss: 1.1139 - val_acc: 0.5714\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 1.1888 - acc: 0.4725 - val_loss: 1.0250 - val_acc: 0.6000\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 1.1306 - acc: 0.4879 - val_loss: 0.9708 - val_acc: 0.6286\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 1.0668 - acc: 0.5255 - val_loss: 0.9548 - val_acc: 0.6743\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.0410 - acc: 0.5575 - val_loss: 0.9341 - val_acc: 0.6686\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 1.0355 - acc: 0.5434 - val_loss: 0.9149 - val_acc: 0.6914\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.9800 - acc: 0.5651 - val_loss: 0.9076 - val_acc: 0.6857\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.9927 - acc: 0.5792 - val_loss: 0.9052 - val_acc: 0.6914\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.9765 - acc: 0.5849 - val_loss: 0.8969 - val_acc: 0.7029\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.9526 - acc: 0.5894 - val_loss: 0.8966 - val_acc: 0.6971\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.9444 - acc: 0.6098 - val_loss: 0.8940 - val_acc: 0.7086\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9171 - acc: 0.6162 - val_loss: 0.8933 - val_acc: 0.6914\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9208 - acc: 0.6086 - val_loss: 0.8746 - val_acc: 0.6914\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.9025 - acc: 0.6239 - val_loss: 0.8720 - val_acc: 0.7029\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.9016 - acc: 0.6271 - val_loss: 0.8759 - val_acc: 0.7029\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.8907 - acc: 0.6309 - val_loss: 0.8783 - val_acc: 0.6800\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8930 - acc: 0.6252 - val_loss: 0.8864 - val_acc: 0.6857\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8654 - acc: 0.6252 - val_loss: 0.8808 - val_acc: 0.6857\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8679 - acc: 0.6430 - val_loss: 0.8905 - val_acc: 0.6743\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8391 - acc: 0.6545 - val_loss: 0.8879 - val_acc: 0.6800\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8518 - acc: 0.6469 - val_loss: 0.8865 - val_acc: 0.6914\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8447 - acc: 0.6533 - val_loss: 0.8892 - val_acc: 0.6800\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.8204 - acc: 0.6711 - val_loss: 0.8981 - val_acc: 0.6914\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.8325 - acc: 0.6596 - val_loss: 0.8951 - val_acc: 0.6857\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.8278 - acc: 0.6609 - val_loss: 0.8906 - val_acc: 0.6971\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.8184 - acc: 0.6724 - val_loss: 0.8930 - val_acc: 0.6914\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.8286 - acc: 0.6571 - val_loss: 0.8918 - val_acc: 0.6857\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8167 - acc: 0.6545 - val_loss: 0.8885 - val_acc: 0.6686\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.8253 - acc: 0.6654 - val_loss: 0.9029 - val_acc: 0.6743\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.8131 - acc: 0.6673 - val_loss: 0.8921 - val_acc: 0.6629\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.7933 - acc: 0.6699 - val_loss: 0.8990 - val_acc: 0.6743\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.7947 - acc: 0.6660 - val_loss: 0.9005 - val_acc: 0.6857\n",
      "Epoch 33/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.8051 - acc: 0.6743 - val_loss: 0.9070 - val_acc: 0.6686\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.7767 - acc: 0.6801 - val_loss: 0.8969 - val_acc: 0.6800\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.7574 - acc: 0.6845 - val_loss: 0.9067 - val_acc: 0.6800\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.7610 - acc: 0.6916 - val_loss: 0.9066 - val_acc: 0.6743\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.7753 - acc: 0.6884 - val_loss: 0.9077 - val_acc: 0.6686\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.7438 - acc: 0.6954 - val_loss: 0.9082 - val_acc: 0.6743\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 116us/step - loss: 0.7450 - acc: 0.6935 - val_loss: 0.9106 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.7511 - acc: 0.7050 - val_loss: 0.9146 - val_acc: 0.6686\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 131us/step - loss: 0.7470 - acc: 0.7063 - val_loss: 0.9182 - val_acc: 0.6514\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7354 - acc: 0.6928 - val_loss: 0.9287 - val_acc: 0.6629\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7232 - acc: 0.6999 - val_loss: 0.9160 - val_acc: 0.6686\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.7128 - acc: 0.7178 - val_loss: 0.9312 - val_acc: 0.6457\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7251 - acc: 0.7037 - val_loss: 0.9384 - val_acc: 0.6457\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7197 - acc: 0.7043 - val_loss: 0.9398 - val_acc: 0.6571\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7193 - acc: 0.7165 - val_loss: 0.9379 - val_acc: 0.6514\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.7077 - acc: 0.7120 - val_loss: 0.9446 - val_acc: 0.6571\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 113us/step - loss: 0.6906 - acc: 0.7197 - val_loss: 0.9357 - val_acc: 0.6400\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 134us/step - loss: 0.7070 - acc: 0.7152 - val_loss: 0.9394 - val_acc: 0.6571\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.7003 - acc: 0.7146 - val_loss: 0.9508 - val_acc: 0.6514\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.6989 - acc: 0.7095 - val_loss: 0.9571 - val_acc: 0.6514\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.6848 - acc: 0.7261 - val_loss: 0.9580 - val_acc: 0.6514\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.6858 - acc: 0.7273 - val_loss: 0.9613 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 127us/step - loss: 0.6969 - acc: 0.7248 - val_loss: 0.9501 - val_acc: 0.6571\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.6803 - acc: 0.7286 - val_loss: 0.9759 - val_acc: 0.6514\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6622 - acc: 0.7273 - val_loss: 0.9608 - val_acc: 0.6800\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.6738 - acc: 0.7292 - val_loss: 0.9764 - val_acc: 0.6457\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.6756 - acc: 0.7171 - val_loss: 0.9757 - val_acc: 0.6400\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.6639 - acc: 0.7267 - val_loss: 0.9615 - val_acc: 0.6457\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.6740 - acc: 0.7350 - val_loss: 0.9661 - val_acc: 0.6514\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.6259 - acc: 0.7414 - val_loss: 0.9733 - val_acc: 0.6571\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.6528 - acc: 0.7407 - val_loss: 0.9711 - val_acc: 0.6514\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.6544 - acc: 0.7344 - val_loss: 0.9690 - val_acc: 0.6514\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.6400 - acc: 0.7580 - val_loss: 0.9717 - val_acc: 0.6457\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.6278 - acc: 0.7567 - val_loss: 0.9797 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.6595 - acc: 0.7299 - val_loss: 0.9751 - val_acc: 0.6571\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.6382 - acc: 0.7395 - val_loss: 0.9857 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.6352 - acc: 0.7395 - val_loss: 1.0031 - val_acc: 0.6457\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.6252 - acc: 0.7427 - val_loss: 0.9925 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.6173 - acc: 0.7529 - val_loss: 0.9969 - val_acc: 0.6400\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.6005 - acc: 0.7599 - val_loss: 1.0054 - val_acc: 0.6457\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.6184 - acc: 0.7529 - val_loss: 1.0089 - val_acc: 0.6400\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.6206 - acc: 0.7567 - val_loss: 1.0162 - val_acc: 0.6457\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.6292 - acc: 0.7458 - val_loss: 1.0038 - val_acc: 0.6514\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.6196 - acc: 0.7478 - val_loss: 0.9984 - val_acc: 0.6571\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.5852 - acc: 0.7644 - val_loss: 1.0078 - val_acc: 0.6457\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5895 - acc: 0.7688 - val_loss: 1.0064 - val_acc: 0.6457\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5723 - acc: 0.7727 - val_loss: 1.0172 - val_acc: 0.6457\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.5781 - acc: 0.7605 - val_loss: 1.0163 - val_acc: 0.6514\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.5992 - acc: 0.7618 - val_loss: 1.0287 - val_acc: 0.6286\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.5910 - acc: 0.7605 - val_loss: 1.0283 - val_acc: 0.6400\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.5789 - acc: 0.7848 - val_loss: 1.0313 - val_acc: 0.6343\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.5672 - acc: 0.7759 - val_loss: 1.0357 - val_acc: 0.6457\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5782 - acc: 0.7880 - val_loss: 1.0401 - val_acc: 0.6457\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.5655 - acc: 0.7791 - val_loss: 1.0466 - val_acc: 0.6457\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 107us/step - loss: 0.5897 - acc: 0.7797 - val_loss: 1.0470 - val_acc: 0.6400\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.5683 - acc: 0.7752 - val_loss: 1.0460 - val_acc: 0.6229\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.5988 - acc: 0.7669 - val_loss: 1.0390 - val_acc: 0.6343\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 115us/step - loss: 0.5493 - acc: 0.7720 - val_loss: 1.0477 - val_acc: 0.6457\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 126us/step - loss: 0.5580 - acc: 0.7880 - val_loss: 1.0636 - val_acc: 0.6571\n",
      "Epoch 92/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.5616 - acc: 0.7746 - val_loss: 1.0576 - val_acc: 0.6343\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5508 - acc: 0.7810 - val_loss: 1.0467 - val_acc: 0.6457\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5748 - acc: 0.7739 - val_loss: 1.0559 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.5418 - acc: 0.7969 - val_loss: 1.0593 - val_acc: 0.6571\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5487 - acc: 0.7771 - val_loss: 1.0555 - val_acc: 0.6457\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.5664 - acc: 0.7746 - val_loss: 1.0742 - val_acc: 0.6343\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.5266 - acc: 0.8103 - val_loss: 1.0751 - val_acc: 0.6343\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5247 - acc: 0.7835 - val_loss: 1.0809 - val_acc: 0.6400\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5434 - acc: 0.7842 - val_loss: 1.0779 - val_acc: 0.6457\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5456 - acc: 0.7829 - val_loss: 1.0803 - val_acc: 0.6400\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5133 - acc: 0.8129 - val_loss: 1.0877 - val_acc: 0.6514\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.5211 - acc: 0.7989 - val_loss: 1.0944 - val_acc: 0.6629\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.5241 - acc: 0.7874 - val_loss: 1.0945 - val_acc: 0.6457\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5201 - acc: 0.7963 - val_loss: 1.1106 - val_acc: 0.6514\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4999 - acc: 0.8078 - val_loss: 1.1207 - val_acc: 0.6343\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5055 - acc: 0.7969 - val_loss: 1.1165 - val_acc: 0.6229\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4931 - acc: 0.8116 - val_loss: 1.1142 - val_acc: 0.6457\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4969 - acc: 0.8174 - val_loss: 1.1394 - val_acc: 0.6400\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5189 - acc: 0.7982 - val_loss: 1.1374 - val_acc: 0.6286\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5086 - acc: 0.8155 - val_loss: 1.1325 - val_acc: 0.6343\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4909 - acc: 0.8174 - val_loss: 1.1260 - val_acc: 0.6400\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4979 - acc: 0.8167 - val_loss: 1.1293 - val_acc: 0.6400\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5056 - acc: 0.8008 - val_loss: 1.1372 - val_acc: 0.6400\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5050 - acc: 0.7931 - val_loss: 1.1294 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4703 - acc: 0.8250 - val_loss: 1.1328 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4908 - acc: 0.8027 - val_loss: 1.1350 - val_acc: 0.6400\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4674 - acc: 0.8135 - val_loss: 1.1546 - val_acc: 0.6229\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4810 - acc: 0.8129 - val_loss: 1.1597 - val_acc: 0.6229\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4959 - acc: 0.8186 - val_loss: 1.1717 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4812 - acc: 0.8218 - val_loss: 1.1572 - val_acc: 0.6343\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4870 - acc: 0.8097 - val_loss: 1.1328 - val_acc: 0.6457\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4763 - acc: 0.8167 - val_loss: 1.1542 - val_acc: 0.6171\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4652 - acc: 0.8263 - val_loss: 1.1501 - val_acc: 0.6400\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4693 - acc: 0.8065 - val_loss: 1.1587 - val_acc: 0.6457\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4736 - acc: 0.8206 - val_loss: 1.1547 - val_acc: 0.6457\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4801 - acc: 0.8206 - val_loss: 1.1503 - val_acc: 0.6400\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4877 - acc: 0.8123 - val_loss: 1.1715 - val_acc: 0.6457\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4590 - acc: 0.8231 - val_loss: 1.1509 - val_acc: 0.6286\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.4727 - acc: 0.8218 - val_loss: 1.1791 - val_acc: 0.6343\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4559 - acc: 0.8238 - val_loss: 1.1974 - val_acc: 0.6286\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4548 - acc: 0.8340 - val_loss: 1.1828 - val_acc: 0.6400\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4686 - acc: 0.8282 - val_loss: 1.1818 - val_acc: 0.6457\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4465 - acc: 0.8321 - val_loss: 1.1984 - val_acc: 0.6400\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4275 - acc: 0.8378 - val_loss: 1.2126 - val_acc: 0.6457\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4548 - acc: 0.8333 - val_loss: 1.2213 - val_acc: 0.6457\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4328 - acc: 0.8372 - val_loss: 1.2302 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4647 - acc: 0.8135 - val_loss: 1.2261 - val_acc: 0.6400\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4357 - acc: 0.8346 - val_loss: 1.2197 - val_acc: 0.6343\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4379 - acc: 0.8378 - val_loss: 1.2206 - val_acc: 0.6343\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4311 - acc: 0.8276 - val_loss: 1.2049 - val_acc: 0.6400\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4537 - acc: 0.8238 - val_loss: 1.2132 - val_acc: 0.6457\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4686 - acc: 0.8250 - val_loss: 1.2443 - val_acc: 0.6400\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4352 - acc: 0.8404 - val_loss: 1.2368 - val_acc: 0.6514\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4400 - acc: 0.8333 - val_loss: 1.2138 - val_acc: 0.6400\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4321 - acc: 0.8423 - val_loss: 1.2336 - val_acc: 0.6343\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4089 - acc: 0.8384 - val_loss: 1.2532 - val_acc: 0.6400\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4257 - acc: 0.8442 - val_loss: 1.2526 - val_acc: 0.6400\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4536 - acc: 0.8250 - val_loss: 1.2555 - val_acc: 0.6343\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4260 - acc: 0.8410 - val_loss: 1.2689 - val_acc: 0.6343\n",
      "194/194 [==============================] - 0s 68us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741/1741 [==============================] - 0s 53us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.4111 - acc: 0.3421 - val_loss: 1.1110 - val_acc: 0.5886\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.1789 - acc: 0.4659 - val_loss: 1.0160 - val_acc: 0.6457\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.1202 - acc: 0.5086 - val_loss: 0.9766 - val_acc: 0.6286\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 1.0662 - acc: 0.5233 - val_loss: 0.9605 - val_acc: 0.6457\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0297 - acc: 0.5578 - val_loss: 0.9490 - val_acc: 0.6571\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 1.0152 - acc: 0.5833 - val_loss: 0.9225 - val_acc: 0.6571\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9906 - acc: 0.5826 - val_loss: 0.9230 - val_acc: 0.6571\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.9800 - acc: 0.5846 - val_loss: 0.9128 - val_acc: 0.6800\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9570 - acc: 0.5839 - val_loss: 0.9062 - val_acc: 0.6743\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9511 - acc: 0.5941 - val_loss: 0.9096 - val_acc: 0.6457\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9138 - acc: 0.6146 - val_loss: 0.9075 - val_acc: 0.6686\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.9143 - acc: 0.6286 - val_loss: 0.9087 - val_acc: 0.6800\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9057 - acc: 0.6158 - val_loss: 0.9029 - val_acc: 0.6743\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8971 - acc: 0.6209 - val_loss: 0.9069 - val_acc: 0.6686\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.9010 - acc: 0.6165 - val_loss: 0.9036 - val_acc: 0.6914\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8873 - acc: 0.6235 - val_loss: 0.8965 - val_acc: 0.6914\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8773 - acc: 0.6343 - val_loss: 0.8937 - val_acc: 0.6914\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.8681 - acc: 0.6401 - val_loss: 0.8963 - val_acc: 0.6914\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8565 - acc: 0.6573 - val_loss: 0.9062 - val_acc: 0.6857\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.8702 - acc: 0.6324 - val_loss: 0.9001 - val_acc: 0.6914\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.8392 - acc: 0.6490 - val_loss: 0.9110 - val_acc: 0.6857\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8301 - acc: 0.6548 - val_loss: 0.9085 - val_acc: 0.6857\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.8383 - acc: 0.6477 - val_loss: 0.9109 - val_acc: 0.6743\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.8227 - acc: 0.6477 - val_loss: 0.9102 - val_acc: 0.6800\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.8098 - acc: 0.6643 - val_loss: 0.9211 - val_acc: 0.6914\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8214 - acc: 0.6624 - val_loss: 0.9215 - val_acc: 0.6914\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.8407 - acc: 0.6541 - val_loss: 0.9253 - val_acc: 0.6914\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.8059 - acc: 0.6726 - val_loss: 0.9278 - val_acc: 0.6800\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.8053 - acc: 0.6713 - val_loss: 0.9335 - val_acc: 0.6857\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 110us/step - loss: 0.7731 - acc: 0.6643 - val_loss: 0.9312 - val_acc: 0.6743\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7866 - acc: 0.6713 - val_loss: 0.9318 - val_acc: 0.6914\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.7921 - acc: 0.6784 - val_loss: 0.9406 - val_acc: 0.6743\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.7954 - acc: 0.6675 - val_loss: 0.9315 - val_acc: 0.6914\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.7772 - acc: 0.6905 - val_loss: 0.9337 - val_acc: 0.6857\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 133us/step - loss: 0.7688 - acc: 0.6981 - val_loss: 0.9347 - val_acc: 0.6800\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 132us/step - loss: 0.7657 - acc: 0.6860 - val_loss: 0.9365 - val_acc: 0.6914\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.7686 - acc: 0.6930 - val_loss: 0.9472 - val_acc: 0.6629\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7604 - acc: 0.6809 - val_loss: 0.9470 - val_acc: 0.6800\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7547 - acc: 0.6899 - val_loss: 0.9506 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7505 - acc: 0.6892 - val_loss: 0.9593 - val_acc: 0.6686\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7454 - acc: 0.6918 - val_loss: 0.9674 - val_acc: 0.6743\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 142us/step - loss: 0.7382 - acc: 0.6969 - val_loss: 0.9636 - val_acc: 0.6800\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 140us/step - loss: 0.7407 - acc: 0.6950 - val_loss: 0.9622 - val_acc: 0.6857\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 143us/step - loss: 0.7328 - acc: 0.7020 - val_loss: 0.9689 - val_acc: 0.6800\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 151us/step - loss: 0.7275 - acc: 0.7064 - val_loss: 0.9746 - val_acc: 0.6629\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.7327 - acc: 0.7045 - val_loss: 0.9868 - val_acc: 0.6743\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7081 - acc: 0.7192 - val_loss: 0.9746 - val_acc: 0.6800\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7125 - acc: 0.6937 - val_loss: 0.9738 - val_acc: 0.6743\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6861 - acc: 0.7154 - val_loss: 0.9762 - val_acc: 0.6800\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7012 - acc: 0.7269 - val_loss: 0.9857 - val_acc: 0.6743\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6920 - acc: 0.7179 - val_loss: 1.0039 - val_acc: 0.6743\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7063 - acc: 0.7103 - val_loss: 0.9903 - val_acc: 0.6686\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6889 - acc: 0.7275 - val_loss: 0.9940 - val_acc: 0.6686\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6817 - acc: 0.7237 - val_loss: 1.0007 - val_acc: 0.6629\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6773 - acc: 0.7096 - val_loss: 1.0148 - val_acc: 0.6629\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6836 - acc: 0.7301 - val_loss: 1.0111 - val_acc: 0.6571\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6810 - acc: 0.7224 - val_loss: 1.0149 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6554 - acc: 0.7294 - val_loss: 1.0257 - val_acc: 0.6629\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6780 - acc: 0.7198 - val_loss: 1.0100 - val_acc: 0.6629\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6588 - acc: 0.7237 - val_loss: 1.0178 - val_acc: 0.6457\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6628 - acc: 0.7275 - val_loss: 1.0308 - val_acc: 0.6629\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6465 - acc: 0.7409 - val_loss: 1.0322 - val_acc: 0.6686\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6414 - acc: 0.7498 - val_loss: 1.0230 - val_acc: 0.6629\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6400 - acc: 0.7511 - val_loss: 1.0308 - val_acc: 0.6571\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6586 - acc: 0.7313 - val_loss: 1.0379 - val_acc: 0.6514\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6390 - acc: 0.7505 - val_loss: 1.0347 - val_acc: 0.6800\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6271 - acc: 0.7454 - val_loss: 1.0328 - val_acc: 0.6914\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6171 - acc: 0.7492 - val_loss: 1.0276 - val_acc: 0.6686\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6052 - acc: 0.7581 - val_loss: 1.0436 - val_acc: 0.6629\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6360 - acc: 0.7364 - val_loss: 1.0463 - val_acc: 0.6743\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6176 - acc: 0.7530 - val_loss: 1.0490 - val_acc: 0.6686\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6168 - acc: 0.7671 - val_loss: 1.0578 - val_acc: 0.6457\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6121 - acc: 0.7524 - val_loss: 1.0547 - val_acc: 0.6629\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6060 - acc: 0.7581 - val_loss: 1.0600 - val_acc: 0.6514\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6055 - acc: 0.7537 - val_loss: 1.0620 - val_acc: 0.6571\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5912 - acc: 0.7735 - val_loss: 1.0665 - val_acc: 0.6686\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6214 - acc: 0.7581 - val_loss: 1.0777 - val_acc: 0.6514\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5858 - acc: 0.7747 - val_loss: 1.0688 - val_acc: 0.6800\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5927 - acc: 0.7754 - val_loss: 1.0784 - val_acc: 0.6629\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5720 - acc: 0.7798 - val_loss: 1.0778 - val_acc: 0.6800\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.5950 - acc: 0.7537 - val_loss: 1.0807 - val_acc: 0.6571\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6080 - acc: 0.7613 - val_loss: 1.0912 - val_acc: 0.6571\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.5728 - acc: 0.7645 - val_loss: 1.0963 - val_acc: 0.6629\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5585 - acc: 0.7786 - val_loss: 1.0920 - val_acc: 0.6514\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5733 - acc: 0.7703 - val_loss: 1.1023 - val_acc: 0.6629\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5744 - acc: 0.7811 - val_loss: 1.0962 - val_acc: 0.6514\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.5531 - acc: 0.7773 - val_loss: 1.1036 - val_acc: 0.6571\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5742 - acc: 0.7811 - val_loss: 1.1123 - val_acc: 0.6571\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5677 - acc: 0.7728 - val_loss: 1.1036 - val_acc: 0.6686\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5630 - acc: 0.7773 - val_loss: 1.1156 - val_acc: 0.6686\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.5533 - acc: 0.7798 - val_loss: 1.1151 - val_acc: 0.6629\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5316 - acc: 0.7837 - val_loss: 1.1138 - val_acc: 0.6743\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5528 - acc: 0.7913 - val_loss: 1.1390 - val_acc: 0.6629\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5549 - acc: 0.7677 - val_loss: 1.1380 - val_acc: 0.6686\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5553 - acc: 0.7888 - val_loss: 1.1354 - val_acc: 0.6800\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5377 - acc: 0.7811 - val_loss: 1.1428 - val_acc: 0.6743\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5328 - acc: 0.8047 - val_loss: 1.1353 - val_acc: 0.6629\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5381 - acc: 0.7786 - val_loss: 1.1607 - val_acc: 0.6743\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5307 - acc: 0.7888 - val_loss: 1.1550 - val_acc: 0.6743\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5383 - acc: 0.7900 - val_loss: 1.1478 - val_acc: 0.6686\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.5280 - acc: 0.7779 - val_loss: 1.1534 - val_acc: 0.6686\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5182 - acc: 0.7990 - val_loss: 1.1499 - val_acc: 0.6571\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5031 - acc: 0.8034 - val_loss: 1.1717 - val_acc: 0.6629\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5155 - acc: 0.8034 - val_loss: 1.1707 - val_acc: 0.6571\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4982 - acc: 0.8022 - val_loss: 1.1842 - val_acc: 0.6457\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5115 - acc: 0.8041 - val_loss: 1.1878 - val_acc: 0.6514\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5265 - acc: 0.7958 - val_loss: 1.1725 - val_acc: 0.6571\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4910 - acc: 0.8124 - val_loss: 1.1880 - val_acc: 0.6571\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5221 - acc: 0.8034 - val_loss: 1.1746 - val_acc: 0.6686\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5187 - acc: 0.8003 - val_loss: 1.1815 - val_acc: 0.6686\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5182 - acc: 0.7920 - val_loss: 1.1784 - val_acc: 0.6629\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.5105 - acc: 0.7932 - val_loss: 1.1827 - val_acc: 0.6629\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.5125 - acc: 0.8041 - val_loss: 1.1949 - val_acc: 0.6800\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4896 - acc: 0.8143 - val_loss: 1.1855 - val_acc: 0.6629\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4892 - acc: 0.8200 - val_loss: 1.1887 - val_acc: 0.6629\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4804 - acc: 0.8168 - val_loss: 1.2017 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5008 - acc: 0.8041 - val_loss: 1.2061 - val_acc: 0.6514\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4671 - acc: 0.8277 - val_loss: 1.2103 - val_acc: 0.6514\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5111 - acc: 0.8022 - val_loss: 1.2060 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4839 - acc: 0.8149 - val_loss: 1.2171 - val_acc: 0.6514\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4931 - acc: 0.8079 - val_loss: 1.2094 - val_acc: 0.6629\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4542 - acc: 0.8315 - val_loss: 1.2310 - val_acc: 0.6514\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4637 - acc: 0.8162 - val_loss: 1.2592 - val_acc: 0.6343\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.4905 - acc: 0.8137 - val_loss: 1.2481 - val_acc: 0.6514\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4939 - acc: 0.8168 - val_loss: 1.2519 - val_acc: 0.6286\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4633 - acc: 0.8207 - val_loss: 1.2429 - val_acc: 0.6457\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4591 - acc: 0.8194 - val_loss: 1.2309 - val_acc: 0.6571\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4831 - acc: 0.8283 - val_loss: 1.2430 - val_acc: 0.6286\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4673 - acc: 0.8194 - val_loss: 1.2489 - val_acc: 0.6629\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4764 - acc: 0.8213 - val_loss: 1.2492 - val_acc: 0.6571\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4558 - acc: 0.8232 - val_loss: 1.2415 - val_acc: 0.6571\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4835 - acc: 0.8162 - val_loss: 1.2427 - val_acc: 0.6571\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4617 - acc: 0.8194 - val_loss: 1.2669 - val_acc: 0.6400\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.4674 - acc: 0.8168 - val_loss: 1.2809 - val_acc: 0.6400\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.4438 - acc: 0.8430 - val_loss: 1.2751 - val_acc: 0.6514\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4604 - acc: 0.8251 - val_loss: 1.2825 - val_acc: 0.6457\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4621 - acc: 0.8226 - val_loss: 1.2908 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4668 - acc: 0.8251 - val_loss: 1.2774 - val_acc: 0.6629\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4469 - acc: 0.8290 - val_loss: 1.2802 - val_acc: 0.6629\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4183 - acc: 0.8360 - val_loss: 1.2799 - val_acc: 0.6629\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4333 - acc: 0.8456 - val_loss: 1.2872 - val_acc: 0.6400\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4365 - acc: 0.8449 - val_loss: 1.2800 - val_acc: 0.6514\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4517 - acc: 0.8251 - val_loss: 1.2947 - val_acc: 0.6457\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4519 - acc: 0.8379 - val_loss: 1.2980 - val_acc: 0.6457\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4246 - acc: 0.8354 - val_loss: 1.2944 - val_acc: 0.6571\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4215 - acc: 0.8417 - val_loss: 1.2951 - val_acc: 0.6571\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4126 - acc: 0.8424 - val_loss: 1.3258 - val_acc: 0.6457\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4338 - acc: 0.8405 - val_loss: 1.3076 - val_acc: 0.6629\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4070 - acc: 0.8475 - val_loss: 1.3249 - val_acc: 0.6457\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4218 - acc: 0.8373 - val_loss: 1.3294 - val_acc: 0.6229\n",
      "193/193 [==============================] - 0s 79us/step\n",
      "1742/1742 [==============================] - 0s 49us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.3663 - acc: 0.3459 - val_loss: 1.1574 - val_acc: 0.4971\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.2064 - acc: 0.4742 - val_loss: 1.0527 - val_acc: 0.6229\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 1.1046 - acc: 0.5080 - val_loss: 0.9990 - val_acc: 0.6114\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.0904 - acc: 0.5297 - val_loss: 0.9611 - val_acc: 0.6400\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.0142 - acc: 0.5750 - val_loss: 0.9316 - val_acc: 0.6514\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9923 - acc: 0.5660 - val_loss: 0.9220 - val_acc: 0.6629\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9772 - acc: 0.5980 - val_loss: 0.9098 - val_acc: 0.6571\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9637 - acc: 0.5954 - val_loss: 0.9089 - val_acc: 0.6571\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9634 - acc: 0.5929 - val_loss: 0.8792 - val_acc: 0.6857\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9442 - acc: 0.6031 - val_loss: 0.8769 - val_acc: 0.6514\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9244 - acc: 0.6043 - val_loss: 0.8795 - val_acc: 0.6686\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9018 - acc: 0.6241 - val_loss: 0.8699 - val_acc: 0.6743\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9167 - acc: 0.6197 - val_loss: 0.8705 - val_acc: 0.6743\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8946 - acc: 0.6311 - val_loss: 0.8636 - val_acc: 0.6571\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8747 - acc: 0.6273 - val_loss: 0.8586 - val_acc: 0.6743\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8789 - acc: 0.6241 - val_loss: 0.8622 - val_acc: 0.6686\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8685 - acc: 0.6401 - val_loss: 0.8541 - val_acc: 0.6571\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.8517 - acc: 0.6554 - val_loss: 0.8576 - val_acc: 0.6914\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8381 - acc: 0.6599 - val_loss: 0.8557 - val_acc: 0.6686\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8432 - acc: 0.6637 - val_loss: 0.8528 - val_acc: 0.6857\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8256 - acc: 0.6605 - val_loss: 0.8618 - val_acc: 0.6686\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.8325 - acc: 0.6586 - val_loss: 0.8615 - val_acc: 0.6571\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8134 - acc: 0.6586 - val_loss: 0.8656 - val_acc: 0.6857\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8037 - acc: 0.6707 - val_loss: 0.8609 - val_acc: 0.6800\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8185 - acc: 0.6567 - val_loss: 0.8595 - val_acc: 0.6686\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7964 - acc: 0.6860 - val_loss: 0.8639 - val_acc: 0.6400\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7974 - acc: 0.6713 - val_loss: 0.8632 - val_acc: 0.6629\n",
      "Epoch 28/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8013 - acc: 0.6688 - val_loss: 0.8770 - val_acc: 0.6571\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7744 - acc: 0.6867 - val_loss: 0.8715 - val_acc: 0.6571\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7648 - acc: 0.6924 - val_loss: 0.8637 - val_acc: 0.6743\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7781 - acc: 0.6777 - val_loss: 0.8659 - val_acc: 0.6514\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7779 - acc: 0.6828 - val_loss: 0.8683 - val_acc: 0.6571\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7764 - acc: 0.6656 - val_loss: 0.8649 - val_acc: 0.6571\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7707 - acc: 0.6892 - val_loss: 0.8696 - val_acc: 0.6514\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 122us/step - loss: 0.7649 - acc: 0.6962 - val_loss: 0.8691 - val_acc: 0.6686\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 144us/step - loss: 0.7287 - acc: 0.7064 - val_loss: 0.8638 - val_acc: 0.6629\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.7609 - acc: 0.6822 - val_loss: 0.8702 - val_acc: 0.6629\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7254 - acc: 0.7084 - val_loss: 0.8665 - val_acc: 0.6743\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.7364 - acc: 0.6988 - val_loss: 0.8666 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7468 - acc: 0.7001 - val_loss: 0.8667 - val_acc: 0.6686\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 119us/step - loss: 0.7136 - acc: 0.7205 - val_loss: 0.8688 - val_acc: 0.6629\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7233 - acc: 0.7033 - val_loss: 0.8667 - val_acc: 0.6457\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.7219 - acc: 0.6994 - val_loss: 0.8694 - val_acc: 0.6457\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7272 - acc: 0.6988 - val_loss: 0.8717 - val_acc: 0.6457\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7245 - acc: 0.7109 - val_loss: 0.8723 - val_acc: 0.6457\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7006 - acc: 0.7109 - val_loss: 0.8861 - val_acc: 0.6571\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7166 - acc: 0.6981 - val_loss: 0.8792 - val_acc: 0.6457\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7078 - acc: 0.7128 - val_loss: 0.8806 - val_acc: 0.6400\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6952 - acc: 0.7294 - val_loss: 0.8786 - val_acc: 0.6457\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6857 - acc: 0.7396 - val_loss: 0.8730 - val_acc: 0.6514\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.6857 - acc: 0.7275 - val_loss: 0.8931 - val_acc: 0.6457\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6790 - acc: 0.7262 - val_loss: 0.8839 - val_acc: 0.6514\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6892 - acc: 0.7167 - val_loss: 0.8812 - val_acc: 0.6457\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6683 - acc: 0.7294 - val_loss: 0.8870 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6922 - acc: 0.7077 - val_loss: 0.8819 - val_acc: 0.6400\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6642 - acc: 0.7358 - val_loss: 0.8780 - val_acc: 0.6629\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6582 - acc: 0.7371 - val_loss: 0.8877 - val_acc: 0.6514\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6486 - acc: 0.7396 - val_loss: 0.8812 - val_acc: 0.6629\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6515 - acc: 0.7364 - val_loss: 0.8922 - val_acc: 0.6571\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6424 - acc: 0.7428 - val_loss: 0.8964 - val_acc: 0.6457\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6542 - acc: 0.7396 - val_loss: 0.8918 - val_acc: 0.6686\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6200 - acc: 0.7632 - val_loss: 0.9065 - val_acc: 0.6400\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6413 - acc: 0.7281 - val_loss: 0.9158 - val_acc: 0.6629\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6508 - acc: 0.7422 - val_loss: 0.9152 - val_acc: 0.6686\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6415 - acc: 0.7352 - val_loss: 0.9044 - val_acc: 0.6514\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6322 - acc: 0.7505 - val_loss: 0.8931 - val_acc: 0.6629\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6176 - acc: 0.7505 - val_loss: 0.8990 - val_acc: 0.6571\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6356 - acc: 0.7358 - val_loss: 0.9019 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6087 - acc: 0.7479 - val_loss: 0.9052 - val_acc: 0.6457\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6322 - acc: 0.7524 - val_loss: 0.9062 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5945 - acc: 0.7652 - val_loss: 0.9084 - val_acc: 0.6457\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.5889 - acc: 0.7632 - val_loss: 0.9093 - val_acc: 0.6514\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5890 - acc: 0.7683 - val_loss: 0.9222 - val_acc: 0.6514\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6116 - acc: 0.7556 - val_loss: 0.9166 - val_acc: 0.6743\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5968 - acc: 0.7715 - val_loss: 0.9229 - val_acc: 0.6686\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5971 - acc: 0.7632 - val_loss: 0.9318 - val_acc: 0.6571\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5935 - acc: 0.7715 - val_loss: 0.9213 - val_acc: 0.6686\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5808 - acc: 0.7715 - val_loss: 0.9400 - val_acc: 0.6514\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5903 - acc: 0.7683 - val_loss: 0.9380 - val_acc: 0.6857\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5746 - acc: 0.7696 - val_loss: 0.9479 - val_acc: 0.6686\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5814 - acc: 0.7607 - val_loss: 0.9451 - val_acc: 0.6686\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5701 - acc: 0.7715 - val_loss: 0.9400 - val_acc: 0.6571\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5691 - acc: 0.7754 - val_loss: 0.9442 - val_acc: 0.6686\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5779 - acc: 0.7709 - val_loss: 0.9374 - val_acc: 0.6800\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.5602 - acc: 0.7728 - val_loss: 0.9286 - val_acc: 0.6800\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5745 - acc: 0.7747 - val_loss: 0.9432 - val_acc: 0.6686\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5574 - acc: 0.7856 - val_loss: 0.9506 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5514 - acc: 0.7856 - val_loss: 0.9571 - val_acc: 0.6743\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5565 - acc: 0.7824 - val_loss: 0.9515 - val_acc: 0.6743\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5421 - acc: 0.7837 - val_loss: 0.9502 - val_acc: 0.6800\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5634 - acc: 0.7837 - val_loss: 0.9588 - val_acc: 0.6457\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5347 - acc: 0.7964 - val_loss: 0.9663 - val_acc: 0.6629\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5466 - acc: 0.7824 - val_loss: 0.9696 - val_acc: 0.6629\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.5641 - acc: 0.7677 - val_loss: 0.9666 - val_acc: 0.6800\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5199 - acc: 0.7920 - val_loss: 0.9726 - val_acc: 0.6743\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5221 - acc: 0.7875 - val_loss: 0.9802 - val_acc: 0.6686\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5277 - acc: 0.7939 - val_loss: 0.9786 - val_acc: 0.6743\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5274 - acc: 0.7939 - val_loss: 0.9749 - val_acc: 0.6629\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.5313 - acc: 0.7939 - val_loss: 0.9774 - val_acc: 0.6686\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5234 - acc: 0.7951 - val_loss: 0.9789 - val_acc: 0.6686\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5227 - acc: 0.7996 - val_loss: 0.9849 - val_acc: 0.6743\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5067 - acc: 0.8054 - val_loss: 0.9871 - val_acc: 0.6800\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5143 - acc: 0.8015 - val_loss: 0.9971 - val_acc: 0.6571\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4836 - acc: 0.8156 - val_loss: 1.0003 - val_acc: 0.6914\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4858 - acc: 0.8047 - val_loss: 0.9845 - val_acc: 0.6629\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4870 - acc: 0.8149 - val_loss: 0.9942 - val_acc: 0.6629\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4846 - acc: 0.8156 - val_loss: 1.0047 - val_acc: 0.6629\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4976 - acc: 0.8073 - val_loss: 1.0152 - val_acc: 0.6629\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5020 - acc: 0.8047 - val_loss: 1.0153 - val_acc: 0.6457\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4782 - acc: 0.8168 - val_loss: 1.0145 - val_acc: 0.6457\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4855 - acc: 0.8098 - val_loss: 0.9937 - val_acc: 0.6686\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4938 - acc: 0.8047 - val_loss: 1.0158 - val_acc: 0.6457\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4819 - acc: 0.8143 - val_loss: 1.0079 - val_acc: 0.6686\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4688 - acc: 0.8360 - val_loss: 1.0124 - val_acc: 0.6514\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4914 - acc: 0.8111 - val_loss: 1.0121 - val_acc: 0.6629\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4980 - acc: 0.8086 - val_loss: 1.0285 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4798 - acc: 0.8060 - val_loss: 1.0388 - val_acc: 0.6514\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4672 - acc: 0.8188 - val_loss: 1.0393 - val_acc: 0.6571\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4571 - acc: 0.8334 - val_loss: 1.0501 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4569 - acc: 0.8328 - val_loss: 1.0521 - val_acc: 0.6457\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4653 - acc: 0.8213 - val_loss: 1.0555 - val_acc: 0.6514\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4681 - acc: 0.8315 - val_loss: 1.0408 - val_acc: 0.6571\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.4667 - acc: 0.8168 - val_loss: 1.0345 - val_acc: 0.6457\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4757 - acc: 0.8258 - val_loss: 1.0355 - val_acc: 0.6629\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4672 - acc: 0.8277 - val_loss: 1.0590 - val_acc: 0.6514\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.4503 - acc: 0.8207 - val_loss: 1.0474 - val_acc: 0.6629\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4779 - acc: 0.8213 - val_loss: 1.0416 - val_acc: 0.6629\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4462 - acc: 0.8302 - val_loss: 1.0339 - val_acc: 0.6686\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4472 - acc: 0.8392 - val_loss: 1.0419 - val_acc: 0.6571\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4293 - acc: 0.8366 - val_loss: 1.0539 - val_acc: 0.6686\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4549 - acc: 0.8245 - val_loss: 1.0596 - val_acc: 0.6571\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4226 - acc: 0.8309 - val_loss: 1.0530 - val_acc: 0.6629\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4675 - acc: 0.8168 - val_loss: 1.0536 - val_acc: 0.6457\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4417 - acc: 0.8430 - val_loss: 1.0634 - val_acc: 0.6629\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4489 - acc: 0.8290 - val_loss: 1.0597 - val_acc: 0.6457\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4456 - acc: 0.8354 - val_loss: 1.0690 - val_acc: 0.6514\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.4208 - acc: 0.8347 - val_loss: 1.0560 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4321 - acc: 0.8373 - val_loss: 1.0690 - val_acc: 0.6514\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4525 - acc: 0.8162 - val_loss: 1.0706 - val_acc: 0.6343\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4262 - acc: 0.8443 - val_loss: 1.0801 - val_acc: 0.6343\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4440 - acc: 0.8290 - val_loss: 1.0806 - val_acc: 0.6514\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4444 - acc: 0.8328 - val_loss: 1.1046 - val_acc: 0.6286\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.4314 - acc: 0.8385 - val_loss: 1.0893 - val_acc: 0.6457\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.4358 - acc: 0.8424 - val_loss: 1.1000 - val_acc: 0.6400\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4197 - acc: 0.8328 - val_loss: 1.1030 - val_acc: 0.6629\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4197 - acc: 0.8347 - val_loss: 1.1003 - val_acc: 0.6457\n",
      "Epoch 147/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4271 - acc: 0.8328 - val_loss: 1.0944 - val_acc: 0.6343\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4031 - acc: 0.8405 - val_loss: 1.0853 - val_acc: 0.6457\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4132 - acc: 0.8417 - val_loss: 1.1103 - val_acc: 0.6400\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4171 - acc: 0.8456 - val_loss: 1.1174 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 81us/step\n",
      "1742/1742 [==============================] - 0s 56us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.3902 - acc: 0.3350 - val_loss: 1.1275 - val_acc: 0.5886\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.2004 - acc: 0.4659 - val_loss: 1.0296 - val_acc: 0.6343\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.0993 - acc: 0.5188 - val_loss: 0.9959 - val_acc: 0.6229\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 1.0770 - acc: 0.5284 - val_loss: 0.9481 - val_acc: 0.6857\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 1.0367 - acc: 0.5514 - val_loss: 0.9406 - val_acc: 0.6343\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0107 - acc: 0.5667 - val_loss: 0.9301 - val_acc: 0.6686\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.9886 - acc: 0.5718 - val_loss: 0.9142 - val_acc: 0.6914\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.9811 - acc: 0.5877 - val_loss: 0.9130 - val_acc: 0.6857\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.9424 - acc: 0.6063 - val_loss: 0.9147 - val_acc: 0.6686\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9477 - acc: 0.5992 - val_loss: 0.8969 - val_acc: 0.6686\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9410 - acc: 0.6088 - val_loss: 0.9054 - val_acc: 0.6800\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.9075 - acc: 0.6107 - val_loss: 0.8976 - val_acc: 0.6914\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8943 - acc: 0.6273 - val_loss: 0.8999 - val_acc: 0.6800\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8853 - acc: 0.6280 - val_loss: 0.9074 - val_acc: 0.6857\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8914 - acc: 0.6273 - val_loss: 0.8950 - val_acc: 0.6743\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8886 - acc: 0.6177 - val_loss: 0.8880 - val_acc: 0.6743\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8690 - acc: 0.6248 - val_loss: 0.8931 - val_acc: 0.6857\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8670 - acc: 0.6343 - val_loss: 0.8972 - val_acc: 0.6800\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8677 - acc: 0.6382 - val_loss: 0.9024 - val_acc: 0.6571\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8467 - acc: 0.6382 - val_loss: 0.9035 - val_acc: 0.6629\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8503 - acc: 0.6426 - val_loss: 0.9091 - val_acc: 0.6686\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8294 - acc: 0.6490 - val_loss: 0.9072 - val_acc: 0.6629\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8255 - acc: 0.6733 - val_loss: 0.9143 - val_acc: 0.6571\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8408 - acc: 0.6471 - val_loss: 0.9165 - val_acc: 0.6457\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8291 - acc: 0.6554 - val_loss: 0.9189 - val_acc: 0.6457\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8089 - acc: 0.6694 - val_loss: 0.9118 - val_acc: 0.6514\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7890 - acc: 0.6675 - val_loss: 0.9205 - val_acc: 0.6686\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8251 - acc: 0.6471 - val_loss: 0.9181 - val_acc: 0.6571\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8014 - acc: 0.6662 - val_loss: 0.9182 - val_acc: 0.6343\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8025 - acc: 0.6879 - val_loss: 0.9176 - val_acc: 0.6343\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8081 - acc: 0.6707 - val_loss: 0.9143 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7747 - acc: 0.6847 - val_loss: 0.9095 - val_acc: 0.6400\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7813 - acc: 0.6720 - val_loss: 0.9096 - val_acc: 0.6400\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7789 - acc: 0.6879 - val_loss: 0.9147 - val_acc: 0.6286\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7732 - acc: 0.6777 - val_loss: 0.9211 - val_acc: 0.6457\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.7562 - acc: 0.6841 - val_loss: 0.9257 - val_acc: 0.6629\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7763 - acc: 0.6892 - val_loss: 0.9301 - val_acc: 0.6571\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7478 - acc: 0.6994 - val_loss: 0.9383 - val_acc: 0.6514\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7543 - acc: 0.6975 - val_loss: 0.9332 - val_acc: 0.6514\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7331 - acc: 0.7013 - val_loss: 0.9368 - val_acc: 0.6571\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7411 - acc: 0.7033 - val_loss: 0.9390 - val_acc: 0.6457\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7364 - acc: 0.6943 - val_loss: 0.9508 - val_acc: 0.6629\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7400 - acc: 0.7001 - val_loss: 0.9570 - val_acc: 0.6457\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7299 - acc: 0.7064 - val_loss: 0.9418 - val_acc: 0.6571\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7255 - acc: 0.6981 - val_loss: 0.9662 - val_acc: 0.6686\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6990 - acc: 0.7198 - val_loss: 0.9642 - val_acc: 0.6686\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7379 - acc: 0.7096 - val_loss: 0.9556 - val_acc: 0.6629\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7273 - acc: 0.7026 - val_loss: 0.9660 - val_acc: 0.6629\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7275 - acc: 0.7033 - val_loss: 0.9592 - val_acc: 0.6400\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7103 - acc: 0.7052 - val_loss: 0.9686 - val_acc: 0.6629\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6913 - acc: 0.7275 - val_loss: 0.9791 - val_acc: 0.6743\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7028 - acc: 0.7186 - val_loss: 0.9821 - val_acc: 0.6286\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6862 - acc: 0.7198 - val_loss: 0.9851 - val_acc: 0.6629\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7009 - acc: 0.7116 - val_loss: 0.9813 - val_acc: 0.6514\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6740 - acc: 0.7211 - val_loss: 0.9679 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6774 - acc: 0.7205 - val_loss: 0.9809 - val_acc: 0.6457\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6800 - acc: 0.7250 - val_loss: 0.9803 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6686 - acc: 0.7326 - val_loss: 0.9793 - val_acc: 0.6400\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6598 - acc: 0.7422 - val_loss: 0.9725 - val_acc: 0.6629\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6755 - acc: 0.7422 - val_loss: 0.9866 - val_acc: 0.6457\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6594 - acc: 0.7390 - val_loss: 0.9871 - val_acc: 0.6629\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6567 - acc: 0.7218 - val_loss: 0.9929 - val_acc: 0.6457\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6392 - acc: 0.7371 - val_loss: 0.9894 - val_acc: 0.6571\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6479 - acc: 0.7428 - val_loss: 1.0005 - val_acc: 0.6571\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 132us/step - loss: 0.6436 - acc: 0.7403 - val_loss: 1.0042 - val_acc: 0.6457\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6368 - acc: 0.7435 - val_loss: 0.9997 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6352 - acc: 0.7562 - val_loss: 1.0121 - val_acc: 0.6457\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6460 - acc: 0.7396 - val_loss: 1.0172 - val_acc: 0.6571\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6350 - acc: 0.7428 - val_loss: 1.0181 - val_acc: 0.6571\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6305 - acc: 0.7524 - val_loss: 1.0374 - val_acc: 0.6571\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.6205 - acc: 0.7594 - val_loss: 1.0215 - val_acc: 0.6571\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6113 - acc: 0.7498 - val_loss: 1.0279 - val_acc: 0.6400\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6056 - acc: 0.7664 - val_loss: 1.0270 - val_acc: 0.6457\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6055 - acc: 0.7588 - val_loss: 1.0403 - val_acc: 0.6400\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6055 - acc: 0.7620 - val_loss: 1.0263 - val_acc: 0.6686\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6072 - acc: 0.7492 - val_loss: 1.0202 - val_acc: 0.6629\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5774 - acc: 0.7581 - val_loss: 1.0489 - val_acc: 0.6343\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5726 - acc: 0.7747 - val_loss: 1.0546 - val_acc: 0.6343\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5939 - acc: 0.7696 - val_loss: 1.0578 - val_acc: 0.6343\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5841 - acc: 0.7639 - val_loss: 1.0751 - val_acc: 0.6343\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5724 - acc: 0.7722 - val_loss: 1.0613 - val_acc: 0.6514\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5810 - acc: 0.7709 - val_loss: 1.0697 - val_acc: 0.6343\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5859 - acc: 0.7683 - val_loss: 1.0678 - val_acc: 0.6229\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5722 - acc: 0.7549 - val_loss: 1.0679 - val_acc: 0.6400\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5820 - acc: 0.7913 - val_loss: 1.0840 - val_acc: 0.6286\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5947 - acc: 0.7664 - val_loss: 1.0723 - val_acc: 0.6286\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.5539 - acc: 0.7773 - val_loss: 1.0703 - val_acc: 0.6514\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5634 - acc: 0.7811 - val_loss: 1.0787 - val_acc: 0.6514\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5674 - acc: 0.7849 - val_loss: 1.0746 - val_acc: 0.6514\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5458 - acc: 0.7875 - val_loss: 1.0837 - val_acc: 0.6286\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5785 - acc: 0.7696 - val_loss: 1.0827 - val_acc: 0.6286\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5392 - acc: 0.7830 - val_loss: 1.0773 - val_acc: 0.6343\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.5520 - acc: 0.7824 - val_loss: 1.0912 - val_acc: 0.6286\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5616 - acc: 0.7754 - val_loss: 1.1020 - val_acc: 0.6171\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5542 - acc: 0.7798 - val_loss: 1.0970 - val_acc: 0.6229\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5408 - acc: 0.7971 - val_loss: 1.0980 - val_acc: 0.6343\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5652 - acc: 0.7805 - val_loss: 1.1013 - val_acc: 0.6171\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5403 - acc: 0.7869 - val_loss: 1.0992 - val_acc: 0.6286\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5536 - acc: 0.7881 - val_loss: 1.1039 - val_acc: 0.6343\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5407 - acc: 0.7830 - val_loss: 1.1173 - val_acc: 0.6457\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5194 - acc: 0.8009 - val_loss: 1.1143 - val_acc: 0.6286\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5241 - acc: 0.7977 - val_loss: 1.1420 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5317 - acc: 0.7996 - val_loss: 1.1153 - val_acc: 0.6229\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5383 - acc: 0.7830 - val_loss: 1.1114 - val_acc: 0.6514\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5154 - acc: 0.7971 - val_loss: 1.1213 - val_acc: 0.6457\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5296 - acc: 0.8003 - val_loss: 1.1163 - val_acc: 0.6343\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5356 - acc: 0.7907 - val_loss: 1.1373 - val_acc: 0.6286\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5341 - acc: 0.7926 - val_loss: 1.1311 - val_acc: 0.6343\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5222 - acc: 0.7990 - val_loss: 1.1276 - val_acc: 0.6457\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5084 - acc: 0.7945 - val_loss: 1.1220 - val_acc: 0.6514\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5280 - acc: 0.7964 - val_loss: 1.1260 - val_acc: 0.6457\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.5035 - acc: 0.7996 - val_loss: 1.1310 - val_acc: 0.6571\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4971 - acc: 0.7951 - val_loss: 1.1391 - val_acc: 0.6400\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4870 - acc: 0.8296 - val_loss: 1.1488 - val_acc: 0.6400\n",
      "Epoch 115/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4857 - acc: 0.8111 - val_loss: 1.1513 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5007 - acc: 0.8066 - val_loss: 1.1580 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4839 - acc: 0.8232 - val_loss: 1.1530 - val_acc: 0.6457\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4816 - acc: 0.8137 - val_loss: 1.1964 - val_acc: 0.6343\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4802 - acc: 0.8092 - val_loss: 1.1956 - val_acc: 0.6400\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.4570 - acc: 0.8277 - val_loss: 1.1983 - val_acc: 0.6457\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4849 - acc: 0.8105 - val_loss: 1.2011 - val_acc: 0.6229\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4570 - acc: 0.8232 - val_loss: 1.2065 - val_acc: 0.6343\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4769 - acc: 0.8143 - val_loss: 1.2056 - val_acc: 0.6457\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4659 - acc: 0.8188 - val_loss: 1.2046 - val_acc: 0.6457\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4743 - acc: 0.8168 - val_loss: 1.2179 - val_acc: 0.6457\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4634 - acc: 0.8226 - val_loss: 1.1946 - val_acc: 0.6343\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4667 - acc: 0.8156 - val_loss: 1.2162 - val_acc: 0.6457\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4694 - acc: 0.8130 - val_loss: 1.2223 - val_acc: 0.6457\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4607 - acc: 0.8315 - val_loss: 1.2277 - val_acc: 0.6400\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4652 - acc: 0.8220 - val_loss: 1.2222 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4584 - acc: 0.8354 - val_loss: 1.2075 - val_acc: 0.6457\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.4647 - acc: 0.8168 - val_loss: 1.2269 - val_acc: 0.6400\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4616 - acc: 0.8200 - val_loss: 1.2212 - val_acc: 0.6286\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4531 - acc: 0.8239 - val_loss: 1.2247 - val_acc: 0.6171\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4405 - acc: 0.8366 - val_loss: 1.2333 - val_acc: 0.6286\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4610 - acc: 0.8213 - val_loss: 1.2392 - val_acc: 0.6343\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4548 - acc: 0.8271 - val_loss: 1.2336 - val_acc: 0.6171\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4303 - acc: 0.8309 - val_loss: 1.2448 - val_acc: 0.6343\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4511 - acc: 0.8271 - val_loss: 1.2631 - val_acc: 0.6114\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4537 - acc: 0.8239 - val_loss: 1.2466 - val_acc: 0.6114\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4286 - acc: 0.8437 - val_loss: 1.2691 - val_acc: 0.6171\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.4474 - acc: 0.8385 - val_loss: 1.3003 - val_acc: 0.6286\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4344 - acc: 0.8373 - val_loss: 1.2897 - val_acc: 0.6229\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.4267 - acc: 0.8392 - val_loss: 1.2907 - val_acc: 0.6229\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4404 - acc: 0.8417 - val_loss: 1.2868 - val_acc: 0.6286\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4382 - acc: 0.8379 - val_loss: 1.2952 - val_acc: 0.6286\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.4231 - acc: 0.8424 - val_loss: 1.2780 - val_acc: 0.6343\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4275 - acc: 0.8437 - val_loss: 1.2884 - val_acc: 0.6343\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4021 - acc: 0.8539 - val_loss: 1.2924 - val_acc: 0.6229\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4090 - acc: 0.8462 - val_loss: 1.2808 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 72us/step\n",
      "1742/1742 [==============================] - 0s 53us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.4487 - acc: 0.3197 - val_loss: 1.1338 - val_acc: 0.5486\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 1.2032 - acc: 0.4716 - val_loss: 1.0250 - val_acc: 0.6057\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0891 - acc: 0.5239 - val_loss: 0.9882 - val_acc: 0.6514\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 1.0845 - acc: 0.5322 - val_loss: 0.9639 - val_acc: 0.6286\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 1.0326 - acc: 0.5514 - val_loss: 0.9444 - val_acc: 0.6400\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0185 - acc: 0.5654 - val_loss: 0.9297 - val_acc: 0.6686\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9877 - acc: 0.5884 - val_loss: 0.9232 - val_acc: 0.6629\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.9757 - acc: 0.5788 - val_loss: 0.9237 - val_acc: 0.6629\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.9760 - acc: 0.5846 - val_loss: 0.9104 - val_acc: 0.6571\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9511 - acc: 0.6082 - val_loss: 0.9152 - val_acc: 0.6686\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.9431 - acc: 0.6063 - val_loss: 0.9112 - val_acc: 0.6629\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9272 - acc: 0.6209 - val_loss: 0.9108 - val_acc: 0.6743\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9165 - acc: 0.6171 - val_loss: 0.9161 - val_acc: 0.6457\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9066 - acc: 0.6114 - val_loss: 0.9151 - val_acc: 0.6571\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8829 - acc: 0.6420 - val_loss: 0.9108 - val_acc: 0.6629\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8855 - acc: 0.6452 - val_loss: 0.9136 - val_acc: 0.6743\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8797 - acc: 0.6318 - val_loss: 0.9021 - val_acc: 0.6857\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8659 - acc: 0.6465 - val_loss: 0.9098 - val_acc: 0.6514\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.8633 - acc: 0.6484 - val_loss: 0.9139 - val_acc: 0.6629\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8488 - acc: 0.6516 - val_loss: 0.9113 - val_acc: 0.6400\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8484 - acc: 0.6592 - val_loss: 0.9142 - val_acc: 0.6629\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8424 - acc: 0.6458 - val_loss: 0.9095 - val_acc: 0.6857\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8310 - acc: 0.6656 - val_loss: 0.9147 - val_acc: 0.6629\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8296 - acc: 0.6662 - val_loss: 0.9164 - val_acc: 0.6514\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8256 - acc: 0.6631 - val_loss: 0.9163 - val_acc: 0.6743\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8224 - acc: 0.6548 - val_loss: 0.9257 - val_acc: 0.6457\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7946 - acc: 0.6675 - val_loss: 0.9171 - val_acc: 0.6571\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8055 - acc: 0.6777 - val_loss: 0.9243 - val_acc: 0.6686\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8079 - acc: 0.6713 - val_loss: 0.9376 - val_acc: 0.6514\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7917 - acc: 0.6790 - val_loss: 0.9296 - val_acc: 0.6629\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7863 - acc: 0.6847 - val_loss: 0.9330 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8052 - acc: 0.6745 - val_loss: 0.9376 - val_acc: 0.6457\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7704 - acc: 0.6879 - val_loss: 0.9416 - val_acc: 0.6286\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7606 - acc: 0.6835 - val_loss: 0.9354 - val_acc: 0.6571\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7672 - acc: 0.6847 - val_loss: 0.9379 - val_acc: 0.6571\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7605 - acc: 0.6905 - val_loss: 0.9357 - val_acc: 0.6514\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7713 - acc: 0.7007 - val_loss: 0.9300 - val_acc: 0.6571\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7590 - acc: 0.7033 - val_loss: 0.9449 - val_acc: 0.6514\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7706 - acc: 0.7001 - val_loss: 0.9531 - val_acc: 0.6286\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7556 - acc: 0.6994 - val_loss: 0.9482 - val_acc: 0.6343\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7421 - acc: 0.7096 - val_loss: 0.9512 - val_acc: 0.6457\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7278 - acc: 0.7186 - val_loss: 0.9497 - val_acc: 0.6400\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.7533 - acc: 0.6969 - val_loss: 0.9476 - val_acc: 0.6343\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7180 - acc: 0.7198 - val_loss: 0.9547 - val_acc: 0.6400\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7233 - acc: 0.7052 - val_loss: 0.9693 - val_acc: 0.6400\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7297 - acc: 0.7033 - val_loss: 0.9655 - val_acc: 0.6171\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7124 - acc: 0.7256 - val_loss: 0.9740 - val_acc: 0.6171\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7144 - acc: 0.7109 - val_loss: 0.9751 - val_acc: 0.6343\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7157 - acc: 0.7039 - val_loss: 0.9770 - val_acc: 0.6171\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7150 - acc: 0.7058 - val_loss: 0.9747 - val_acc: 0.6171\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.7182 - acc: 0.7154 - val_loss: 0.9876 - val_acc: 0.6343\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.7022 - acc: 0.7192 - val_loss: 0.9885 - val_acc: 0.6171\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.6925 - acc: 0.7256 - val_loss: 0.9847 - val_acc: 0.6343\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.6812 - acc: 0.7339 - val_loss: 0.9922 - val_acc: 0.6343\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6732 - acc: 0.7364 - val_loss: 0.9859 - val_acc: 0.6400\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6885 - acc: 0.7135 - val_loss: 0.9832 - val_acc: 0.6400\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 113us/step - loss: 0.6773 - acc: 0.7250 - val_loss: 0.9841 - val_acc: 0.6286\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6767 - acc: 0.7358 - val_loss: 0.9905 - val_acc: 0.6457\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6550 - acc: 0.7511 - val_loss: 1.0038 - val_acc: 0.6571\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6680 - acc: 0.7320 - val_loss: 1.0031 - val_acc: 0.6400\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6693 - acc: 0.7466 - val_loss: 1.0086 - val_acc: 0.6286\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6468 - acc: 0.7358 - val_loss: 1.0094 - val_acc: 0.6171\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6409 - acc: 0.7371 - val_loss: 1.0217 - val_acc: 0.6229\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6378 - acc: 0.7435 - val_loss: 1.0143 - val_acc: 0.6286\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6524 - acc: 0.7409 - val_loss: 1.0246 - val_acc: 0.6400\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6385 - acc: 0.7620 - val_loss: 1.0229 - val_acc: 0.6229\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6428 - acc: 0.7396 - val_loss: 1.0331 - val_acc: 0.6171\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6264 - acc: 0.7594 - val_loss: 1.0364 - val_acc: 0.6286\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6225 - acc: 0.7530 - val_loss: 1.0289 - val_acc: 0.6171\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6360 - acc: 0.7549 - val_loss: 1.0333 - val_acc: 0.6114\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6398 - acc: 0.7581 - val_loss: 1.0431 - val_acc: 0.6114\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6190 - acc: 0.7588 - val_loss: 1.0479 - val_acc: 0.6000\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6310 - acc: 0.7569 - val_loss: 1.0542 - val_acc: 0.6171\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6070 - acc: 0.7645 - val_loss: 1.0531 - val_acc: 0.6229\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6150 - acc: 0.7588 - val_loss: 1.0531 - val_acc: 0.6229\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5955 - acc: 0.7652 - val_loss: 1.0491 - val_acc: 0.6343\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 110us/step - loss: 0.6011 - acc: 0.7741 - val_loss: 1.0610 - val_acc: 0.6171\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5982 - acc: 0.7703 - val_loss: 1.0745 - val_acc: 0.6171\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.5989 - acc: 0.7594 - val_loss: 1.0692 - val_acc: 0.6286\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5829 - acc: 0.7741 - val_loss: 1.0643 - val_acc: 0.6229\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5994 - acc: 0.7722 - val_loss: 1.0588 - val_acc: 0.6229\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.5863 - acc: 0.7607 - val_loss: 1.0604 - val_acc: 0.6286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5644 - acc: 0.7849 - val_loss: 1.0710 - val_acc: 0.6229\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5978 - acc: 0.7760 - val_loss: 1.0660 - val_acc: 0.6171\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6081 - acc: 0.7798 - val_loss: 1.0776 - val_acc: 0.6114\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5888 - acc: 0.7575 - val_loss: 1.0839 - val_acc: 0.6057\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5647 - acc: 0.7786 - val_loss: 1.0914 - val_acc: 0.6229\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5744 - acc: 0.7786 - val_loss: 1.0787 - val_acc: 0.6343\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5683 - acc: 0.7849 - val_loss: 1.0836 - val_acc: 0.6171\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 118us/step - loss: 0.5522 - acc: 0.7773 - val_loss: 1.0813 - val_acc: 0.6114\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5535 - acc: 0.7939 - val_loss: 1.0871 - val_acc: 0.5943\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5580 - acc: 0.7735 - val_loss: 1.0959 - val_acc: 0.6114\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.5527 - acc: 0.7798 - val_loss: 1.1001 - val_acc: 0.6229\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5296 - acc: 0.7939 - val_loss: 1.0996 - val_acc: 0.6343\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5435 - acc: 0.7932 - val_loss: 1.0987 - val_acc: 0.6286\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 120us/step - loss: 0.5397 - acc: 0.7983 - val_loss: 1.1096 - val_acc: 0.6229\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.5493 - acc: 0.7862 - val_loss: 1.1053 - val_acc: 0.6171\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5336 - acc: 0.7843 - val_loss: 1.1049 - val_acc: 0.6057\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5453 - acc: 0.7945 - val_loss: 1.1196 - val_acc: 0.6286\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5335 - acc: 0.8054 - val_loss: 1.1230 - val_acc: 0.6229\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5425 - acc: 0.7945 - val_loss: 1.1271 - val_acc: 0.6286\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5176 - acc: 0.8022 - val_loss: 1.1454 - val_acc: 0.6229\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5248 - acc: 0.8022 - val_loss: 1.1340 - val_acc: 0.5886\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.5147 - acc: 0.7951 - val_loss: 1.1358 - val_acc: 0.6057\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 127us/step - loss: 0.5255 - acc: 0.8041 - val_loss: 1.1389 - val_acc: 0.6229\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5202 - acc: 0.7983 - val_loss: 1.1220 - val_acc: 0.6229\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.5164 - acc: 0.8130 - val_loss: 1.1351 - val_acc: 0.6286\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5116 - acc: 0.7996 - val_loss: 1.1271 - val_acc: 0.6343\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4995 - acc: 0.8168 - val_loss: 1.1431 - val_acc: 0.6171\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5189 - acc: 0.8041 - val_loss: 1.1420 - val_acc: 0.6286\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5104 - acc: 0.8194 - val_loss: 1.1515 - val_acc: 0.6229\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.5091 - acc: 0.7958 - val_loss: 1.1396 - val_acc: 0.6400\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.4928 - acc: 0.8175 - val_loss: 1.1564 - val_acc: 0.6286\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5031 - acc: 0.8073 - val_loss: 1.1573 - val_acc: 0.6286\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 109us/step - loss: 0.4970 - acc: 0.8092 - val_loss: 1.1631 - val_acc: 0.6171\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4796 - acc: 0.8258 - val_loss: 1.1632 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4746 - acc: 0.8188 - val_loss: 1.1502 - val_acc: 0.6457\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4849 - acc: 0.8054 - val_loss: 1.1703 - val_acc: 0.6400\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.4736 - acc: 0.8251 - val_loss: 1.1653 - val_acc: 0.6400\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 159us/step - loss: 0.4873 - acc: 0.8271 - val_loss: 1.1622 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 142us/step - loss: 0.4991 - acc: 0.8054 - val_loss: 1.1685 - val_acc: 0.6171\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 169us/step - loss: 0.4501 - acc: 0.8385 - val_loss: 1.1787 - val_acc: 0.6286\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 150us/step - loss: 0.4946 - acc: 0.8175 - val_loss: 1.1905 - val_acc: 0.6000\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 184us/step - loss: 0.4722 - acc: 0.8207 - val_loss: 1.1948 - val_acc: 0.6114\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 138us/step - loss: 0.4623 - acc: 0.8258 - val_loss: 1.1974 - val_acc: 0.6114\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 125us/step - loss: 0.4780 - acc: 0.8251 - val_loss: 1.1968 - val_acc: 0.6171\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 162us/step - loss: 0.4666 - acc: 0.8232 - val_loss: 1.2128 - val_acc: 0.6286\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 146us/step - loss: 0.4926 - acc: 0.8162 - val_loss: 1.1795 - val_acc: 0.6457\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 134us/step - loss: 0.4525 - acc: 0.8347 - val_loss: 1.1996 - val_acc: 0.6171\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 148us/step - loss: 0.4419 - acc: 0.8411 - val_loss: 1.1967 - val_acc: 0.6229\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 139us/step - loss: 0.4760 - acc: 0.8245 - val_loss: 1.1988 - val_acc: 0.6343\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.4291 - acc: 0.8385 - val_loss: 1.1969 - val_acc: 0.6057\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4486 - acc: 0.8309 - val_loss: 1.2035 - val_acc: 0.6171\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4436 - acc: 0.8379 - val_loss: 1.2060 - val_acc: 0.6114\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4581 - acc: 0.8302 - val_loss: 1.2233 - val_acc: 0.6000\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4412 - acc: 0.8322 - val_loss: 1.2201 - val_acc: 0.6229\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4348 - acc: 0.8271 - val_loss: 1.2083 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.4370 - acc: 0.8315 - val_loss: 1.2153 - val_acc: 0.6171\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 118us/step - loss: 0.4564 - acc: 0.8424 - val_loss: 1.2147 - val_acc: 0.6286\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 149us/step - loss: 0.4638 - acc: 0.8258 - val_loss: 1.2248 - val_acc: 0.6343\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.4507 - acc: 0.8283 - val_loss: 1.2353 - val_acc: 0.6400\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.4136 - acc: 0.8462 - val_loss: 1.2409 - val_acc: 0.6286\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4649 - acc: 0.8271 - val_loss: 1.2446 - val_acc: 0.6171\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4386 - acc: 0.8462 - val_loss: 1.2466 - val_acc: 0.6343\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4098 - acc: 0.8456 - val_loss: 1.2532 - val_acc: 0.6343\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4429 - acc: 0.8385 - val_loss: 1.2546 - val_acc: 0.6286\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4318 - acc: 0.8462 - val_loss: 1.2795 - val_acc: 0.6229\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4452 - acc: 0.8302 - val_loss: 1.2736 - val_acc: 0.6229\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4185 - acc: 0.8430 - val_loss: 1.2679 - val_acc: 0.6229\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.3970 - acc: 0.8551 - val_loss: 1.2596 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 68us/step\n",
      "1742/1742 [==============================] - 0s 47us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.3924 - acc: 0.3472 - val_loss: 1.1536 - val_acc: 0.4571\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 1.2106 - acc: 0.4550 - val_loss: 1.0598 - val_acc: 0.5314\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.1326 - acc: 0.4952 - val_loss: 1.0142 - val_acc: 0.5429\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 1.0543 - acc: 0.5450 - val_loss: 1.0026 - val_acc: 0.5657\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.0488 - acc: 0.5482 - val_loss: 0.9929 - val_acc: 0.5486\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9871 - acc: 0.5871 - val_loss: 0.9875 - val_acc: 0.5543\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.9845 - acc: 0.5750 - val_loss: 0.9720 - val_acc: 0.5714\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.9790 - acc: 0.5922 - val_loss: 0.9680 - val_acc: 0.5771\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9563 - acc: 0.5948 - val_loss: 0.9554 - val_acc: 0.5714\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9286 - acc: 0.6088 - val_loss: 0.9676 - val_acc: 0.5714\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9395 - acc: 0.5992 - val_loss: 0.9535 - val_acc: 0.5600\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9187 - acc: 0.6050 - val_loss: 0.9793 - val_acc: 0.5543\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9265 - acc: 0.6241 - val_loss: 0.9681 - val_acc: 0.5429\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9009 - acc: 0.6241 - val_loss: 0.9793 - val_acc: 0.5657\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8915 - acc: 0.6228 - val_loss: 0.9724 - val_acc: 0.5657\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.9081 - acc: 0.6254 - val_loss: 0.9689 - val_acc: 0.5600\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8726 - acc: 0.6394 - val_loss: 0.9567 - val_acc: 0.5543\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8576 - acc: 0.6394 - val_loss: 0.9579 - val_acc: 0.5714\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8609 - acc: 0.6388 - val_loss: 0.9727 - val_acc: 0.5657\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8450 - acc: 0.6548 - val_loss: 0.9730 - val_acc: 0.5714\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8407 - acc: 0.6362 - val_loss: 0.9789 - val_acc: 0.5771\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8407 - acc: 0.6567 - val_loss: 0.9762 - val_acc: 0.5771\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8355 - acc: 0.6458 - val_loss: 0.9827 - val_acc: 0.5886\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8113 - acc: 0.6675 - val_loss: 0.9768 - val_acc: 0.5600\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8254 - acc: 0.6503 - val_loss: 0.9725 - val_acc: 0.5829\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8149 - acc: 0.6656 - val_loss: 0.9708 - val_acc: 0.5657\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8228 - acc: 0.6624 - val_loss: 0.9803 - val_acc: 0.5829\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8286 - acc: 0.6599 - val_loss: 0.9906 - val_acc: 0.5886\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8127 - acc: 0.6669 - val_loss: 0.9888 - val_acc: 0.5886\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7765 - acc: 0.6867 - val_loss: 0.9750 - val_acc: 0.6000\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7904 - acc: 0.6720 - val_loss: 0.9817 - val_acc: 0.5886\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7906 - acc: 0.6707 - val_loss: 0.9874 - val_acc: 0.5829\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7817 - acc: 0.6867 - val_loss: 0.9873 - val_acc: 0.5943\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7839 - acc: 0.6867 - val_loss: 1.0071 - val_acc: 0.6000\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7702 - acc: 0.6822 - val_loss: 1.0096 - val_acc: 0.5886\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7670 - acc: 0.6943 - val_loss: 1.0165 - val_acc: 0.5829\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7494 - acc: 0.6803 - val_loss: 1.0143 - val_acc: 0.6057\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7733 - acc: 0.6911 - val_loss: 1.0070 - val_acc: 0.5943\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7401 - acc: 0.7013 - val_loss: 1.0134 - val_acc: 0.5829\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7539 - acc: 0.6943 - val_loss: 1.0124 - val_acc: 0.5714\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7601 - acc: 0.6911 - val_loss: 1.0096 - val_acc: 0.5714\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7466 - acc: 0.6918 - val_loss: 1.0326 - val_acc: 0.5657\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7469 - acc: 0.7026 - val_loss: 1.0298 - val_acc: 0.5714\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7412 - acc: 0.7122 - val_loss: 1.0184 - val_acc: 0.5771\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7267 - acc: 0.6930 - val_loss: 1.0216 - val_acc: 0.5886\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7459 - acc: 0.6988 - val_loss: 1.0249 - val_acc: 0.5943\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7117 - acc: 0.7167 - val_loss: 1.0208 - val_acc: 0.5943\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7049 - acc: 0.7084 - val_loss: 1.0270 - val_acc: 0.5886\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7008 - acc: 0.7288 - val_loss: 1.0423 - val_acc: 0.5829\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6889 - acc: 0.7332 - val_loss: 1.0464 - val_acc: 0.5771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7072 - acc: 0.7096 - val_loss: 1.0335 - val_acc: 0.5886\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7144 - acc: 0.7020 - val_loss: 1.0574 - val_acc: 0.5829\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6957 - acc: 0.7192 - val_loss: 1.0543 - val_acc: 0.5829\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7121 - acc: 0.7205 - val_loss: 1.0681 - val_acc: 0.5600\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6947 - acc: 0.7230 - val_loss: 1.0700 - val_acc: 0.5657\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6987 - acc: 0.7269 - val_loss: 1.0650 - val_acc: 0.5714\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6613 - acc: 0.7313 - val_loss: 1.0614 - val_acc: 0.5657\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6654 - acc: 0.7396 - val_loss: 1.0587 - val_acc: 0.5714\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6683 - acc: 0.7320 - val_loss: 1.0713 - val_acc: 0.5714\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6924 - acc: 0.7301 - val_loss: 1.0736 - val_acc: 0.5714\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6575 - acc: 0.7320 - val_loss: 1.0664 - val_acc: 0.5886\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6643 - acc: 0.7403 - val_loss: 1.0532 - val_acc: 0.5943\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6609 - acc: 0.7428 - val_loss: 1.0911 - val_acc: 0.5714\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6768 - acc: 0.7371 - val_loss: 1.0722 - val_acc: 0.5886\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6651 - acc: 0.7403 - val_loss: 1.0835 - val_acc: 0.5829\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6308 - acc: 0.7524 - val_loss: 1.0860 - val_acc: 0.5714\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6590 - acc: 0.7332 - val_loss: 1.0878 - val_acc: 0.5771\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6472 - acc: 0.7422 - val_loss: 1.0877 - val_acc: 0.5600\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6295 - acc: 0.7479 - val_loss: 1.1079 - val_acc: 0.5657\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6431 - acc: 0.7454 - val_loss: 1.1042 - val_acc: 0.5543\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6438 - acc: 0.7384 - val_loss: 1.0973 - val_acc: 0.5771\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6291 - acc: 0.7588 - val_loss: 1.0930 - val_acc: 0.5771\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6264 - acc: 0.7613 - val_loss: 1.0976 - val_acc: 0.5714\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6167 - acc: 0.7645 - val_loss: 1.0875 - val_acc: 0.5600\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6180 - acc: 0.7518 - val_loss: 1.1028 - val_acc: 0.5714\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6176 - acc: 0.7632 - val_loss: 1.1172 - val_acc: 0.5714\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6019 - acc: 0.7760 - val_loss: 1.1186 - val_acc: 0.5657\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5965 - acc: 0.7594 - val_loss: 1.1291 - val_acc: 0.5657\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6019 - acc: 0.7626 - val_loss: 1.1423 - val_acc: 0.5829\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5926 - acc: 0.7766 - val_loss: 1.1502 - val_acc: 0.5771\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6153 - acc: 0.7626 - val_loss: 1.1494 - val_acc: 0.5714\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6120 - acc: 0.7511 - val_loss: 1.1452 - val_acc: 0.5714\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5873 - acc: 0.7703 - val_loss: 1.1275 - val_acc: 0.5771\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5954 - acc: 0.7639 - val_loss: 1.1235 - val_acc: 0.5714\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5881 - acc: 0.7652 - val_loss: 1.1276 - val_acc: 0.5714\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5888 - acc: 0.7658 - val_loss: 1.1300 - val_acc: 0.5714\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5731 - acc: 0.7779 - val_loss: 1.1460 - val_acc: 0.5543\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5750 - acc: 0.7722 - val_loss: 1.1434 - val_acc: 0.5657\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5778 - acc: 0.7862 - val_loss: 1.1477 - val_acc: 0.5600\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5870 - acc: 0.7773 - val_loss: 1.1420 - val_acc: 0.5714\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5561 - acc: 0.7811 - val_loss: 1.1405 - val_acc: 0.5714\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5754 - acc: 0.7766 - val_loss: 1.1599 - val_acc: 0.5600\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5720 - acc: 0.7849 - val_loss: 1.1736 - val_acc: 0.5600\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5629 - acc: 0.7760 - val_loss: 1.1777 - val_acc: 0.5714\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5627 - acc: 0.7875 - val_loss: 1.1760 - val_acc: 0.5600\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5651 - acc: 0.7779 - val_loss: 1.1661 - val_acc: 0.5600\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5524 - acc: 0.8086 - val_loss: 1.1556 - val_acc: 0.5714\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5442 - acc: 0.7856 - val_loss: 1.1601 - val_acc: 0.5657\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5582 - acc: 0.7913 - val_loss: 1.1836 - val_acc: 0.5657\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5311 - acc: 0.7958 - val_loss: 1.1793 - val_acc: 0.5714\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5536 - acc: 0.7875 - val_loss: 1.1808 - val_acc: 0.5486\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5545 - acc: 0.7920 - val_loss: 1.1771 - val_acc: 0.5600\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5142 - acc: 0.7971 - val_loss: 1.2194 - val_acc: 0.5600\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5190 - acc: 0.8009 - val_loss: 1.1932 - val_acc: 0.5486\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5513 - acc: 0.8009 - val_loss: 1.2390 - val_acc: 0.5314\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5315 - acc: 0.7990 - val_loss: 1.2119 - val_acc: 0.5600\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5301 - acc: 0.7964 - val_loss: 1.2131 - val_acc: 0.5600\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5146 - acc: 0.8054 - val_loss: 1.2204 - val_acc: 0.5600\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5456 - acc: 0.7830 - val_loss: 1.2164 - val_acc: 0.5486\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5080 - acc: 0.8041 - val_loss: 1.1995 - val_acc: 0.5543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5229 - acc: 0.7932 - val_loss: 1.2216 - val_acc: 0.5714\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5223 - acc: 0.7990 - val_loss: 1.2252 - val_acc: 0.5657\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4845 - acc: 0.8226 - val_loss: 1.1930 - val_acc: 0.5657\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4932 - acc: 0.8175 - val_loss: 1.1993 - val_acc: 0.5714\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5382 - acc: 0.7881 - val_loss: 1.2165 - val_acc: 0.5657\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5132 - acc: 0.8156 - val_loss: 1.2343 - val_acc: 0.5600\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5013 - acc: 0.8105 - val_loss: 1.2509 - val_acc: 0.5600\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5209 - acc: 0.7958 - val_loss: 1.2354 - val_acc: 0.5714\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.4860 - acc: 0.8079 - val_loss: 1.2612 - val_acc: 0.5543\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5096 - acc: 0.8105 - val_loss: 1.2327 - val_acc: 0.5543\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4932 - acc: 0.8156 - val_loss: 1.2364 - val_acc: 0.5543\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4996 - acc: 0.8079 - val_loss: 1.2512 - val_acc: 0.5600\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4865 - acc: 0.8245 - val_loss: 1.2432 - val_acc: 0.5600\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.4999 - acc: 0.8111 - val_loss: 1.2592 - val_acc: 0.5600\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5074 - acc: 0.8079 - val_loss: 1.2487 - val_acc: 0.5657\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5312 - acc: 0.8015 - val_loss: 1.2665 - val_acc: 0.5600\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4755 - acc: 0.8302 - val_loss: 1.2420 - val_acc: 0.5657\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4811 - acc: 0.8041 - val_loss: 1.2617 - val_acc: 0.5829\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4707 - acc: 0.8200 - val_loss: 1.2552 - val_acc: 0.5714\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4828 - acc: 0.8194 - val_loss: 1.2907 - val_acc: 0.5371\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4724 - acc: 0.8162 - val_loss: 1.2843 - val_acc: 0.5371\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4434 - acc: 0.8328 - val_loss: 1.2823 - val_acc: 0.5486\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.4709 - acc: 0.8239 - val_loss: 1.3010 - val_acc: 0.5429\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4765 - acc: 0.8264 - val_loss: 1.2757 - val_acc: 0.5600\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4525 - acc: 0.8328 - val_loss: 1.3013 - val_acc: 0.5543\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4345 - acc: 0.8379 - val_loss: 1.2992 - val_acc: 0.5657\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4345 - acc: 0.8366 - val_loss: 1.3143 - val_acc: 0.5486\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4704 - acc: 0.8207 - val_loss: 1.3256 - val_acc: 0.5314\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4658 - acc: 0.8341 - val_loss: 1.3479 - val_acc: 0.5314\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.4592 - acc: 0.8309 - val_loss: 1.3282 - val_acc: 0.5314\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4562 - acc: 0.8277 - val_loss: 1.3233 - val_acc: 0.5429\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.4581 - acc: 0.8354 - val_loss: 1.3025 - val_acc: 0.5600\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4687 - acc: 0.8162 - val_loss: 1.3137 - val_acc: 0.5600\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4372 - acc: 0.8385 - val_loss: 1.3160 - val_acc: 0.5600\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4717 - acc: 0.8264 - val_loss: 1.3279 - val_acc: 0.5543\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4439 - acc: 0.8334 - val_loss: 1.3429 - val_acc: 0.5657\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4426 - acc: 0.8405 - val_loss: 1.3399 - val_acc: 0.5543\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4576 - acc: 0.8264 - val_loss: 1.3434 - val_acc: 0.5600\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4325 - acc: 0.8366 - val_loss: 1.3459 - val_acc: 0.5543\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4339 - acc: 0.8494 - val_loss: 1.3256 - val_acc: 0.5600\n",
      "193/193 [==============================] - 0s 78us/step\n",
      "1742/1742 [==============================] - 0s 48us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.6147 - acc: 0.2937 - val_loss: 1.3140 - val_acc: 0.3486\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.3003 - acc: 0.4144 - val_loss: 1.1277 - val_acc: 0.5829\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.2109 - acc: 0.4700 - val_loss: 1.0541 - val_acc: 0.6057\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 1.1662 - acc: 0.4751 - val_loss: 1.0155 - val_acc: 0.6286\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 1.1024 - acc: 0.5249 - val_loss: 0.9945 - val_acc: 0.6400\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 1.0634 - acc: 0.5409 - val_loss: 0.9775 - val_acc: 0.6457\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0641 - acc: 0.5345 - val_loss: 0.9608 - val_acc: 0.6400\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.0475 - acc: 0.5556 - val_loss: 0.9429 - val_acc: 0.6571\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0278 - acc: 0.5600 - val_loss: 0.9398 - val_acc: 0.6457\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 1.0112 - acc: 0.5549 - val_loss: 0.9272 - val_acc: 0.6629\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9903 - acc: 0.5741 - val_loss: 0.9329 - val_acc: 0.6686\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9960 - acc: 0.5670 - val_loss: 0.9193 - val_acc: 0.6743\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9844 - acc: 0.5760 - val_loss: 0.9151 - val_acc: 0.6800\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9695 - acc: 0.5951 - val_loss: 0.9049 - val_acc: 0.6800\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9670 - acc: 0.6015 - val_loss: 0.8996 - val_acc: 0.6800\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9464 - acc: 0.5907 - val_loss: 0.8976 - val_acc: 0.6686\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9397 - acc: 0.6086 - val_loss: 0.8978 - val_acc: 0.6800\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9270 - acc: 0.6194 - val_loss: 0.8952 - val_acc: 0.6800\n",
      "Epoch 19/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9238 - acc: 0.6213 - val_loss: 0.8947 - val_acc: 0.6800\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9405 - acc: 0.6015 - val_loss: 0.8916 - val_acc: 0.6800\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9198 - acc: 0.6220 - val_loss: 0.8912 - val_acc: 0.6857\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9166 - acc: 0.6188 - val_loss: 0.8906 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9233 - acc: 0.6181 - val_loss: 0.8901 - val_acc: 0.6743\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9066 - acc: 0.6290 - val_loss: 0.8865 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9041 - acc: 0.6271 - val_loss: 0.8850 - val_acc: 0.6800\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8930 - acc: 0.6252 - val_loss: 0.8914 - val_acc: 0.6857\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8807 - acc: 0.6392 - val_loss: 0.8941 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8766 - acc: 0.6373 - val_loss: 0.8916 - val_acc: 0.6743\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8749 - acc: 0.6303 - val_loss: 0.8843 - val_acc: 0.6800\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.8564 - acc: 0.6373 - val_loss: 0.8865 - val_acc: 0.6800\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8702 - acc: 0.6360 - val_loss: 0.8811 - val_acc: 0.6800\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8749 - acc: 0.6335 - val_loss: 0.8829 - val_acc: 0.6743\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8597 - acc: 0.6373 - val_loss: 0.8882 - val_acc: 0.6686\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8603 - acc: 0.6341 - val_loss: 0.8840 - val_acc: 0.6686\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8585 - acc: 0.6296 - val_loss: 0.8921 - val_acc: 0.6743\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8607 - acc: 0.6392 - val_loss: 0.8894 - val_acc: 0.6571\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8440 - acc: 0.6590 - val_loss: 0.8913 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8215 - acc: 0.6711 - val_loss: 0.8837 - val_acc: 0.6571\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8513 - acc: 0.6603 - val_loss: 0.8900 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8395 - acc: 0.6564 - val_loss: 0.8964 - val_acc: 0.6629\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8377 - acc: 0.6552 - val_loss: 0.9020 - val_acc: 0.6571\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8363 - acc: 0.6635 - val_loss: 0.8972 - val_acc: 0.6629\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8174 - acc: 0.6788 - val_loss: 0.8909 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8190 - acc: 0.6743 - val_loss: 0.8927 - val_acc: 0.6514\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8191 - acc: 0.6616 - val_loss: 0.8941 - val_acc: 0.6343\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8278 - acc: 0.6635 - val_loss: 0.8932 - val_acc: 0.6514\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8101 - acc: 0.6705 - val_loss: 0.8924 - val_acc: 0.6457\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8310 - acc: 0.6609 - val_loss: 0.8906 - val_acc: 0.6457\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8151 - acc: 0.6801 - val_loss: 0.8961 - val_acc: 0.6343\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8070 - acc: 0.6692 - val_loss: 0.8940 - val_acc: 0.6571\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7998 - acc: 0.6788 - val_loss: 0.8962 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8183 - acc: 0.6788 - val_loss: 0.8917 - val_acc: 0.6571\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7915 - acc: 0.6807 - val_loss: 0.8964 - val_acc: 0.6571\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7805 - acc: 0.6820 - val_loss: 0.8984 - val_acc: 0.6571\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7865 - acc: 0.6865 - val_loss: 0.9003 - val_acc: 0.6571\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8044 - acc: 0.6673 - val_loss: 0.9117 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8020 - acc: 0.6616 - val_loss: 0.9045 - val_acc: 0.6514\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7940 - acc: 0.6711 - val_loss: 0.8932 - val_acc: 0.6400\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7910 - acc: 0.6884 - val_loss: 0.9108 - val_acc: 0.6571\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7679 - acc: 0.6992 - val_loss: 0.9070 - val_acc: 0.6457\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7851 - acc: 0.6814 - val_loss: 0.9088 - val_acc: 0.6514\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7814 - acc: 0.6814 - val_loss: 0.9052 - val_acc: 0.6457\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7551 - acc: 0.6992 - val_loss: 0.9040 - val_acc: 0.6514\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7680 - acc: 0.6922 - val_loss: 0.9059 - val_acc: 0.6400\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7585 - acc: 0.6941 - val_loss: 0.9119 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7625 - acc: 0.7043 - val_loss: 0.9079 - val_acc: 0.6629\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7691 - acc: 0.6935 - val_loss: 0.9092 - val_acc: 0.6514\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7563 - acc: 0.7050 - val_loss: 0.9075 - val_acc: 0.6457\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7580 - acc: 0.6954 - val_loss: 0.9065 - val_acc: 0.6514\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7688 - acc: 0.6954 - val_loss: 0.9107 - val_acc: 0.6457\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7640 - acc: 0.6922 - val_loss: 0.9145 - val_acc: 0.6514\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7446 - acc: 0.7095 - val_loss: 0.9158 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7484 - acc: 0.7018 - val_loss: 0.9167 - val_acc: 0.6400\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7523 - acc: 0.6916 - val_loss: 0.9141 - val_acc: 0.6514\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7351 - acc: 0.7011 - val_loss: 0.9221 - val_acc: 0.6400\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7427 - acc: 0.6999 - val_loss: 0.9177 - val_acc: 0.6457\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7381 - acc: 0.7050 - val_loss: 0.9162 - val_acc: 0.6514\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7347 - acc: 0.7133 - val_loss: 0.9135 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7332 - acc: 0.7063 - val_loss: 0.9216 - val_acc: 0.6343\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7442 - acc: 0.7114 - val_loss: 0.9237 - val_acc: 0.6400\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7227 - acc: 0.7178 - val_loss: 0.9230 - val_acc: 0.6571\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7302 - acc: 0.7082 - val_loss: 0.9283 - val_acc: 0.6514\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7171 - acc: 0.7190 - val_loss: 0.9326 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7084 - acc: 0.7184 - val_loss: 0.9276 - val_acc: 0.6571\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7235 - acc: 0.7146 - val_loss: 0.9298 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7236 - acc: 0.7101 - val_loss: 0.9309 - val_acc: 0.6514\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7086 - acc: 0.7229 - val_loss: 0.9358 - val_acc: 0.6400\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7103 - acc: 0.7133 - val_loss: 0.9385 - val_acc: 0.6457\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6994 - acc: 0.7184 - val_loss: 0.9401 - val_acc: 0.6457\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6977 - acc: 0.7222 - val_loss: 0.9422 - val_acc: 0.6457\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6788 - acc: 0.7299 - val_loss: 0.9490 - val_acc: 0.6400\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7229 - acc: 0.7152 - val_loss: 0.9505 - val_acc: 0.6400\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7115 - acc: 0.7165 - val_loss: 0.9545 - val_acc: 0.6457\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6878 - acc: 0.7286 - val_loss: 0.9479 - val_acc: 0.6457\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.6891 - acc: 0.7350 - val_loss: 0.9599 - val_acc: 0.6457\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6997 - acc: 0.7209 - val_loss: 0.9607 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6838 - acc: 0.7337 - val_loss: 0.9556 - val_acc: 0.6514\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7055 - acc: 0.7107 - val_loss: 0.9601 - val_acc: 0.6400\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6912 - acc: 0.7254 - val_loss: 0.9592 - val_acc: 0.6400\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6857 - acc: 0.7241 - val_loss: 0.9647 - val_acc: 0.6514\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7016 - acc: 0.7305 - val_loss: 0.9715 - val_acc: 0.6457\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6652 - acc: 0.7446 - val_loss: 0.9687 - val_acc: 0.6400\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6891 - acc: 0.7178 - val_loss: 0.9724 - val_acc: 0.6457\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6772 - acc: 0.7299 - val_loss: 0.9633 - val_acc: 0.6629\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6697 - acc: 0.7286 - val_loss: 0.9716 - val_acc: 0.6400\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6788 - acc: 0.7241 - val_loss: 0.9702 - val_acc: 0.6343\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6704 - acc: 0.7292 - val_loss: 0.9702 - val_acc: 0.6343\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6470 - acc: 0.7446 - val_loss: 0.9742 - val_acc: 0.6514\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6746 - acc: 0.7184 - val_loss: 0.9717 - val_acc: 0.6400\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6563 - acc: 0.7395 - val_loss: 0.9654 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6474 - acc: 0.7452 - val_loss: 0.9717 - val_acc: 0.6457\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6580 - acc: 0.7407 - val_loss: 0.9687 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6618 - acc: 0.7439 - val_loss: 0.9705 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6698 - acc: 0.7318 - val_loss: 0.9808 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6676 - acc: 0.7356 - val_loss: 0.9790 - val_acc: 0.6514\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6514 - acc: 0.7324 - val_loss: 0.9737 - val_acc: 0.6571\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6584 - acc: 0.7433 - val_loss: 0.9823 - val_acc: 0.6514\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6591 - acc: 0.7356 - val_loss: 0.9926 - val_acc: 0.6400\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6352 - acc: 0.7593 - val_loss: 0.9805 - val_acc: 0.6571\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6437 - acc: 0.7490 - val_loss: 0.9747 - val_acc: 0.6571\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6527 - acc: 0.7337 - val_loss: 0.9892 - val_acc: 0.6400\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6571 - acc: 0.7401 - val_loss: 0.9811 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6429 - acc: 0.7427 - val_loss: 0.9875 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6313 - acc: 0.7503 - val_loss: 0.9925 - val_acc: 0.6400\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6537 - acc: 0.7420 - val_loss: 0.9865 - val_acc: 0.6514\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6052 - acc: 0.7625 - val_loss: 0.9898 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6276 - acc: 0.7567 - val_loss: 0.9982 - val_acc: 0.6400\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6120 - acc: 0.7567 - val_loss: 1.0008 - val_acc: 0.6457\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6353 - acc: 0.7382 - val_loss: 0.9946 - val_acc: 0.6343\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6196 - acc: 0.7599 - val_loss: 0.9982 - val_acc: 0.6286\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.6265 - acc: 0.7599 - val_loss: 0.9948 - val_acc: 0.6400\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6332 - acc: 0.7586 - val_loss: 1.0092 - val_acc: 0.6457\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6210 - acc: 0.7548 - val_loss: 1.0048 - val_acc: 0.6400\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6061 - acc: 0.7752 - val_loss: 1.0003 - val_acc: 0.6514\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5983 - acc: 0.7669 - val_loss: 1.0051 - val_acc: 0.6400\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6227 - acc: 0.7478 - val_loss: 1.0082 - val_acc: 0.6400\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6159 - acc: 0.7561 - val_loss: 1.0043 - val_acc: 0.6571\n",
      "Epoch 138/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6172 - acc: 0.7599 - val_loss: 1.0107 - val_acc: 0.6343\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6121 - acc: 0.7637 - val_loss: 1.0213 - val_acc: 0.6400\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5992 - acc: 0.7618 - val_loss: 1.0251 - val_acc: 0.6400\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6012 - acc: 0.7554 - val_loss: 1.0157 - val_acc: 0.6629\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6198 - acc: 0.7554 - val_loss: 1.0114 - val_acc: 0.6514\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6042 - acc: 0.7618 - val_loss: 1.0223 - val_acc: 0.6514\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.6167 - acc: 0.7529 - val_loss: 1.0132 - val_acc: 0.6571\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6018 - acc: 0.7688 - val_loss: 1.0124 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5936 - acc: 0.7746 - val_loss: 1.0227 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5919 - acc: 0.7759 - val_loss: 1.0240 - val_acc: 0.6514\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5790 - acc: 0.7803 - val_loss: 1.0169 - val_acc: 0.6571\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6021 - acc: 0.7656 - val_loss: 1.0120 - val_acc: 0.6629\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5775 - acc: 0.7861 - val_loss: 1.0267 - val_acc: 0.6571\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5821 - acc: 0.7797 - val_loss: 1.0413 - val_acc: 0.6514\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6033 - acc: 0.7580 - val_loss: 1.0326 - val_acc: 0.6686\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5948 - acc: 0.7720 - val_loss: 1.0320 - val_acc: 0.6629\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5957 - acc: 0.7663 - val_loss: 1.0360 - val_acc: 0.6629\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6039 - acc: 0.7631 - val_loss: 1.0471 - val_acc: 0.6571\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5661 - acc: 0.7886 - val_loss: 1.0411 - val_acc: 0.6571\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5749 - acc: 0.7701 - val_loss: 1.0394 - val_acc: 0.6571\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5685 - acc: 0.7829 - val_loss: 1.0327 - val_acc: 0.6514\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5966 - acc: 0.7593 - val_loss: 1.0358 - val_acc: 0.6514\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5813 - acc: 0.7797 - val_loss: 1.0380 - val_acc: 0.6686\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5785 - acc: 0.7695 - val_loss: 1.0471 - val_acc: 0.6571\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5594 - acc: 0.7829 - val_loss: 1.0430 - val_acc: 0.6629\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5771 - acc: 0.7752 - val_loss: 1.0524 - val_acc: 0.6457\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5916 - acc: 0.7848 - val_loss: 1.0440 - val_acc: 0.6571\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5772 - acc: 0.7656 - val_loss: 1.0372 - val_acc: 0.6571\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5867 - acc: 0.7771 - val_loss: 1.0513 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5642 - acc: 0.7791 - val_loss: 1.0660 - val_acc: 0.6457\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5570 - acc: 0.7842 - val_loss: 1.0723 - val_acc: 0.6457\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5572 - acc: 0.7829 - val_loss: 1.0644 - val_acc: 0.6514\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5577 - acc: 0.7893 - val_loss: 1.0511 - val_acc: 0.6629\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5711 - acc: 0.7829 - val_loss: 1.0617 - val_acc: 0.6457\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5625 - acc: 0.7835 - val_loss: 1.0619 - val_acc: 0.6514\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5659 - acc: 0.7765 - val_loss: 1.0732 - val_acc: 0.6457\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5476 - acc: 0.7771 - val_loss: 1.0763 - val_acc: 0.6571\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5425 - acc: 0.7778 - val_loss: 1.0720 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5597 - acc: 0.7848 - val_loss: 1.0717 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5621 - acc: 0.7759 - val_loss: 1.0853 - val_acc: 0.6514\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5355 - acc: 0.7899 - val_loss: 1.0912 - val_acc: 0.6457\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5151 - acc: 0.8078 - val_loss: 1.0900 - val_acc: 0.6514\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5476 - acc: 0.7918 - val_loss: 1.0859 - val_acc: 0.6571\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5611 - acc: 0.7861 - val_loss: 1.0799 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5393 - acc: 0.7944 - val_loss: 1.0840 - val_acc: 0.6343\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5289 - acc: 0.7995 - val_loss: 1.0952 - val_acc: 0.6343\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5495 - acc: 0.7861 - val_loss: 1.0867 - val_acc: 0.6514\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.5237 - acc: 0.7969 - val_loss: 1.0921 - val_acc: 0.6457\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5376 - acc: 0.7976 - val_loss: 1.0950 - val_acc: 0.6457\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5213 - acc: 0.7880 - val_loss: 1.1072 - val_acc: 0.6514\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5322 - acc: 0.7957 - val_loss: 1.1143 - val_acc: 0.6400\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5050 - acc: 0.8193 - val_loss: 1.1103 - val_acc: 0.6400\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.5326 - acc: 0.7822 - val_loss: 1.1069 - val_acc: 0.6571\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5235 - acc: 0.7976 - val_loss: 1.1202 - val_acc: 0.6514\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.5188 - acc: 0.8052 - val_loss: 1.1109 - val_acc: 0.6400\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.5289 - acc: 0.7976 - val_loss: 1.1110 - val_acc: 0.6457\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.4978 - acc: 0.8123 - val_loss: 1.1110 - val_acc: 0.6457\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.5480 - acc: 0.7886 - val_loss: 1.1122 - val_acc: 0.6571\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5339 - acc: 0.7842 - val_loss: 1.1105 - val_acc: 0.6457\n",
      "Epoch 197/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.5015 - acc: 0.8116 - val_loss: 1.1222 - val_acc: 0.6457\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5328 - acc: 0.7944 - val_loss: 1.1169 - val_acc: 0.6514\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5142 - acc: 0.8001 - val_loss: 1.1229 - val_acc: 0.6514\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.5171 - acc: 0.7976 - val_loss: 1.1186 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 87us/step\n",
      "1741/1741 [==============================] - 0s 61us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.4781 - acc: 0.3180 - val_loss: 1.2099 - val_acc: 0.4571\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 125us/step - loss: 1.2794 - acc: 0.4291 - val_loss: 1.1045 - val_acc: 0.5829\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 125us/step - loss: 1.1837 - acc: 0.4579 - val_loss: 1.0559 - val_acc: 0.5943\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 1.1255 - acc: 0.5128 - val_loss: 1.0141 - val_acc: 0.6000\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 1.0952 - acc: 0.5013 - val_loss: 0.9930 - val_acc: 0.6286\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 114us/step - loss: 1.0696 - acc: 0.5409 - val_loss: 0.9824 - val_acc: 0.6343\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 1.0546 - acc: 0.5524 - val_loss: 0.9607 - val_acc: 0.6343\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 1.0273 - acc: 0.5600 - val_loss: 0.9441 - val_acc: 0.6286\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9991 - acc: 0.5734 - val_loss: 0.9352 - val_acc: 0.6343\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.9985 - acc: 0.5779 - val_loss: 0.9273 - val_acc: 0.6514\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9867 - acc: 0.5747 - val_loss: 0.9291 - val_acc: 0.6571\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.9812 - acc: 0.5779 - val_loss: 0.9198 - val_acc: 0.6743\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.9633 - acc: 0.5875 - val_loss: 0.9112 - val_acc: 0.6629\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9664 - acc: 0.6003 - val_loss: 0.9094 - val_acc: 0.6571\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 153us/step - loss: 0.9444 - acc: 0.6041 - val_loss: 0.8999 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.9302 - acc: 0.6111 - val_loss: 0.8993 - val_acc: 0.6800\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.9376 - acc: 0.6111 - val_loss: 0.8999 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 120us/step - loss: 0.9284 - acc: 0.6117 - val_loss: 0.8940 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.9219 - acc: 0.6207 - val_loss: 0.8941 - val_acc: 0.6514\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.9224 - acc: 0.6245 - val_loss: 0.8956 - val_acc: 0.6571\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9154 - acc: 0.6137 - val_loss: 0.8933 - val_acc: 0.6514\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8931 - acc: 0.6130 - val_loss: 0.9013 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8996 - acc: 0.6290 - val_loss: 0.8891 - val_acc: 0.6629\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.9169 - acc: 0.6143 - val_loss: 0.8924 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8985 - acc: 0.6386 - val_loss: 0.8918 - val_acc: 0.6571\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8954 - acc: 0.6188 - val_loss: 0.8961 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8736 - acc: 0.6450 - val_loss: 0.8856 - val_acc: 0.6686\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.8976 - acc: 0.6252 - val_loss: 0.8850 - val_acc: 0.6457\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8565 - acc: 0.6501 - val_loss: 0.8835 - val_acc: 0.6686\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.8609 - acc: 0.6507 - val_loss: 0.8864 - val_acc: 0.6629\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8674 - acc: 0.6328 - val_loss: 0.8876 - val_acc: 0.6571\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.8610 - acc: 0.6469 - val_loss: 0.8886 - val_acc: 0.6629\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8723 - acc: 0.6469 - val_loss: 0.8837 - val_acc: 0.6686\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8497 - acc: 0.6558 - val_loss: 0.8886 - val_acc: 0.6514\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8407 - acc: 0.6616 - val_loss: 0.8857 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8493 - acc: 0.6577 - val_loss: 0.8872 - val_acc: 0.6571\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8537 - acc: 0.6494 - val_loss: 0.8897 - val_acc: 0.6571\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8438 - acc: 0.6513 - val_loss: 0.8899 - val_acc: 0.6514\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8335 - acc: 0.6622 - val_loss: 0.8935 - val_acc: 0.6571\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8316 - acc: 0.6596 - val_loss: 0.8952 - val_acc: 0.6629\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8211 - acc: 0.6577 - val_loss: 0.8965 - val_acc: 0.6400\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.8137 - acc: 0.6673 - val_loss: 0.8940 - val_acc: 0.6514\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8169 - acc: 0.6731 - val_loss: 0.8898 - val_acc: 0.6457\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8252 - acc: 0.6724 - val_loss: 0.8956 - val_acc: 0.6457\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8144 - acc: 0.6737 - val_loss: 0.8958 - val_acc: 0.6457\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7967 - acc: 0.6641 - val_loss: 0.9031 - val_acc: 0.6400\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8263 - acc: 0.6616 - val_loss: 0.8958 - val_acc: 0.6686\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8044 - acc: 0.6654 - val_loss: 0.9035 - val_acc: 0.6457\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8038 - acc: 0.6782 - val_loss: 0.8966 - val_acc: 0.6629\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8048 - acc: 0.6762 - val_loss: 0.9002 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8068 - acc: 0.6750 - val_loss: 0.8967 - val_acc: 0.6571\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7873 - acc: 0.6801 - val_loss: 0.8951 - val_acc: 0.6514\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7914 - acc: 0.6762 - val_loss: 0.8954 - val_acc: 0.6514\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7833 - acc: 0.6705 - val_loss: 0.8975 - val_acc: 0.6629\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7855 - acc: 0.6814 - val_loss: 0.8985 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7804 - acc: 0.6954 - val_loss: 0.9038 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7810 - acc: 0.6788 - val_loss: 0.9010 - val_acc: 0.6514\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7744 - acc: 0.6833 - val_loss: 0.9004 - val_acc: 0.6686\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7763 - acc: 0.6897 - val_loss: 0.9054 - val_acc: 0.6629\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7623 - acc: 0.6909 - val_loss: 0.9006 - val_acc: 0.6686\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7604 - acc: 0.6852 - val_loss: 0.9091 - val_acc: 0.6686\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7676 - acc: 0.6807 - val_loss: 0.9143 - val_acc: 0.6571\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7664 - acc: 0.6903 - val_loss: 0.9046 - val_acc: 0.6514\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7499 - acc: 0.6960 - val_loss: 0.9194 - val_acc: 0.6400\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7611 - acc: 0.6820 - val_loss: 0.9240 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7629 - acc: 0.6948 - val_loss: 0.9178 - val_acc: 0.6400\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7652 - acc: 0.6948 - val_loss: 0.9171 - val_acc: 0.6629\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7435 - acc: 0.6845 - val_loss: 0.9122 - val_acc: 0.6571\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7444 - acc: 0.6980 - val_loss: 0.9176 - val_acc: 0.6514\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7284 - acc: 0.7082 - val_loss: 0.9211 - val_acc: 0.6457\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7427 - acc: 0.7031 - val_loss: 0.9222 - val_acc: 0.6457\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7571 - acc: 0.6967 - val_loss: 0.9216 - val_acc: 0.6514\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7485 - acc: 0.6948 - val_loss: 0.9239 - val_acc: 0.6457\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7475 - acc: 0.6897 - val_loss: 0.9242 - val_acc: 0.6457\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7522 - acc: 0.6820 - val_loss: 0.9144 - val_acc: 0.6457\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7322 - acc: 0.7114 - val_loss: 0.9173 - val_acc: 0.6571\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.7299 - acc: 0.7095 - val_loss: 0.9197 - val_acc: 0.6571\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7297 - acc: 0.7107 - val_loss: 0.9201 - val_acc: 0.6343\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7398 - acc: 0.6986 - val_loss: 0.9246 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7245 - acc: 0.7184 - val_loss: 0.9244 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.7313 - acc: 0.7056 - val_loss: 0.9226 - val_acc: 0.6457\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7227 - acc: 0.6967 - val_loss: 0.9294 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.7244 - acc: 0.7031 - val_loss: 0.9264 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7087 - acc: 0.7165 - val_loss: 0.9312 - val_acc: 0.6457\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7198 - acc: 0.6948 - val_loss: 0.9319 - val_acc: 0.6343\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.7111 - acc: 0.7331 - val_loss: 0.9333 - val_acc: 0.6400\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7074 - acc: 0.7139 - val_loss: 0.9273 - val_acc: 0.6514\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.7231 - acc: 0.7146 - val_loss: 0.9288 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.7183 - acc: 0.7158 - val_loss: 0.9299 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 116us/step - loss: 0.6877 - acc: 0.7241 - val_loss: 0.9317 - val_acc: 0.6457\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7130 - acc: 0.7158 - val_loss: 0.9339 - val_acc: 0.6514\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7047 - acc: 0.7146 - val_loss: 0.9362 - val_acc: 0.6514\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6873 - acc: 0.7318 - val_loss: 0.9369 - val_acc: 0.6514\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.6885 - acc: 0.7261 - val_loss: 0.9407 - val_acc: 0.6629\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6865 - acc: 0.7248 - val_loss: 0.9419 - val_acc: 0.6629\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6598 - acc: 0.7363 - val_loss: 0.9541 - val_acc: 0.6400\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6994 - acc: 0.7222 - val_loss: 0.9448 - val_acc: 0.6343\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6869 - acc: 0.7209 - val_loss: 0.9449 - val_acc: 0.6286\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.6553 - acc: 0.7433 - val_loss: 0.9472 - val_acc: 0.6343\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6788 - acc: 0.7369 - val_loss: 0.9456 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6764 - acc: 0.7261 - val_loss: 0.9473 - val_acc: 0.6343\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6560 - acc: 0.7446 - val_loss: 0.9512 - val_acc: 0.6343\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6805 - acc: 0.7299 - val_loss: 0.9658 - val_acc: 0.6343\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.6759 - acc: 0.7331 - val_loss: 0.9443 - val_acc: 0.6457\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6698 - acc: 0.7203 - val_loss: 0.9516 - val_acc: 0.6571\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.6686 - acc: 0.7267 - val_loss: 0.9543 - val_acc: 0.6514\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.6650 - acc: 0.7324 - val_loss: 0.9600 - val_acc: 0.6343\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6547 - acc: 0.7471 - val_loss: 0.9591 - val_acc: 0.6286\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6609 - acc: 0.7458 - val_loss: 0.9677 - val_acc: 0.6343\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6460 - acc: 0.7420 - val_loss: 0.9639 - val_acc: 0.6286\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6615 - acc: 0.7414 - val_loss: 0.9582 - val_acc: 0.6400\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.6623 - acc: 0.7407 - val_loss: 0.9566 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6588 - acc: 0.7458 - val_loss: 0.9598 - val_acc: 0.6400\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6520 - acc: 0.7458 - val_loss: 0.9680 - val_acc: 0.6514\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6332 - acc: 0.7458 - val_loss: 0.9685 - val_acc: 0.6343\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6321 - acc: 0.7471 - val_loss: 0.9717 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6531 - acc: 0.7452 - val_loss: 0.9690 - val_acc: 0.6229\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6396 - acc: 0.7388 - val_loss: 0.9695 - val_acc: 0.6286\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6440 - acc: 0.7478 - val_loss: 0.9743 - val_acc: 0.6286\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6448 - acc: 0.7612 - val_loss: 0.9737 - val_acc: 0.6286\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6444 - acc: 0.7414 - val_loss: 0.9875 - val_acc: 0.6457\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6423 - acc: 0.7516 - val_loss: 0.9946 - val_acc: 0.6171\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6572 - acc: 0.7350 - val_loss: 0.9833 - val_acc: 0.6229\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6348 - acc: 0.7478 - val_loss: 0.9952 - val_acc: 0.6171\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6559 - acc: 0.7267 - val_loss: 0.9917 - val_acc: 0.6400\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6458 - acc: 0.7471 - val_loss: 0.9870 - val_acc: 0.6343\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6253 - acc: 0.7522 - val_loss: 0.9934 - val_acc: 0.6171\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6339 - acc: 0.7593 - val_loss: 1.0103 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6394 - acc: 0.7388 - val_loss: 0.9923 - val_acc: 0.6286\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6248 - acc: 0.7516 - val_loss: 0.9952 - val_acc: 0.6343\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6147 - acc: 0.7535 - val_loss: 0.9925 - val_acc: 0.6286\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6303 - acc: 0.7458 - val_loss: 1.0001 - val_acc: 0.6114\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5948 - acc: 0.7599 - val_loss: 0.9955 - val_acc: 0.6286\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6006 - acc: 0.7701 - val_loss: 0.9922 - val_acc: 0.6171\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.5882 - acc: 0.7637 - val_loss: 0.9958 - val_acc: 0.6114\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5936 - acc: 0.7612 - val_loss: 0.9993 - val_acc: 0.6171\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5991 - acc: 0.7708 - val_loss: 1.0013 - val_acc: 0.6171\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6176 - acc: 0.7458 - val_loss: 1.0068 - val_acc: 0.6057\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6133 - acc: 0.7612 - val_loss: 1.0081 - val_acc: 0.6171\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6151 - acc: 0.7522 - val_loss: 1.0120 - val_acc: 0.6171\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5782 - acc: 0.7637 - val_loss: 1.0088 - val_acc: 0.6057\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5728 - acc: 0.7816 - val_loss: 1.0033 - val_acc: 0.6114\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5982 - acc: 0.7548 - val_loss: 1.0203 - val_acc: 0.6057\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5862 - acc: 0.7663 - val_loss: 1.0245 - val_acc: 0.6114\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5935 - acc: 0.7644 - val_loss: 1.0299 - val_acc: 0.6057\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5921 - acc: 0.7701 - val_loss: 1.0145 - val_acc: 0.6229\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5867 - acc: 0.7682 - val_loss: 1.0118 - val_acc: 0.6057\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6058 - acc: 0.7497 - val_loss: 1.0203 - val_acc: 0.6171\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5991 - acc: 0.7631 - val_loss: 1.0170 - val_acc: 0.6229\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5837 - acc: 0.7822 - val_loss: 1.0083 - val_acc: 0.6171\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5967 - acc: 0.7650 - val_loss: 1.0124 - val_acc: 0.6457\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5780 - acc: 0.7720 - val_loss: 1.0207 - val_acc: 0.6286\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5733 - acc: 0.7695 - val_loss: 1.0356 - val_acc: 0.6229\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5546 - acc: 0.7905 - val_loss: 1.0259 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5824 - acc: 0.7752 - val_loss: 1.0281 - val_acc: 0.6229\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5943 - acc: 0.7625 - val_loss: 1.0358 - val_acc: 0.6229\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5656 - acc: 0.7791 - val_loss: 1.0379 - val_acc: 0.6286\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5854 - acc: 0.7867 - val_loss: 1.0337 - val_acc: 0.6171\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5656 - acc: 0.7835 - val_loss: 1.0318 - val_acc: 0.6171\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5904 - acc: 0.7586 - val_loss: 1.0373 - val_acc: 0.6229\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.5772 - acc: 0.7739 - val_loss: 1.0293 - val_acc: 0.6286\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5551 - acc: 0.7905 - val_loss: 1.0291 - val_acc: 0.6229\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5581 - acc: 0.7893 - val_loss: 1.0298 - val_acc: 0.6229\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5781 - acc: 0.7688 - val_loss: 1.0350 - val_acc: 0.6229\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5585 - acc: 0.7739 - val_loss: 1.0415 - val_acc: 0.6229\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5591 - acc: 0.7810 - val_loss: 1.0425 - val_acc: 0.6171\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5615 - acc: 0.7803 - val_loss: 1.0447 - val_acc: 0.6229\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5690 - acc: 0.7727 - val_loss: 1.0422 - val_acc: 0.6286\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5529 - acc: 0.7905 - val_loss: 1.0402 - val_acc: 0.6229\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5582 - acc: 0.7612 - val_loss: 1.0527 - val_acc: 0.6229\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5390 - acc: 0.7944 - val_loss: 1.0529 - val_acc: 0.6286\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5591 - acc: 0.7720 - val_loss: 1.0638 - val_acc: 0.6229\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5485 - acc: 0.7784 - val_loss: 1.0575 - val_acc: 0.6229\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5463 - acc: 0.7963 - val_loss: 1.0624 - val_acc: 0.6286\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5406 - acc: 0.7829 - val_loss: 1.0699 - val_acc: 0.6114\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5309 - acc: 0.7925 - val_loss: 1.0632 - val_acc: 0.6229\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5405 - acc: 0.7861 - val_loss: 1.0694 - val_acc: 0.6171\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5385 - acc: 0.7880 - val_loss: 1.0741 - val_acc: 0.6171\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.5462 - acc: 0.7969 - val_loss: 1.0569 - val_acc: 0.6343\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5195 - acc: 0.7944 - val_loss: 1.0629 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5470 - acc: 0.7854 - val_loss: 1.0697 - val_acc: 0.6343\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5290 - acc: 0.8059 - val_loss: 1.0644 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5240 - acc: 0.7969 - val_loss: 1.0733 - val_acc: 0.6400\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5451 - acc: 0.7950 - val_loss: 1.0861 - val_acc: 0.6286\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5327 - acc: 0.7912 - val_loss: 1.0845 - val_acc: 0.6343\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5401 - acc: 0.7976 - val_loss: 1.0777 - val_acc: 0.6229\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5264 - acc: 0.7950 - val_loss: 1.0774 - val_acc: 0.6286\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5395 - acc: 0.7905 - val_loss: 1.0874 - val_acc: 0.6286\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5059 - acc: 0.8129 - val_loss: 1.0884 - val_acc: 0.6286\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5200 - acc: 0.7969 - val_loss: 1.0900 - val_acc: 0.6343\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5095 - acc: 0.7950 - val_loss: 1.0909 - val_acc: 0.6343\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5245 - acc: 0.7963 - val_loss: 1.0842 - val_acc: 0.6229\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5207 - acc: 0.8116 - val_loss: 1.0880 - val_acc: 0.6229\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5076 - acc: 0.8027 - val_loss: 1.0967 - val_acc: 0.6171\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5223 - acc: 0.7905 - val_loss: 1.1039 - val_acc: 0.6286\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5069 - acc: 0.8040 - val_loss: 1.0974 - val_acc: 0.6286\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5229 - acc: 0.8020 - val_loss: 1.1128 - val_acc: 0.6114\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5134 - acc: 0.7937 - val_loss: 1.1119 - val_acc: 0.6286\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5210 - acc: 0.8001 - val_loss: 1.1109 - val_acc: 0.6286\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5151 - acc: 0.7982 - val_loss: 1.1074 - val_acc: 0.6286\n",
      "194/194 [==============================] - 0s 71us/step\n",
      "1741/1741 [==============================] - 0s 42us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.5291 - acc: 0.3186 - val_loss: 1.2226 - val_acc: 0.5086\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 1.2757 - acc: 0.4240 - val_loss: 1.1043 - val_acc: 0.5886\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.2034 - acc: 0.4496 - val_loss: 1.0556 - val_acc: 0.5886\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 1.1505 - acc: 0.4828 - val_loss: 1.0187 - val_acc: 0.6286\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.1217 - acc: 0.5077 - val_loss: 0.9997 - val_acc: 0.6000\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.1039 - acc: 0.5185 - val_loss: 0.9840 - val_acc: 0.6000\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.0653 - acc: 0.5396 - val_loss: 0.9667 - val_acc: 0.6343\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.0577 - acc: 0.5275 - val_loss: 0.9603 - val_acc: 0.6514\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.0243 - acc: 0.5613 - val_loss: 0.9496 - val_acc: 0.6286\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0183 - acc: 0.5658 - val_loss: 0.9427 - val_acc: 0.6457\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.0020 - acc: 0.5734 - val_loss: 0.9319 - val_acc: 0.6457\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9936 - acc: 0.5798 - val_loss: 0.9299 - val_acc: 0.6457\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9915 - acc: 0.5760 - val_loss: 0.9186 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.9712 - acc: 0.5792 - val_loss: 0.9129 - val_acc: 0.6571\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9547 - acc: 0.5964 - val_loss: 0.9127 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9507 - acc: 0.6034 - val_loss: 0.9129 - val_acc: 0.6743\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.9505 - acc: 0.5977 - val_loss: 0.9083 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.9420 - acc: 0.6220 - val_loss: 0.9015 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9224 - acc: 0.6041 - val_loss: 0.9051 - val_acc: 0.6514\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9163 - acc: 0.6181 - val_loss: 0.9062 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9229 - acc: 0.6098 - val_loss: 0.8973 - val_acc: 0.6857\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9116 - acc: 0.6022 - val_loss: 0.8986 - val_acc: 0.6743\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.9264 - acc: 0.6194 - val_loss: 0.9012 - val_acc: 0.6629\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.8877 - acc: 0.6258 - val_loss: 0.9066 - val_acc: 0.6800\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.9122 - acc: 0.6175 - val_loss: 0.8951 - val_acc: 0.6686\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8911 - acc: 0.6175 - val_loss: 0.8996 - val_acc: 0.6800\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.8709 - acc: 0.6277 - val_loss: 0.8970 - val_acc: 0.6629\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.8801 - acc: 0.6424 - val_loss: 0.8929 - val_acc: 0.6857\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8759 - acc: 0.6232 - val_loss: 0.8884 - val_acc: 0.6857\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 122us/step - loss: 0.8600 - acc: 0.6513 - val_loss: 0.8901 - val_acc: 0.6857\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8563 - acc: 0.6584 - val_loss: 0.8924 - val_acc: 0.6743\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8427 - acc: 0.6526 - val_loss: 0.8976 - val_acc: 0.6686\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8531 - acc: 0.6494 - val_loss: 0.8942 - val_acc: 0.6686\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8548 - acc: 0.6481 - val_loss: 0.8921 - val_acc: 0.6571\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8551 - acc: 0.6533 - val_loss: 0.8950 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8589 - acc: 0.6558 - val_loss: 0.8929 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8529 - acc: 0.6411 - val_loss: 0.8932 - val_acc: 0.6514\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8403 - acc: 0.6411 - val_loss: 0.8949 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8363 - acc: 0.6571 - val_loss: 0.8939 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8302 - acc: 0.6545 - val_loss: 0.8998 - val_acc: 0.6629\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8141 - acc: 0.6699 - val_loss: 0.9015 - val_acc: 0.6629\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8272 - acc: 0.6616 - val_loss: 0.8955 - val_acc: 0.6686\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8086 - acc: 0.6660 - val_loss: 0.8913 - val_acc: 0.6629\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8120 - acc: 0.6679 - val_loss: 0.8995 - val_acc: 0.6686\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8186 - acc: 0.6628 - val_loss: 0.8990 - val_acc: 0.6571\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.8275 - acc: 0.6513 - val_loss: 0.9013 - val_acc: 0.6514\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 113us/step - loss: 0.8304 - acc: 0.6577 - val_loss: 0.8959 - val_acc: 0.6686\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8220 - acc: 0.6596 - val_loss: 0.9043 - val_acc: 0.6571\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.8101 - acc: 0.6814 - val_loss: 0.8991 - val_acc: 0.6514\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 121us/step - loss: 0.8025 - acc: 0.6609 - val_loss: 0.9009 - val_acc: 0.6629\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8027 - acc: 0.6526 - val_loss: 0.9026 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7924 - acc: 0.6750 - val_loss: 0.8969 - val_acc: 0.6629\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7888 - acc: 0.6679 - val_loss: 0.9008 - val_acc: 0.6514\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7615 - acc: 0.6890 - val_loss: 0.8996 - val_acc: 0.6571\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7698 - acc: 0.6801 - val_loss: 0.8967 - val_acc: 0.6629\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7877 - acc: 0.6692 - val_loss: 0.9061 - val_acc: 0.6514\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7953 - acc: 0.6877 - val_loss: 0.9099 - val_acc: 0.6514\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7748 - acc: 0.6826 - val_loss: 0.9127 - val_acc: 0.6514\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7766 - acc: 0.6954 - val_loss: 0.9152 - val_acc: 0.6400\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7851 - acc: 0.6686 - val_loss: 0.9078 - val_acc: 0.6286\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7811 - acc: 0.6775 - val_loss: 0.9030 - val_acc: 0.6457\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7783 - acc: 0.6852 - val_loss: 0.9004 - val_acc: 0.6571\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7750 - acc: 0.6820 - val_loss: 0.9066 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7700 - acc: 0.6897 - val_loss: 0.9067 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7469 - acc: 0.6871 - val_loss: 0.9046 - val_acc: 0.6686\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7813 - acc: 0.6737 - val_loss: 0.9034 - val_acc: 0.6629\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7510 - acc: 0.6980 - val_loss: 0.9104 - val_acc: 0.6514\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7712 - acc: 0.6775 - val_loss: 0.9072 - val_acc: 0.6457\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7523 - acc: 0.6858 - val_loss: 0.9125 - val_acc: 0.6400\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7502 - acc: 0.6986 - val_loss: 0.9127 - val_acc: 0.6514\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7403 - acc: 0.6973 - val_loss: 0.9120 - val_acc: 0.6629\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7551 - acc: 0.6807 - val_loss: 0.9179 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7391 - acc: 0.7024 - val_loss: 0.9184 - val_acc: 0.6514\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7388 - acc: 0.7069 - val_loss: 0.9174 - val_acc: 0.6571\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7295 - acc: 0.7018 - val_loss: 0.9209 - val_acc: 0.6457\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7490 - acc: 0.6992 - val_loss: 0.9204 - val_acc: 0.6514\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7349 - acc: 0.7037 - val_loss: 0.9231 - val_acc: 0.6571\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7432 - acc: 0.7005 - val_loss: 0.9205 - val_acc: 0.6400\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7169 - acc: 0.6999 - val_loss: 0.9265 - val_acc: 0.6400\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7171 - acc: 0.7063 - val_loss: 0.9223 - val_acc: 0.6400\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7232 - acc: 0.7063 - val_loss: 0.9260 - val_acc: 0.6400\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7248 - acc: 0.7101 - val_loss: 0.9249 - val_acc: 0.6571\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7219 - acc: 0.7190 - val_loss: 0.9355 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6972 - acc: 0.7095 - val_loss: 0.9381 - val_acc: 0.6400\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7129 - acc: 0.7063 - val_loss: 0.9374 - val_acc: 0.6400\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7150 - acc: 0.6954 - val_loss: 0.9400 - val_acc: 0.6514\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7098 - acc: 0.7184 - val_loss: 0.9429 - val_acc: 0.6457\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.7157 - acc: 0.7139 - val_loss: 0.9433 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.7192 - acc: 0.7011 - val_loss: 0.9466 - val_acc: 0.6400\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.7089 - acc: 0.7158 - val_loss: 0.9474 - val_acc: 0.6514\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.6848 - acc: 0.7331 - val_loss: 0.9461 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7052 - acc: 0.7158 - val_loss: 0.9452 - val_acc: 0.6629\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6950 - acc: 0.7235 - val_loss: 0.9461 - val_acc: 0.6400\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6824 - acc: 0.7209 - val_loss: 0.9499 - val_acc: 0.6457\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6861 - acc: 0.7203 - val_loss: 0.9477 - val_acc: 0.6571\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6820 - acc: 0.7209 - val_loss: 0.9497 - val_acc: 0.6400\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6826 - acc: 0.7222 - val_loss: 0.9525 - val_acc: 0.6400\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - ETA: 0s - loss: 0.6926 - acc: 0.718 - 0s 81us/step - loss: 0.6843 - acc: 0.7203 - val_loss: 0.9520 - val_acc: 0.6343\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6809 - acc: 0.7382 - val_loss: 0.9561 - val_acc: 0.6343\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6965 - acc: 0.7101 - val_loss: 0.9522 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6763 - acc: 0.7216 - val_loss: 0.9513 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6834 - acc: 0.7203 - val_loss: 0.9559 - val_acc: 0.6571\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6559 - acc: 0.7465 - val_loss: 0.9629 - val_acc: 0.6457\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6703 - acc: 0.7229 - val_loss: 0.9663 - val_acc: 0.6400\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6843 - acc: 0.7178 - val_loss: 0.9674 - val_acc: 0.6514\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6767 - acc: 0.7241 - val_loss: 0.9623 - val_acc: 0.6629\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6605 - acc: 0.7241 - val_loss: 0.9600 - val_acc: 0.6457\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6607 - acc: 0.7299 - val_loss: 0.9734 - val_acc: 0.6343\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.6576 - acc: 0.7407 - val_loss: 0.9709 - val_acc: 0.6343\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6716 - acc: 0.7209 - val_loss: 0.9687 - val_acc: 0.6286\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6596 - acc: 0.7331 - val_loss: 0.9668 - val_acc: 0.6457\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6583 - acc: 0.7363 - val_loss: 0.9664 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6690 - acc: 0.7337 - val_loss: 0.9695 - val_acc: 0.6400\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6586 - acc: 0.7241 - val_loss: 0.9776 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6463 - acc: 0.7439 - val_loss: 0.9771 - val_acc: 0.6457\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6451 - acc: 0.7382 - val_loss: 0.9828 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.6348 - acc: 0.7395 - val_loss: 0.9779 - val_acc: 0.6571\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.6304 - acc: 0.7465 - val_loss: 0.9864 - val_acc: 0.6457\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6397 - acc: 0.7369 - val_loss: 0.9924 - val_acc: 0.6514\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6406 - acc: 0.7427 - val_loss: 0.9948 - val_acc: 0.6400\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6094 - acc: 0.7407 - val_loss: 0.9887 - val_acc: 0.6343\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6255 - acc: 0.7503 - val_loss: 0.9884 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6254 - acc: 0.7471 - val_loss: 0.9953 - val_acc: 0.6457\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6534 - acc: 0.7375 - val_loss: 0.9896 - val_acc: 0.6514\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6321 - acc: 0.7465 - val_loss: 0.9938 - val_acc: 0.6457\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6279 - acc: 0.7586 - val_loss: 0.9918 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6338 - acc: 0.7350 - val_loss: 0.9963 - val_acc: 0.6400\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6105 - acc: 0.7516 - val_loss: 1.0020 - val_acc: 0.6400\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6264 - acc: 0.7535 - val_loss: 1.0087 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6438 - acc: 0.7414 - val_loss: 1.0124 - val_acc: 0.6400\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.6341 - acc: 0.7548 - val_loss: 1.0140 - val_acc: 0.6514\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.6097 - acc: 0.7605 - val_loss: 0.9981 - val_acc: 0.6286\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.6183 - acc: 0.7605 - val_loss: 1.0026 - val_acc: 0.6571\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.6257 - acc: 0.7458 - val_loss: 1.0124 - val_acc: 0.6514\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.6175 - acc: 0.7490 - val_loss: 1.0150 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.6198 - acc: 0.7580 - val_loss: 1.0023 - val_acc: 0.6457\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.6141 - acc: 0.7593 - val_loss: 1.0061 - val_acc: 0.6400\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.6070 - acc: 0.7510 - val_loss: 1.0034 - val_acc: 0.6514\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 107us/step - loss: 0.6173 - acc: 0.7497 - val_loss: 1.0008 - val_acc: 0.6400\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.6029 - acc: 0.7522 - val_loss: 0.9974 - val_acc: 0.6514\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5912 - acc: 0.7497 - val_loss: 1.0113 - val_acc: 0.6514\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.6126 - acc: 0.7573 - val_loss: 1.0248 - val_acc: 0.6457\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.5874 - acc: 0.7650 - val_loss: 1.0174 - val_acc: 0.6457\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.5961 - acc: 0.7548 - val_loss: 1.0135 - val_acc: 0.6514\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.5981 - acc: 0.7612 - val_loss: 1.0178 - val_acc: 0.6571\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.5839 - acc: 0.7542 - val_loss: 1.0232 - val_acc: 0.6514\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.5938 - acc: 0.7567 - val_loss: 1.0232 - val_acc: 0.6571\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.5923 - acc: 0.7650 - val_loss: 1.0299 - val_acc: 0.6343\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.6019 - acc: 0.7599 - val_loss: 1.0295 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.5861 - acc: 0.7554 - val_loss: 1.0230 - val_acc: 0.6400\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.5956 - acc: 0.7612 - val_loss: 1.0193 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.5639 - acc: 0.7708 - val_loss: 1.0270 - val_acc: 0.6343\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 112us/step - loss: 0.5842 - acc: 0.7561 - val_loss: 1.0312 - val_acc: 0.6400\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.5733 - acc: 0.7727 - val_loss: 1.0341 - val_acc: 0.6343\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.5558 - acc: 0.7810 - val_loss: 1.0341 - val_acc: 0.6457\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.5728 - acc: 0.7739 - val_loss: 1.0377 - val_acc: 0.6457\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.5784 - acc: 0.7663 - val_loss: 1.0347 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.5688 - acc: 0.7733 - val_loss: 1.0382 - val_acc: 0.6571\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.5880 - acc: 0.7612 - val_loss: 1.0450 - val_acc: 0.6457\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.5442 - acc: 0.7886 - val_loss: 1.0474 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.5793 - acc: 0.7695 - val_loss: 1.0476 - val_acc: 0.6514\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.5762 - acc: 0.7688 - val_loss: 1.0593 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.5529 - acc: 0.7752 - val_loss: 1.0570 - val_acc: 0.6286\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.5630 - acc: 0.7931 - val_loss: 1.0494 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 114us/step - loss: 0.5627 - acc: 0.7899 - val_loss: 1.0559 - val_acc: 0.6400\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.5421 - acc: 0.8059 - val_loss: 1.0648 - val_acc: 0.6514\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.5382 - acc: 0.7848 - val_loss: 1.0731 - val_acc: 0.6457\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.5794 - acc: 0.7612 - val_loss: 1.0715 - val_acc: 0.6457\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.5642 - acc: 0.7816 - val_loss: 1.0668 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5528 - acc: 0.7727 - val_loss: 1.0651 - val_acc: 0.6343\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.5568 - acc: 0.7784 - val_loss: 1.0634 - val_acc: 0.6457\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.5479 - acc: 0.7912 - val_loss: 1.0663 - val_acc: 0.6514\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.5559 - acc: 0.7842 - val_loss: 1.0676 - val_acc: 0.6343\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5664 - acc: 0.7803 - val_loss: 1.0686 - val_acc: 0.6400\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.5480 - acc: 0.7880 - val_loss: 1.0630 - val_acc: 0.6286\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.5200 - acc: 0.8052 - val_loss: 1.0622 - val_acc: 0.6229\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.5275 - acc: 0.7899 - val_loss: 1.0613 - val_acc: 0.6400\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.5578 - acc: 0.7695 - val_loss: 1.0655 - val_acc: 0.6457\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 113us/step - loss: 0.5297 - acc: 0.7925 - val_loss: 1.0748 - val_acc: 0.6400\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.5501 - acc: 0.7797 - val_loss: 1.0790 - val_acc: 0.6457\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.5412 - acc: 0.7874 - val_loss: 1.0899 - val_acc: 0.6343\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.5452 - acc: 0.7905 - val_loss: 1.0792 - val_acc: 0.6343\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.5303 - acc: 0.7861 - val_loss: 1.0794 - val_acc: 0.6457\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.5306 - acc: 0.7944 - val_loss: 1.0906 - val_acc: 0.6400\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.5404 - acc: 0.7867 - val_loss: 1.0929 - val_acc: 0.6286\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.5271 - acc: 0.7854 - val_loss: 1.0859 - val_acc: 0.6343\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.5148 - acc: 0.7829 - val_loss: 1.0914 - val_acc: 0.6229\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.5157 - acc: 0.7950 - val_loss: 1.0962 - val_acc: 0.6171\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 120us/step - loss: 0.5224 - acc: 0.7969 - val_loss: 1.1024 - val_acc: 0.6171\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 130us/step - loss: 0.5303 - acc: 0.7842 - val_loss: 1.1004 - val_acc: 0.6229\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 127us/step - loss: 0.5420 - acc: 0.7803 - val_loss: 1.0981 - val_acc: 0.6286\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.4965 - acc: 0.8155 - val_loss: 1.0949 - val_acc: 0.6400\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5150 - acc: 0.8091 - val_loss: 1.1097 - val_acc: 0.6114\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.5164 - acc: 0.7982 - val_loss: 1.1180 - val_acc: 0.6286\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5335 - acc: 0.7918 - val_loss: 1.1092 - val_acc: 0.6286\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.5179 - acc: 0.8078 - val_loss: 1.1154 - val_acc: 0.6057\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.5191 - acc: 0.7931 - val_loss: 1.1125 - val_acc: 0.6171\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.5293 - acc: 0.7925 - val_loss: 1.1095 - val_acc: 0.6229\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4998 - acc: 0.8078 - val_loss: 1.1280 - val_acc: 0.6171\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.4842 - acc: 0.8155 - val_loss: 1.1203 - val_acc: 0.6171\n",
      "194/194 [==============================] - 0s 82us/step\n",
      "1741/1741 [==============================] - 0s 58us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.4716 - acc: 0.3065 - val_loss: 1.2019 - val_acc: 0.5029\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.2690 - acc: 0.4106 - val_loss: 1.1027 - val_acc: 0.5429\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.1998 - acc: 0.4585 - val_loss: 1.0514 - val_acc: 0.5886\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.1471 - acc: 0.4994 - val_loss: 1.0179 - val_acc: 0.5943\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 1.1302 - acc: 0.5109 - val_loss: 0.9915 - val_acc: 0.6171\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.1018 - acc: 0.5198 - val_loss: 0.9746 - val_acc: 0.6114\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.1004 - acc: 0.5230 - val_loss: 0.9527 - val_acc: 0.6286\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0730 - acc: 0.5441 - val_loss: 0.9471 - val_acc: 0.6457\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 84us/step - loss: 1.0518 - acc: 0.5460 - val_loss: 0.9355 - val_acc: 0.6629\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.0324 - acc: 0.5594 - val_loss: 0.9304 - val_acc: 0.6514\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0118 - acc: 0.5670 - val_loss: 0.9184 - val_acc: 0.6571\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9941 - acc: 0.5824 - val_loss: 0.9150 - val_acc: 0.6743\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9777 - acc: 0.5926 - val_loss: 0.9134 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9843 - acc: 0.5811 - val_loss: 0.9134 - val_acc: 0.6686\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9783 - acc: 0.5754 - val_loss: 0.9058 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9386 - acc: 0.6060 - val_loss: 0.8962 - val_acc: 0.6800\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9527 - acc: 0.6015 - val_loss: 0.9019 - val_acc: 0.6743\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9420 - acc: 0.6009 - val_loss: 0.9009 - val_acc: 0.6743\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9508 - acc: 0.6022 - val_loss: 0.8956 - val_acc: 0.6800\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9423 - acc: 0.5971 - val_loss: 0.8969 - val_acc: 0.6800\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.9374 - acc: 0.6047 - val_loss: 0.8938 - val_acc: 0.6857\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9251 - acc: 0.6220 - val_loss: 0.8975 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.9092 - acc: 0.6309 - val_loss: 0.8863 - val_acc: 0.6857\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9133 - acc: 0.6201 - val_loss: 0.8883 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9108 - acc: 0.6181 - val_loss: 0.8876 - val_acc: 0.6914\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8872 - acc: 0.6239 - val_loss: 0.8842 - val_acc: 0.6629\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.8749 - acc: 0.6322 - val_loss: 0.8868 - val_acc: 0.6857\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9034 - acc: 0.6284 - val_loss: 0.8855 - val_acc: 0.6800\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8793 - acc: 0.6322 - val_loss: 0.8852 - val_acc: 0.6857\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 115us/step - loss: 0.8740 - acc: 0.6456 - val_loss: 0.8821 - val_acc: 0.6800\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8720 - acc: 0.6392 - val_loss: 0.8825 - val_acc: 0.6800\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8704 - acc: 0.6360 - val_loss: 0.8819 - val_acc: 0.6800\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.8743 - acc: 0.6386 - val_loss: 0.8817 - val_acc: 0.6800\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8718 - acc: 0.6437 - val_loss: 0.8900 - val_acc: 0.6743\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8532 - acc: 0.6469 - val_loss: 0.8866 - val_acc: 0.6800\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8592 - acc: 0.6456 - val_loss: 0.8832 - val_acc: 0.6800\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8529 - acc: 0.6571 - val_loss: 0.8800 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8489 - acc: 0.6469 - val_loss: 0.8854 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8454 - acc: 0.6501 - val_loss: 0.8820 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8448 - acc: 0.6494 - val_loss: 0.8916 - val_acc: 0.6686\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8457 - acc: 0.6469 - val_loss: 0.8865 - val_acc: 0.6857\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8494 - acc: 0.6520 - val_loss: 0.8946 - val_acc: 0.6571\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8204 - acc: 0.6596 - val_loss: 0.8840 - val_acc: 0.6743\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8528 - acc: 0.6469 - val_loss: 0.8815 - val_acc: 0.6686\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8256 - acc: 0.6673 - val_loss: 0.8866 - val_acc: 0.6800\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8204 - acc: 0.6654 - val_loss: 0.8862 - val_acc: 0.6857\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8202 - acc: 0.6577 - val_loss: 0.8953 - val_acc: 0.6686\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8392 - acc: 0.6552 - val_loss: 0.8999 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8104 - acc: 0.6564 - val_loss: 0.9019 - val_acc: 0.6629\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7935 - acc: 0.6814 - val_loss: 0.9057 - val_acc: 0.6743\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8043 - acc: 0.6782 - val_loss: 0.9039 - val_acc: 0.6800\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8068 - acc: 0.6699 - val_loss: 0.9031 - val_acc: 0.6571\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8143 - acc: 0.6609 - val_loss: 0.8953 - val_acc: 0.6743\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7985 - acc: 0.6609 - val_loss: 0.9057 - val_acc: 0.6686\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8076 - acc: 0.6737 - val_loss: 0.9089 - val_acc: 0.6514\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7929 - acc: 0.6769 - val_loss: 0.9052 - val_acc: 0.6514\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7826 - acc: 0.6782 - val_loss: 0.9007 - val_acc: 0.6514\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7870 - acc: 0.6788 - val_loss: 0.9061 - val_acc: 0.6629\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7908 - acc: 0.6737 - val_loss: 0.9066 - val_acc: 0.6514\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7932 - acc: 0.6801 - val_loss: 0.9072 - val_acc: 0.6514\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7738 - acc: 0.6954 - val_loss: 0.9156 - val_acc: 0.6629\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.7705 - acc: 0.6820 - val_loss: 0.9165 - val_acc: 0.6571\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.7673 - acc: 0.6999 - val_loss: 0.9098 - val_acc: 0.6514\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7809 - acc: 0.6845 - val_loss: 0.9199 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7590 - acc: 0.6909 - val_loss: 0.9210 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7608 - acc: 0.6909 - val_loss: 0.9234 - val_acc: 0.6400\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7745 - acc: 0.6833 - val_loss: 0.9261 - val_acc: 0.6686\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7648 - acc: 0.6884 - val_loss: 0.9231 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 114us/step - loss: 0.7468 - acc: 0.7024 - val_loss: 0.9228 - val_acc: 0.6571\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7643 - acc: 0.6762 - val_loss: 0.9280 - val_acc: 0.6514\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7688 - acc: 0.6954 - val_loss: 0.9269 - val_acc: 0.6343\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7521 - acc: 0.6903 - val_loss: 0.9242 - val_acc: 0.6514\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7581 - acc: 0.6954 - val_loss: 0.9260 - val_acc: 0.6514\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7453 - acc: 0.7024 - val_loss: 0.9225 - val_acc: 0.6400\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7396 - acc: 0.6935 - val_loss: 0.9278 - val_acc: 0.6457\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7194 - acc: 0.7114 - val_loss: 0.9276 - val_acc: 0.6343\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7385 - acc: 0.6992 - val_loss: 0.9366 - val_acc: 0.6400\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.7404 - acc: 0.7075 - val_loss: 0.9465 - val_acc: 0.6514\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.7562 - acc: 0.6986 - val_loss: 0.9377 - val_acc: 0.6457\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 107us/step - loss: 0.7488 - acc: 0.6922 - val_loss: 0.9414 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7317 - acc: 0.6935 - val_loss: 0.9471 - val_acc: 0.6457\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7231 - acc: 0.7197 - val_loss: 0.9485 - val_acc: 0.6286\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7156 - acc: 0.7114 - val_loss: 0.9444 - val_acc: 0.6571\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7308 - acc: 0.6992 - val_loss: 0.9443 - val_acc: 0.6514\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.7303 - acc: 0.7050 - val_loss: 0.9502 - val_acc: 0.6457\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7172 - acc: 0.7069 - val_loss: 0.9469 - val_acc: 0.6686\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7269 - acc: 0.6999 - val_loss: 0.9451 - val_acc: 0.6629\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7061 - acc: 0.7241 - val_loss: 0.9537 - val_acc: 0.6629\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7110 - acc: 0.7209 - val_loss: 0.9494 - val_acc: 0.6743\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7131 - acc: 0.7120 - val_loss: 0.9519 - val_acc: 0.6400\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7051 - acc: 0.7158 - val_loss: 0.9541 - val_acc: 0.6514\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7025 - acc: 0.7261 - val_loss: 0.9634 - val_acc: 0.6514\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7116 - acc: 0.7101 - val_loss: 0.9582 - val_acc: 0.6457\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7238 - acc: 0.7082 - val_loss: 0.9594 - val_acc: 0.6629\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7029 - acc: 0.7018 - val_loss: 0.9612 - val_acc: 0.6571\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6830 - acc: 0.7216 - val_loss: 0.9648 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6965 - acc: 0.7248 - val_loss: 0.9626 - val_acc: 0.6457\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7010 - acc: 0.7158 - val_loss: 0.9671 - val_acc: 0.6514\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6912 - acc: 0.7254 - val_loss: 0.9679 - val_acc: 0.6743\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6924 - acc: 0.7152 - val_loss: 0.9690 - val_acc: 0.6743\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6857 - acc: 0.7241 - val_loss: 0.9730 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6979 - acc: 0.7286 - val_loss: 0.9687 - val_acc: 0.6686\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6800 - acc: 0.7331 - val_loss: 0.9654 - val_acc: 0.6571\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6713 - acc: 0.7324 - val_loss: 0.9675 - val_acc: 0.6457\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6734 - acc: 0.7222 - val_loss: 0.9749 - val_acc: 0.6686\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6537 - acc: 0.7420 - val_loss: 0.9794 - val_acc: 0.6571\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6689 - acc: 0.7254 - val_loss: 0.9781 - val_acc: 0.6571\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6674 - acc: 0.7388 - val_loss: 0.9799 - val_acc: 0.6800\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6655 - acc: 0.7414 - val_loss: 0.9815 - val_acc: 0.6629\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6617 - acc: 0.7388 - val_loss: 0.9857 - val_acc: 0.6629\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6839 - acc: 0.7203 - val_loss: 0.9885 - val_acc: 0.6571\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6524 - acc: 0.7344 - val_loss: 0.9916 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6512 - acc: 0.7414 - val_loss: 0.9830 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6639 - acc: 0.7344 - val_loss: 0.9960 - val_acc: 0.6514\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6710 - acc: 0.7305 - val_loss: 0.9848 - val_acc: 0.6629\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6646 - acc: 0.7312 - val_loss: 0.9901 - val_acc: 0.6686\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6273 - acc: 0.7490 - val_loss: 0.9936 - val_acc: 0.6629\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6600 - acc: 0.7401 - val_loss: 0.9903 - val_acc: 0.6571\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6280 - acc: 0.7612 - val_loss: 0.9958 - val_acc: 0.6457\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6600 - acc: 0.7446 - val_loss: 1.0037 - val_acc: 0.6629\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6420 - acc: 0.7510 - val_loss: 1.0089 - val_acc: 0.6571\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6773 - acc: 0.7254 - val_loss: 1.0084 - val_acc: 0.6514\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6398 - acc: 0.7503 - val_loss: 1.0038 - val_acc: 0.6514\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6458 - acc: 0.7484 - val_loss: 1.0134 - val_acc: 0.6400\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6394 - acc: 0.7548 - val_loss: 1.0147 - val_acc: 0.6571\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6279 - acc: 0.7656 - val_loss: 1.0186 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6225 - acc: 0.7388 - val_loss: 1.0204 - val_acc: 0.6514\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6313 - acc: 0.7452 - val_loss: 1.0251 - val_acc: 0.6629\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6543 - acc: 0.7312 - val_loss: 1.0170 - val_acc: 0.6686\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6213 - acc: 0.7510 - val_loss: 1.0171 - val_acc: 0.6571\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6275 - acc: 0.7458 - val_loss: 1.0213 - val_acc: 0.6457\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6298 - acc: 0.7382 - val_loss: 1.0235 - val_acc: 0.6514\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6261 - acc: 0.7567 - val_loss: 1.0272 - val_acc: 0.6571\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6166 - acc: 0.7510 - val_loss: 1.0333 - val_acc: 0.6457\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6306 - acc: 0.7490 - val_loss: 1.0270 - val_acc: 0.6571\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6172 - acc: 0.7522 - val_loss: 1.0294 - val_acc: 0.6514\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6177 - acc: 0.7452 - val_loss: 1.0323 - val_acc: 0.6571\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6043 - acc: 0.7625 - val_loss: 1.0458 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6220 - acc: 0.7567 - val_loss: 1.0458 - val_acc: 0.6629\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6181 - acc: 0.7561 - val_loss: 1.0519 - val_acc: 0.6400\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6103 - acc: 0.7656 - val_loss: 1.0462 - val_acc: 0.6571\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6270 - acc: 0.7458 - val_loss: 1.0449 - val_acc: 0.6400\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6073 - acc: 0.7637 - val_loss: 1.0402 - val_acc: 0.6457\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6380 - acc: 0.7452 - val_loss: 1.0522 - val_acc: 0.6343\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6128 - acc: 0.7471 - val_loss: 1.0503 - val_acc: 0.6514\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6113 - acc: 0.7580 - val_loss: 1.0518 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6043 - acc: 0.7612 - val_loss: 1.0620 - val_acc: 0.6571\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5891 - acc: 0.7746 - val_loss: 1.0530 - val_acc: 0.6457\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5928 - acc: 0.7688 - val_loss: 1.0517 - val_acc: 0.6514\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5940 - acc: 0.7637 - val_loss: 1.0521 - val_acc: 0.6343\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5818 - acc: 0.7688 - val_loss: 1.0497 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5896 - acc: 0.7695 - val_loss: 1.0610 - val_acc: 0.6514\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5828 - acc: 0.7733 - val_loss: 1.0649 - val_acc: 0.6457\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.5948 - acc: 0.7631 - val_loss: 1.0592 - val_acc: 0.6457\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.5913 - acc: 0.7612 - val_loss: 1.0651 - val_acc: 0.6514\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5822 - acc: 0.7752 - val_loss: 1.0693 - val_acc: 0.6571\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5728 - acc: 0.7695 - val_loss: 1.0653 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5650 - acc: 0.7803 - val_loss: 1.0676 - val_acc: 0.6571\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.5778 - acc: 0.7918 - val_loss: 1.0812 - val_acc: 0.6571\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5890 - acc: 0.7676 - val_loss: 1.0819 - val_acc: 0.6457\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5873 - acc: 0.7637 - val_loss: 1.0819 - val_acc: 0.6400\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5711 - acc: 0.7708 - val_loss: 1.0774 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5705 - acc: 0.7695 - val_loss: 1.0865 - val_acc: 0.6400\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5788 - acc: 0.7605 - val_loss: 1.0916 - val_acc: 0.6457\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5461 - acc: 0.7835 - val_loss: 1.0882 - val_acc: 0.6343\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5660 - acc: 0.7720 - val_loss: 1.0990 - val_acc: 0.6400\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5816 - acc: 0.7625 - val_loss: 1.0994 - val_acc: 0.6343\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5501 - acc: 0.7842 - val_loss: 1.1013 - val_acc: 0.6343\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5573 - acc: 0.7835 - val_loss: 1.0945 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5582 - acc: 0.7791 - val_loss: 1.0922 - val_acc: 0.6571\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5560 - acc: 0.7937 - val_loss: 1.1004 - val_acc: 0.6343\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5416 - acc: 0.7803 - val_loss: 1.0991 - val_acc: 0.6286\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5581 - acc: 0.7746 - val_loss: 1.1061 - val_acc: 0.6457\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5812 - acc: 0.7663 - val_loss: 1.1143 - val_acc: 0.6457\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.5632 - acc: 0.7746 - val_loss: 1.1120 - val_acc: 0.6571\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5349 - acc: 0.7861 - val_loss: 1.1053 - val_acc: 0.6457\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5766 - acc: 0.7752 - val_loss: 1.1114 - val_acc: 0.6457\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5563 - acc: 0.7822 - val_loss: 1.1100 - val_acc: 0.6400\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5329 - acc: 0.7867 - val_loss: 1.1116 - val_acc: 0.6571\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5373 - acc: 0.7810 - val_loss: 1.1117 - val_acc: 0.6457\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5194 - acc: 0.7912 - val_loss: 1.1142 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5562 - acc: 0.7861 - val_loss: 1.1179 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5408 - acc: 0.7810 - val_loss: 1.1187 - val_acc: 0.6457\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5525 - acc: 0.7759 - val_loss: 1.1180 - val_acc: 0.6400\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5486 - acc: 0.7880 - val_loss: 1.1167 - val_acc: 0.6457\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5541 - acc: 0.7771 - val_loss: 1.1225 - val_acc: 0.6571\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5384 - acc: 0.7925 - val_loss: 1.1198 - val_acc: 0.6457\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5423 - acc: 0.7771 - val_loss: 1.1067 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5383 - acc: 0.7835 - val_loss: 1.1257 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5526 - acc: 0.7746 - val_loss: 1.1317 - val_acc: 0.6286\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5294 - acc: 0.7989 - val_loss: 1.1402 - val_acc: 0.6457\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5402 - acc: 0.7771 - val_loss: 1.1358 - val_acc: 0.6571\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5414 - acc: 0.7797 - val_loss: 1.1310 - val_acc: 0.6514\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5173 - acc: 0.7937 - val_loss: 1.1334 - val_acc: 0.6629\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5145 - acc: 0.7867 - val_loss: 1.1304 - val_acc: 0.6400\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.5219 - acc: 0.8027 - val_loss: 1.1303 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5223 - acc: 0.8001 - val_loss: 1.1460 - val_acc: 0.6343\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5249 - acc: 0.7861 - val_loss: 1.1472 - val_acc: 0.6514\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5278 - acc: 0.7957 - val_loss: 1.1486 - val_acc: 0.6457\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5344 - acc: 0.7816 - val_loss: 1.1518 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 68us/step\n",
      "1741/1741 [==============================] - 0s 51us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.5045 - acc: 0.3001 - val_loss: 1.1818 - val_acc: 0.4971\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 1.2751 - acc: 0.4266 - val_loss: 1.0826 - val_acc: 0.6171\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.2104 - acc: 0.4693 - val_loss: 1.0362 - val_acc: 0.6000\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.1520 - acc: 0.4891 - val_loss: 1.0019 - val_acc: 0.6457\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 1.1193 - acc: 0.5006 - val_loss: 0.9870 - val_acc: 0.6286\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.1064 - acc: 0.5307 - val_loss: 0.9682 - val_acc: 0.6514\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0872 - acc: 0.5262 - val_loss: 0.9508 - val_acc: 0.6743\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.0474 - acc: 0.5351 - val_loss: 0.9450 - val_acc: 0.6686\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0242 - acc: 0.5594 - val_loss: 0.9276 - val_acc: 0.6686\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 1.0030 - acc: 0.5632 - val_loss: 0.9210 - val_acc: 0.6743\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9987 - acc: 0.5709 - val_loss: 0.9183 - val_acc: 0.6743\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9949 - acc: 0.5696 - val_loss: 0.9107 - val_acc: 0.6686\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9911 - acc: 0.5862 - val_loss: 0.9090 - val_acc: 0.6800\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9872 - acc: 0.5670 - val_loss: 0.9025 - val_acc: 0.6800\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9814 - acc: 0.5728 - val_loss: 0.8973 - val_acc: 0.6914\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.9576 - acc: 0.6022 - val_loss: 0.8970 - val_acc: 0.6629\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9419 - acc: 0.5939 - val_loss: 0.8902 - val_acc: 0.6914\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9324 - acc: 0.5977 - val_loss: 0.8911 - val_acc: 0.6800\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9293 - acc: 0.6079 - val_loss: 0.8918 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9270 - acc: 0.6028 - val_loss: 0.8870 - val_acc: 0.6914\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.9354 - acc: 0.5951 - val_loss: 0.8830 - val_acc: 0.6914\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9062 - acc: 0.6181 - val_loss: 0.8897 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9208 - acc: 0.5977 - val_loss: 0.8840 - val_acc: 0.6800\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8956 - acc: 0.6232 - val_loss: 0.8878 - val_acc: 0.6800\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9091 - acc: 0.6175 - val_loss: 0.8815 - val_acc: 0.6857\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8848 - acc: 0.6341 - val_loss: 0.8787 - val_acc: 0.6800\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8937 - acc: 0.6347 - val_loss: 0.8828 - val_acc: 0.6857\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8897 - acc: 0.6188 - val_loss: 0.8797 - val_acc: 0.6857\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8769 - acc: 0.6392 - val_loss: 0.8824 - val_acc: 0.6857\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8899 - acc: 0.6232 - val_loss: 0.8785 - val_acc: 0.6743\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8825 - acc: 0.6232 - val_loss: 0.8765 - val_acc: 0.6800\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8521 - acc: 0.6533 - val_loss: 0.8790 - val_acc: 0.6800\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8799 - acc: 0.6392 - val_loss: 0.8756 - val_acc: 0.6857\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8654 - acc: 0.6456 - val_loss: 0.8741 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8546 - acc: 0.6577 - val_loss: 0.8727 - val_acc: 0.6914\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8569 - acc: 0.6475 - val_loss: 0.8773 - val_acc: 0.6857\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8586 - acc: 0.6347 - val_loss: 0.8797 - val_acc: 0.6800\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8426 - acc: 0.6571 - val_loss: 0.8810 - val_acc: 0.6857\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8408 - acc: 0.6584 - val_loss: 0.8762 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8229 - acc: 0.6609 - val_loss: 0.8796 - val_acc: 0.6800\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8355 - acc: 0.6571 - val_loss: 0.8723 - val_acc: 0.6914\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8184 - acc: 0.6590 - val_loss: 0.8808 - val_acc: 0.6857\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.8310 - acc: 0.6571 - val_loss: 0.8774 - val_acc: 0.6857\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8240 - acc: 0.6571 - val_loss: 0.8796 - val_acc: 0.6743\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8292 - acc: 0.6660 - val_loss: 0.8768 - val_acc: 0.6914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8234 - acc: 0.6692 - val_loss: 0.8808 - val_acc: 0.7029\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8188 - acc: 0.6654 - val_loss: 0.8774 - val_acc: 0.6914\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8085 - acc: 0.6628 - val_loss: 0.8821 - val_acc: 0.6914\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8105 - acc: 0.6731 - val_loss: 0.8811 - val_acc: 0.6914\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8065 - acc: 0.6711 - val_loss: 0.8845 - val_acc: 0.6800\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8090 - acc: 0.6762 - val_loss: 0.8834 - val_acc: 0.6914\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7877 - acc: 0.6750 - val_loss: 0.8778 - val_acc: 0.6914\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7807 - acc: 0.6782 - val_loss: 0.8768 - val_acc: 0.6971\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8072 - acc: 0.6686 - val_loss: 0.8828 - val_acc: 0.7029\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7945 - acc: 0.6711 - val_loss: 0.8878 - val_acc: 0.7086\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7786 - acc: 0.6852 - val_loss: 0.8888 - val_acc: 0.6914\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7759 - acc: 0.6845 - val_loss: 0.8926 - val_acc: 0.6743\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7966 - acc: 0.6769 - val_loss: 0.8930 - val_acc: 0.6914\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7798 - acc: 0.6903 - val_loss: 0.8961 - val_acc: 0.6743\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7791 - acc: 0.6890 - val_loss: 0.8978 - val_acc: 0.6743\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7625 - acc: 0.6948 - val_loss: 0.8937 - val_acc: 0.6800\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7765 - acc: 0.6807 - val_loss: 0.8970 - val_acc: 0.6800\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7709 - acc: 0.6750 - val_loss: 0.8953 - val_acc: 0.6800\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7612 - acc: 0.6858 - val_loss: 0.8994 - val_acc: 0.6629\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7696 - acc: 0.6980 - val_loss: 0.9049 - val_acc: 0.6686\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7722 - acc: 0.6839 - val_loss: 0.9117 - val_acc: 0.6686\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7635 - acc: 0.6999 - val_loss: 0.9113 - val_acc: 0.6800\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7496 - acc: 0.7037 - val_loss: 0.9114 - val_acc: 0.6743\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7530 - acc: 0.6928 - val_loss: 0.9084 - val_acc: 0.6743\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7465 - acc: 0.6986 - val_loss: 0.9067 - val_acc: 0.6686\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7318 - acc: 0.6986 - val_loss: 0.9060 - val_acc: 0.6343\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7424 - acc: 0.6941 - val_loss: 0.9071 - val_acc: 0.6800\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7574 - acc: 0.7024 - val_loss: 0.9037 - val_acc: 0.6743\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7359 - acc: 0.6820 - val_loss: 0.9084 - val_acc: 0.6629\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7324 - acc: 0.6948 - val_loss: 0.9120 - val_acc: 0.6686\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7380 - acc: 0.6960 - val_loss: 0.9103 - val_acc: 0.6686\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7184 - acc: 0.7056 - val_loss: 0.9111 - val_acc: 0.6629\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7240 - acc: 0.6980 - val_loss: 0.9136 - val_acc: 0.6800\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7166 - acc: 0.7197 - val_loss: 0.9171 - val_acc: 0.6743\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7303 - acc: 0.7018 - val_loss: 0.9183 - val_acc: 0.6743\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7262 - acc: 0.6948 - val_loss: 0.9270 - val_acc: 0.6571\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7286 - acc: 0.7063 - val_loss: 0.9170 - val_acc: 0.6743\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7152 - acc: 0.7152 - val_loss: 0.9333 - val_acc: 0.6571\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7308 - acc: 0.6992 - val_loss: 0.9183 - val_acc: 0.6571\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7263 - acc: 0.6973 - val_loss: 0.9196 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6900 - acc: 0.7305 - val_loss: 0.9163 - val_acc: 0.6457\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7234 - acc: 0.7088 - val_loss: 0.9229 - val_acc: 0.6571\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7006 - acc: 0.7248 - val_loss: 0.9341 - val_acc: 0.6686\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7046 - acc: 0.7005 - val_loss: 0.9272 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6955 - acc: 0.7209 - val_loss: 0.9251 - val_acc: 0.6686\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7048 - acc: 0.7133 - val_loss: 0.9280 - val_acc: 0.6571\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6861 - acc: 0.7299 - val_loss: 0.9326 - val_acc: 0.6400\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6800 - acc: 0.7261 - val_loss: 0.9372 - val_acc: 0.6514\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6927 - acc: 0.7209 - val_loss: 0.9337 - val_acc: 0.6571\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7014 - acc: 0.7152 - val_loss: 0.9332 - val_acc: 0.6571\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7055 - acc: 0.7229 - val_loss: 0.9320 - val_acc: 0.6514\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6972 - acc: 0.7178 - val_loss: 0.9435 - val_acc: 0.6514\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6878 - acc: 0.7114 - val_loss: 0.9381 - val_acc: 0.6571\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6772 - acc: 0.7222 - val_loss: 0.9384 - val_acc: 0.6629\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7102 - acc: 0.7235 - val_loss: 0.9337 - val_acc: 0.6686\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6708 - acc: 0.7337 - val_loss: 0.9434 - val_acc: 0.6514\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6806 - acc: 0.7305 - val_loss: 0.9379 - val_acc: 0.6514\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6875 - acc: 0.7165 - val_loss: 0.9374 - val_acc: 0.6343\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6780 - acc: 0.7273 - val_loss: 0.9377 - val_acc: 0.6400\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6628 - acc: 0.7350 - val_loss: 0.9460 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6788 - acc: 0.7254 - val_loss: 0.9564 - val_acc: 0.6571\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6665 - acc: 0.7286 - val_loss: 0.9479 - val_acc: 0.6571\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6688 - acc: 0.7216 - val_loss: 0.9489 - val_acc: 0.6457\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6753 - acc: 0.7235 - val_loss: 0.9551 - val_acc: 0.6400\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6569 - acc: 0.7305 - val_loss: 0.9566 - val_acc: 0.6514\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6357 - acc: 0.7388 - val_loss: 0.9564 - val_acc: 0.6571\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6514 - acc: 0.7388 - val_loss: 0.9566 - val_acc: 0.6514\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6622 - acc: 0.7331 - val_loss: 0.9512 - val_acc: 0.6514\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6438 - acc: 0.7446 - val_loss: 0.9568 - val_acc: 0.6686\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6425 - acc: 0.7261 - val_loss: 0.9573 - val_acc: 0.6629\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6632 - acc: 0.7401 - val_loss: 0.9631 - val_acc: 0.6629\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6322 - acc: 0.7446 - val_loss: 0.9619 - val_acc: 0.6571\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6562 - acc: 0.7350 - val_loss: 0.9703 - val_acc: 0.6571\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6284 - acc: 0.7484 - val_loss: 0.9699 - val_acc: 0.6629\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6454 - acc: 0.7484 - val_loss: 0.9698 - val_acc: 0.6514\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6146 - acc: 0.7580 - val_loss: 0.9664 - val_acc: 0.6514\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6242 - acc: 0.7312 - val_loss: 0.9745 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6379 - acc: 0.7312 - val_loss: 0.9696 - val_acc: 0.6457\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6231 - acc: 0.7593 - val_loss: 0.9759 - val_acc: 0.6629\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6602 - acc: 0.7305 - val_loss: 0.9784 - val_acc: 0.6400\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6334 - acc: 0.7305 - val_loss: 0.9768 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6464 - acc: 0.7356 - val_loss: 0.9688 - val_acc: 0.6629\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6152 - acc: 0.7644 - val_loss: 0.9697 - val_acc: 0.6514\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6201 - acc: 0.7497 - val_loss: 0.9772 - val_acc: 0.6457\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6012 - acc: 0.7625 - val_loss: 0.9798 - val_acc: 0.6514\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6223 - acc: 0.7497 - val_loss: 0.9811 - val_acc: 0.6629\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6140 - acc: 0.7375 - val_loss: 0.9785 - val_acc: 0.6457\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6135 - acc: 0.7522 - val_loss: 0.9855 - val_acc: 0.6400\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6040 - acc: 0.7561 - val_loss: 0.9913 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6055 - acc: 0.7484 - val_loss: 0.9852 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6170 - acc: 0.7535 - val_loss: 0.9842 - val_acc: 0.6571\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6211 - acc: 0.7490 - val_loss: 0.9874 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6200 - acc: 0.7458 - val_loss: 0.9954 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5799 - acc: 0.7656 - val_loss: 0.9957 - val_acc: 0.6514\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5891 - acc: 0.7676 - val_loss: 1.0072 - val_acc: 0.6457\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6139 - acc: 0.7542 - val_loss: 0.9980 - val_acc: 0.6571\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5988 - acc: 0.7554 - val_loss: 1.0035 - val_acc: 0.6457\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5871 - acc: 0.7522 - val_loss: 1.0038 - val_acc: 0.6571\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5780 - acc: 0.7688 - val_loss: 1.0152 - val_acc: 0.6400\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5762 - acc: 0.7650 - val_loss: 1.0058 - val_acc: 0.6514\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5852 - acc: 0.7669 - val_loss: 1.0010 - val_acc: 0.6457\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5809 - acc: 0.7739 - val_loss: 1.0174 - val_acc: 0.6571\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5877 - acc: 0.7605 - val_loss: 1.0230 - val_acc: 0.6286\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5704 - acc: 0.7733 - val_loss: 1.0186 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5805 - acc: 0.7688 - val_loss: 1.0140 - val_acc: 0.6286\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5762 - acc: 0.7682 - val_loss: 1.0184 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5932 - acc: 0.7510 - val_loss: 1.0124 - val_acc: 0.6343\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5984 - acc: 0.7497 - val_loss: 1.0139 - val_acc: 0.6400\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5679 - acc: 0.7714 - val_loss: 1.0170 - val_acc: 0.6400\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5842 - acc: 0.7720 - val_loss: 1.0266 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5756 - acc: 0.7682 - val_loss: 1.0260 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5728 - acc: 0.7669 - val_loss: 1.0255 - val_acc: 0.6457\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5902 - acc: 0.7708 - val_loss: 1.0201 - val_acc: 0.6457\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5665 - acc: 0.7791 - val_loss: 1.0187 - val_acc: 0.6400\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5680 - acc: 0.7759 - val_loss: 1.0290 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5664 - acc: 0.7720 - val_loss: 1.0360 - val_acc: 0.6400\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5569 - acc: 0.7848 - val_loss: 1.0352 - val_acc: 0.6343\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5726 - acc: 0.7752 - val_loss: 1.0280 - val_acc: 0.6343\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5582 - acc: 0.7727 - val_loss: 1.0330 - val_acc: 0.6343\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5578 - acc: 0.7816 - val_loss: 1.0347 - val_acc: 0.6343\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5655 - acc: 0.7688 - val_loss: 1.0383 - val_acc: 0.6514\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5376 - acc: 0.7861 - val_loss: 1.0313 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5628 - acc: 0.7784 - val_loss: 1.0363 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5503 - acc: 0.7714 - val_loss: 1.0448 - val_acc: 0.6400\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.5629 - acc: 0.7848 - val_loss: 1.0395 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5485 - acc: 0.7854 - val_loss: 1.0456 - val_acc: 0.6571\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5709 - acc: 0.7765 - val_loss: 1.0518 - val_acc: 0.6343\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5416 - acc: 0.7714 - val_loss: 1.0434 - val_acc: 0.6400\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5575 - acc: 0.7835 - val_loss: 1.0468 - val_acc: 0.6400\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5344 - acc: 0.7886 - val_loss: 1.0476 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5318 - acc: 0.7982 - val_loss: 1.0539 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5442 - acc: 0.7905 - val_loss: 1.0493 - val_acc: 0.6571\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5306 - acc: 0.7752 - val_loss: 1.0498 - val_acc: 0.6514\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5240 - acc: 0.7963 - val_loss: 1.0640 - val_acc: 0.6514\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5323 - acc: 0.7848 - val_loss: 1.0684 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5426 - acc: 0.7797 - val_loss: 1.0680 - val_acc: 0.6514\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5364 - acc: 0.7848 - val_loss: 1.0623 - val_acc: 0.6514\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5380 - acc: 0.7874 - val_loss: 1.0719 - val_acc: 0.6514\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5352 - acc: 0.7957 - val_loss: 1.0743 - val_acc: 0.6400\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4905 - acc: 0.8046 - val_loss: 1.0753 - val_acc: 0.6457\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5387 - acc: 0.7829 - val_loss: 1.0620 - val_acc: 0.6457\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5187 - acc: 0.7867 - val_loss: 1.0667 - val_acc: 0.6400\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5317 - acc: 0.7861 - val_loss: 1.0743 - val_acc: 0.6400\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5205 - acc: 0.7950 - val_loss: 1.0690 - val_acc: 0.6571\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5321 - acc: 0.7842 - val_loss: 1.0732 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5361 - acc: 0.7893 - val_loss: 1.0709 - val_acc: 0.6457\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5280 - acc: 0.7912 - val_loss: 1.0797 - val_acc: 0.6343\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5407 - acc: 0.7791 - val_loss: 1.0785 - val_acc: 0.6400\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4932 - acc: 0.8091 - val_loss: 1.0855 - val_acc: 0.6343\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5043 - acc: 0.8027 - val_loss: 1.0816 - val_acc: 0.6343\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5079 - acc: 0.7976 - val_loss: 1.0851 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4946 - acc: 0.7899 - val_loss: 1.0904 - val_acc: 0.6457\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5403 - acc: 0.7905 - val_loss: 1.0899 - val_acc: 0.6400\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5068 - acc: 0.8084 - val_loss: 1.0922 - val_acc: 0.6286\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5126 - acc: 0.7905 - val_loss: 1.1031 - val_acc: 0.6514\n",
      "194/194 [==============================] - 0s 78us/step\n",
      "1741/1741 [==============================] - 0s 53us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.4445 - acc: 0.3159 - val_loss: 1.2097 - val_acc: 0.5143\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.2584 - acc: 0.4084 - val_loss: 1.1143 - val_acc: 0.5657\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 1.1857 - acc: 0.4786 - val_loss: 1.0510 - val_acc: 0.6171\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.1498 - acc: 0.4997 - val_loss: 1.0202 - val_acc: 0.6229\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 1.1051 - acc: 0.5131 - val_loss: 0.9996 - val_acc: 0.6400\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 1.0839 - acc: 0.5373 - val_loss: 0.9831 - val_acc: 0.6514\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0655 - acc: 0.5380 - val_loss: 0.9694 - val_acc: 0.6286\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.0367 - acc: 0.5469 - val_loss: 0.9539 - val_acc: 0.6743\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 1.0572 - acc: 0.5482 - val_loss: 0.9426 - val_acc: 0.6743\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 1.0301 - acc: 0.5539 - val_loss: 0.9417 - val_acc: 0.6914\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9962 - acc: 0.5814 - val_loss: 0.9395 - val_acc: 0.6800\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9931 - acc: 0.5686 - val_loss: 0.9271 - val_acc: 0.6857\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9739 - acc: 0.5641 - val_loss: 0.9291 - val_acc: 0.6800\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9745 - acc: 0.5986 - val_loss: 0.9203 - val_acc: 0.6914\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9699 - acc: 0.5724 - val_loss: 0.9123 - val_acc: 0.6914\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9582 - acc: 0.5865 - val_loss: 0.9149 - val_acc: 0.6800\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9504 - acc: 0.6024 - val_loss: 0.9115 - val_acc: 0.6857\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9486 - acc: 0.5980 - val_loss: 0.9100 - val_acc: 0.6800\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9272 - acc: 0.6056 - val_loss: 0.9070 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9173 - acc: 0.6273 - val_loss: 0.9078 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.9182 - acc: 0.6146 - val_loss: 0.9070 - val_acc: 0.6686\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9126 - acc: 0.6126 - val_loss: 0.9059 - val_acc: 0.6743\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9215 - acc: 0.6158 - val_loss: 0.9054 - val_acc: 0.6743\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9287 - acc: 0.6037 - val_loss: 0.9052 - val_acc: 0.6743\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8845 - acc: 0.6280 - val_loss: 0.8978 - val_acc: 0.6857\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8973 - acc: 0.6177 - val_loss: 0.8966 - val_acc: 0.6743\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8946 - acc: 0.6216 - val_loss: 0.8991 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8705 - acc: 0.6324 - val_loss: 0.8994 - val_acc: 0.6914\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8810 - acc: 0.6331 - val_loss: 0.9014 - val_acc: 0.6857\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.8726 - acc: 0.6362 - val_loss: 0.8958 - val_acc: 0.6857\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8533 - acc: 0.6452 - val_loss: 0.9013 - val_acc: 0.6800\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8502 - acc: 0.6433 - val_loss: 0.8990 - val_acc: 0.6857\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8648 - acc: 0.6452 - val_loss: 0.9058 - val_acc: 0.6914\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8570 - acc: 0.6490 - val_loss: 0.8985 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8434 - acc: 0.6599 - val_loss: 0.9053 - val_acc: 0.6743\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8484 - acc: 0.6439 - val_loss: 0.9048 - val_acc: 0.6914\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8369 - acc: 0.6426 - val_loss: 0.9024 - val_acc: 0.6800\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8322 - acc: 0.6471 - val_loss: 0.9093 - val_acc: 0.6914\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8173 - acc: 0.6739 - val_loss: 0.9099 - val_acc: 0.6857\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8312 - acc: 0.6433 - val_loss: 0.9028 - val_acc: 0.6857\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8312 - acc: 0.6579 - val_loss: 0.9067 - val_acc: 0.6857\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8328 - acc: 0.6516 - val_loss: 0.9077 - val_acc: 0.6914\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.8177 - acc: 0.6560 - val_loss: 0.9133 - val_acc: 0.6914\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8185 - acc: 0.6618 - val_loss: 0.9118 - val_acc: 0.6914\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8136 - acc: 0.6682 - val_loss: 0.9148 - val_acc: 0.6800\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8238 - acc: 0.6713 - val_loss: 0.9103 - val_acc: 0.6914\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8209 - acc: 0.6726 - val_loss: 0.9084 - val_acc: 0.6914\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7936 - acc: 0.6777 - val_loss: 0.9163 - val_acc: 0.6971\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7933 - acc: 0.6796 - val_loss: 0.9129 - val_acc: 0.6800\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8059 - acc: 0.6560 - val_loss: 0.9124 - val_acc: 0.6857\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8158 - acc: 0.6713 - val_loss: 0.9059 - val_acc: 0.7029\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7968 - acc: 0.6713 - val_loss: 0.9110 - val_acc: 0.7029\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7818 - acc: 0.6771 - val_loss: 0.9095 - val_acc: 0.6800\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7952 - acc: 0.6771 - val_loss: 0.9167 - val_acc: 0.6800\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7787 - acc: 0.6867 - val_loss: 0.9198 - val_acc: 0.6800\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7681 - acc: 0.6765 - val_loss: 0.9140 - val_acc: 0.7029\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7803 - acc: 0.6816 - val_loss: 0.9180 - val_acc: 0.6686\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7810 - acc: 0.6803 - val_loss: 0.9256 - val_acc: 0.6914\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7995 - acc: 0.6573 - val_loss: 0.9197 - val_acc: 0.7086\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7605 - acc: 0.6924 - val_loss: 0.9260 - val_acc: 0.6800\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7675 - acc: 0.6975 - val_loss: 0.9255 - val_acc: 0.6857\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7743 - acc: 0.6796 - val_loss: 0.9252 - val_acc: 0.6857\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7637 - acc: 0.6924 - val_loss: 0.9304 - val_acc: 0.6857\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7659 - acc: 0.6822 - val_loss: 0.9280 - val_acc: 0.6971\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7624 - acc: 0.6943 - val_loss: 0.9313 - val_acc: 0.6971\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7518 - acc: 0.6790 - val_loss: 0.9333 - val_acc: 0.6743\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.7567 - acc: 0.6975 - val_loss: 0.9422 - val_acc: 0.6800\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7439 - acc: 0.6867 - val_loss: 0.9365 - val_acc: 0.6914\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7357 - acc: 0.6962 - val_loss: 0.9315 - val_acc: 0.6914\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7408 - acc: 0.6905 - val_loss: 0.9377 - val_acc: 0.6857\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7198 - acc: 0.7052 - val_loss: 0.9354 - val_acc: 0.6743\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7380 - acc: 0.6937 - val_loss: 0.9449 - val_acc: 0.6800\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7317 - acc: 0.6924 - val_loss: 0.9432 - val_acc: 0.6914\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7267 - acc: 0.7020 - val_loss: 0.9447 - val_acc: 0.6800\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7236 - acc: 0.7173 - val_loss: 0.9442 - val_acc: 0.6857\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7141 - acc: 0.7084 - val_loss: 0.9478 - val_acc: 0.6857\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7331 - acc: 0.7116 - val_loss: 0.9568 - val_acc: 0.6800\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7362 - acc: 0.6956 - val_loss: 0.9600 - val_acc: 0.6857\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7166 - acc: 0.6988 - val_loss: 0.9603 - val_acc: 0.6800\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6933 - acc: 0.7269 - val_loss: 0.9714 - val_acc: 0.6857\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7326 - acc: 0.6975 - val_loss: 0.9574 - val_acc: 0.6971\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7192 - acc: 0.6981 - val_loss: 0.9623 - val_acc: 0.6857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7021 - acc: 0.7141 - val_loss: 0.9564 - val_acc: 0.6857\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7064 - acc: 0.7064 - val_loss: 0.9560 - val_acc: 0.6914\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7141 - acc: 0.7090 - val_loss: 0.9651 - val_acc: 0.6743\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7154 - acc: 0.7020 - val_loss: 0.9637 - val_acc: 0.6857\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6925 - acc: 0.7084 - val_loss: 0.9532 - val_acc: 0.6800\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7182 - acc: 0.7135 - val_loss: 0.9611 - val_acc: 0.6971\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6994 - acc: 0.7033 - val_loss: 0.9695 - val_acc: 0.6857\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6948 - acc: 0.7128 - val_loss: 0.9622 - val_acc: 0.6800\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7002 - acc: 0.7192 - val_loss: 0.9613 - val_acc: 0.6800\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6971 - acc: 0.7352 - val_loss: 0.9742 - val_acc: 0.6800\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6949 - acc: 0.7135 - val_loss: 0.9732 - val_acc: 0.6914\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 134us/step - loss: 0.7029 - acc: 0.7122 - val_loss: 0.9727 - val_acc: 0.6743\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6870 - acc: 0.7167 - val_loss: 0.9864 - val_acc: 0.6629\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6555 - acc: 0.7301 - val_loss: 0.9765 - val_acc: 0.6743\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6613 - acc: 0.7301 - val_loss: 0.9788 - val_acc: 0.6686\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6732 - acc: 0.7352 - val_loss: 0.9760 - val_acc: 0.6857\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6763 - acc: 0.7230 - val_loss: 0.9889 - val_acc: 0.6686\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6780 - acc: 0.7154 - val_loss: 0.9858 - val_acc: 0.6686\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6643 - acc: 0.7230 - val_loss: 0.9884 - val_acc: 0.6800\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.6675 - acc: 0.7224 - val_loss: 0.9922 - val_acc: 0.6743\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6634 - acc: 0.7320 - val_loss: 0.9839 - val_acc: 0.6800\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.6496 - acc: 0.7435 - val_loss: 0.9969 - val_acc: 0.6514\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6549 - acc: 0.7269 - val_loss: 0.9948 - val_acc: 0.6743\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.6634 - acc: 0.7422 - val_loss: 0.9979 - val_acc: 0.6571\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6485 - acc: 0.7352 - val_loss: 0.9978 - val_acc: 0.6571\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6582 - acc: 0.7371 - val_loss: 0.9954 - val_acc: 0.6457\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6519 - acc: 0.7294 - val_loss: 0.9960 - val_acc: 0.6571\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6541 - acc: 0.7275 - val_loss: 1.0043 - val_acc: 0.6629\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6641 - acc: 0.7390 - val_loss: 1.0024 - val_acc: 0.6514\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6368 - acc: 0.7435 - val_loss: 1.0057 - val_acc: 0.6629\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6371 - acc: 0.7466 - val_loss: 1.0005 - val_acc: 0.6629\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6590 - acc: 0.7371 - val_loss: 1.0026 - val_acc: 0.6629\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6411 - acc: 0.7435 - val_loss: 1.0170 - val_acc: 0.6514\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6586 - acc: 0.7352 - val_loss: 1.0132 - val_acc: 0.6457\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6359 - acc: 0.7358 - val_loss: 1.0209 - val_acc: 0.6457\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.6391 - acc: 0.7377 - val_loss: 1.0185 - val_acc: 0.6514\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6283 - acc: 0.7530 - val_loss: 1.0156 - val_acc: 0.6514\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6475 - acc: 0.7422 - val_loss: 1.0184 - val_acc: 0.6457\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6320 - acc: 0.7549 - val_loss: 1.0163 - val_acc: 0.6571\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6301 - acc: 0.7562 - val_loss: 1.0183 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6417 - acc: 0.7377 - val_loss: 1.0277 - val_acc: 0.6571\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6215 - acc: 0.7396 - val_loss: 1.0308 - val_acc: 0.6571\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6427 - acc: 0.7364 - val_loss: 1.0308 - val_acc: 0.6571\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6190 - acc: 0.7473 - val_loss: 1.0291 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6320 - acc: 0.7313 - val_loss: 1.0410 - val_acc: 0.6514\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6282 - acc: 0.7371 - val_loss: 1.0350 - val_acc: 0.6514\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6263 - acc: 0.7543 - val_loss: 1.0454 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6096 - acc: 0.7581 - val_loss: 1.0490 - val_acc: 0.6457\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6350 - acc: 0.7371 - val_loss: 1.0339 - val_acc: 0.6629\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6138 - acc: 0.7479 - val_loss: 1.0472 - val_acc: 0.6571\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5862 - acc: 0.7530 - val_loss: 1.0513 - val_acc: 0.6514\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 113us/step - loss: 0.6012 - acc: 0.7569 - val_loss: 1.0495 - val_acc: 0.6571\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 160us/step - loss: 0.5997 - acc: 0.7524 - val_loss: 1.0608 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.5916 - acc: 0.7709 - val_loss: 1.0593 - val_acc: 0.6571\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.6074 - acc: 0.7581 - val_loss: 1.0528 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6058 - acc: 0.7594 - val_loss: 1.0643 - val_acc: 0.6514\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6070 - acc: 0.7505 - val_loss: 1.0673 - val_acc: 0.6571\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5977 - acc: 0.7588 - val_loss: 1.0684 - val_acc: 0.6343\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6042 - acc: 0.7601 - val_loss: 1.0784 - val_acc: 0.6629\n",
      "Epoch 142/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5899 - acc: 0.7556 - val_loss: 1.0698 - val_acc: 0.6571\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5857 - acc: 0.7741 - val_loss: 1.0680 - val_acc: 0.6400\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6018 - acc: 0.7588 - val_loss: 1.0779 - val_acc: 0.6400\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5740 - acc: 0.7594 - val_loss: 1.0781 - val_acc: 0.6514\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5663 - acc: 0.7747 - val_loss: 1.0734 - val_acc: 0.6457\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5772 - acc: 0.7703 - val_loss: 1.0805 - val_acc: 0.6457\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5888 - acc: 0.7594 - val_loss: 1.0818 - val_acc: 0.6400\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5781 - acc: 0.7683 - val_loss: 1.0881 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6021 - acc: 0.7549 - val_loss: 1.0759 - val_acc: 0.6629\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5759 - acc: 0.7664 - val_loss: 1.0885 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5602 - acc: 0.7766 - val_loss: 1.0968 - val_acc: 0.6400\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5734 - acc: 0.7805 - val_loss: 1.0861 - val_acc: 0.6400\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5519 - acc: 0.7824 - val_loss: 1.0996 - val_acc: 0.6400\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5581 - acc: 0.7805 - val_loss: 1.0870 - val_acc: 0.6457\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5909 - acc: 0.7671 - val_loss: 1.0989 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5564 - acc: 0.7735 - val_loss: 1.0889 - val_acc: 0.6571\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5683 - acc: 0.7754 - val_loss: 1.1008 - val_acc: 0.6400\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5663 - acc: 0.7690 - val_loss: 1.0985 - val_acc: 0.6286\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5427 - acc: 0.7964 - val_loss: 1.1163 - val_acc: 0.6343\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5592 - acc: 0.7741 - val_loss: 1.1145 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5505 - acc: 0.7830 - val_loss: 1.1137 - val_acc: 0.6286\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5780 - acc: 0.7639 - val_loss: 1.1167 - val_acc: 0.6286\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5598 - acc: 0.7741 - val_loss: 1.1118 - val_acc: 0.6343\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5571 - acc: 0.7773 - val_loss: 1.1089 - val_acc: 0.6286\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5505 - acc: 0.7830 - val_loss: 1.1191 - val_acc: 0.6400\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.5455 - acc: 0.7843 - val_loss: 1.1197 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5631 - acc: 0.7741 - val_loss: 1.1197 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5780 - acc: 0.7703 - val_loss: 1.1244 - val_acc: 0.6400\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5484 - acc: 0.7811 - val_loss: 1.1197 - val_acc: 0.6400\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5600 - acc: 0.7741 - val_loss: 1.1195 - val_acc: 0.6343\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5428 - acc: 0.7856 - val_loss: 1.1280 - val_acc: 0.6286\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5356 - acc: 0.7875 - val_loss: 1.1245 - val_acc: 0.6400\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5512 - acc: 0.7837 - val_loss: 1.1408 - val_acc: 0.6457\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5483 - acc: 0.7900 - val_loss: 1.1392 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5412 - acc: 0.7786 - val_loss: 1.1464 - val_acc: 0.6400\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5554 - acc: 0.7843 - val_loss: 1.1369 - val_acc: 0.6400\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5394 - acc: 0.7881 - val_loss: 1.1439 - val_acc: 0.6343\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5339 - acc: 0.7798 - val_loss: 1.1483 - val_acc: 0.6286\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5382 - acc: 0.7945 - val_loss: 1.1435 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5325 - acc: 0.7869 - val_loss: 1.1507 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5126 - acc: 0.7958 - val_loss: 1.1548 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5459 - acc: 0.7926 - val_loss: 1.1501 - val_acc: 0.6457\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5197 - acc: 0.8028 - val_loss: 1.1557 - val_acc: 0.6457\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5242 - acc: 0.7894 - val_loss: 1.1613 - val_acc: 0.6457\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5115 - acc: 0.8022 - val_loss: 1.1589 - val_acc: 0.6571\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5157 - acc: 0.7996 - val_loss: 1.1631 - val_acc: 0.6457\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5075 - acc: 0.7939 - val_loss: 1.1594 - val_acc: 0.6457\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5228 - acc: 0.8015 - val_loss: 1.1677 - val_acc: 0.6514\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5179 - acc: 0.7856 - val_loss: 1.1726 - val_acc: 0.6514\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.5358 - acc: 0.7843 - val_loss: 1.1763 - val_acc: 0.6400\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.4998 - acc: 0.8111 - val_loss: 1.1617 - val_acc: 0.6400\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5119 - acc: 0.7983 - val_loss: 1.1711 - val_acc: 0.6343\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5137 - acc: 0.7932 - val_loss: 1.1752 - val_acc: 0.6343\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4973 - acc: 0.7996 - val_loss: 1.1641 - val_acc: 0.6400\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5032 - acc: 0.8009 - val_loss: 1.1745 - val_acc: 0.6229\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5009 - acc: 0.8117 - val_loss: 1.1875 - val_acc: 0.6171\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5171 - acc: 0.7990 - val_loss: 1.1857 - val_acc: 0.6229\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5188 - acc: 0.7939 - val_loss: 1.1827 - val_acc: 0.6229\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.4944 - acc: 0.7913 - val_loss: 1.1858 - val_acc: 0.6171\n",
      "193/193 [==============================] - 0s 75us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742/1742 [==============================] - 0s 53us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 7s 4ms/step - loss: 1.5581 - acc: 0.2999 - val_loss: 1.2316 - val_acc: 0.4800\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 1.3056 - acc: 0.3931 - val_loss: 1.1104 - val_acc: 0.5600\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.2204 - acc: 0.4620 - val_loss: 1.0568 - val_acc: 0.6000\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 1.1754 - acc: 0.4793 - val_loss: 1.0169 - val_acc: 0.6229\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.1231 - acc: 0.5105 - val_loss: 0.9950 - val_acc: 0.6343\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0870 - acc: 0.5227 - val_loss: 0.9766 - val_acc: 0.6343\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 1.0725 - acc: 0.5322 - val_loss: 0.9613 - val_acc: 0.6286\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 1.0274 - acc: 0.5648 - val_loss: 0.9411 - val_acc: 0.6400\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.0116 - acc: 0.5712 - val_loss: 0.9344 - val_acc: 0.6343\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 1.0313 - acc: 0.5680 - val_loss: 0.9260 - val_acc: 0.6514\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9963 - acc: 0.5705 - val_loss: 0.9172 - val_acc: 0.6400\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9958 - acc: 0.5775 - val_loss: 0.9076 - val_acc: 0.6400\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.9783 - acc: 0.5686 - val_loss: 0.9010 - val_acc: 0.6343\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.9626 - acc: 0.6069 - val_loss: 0.8971 - val_acc: 0.6571\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9372 - acc: 0.6082 - val_loss: 0.8956 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9401 - acc: 0.6031 - val_loss: 0.8822 - val_acc: 0.6629\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9430 - acc: 0.6101 - val_loss: 0.8856 - val_acc: 0.6514\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9317 - acc: 0.6158 - val_loss: 0.8792 - val_acc: 0.6457\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9378 - acc: 0.6050 - val_loss: 0.8803 - val_acc: 0.6400\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9191 - acc: 0.6031 - val_loss: 0.8742 - val_acc: 0.6457\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8938 - acc: 0.6362 - val_loss: 0.8775 - val_acc: 0.6514\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9161 - acc: 0.6011 - val_loss: 0.8712 - val_acc: 0.6457\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9022 - acc: 0.6101 - val_loss: 0.8695 - val_acc: 0.6457\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9162 - acc: 0.6063 - val_loss: 0.8665 - val_acc: 0.6571\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8954 - acc: 0.6331 - val_loss: 0.8676 - val_acc: 0.6686\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.8822 - acc: 0.6420 - val_loss: 0.8735 - val_acc: 0.6629\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8764 - acc: 0.6465 - val_loss: 0.8712 - val_acc: 0.6629\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8860 - acc: 0.6267 - val_loss: 0.8643 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8717 - acc: 0.6388 - val_loss: 0.8648 - val_acc: 0.6743\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8651 - acc: 0.6477 - val_loss: 0.8629 - val_acc: 0.6514\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8603 - acc: 0.6439 - val_loss: 0.8645 - val_acc: 0.6571\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8656 - acc: 0.6490 - val_loss: 0.8658 - val_acc: 0.6743\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8360 - acc: 0.6458 - val_loss: 0.8593 - val_acc: 0.6686\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.8491 - acc: 0.6675 - val_loss: 0.8641 - val_acc: 0.6629\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.8403 - acc: 0.6592 - val_loss: 0.8639 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.8391 - acc: 0.6631 - val_loss: 0.8618 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8521 - acc: 0.6592 - val_loss: 0.8636 - val_acc: 0.6686\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8401 - acc: 0.6675 - val_loss: 0.8584 - val_acc: 0.6743\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8321 - acc: 0.6624 - val_loss: 0.8625 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8431 - acc: 0.6426 - val_loss: 0.8587 - val_acc: 0.6857\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8439 - acc: 0.6541 - val_loss: 0.8573 - val_acc: 0.6743\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8199 - acc: 0.6631 - val_loss: 0.8573 - val_acc: 0.6800\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8217 - acc: 0.6662 - val_loss: 0.8586 - val_acc: 0.6800\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8390 - acc: 0.6554 - val_loss: 0.8606 - val_acc: 0.6743\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8267 - acc: 0.6592 - val_loss: 0.8611 - val_acc: 0.6686\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8028 - acc: 0.6618 - val_loss: 0.8570 - val_acc: 0.6686\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8144 - acc: 0.6631 - val_loss: 0.8602 - val_acc: 0.6743\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7892 - acc: 0.6694 - val_loss: 0.8600 - val_acc: 0.6743\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7870 - acc: 0.6790 - val_loss: 0.8658 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7901 - acc: 0.6694 - val_loss: 0.8622 - val_acc: 0.6857\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8012 - acc: 0.6701 - val_loss: 0.8569 - val_acc: 0.6971\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7780 - acc: 0.6809 - val_loss: 0.8618 - val_acc: 0.6857\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7738 - acc: 0.6809 - val_loss: 0.8569 - val_acc: 0.6686\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7870 - acc: 0.6892 - val_loss: 0.8570 - val_acc: 0.6800\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7751 - acc: 0.6867 - val_loss: 0.8624 - val_acc: 0.6857\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7644 - acc: 0.6860 - val_loss: 0.8683 - val_acc: 0.6686\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7865 - acc: 0.6879 - val_loss: 0.8674 - val_acc: 0.6857\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7677 - acc: 0.6911 - val_loss: 0.8625 - val_acc: 0.6743\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7605 - acc: 0.6937 - val_loss: 0.8603 - val_acc: 0.6800\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7677 - acc: 0.6828 - val_loss: 0.8605 - val_acc: 0.6686\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7729 - acc: 0.6822 - val_loss: 0.8635 - val_acc: 0.6800\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7490 - acc: 0.7001 - val_loss: 0.8703 - val_acc: 0.6629\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7563 - acc: 0.6847 - val_loss: 0.8643 - val_acc: 0.6857\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7396 - acc: 0.7090 - val_loss: 0.8635 - val_acc: 0.6914\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7498 - acc: 0.6969 - val_loss: 0.8682 - val_acc: 0.6743\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7558 - acc: 0.6956 - val_loss: 0.8639 - val_acc: 0.6686\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7428 - acc: 0.7026 - val_loss: 0.8641 - val_acc: 0.6629\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7273 - acc: 0.7135 - val_loss: 0.8707 - val_acc: 0.6686\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.7310 - acc: 0.7096 - val_loss: 0.8692 - val_acc: 0.6743\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.7492 - acc: 0.6994 - val_loss: 0.8720 - val_acc: 0.6743\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 109us/step - loss: 0.7319 - acc: 0.7039 - val_loss: 0.8692 - val_acc: 0.6800\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 137us/step - loss: 0.7458 - acc: 0.7001 - val_loss: 0.8737 - val_acc: 0.6743\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.7351 - acc: 0.6969 - val_loss: 0.8740 - val_acc: 0.6686\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7342 - acc: 0.6994 - val_loss: 0.8711 - val_acc: 0.6857\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7175 - acc: 0.7045 - val_loss: 0.8745 - val_acc: 0.6743\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7100 - acc: 0.7116 - val_loss: 0.8766 - val_acc: 0.6686\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7129 - acc: 0.7052 - val_loss: 0.8787 - val_acc: 0.6743\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7105 - acc: 0.7198 - val_loss: 0.8764 - val_acc: 0.6743\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6962 - acc: 0.7186 - val_loss: 0.8774 - val_acc: 0.6743\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7244 - acc: 0.7077 - val_loss: 0.8735 - val_acc: 0.6857\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7027 - acc: 0.7122 - val_loss: 0.8842 - val_acc: 0.6629\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7188 - acc: 0.7192 - val_loss: 0.8883 - val_acc: 0.6800\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7061 - acc: 0.7154 - val_loss: 0.8806 - val_acc: 0.6743\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7078 - acc: 0.6994 - val_loss: 0.8849 - val_acc: 0.6743\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6949 - acc: 0.7186 - val_loss: 0.8803 - val_acc: 0.6800\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6930 - acc: 0.7128 - val_loss: 0.8819 - val_acc: 0.6800\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7046 - acc: 0.7116 - val_loss: 0.8808 - val_acc: 0.6800\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7020 - acc: 0.7237 - val_loss: 0.8799 - val_acc: 0.6800\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.7028 - acc: 0.7147 - val_loss: 0.8770 - val_acc: 0.6800\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6957 - acc: 0.7167 - val_loss: 0.8750 - val_acc: 0.6857\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6870 - acc: 0.7192 - val_loss: 0.8809 - val_acc: 0.6686\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7026 - acc: 0.7269 - val_loss: 0.8771 - val_acc: 0.6800\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6752 - acc: 0.7262 - val_loss: 0.8830 - val_acc: 0.6800\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6820 - acc: 0.7243 - val_loss: 0.8904 - val_acc: 0.6800\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6923 - acc: 0.7218 - val_loss: 0.8881 - val_acc: 0.6857\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.6634 - acc: 0.7307 - val_loss: 0.8806 - val_acc: 0.6743\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6624 - acc: 0.7262 - val_loss: 0.8792 - val_acc: 0.6743\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6747 - acc: 0.7237 - val_loss: 0.8846 - val_acc: 0.6800\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.6824 - acc: 0.7211 - val_loss: 0.8767 - val_acc: 0.6914\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6546 - acc: 0.7403 - val_loss: 0.8825 - val_acc: 0.6743\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6625 - acc: 0.7262 - val_loss: 0.8936 - val_acc: 0.6743\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6832 - acc: 0.7313 - val_loss: 0.8939 - val_acc: 0.6800\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6784 - acc: 0.7294 - val_loss: 0.8880 - val_acc: 0.6743\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6819 - acc: 0.7269 - val_loss: 0.8990 - val_acc: 0.6800\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6767 - acc: 0.7511 - val_loss: 0.8964 - val_acc: 0.6857\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6455 - acc: 0.7256 - val_loss: 0.8964 - val_acc: 0.6743\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6471 - acc: 0.7415 - val_loss: 0.9056 - val_acc: 0.6629\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6557 - acc: 0.7435 - val_loss: 0.9059 - val_acc: 0.6686\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6530 - acc: 0.7403 - val_loss: 0.9004 - val_acc: 0.6743\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6468 - acc: 0.7371 - val_loss: 0.9037 - val_acc: 0.6800\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.6397 - acc: 0.7473 - val_loss: 0.9051 - val_acc: 0.6800\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6584 - acc: 0.7428 - val_loss: 0.9108 - val_acc: 0.6686\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6380 - acc: 0.7326 - val_loss: 0.9125 - val_acc: 0.6857\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6498 - acc: 0.7415 - val_loss: 0.9084 - val_acc: 0.6857\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.6307 - acc: 0.7511 - val_loss: 0.9091 - val_acc: 0.6857\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6209 - acc: 0.7645 - val_loss: 0.9062 - val_acc: 0.6743\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.6431 - acc: 0.7435 - val_loss: 0.9105 - val_acc: 0.6686\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6273 - acc: 0.7345 - val_loss: 0.9097 - val_acc: 0.6857\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.6127 - acc: 0.7575 - val_loss: 0.9183 - val_acc: 0.6914\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.6327 - acc: 0.7415 - val_loss: 0.9206 - val_acc: 0.6857\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6282 - acc: 0.7588 - val_loss: 0.9212 - val_acc: 0.6629\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6288 - acc: 0.7428 - val_loss: 0.9260 - val_acc: 0.6686\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6166 - acc: 0.7556 - val_loss: 0.9193 - val_acc: 0.6743\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.6294 - acc: 0.7384 - val_loss: 0.9205 - val_acc: 0.6800\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.6160 - acc: 0.7690 - val_loss: 0.9213 - val_acc: 0.6743\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6252 - acc: 0.7447 - val_loss: 0.9205 - val_acc: 0.6800\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6232 - acc: 0.7581 - val_loss: 0.9204 - val_acc: 0.6857\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6011 - acc: 0.7632 - val_loss: 0.9223 - val_acc: 0.6800\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6123 - acc: 0.7549 - val_loss: 0.9284 - val_acc: 0.6686\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5915 - acc: 0.7601 - val_loss: 0.9267 - val_acc: 0.6743\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.6105 - acc: 0.7492 - val_loss: 0.9263 - val_acc: 0.6571\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.6177 - acc: 0.7524 - val_loss: 0.9349 - val_acc: 0.6629\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6116 - acc: 0.7601 - val_loss: 0.9331 - val_acc: 0.6743\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6096 - acc: 0.7607 - val_loss: 0.9342 - val_acc: 0.6686\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 117us/step - loss: 0.5934 - acc: 0.7696 - val_loss: 0.9391 - val_acc: 0.6743\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 126us/step - loss: 0.5927 - acc: 0.7652 - val_loss: 0.9404 - val_acc: 0.6686\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 143us/step - loss: 0.6040 - acc: 0.7492 - val_loss: 0.9299 - val_acc: 0.6857\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5942 - acc: 0.7594 - val_loss: 0.9343 - val_acc: 0.6743\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5891 - acc: 0.7601 - val_loss: 0.9307 - val_acc: 0.7029\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5890 - acc: 0.7511 - val_loss: 0.9353 - val_acc: 0.6914\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6017 - acc: 0.7479 - val_loss: 0.9458 - val_acc: 0.6743\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6065 - acc: 0.7492 - val_loss: 0.9392 - val_acc: 0.6743\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6108 - acc: 0.7703 - val_loss: 0.9443 - val_acc: 0.6857\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5862 - acc: 0.7658 - val_loss: 0.9474 - val_acc: 0.6800\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5794 - acc: 0.7728 - val_loss: 0.9425 - val_acc: 0.6686\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5901 - acc: 0.7626 - val_loss: 0.9431 - val_acc: 0.6800\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5795 - acc: 0.7798 - val_loss: 0.9460 - val_acc: 0.6686\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5789 - acc: 0.7613 - val_loss: 0.9498 - val_acc: 0.6629\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5989 - acc: 0.7639 - val_loss: 0.9500 - val_acc: 0.6686\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5470 - acc: 0.7932 - val_loss: 0.9530 - val_acc: 0.6743\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5972 - acc: 0.7652 - val_loss: 0.9574 - val_acc: 0.6800\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5798 - acc: 0.7569 - val_loss: 0.9517 - val_acc: 0.6857\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5655 - acc: 0.7766 - val_loss: 0.9638 - val_acc: 0.6514\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5673 - acc: 0.7760 - val_loss: 0.9636 - val_acc: 0.6686\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5542 - acc: 0.7881 - val_loss: 0.9633 - val_acc: 0.6857\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5721 - acc: 0.7779 - val_loss: 0.9631 - val_acc: 0.6743\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5705 - acc: 0.7664 - val_loss: 0.9619 - val_acc: 0.6571\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5578 - acc: 0.7677 - val_loss: 0.9611 - val_acc: 0.6686\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5563 - acc: 0.7869 - val_loss: 0.9553 - val_acc: 0.6686\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5557 - acc: 0.7875 - val_loss: 0.9637 - val_acc: 0.6743\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5517 - acc: 0.7849 - val_loss: 0.9730 - val_acc: 0.6514\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5823 - acc: 0.7728 - val_loss: 0.9683 - val_acc: 0.6629\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5655 - acc: 0.7703 - val_loss: 0.9625 - val_acc: 0.6571\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5560 - acc: 0.7843 - val_loss: 0.9703 - val_acc: 0.6514\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5384 - acc: 0.7926 - val_loss: 0.9732 - val_acc: 0.6514\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5396 - acc: 0.7792 - val_loss: 0.9718 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5464 - acc: 0.7824 - val_loss: 0.9803 - val_acc: 0.6629\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5305 - acc: 0.7862 - val_loss: 0.9835 - val_acc: 0.6514\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5553 - acc: 0.7881 - val_loss: 0.9789 - val_acc: 0.6571\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5433 - acc: 0.7817 - val_loss: 0.9798 - val_acc: 0.6514\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5343 - acc: 0.7888 - val_loss: 0.9838 - val_acc: 0.6514\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5377 - acc: 0.7958 - val_loss: 0.9820 - val_acc: 0.6514\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5393 - acc: 0.7843 - val_loss: 0.9917 - val_acc: 0.6571\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5309 - acc: 0.7990 - val_loss: 0.9895 - val_acc: 0.6514\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5471 - acc: 0.7926 - val_loss: 0.9949 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5314 - acc: 0.7824 - val_loss: 0.9923 - val_acc: 0.6571\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5193 - acc: 0.8041 - val_loss: 0.9942 - val_acc: 0.6400\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5468 - acc: 0.7779 - val_loss: 0.9961 - val_acc: 0.6514\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5182 - acc: 0.7964 - val_loss: 0.9906 - val_acc: 0.6629\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5433 - acc: 0.7805 - val_loss: 0.9932 - val_acc: 0.6629\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5286 - acc: 0.7805 - val_loss: 0.9932 - val_acc: 0.6800\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5310 - acc: 0.7824 - val_loss: 0.9975 - val_acc: 0.6571\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5095 - acc: 0.7926 - val_loss: 0.9970 - val_acc: 0.6629\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5168 - acc: 0.7958 - val_loss: 0.9968 - val_acc: 0.6571\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5183 - acc: 0.8003 - val_loss: 0.9984 - val_acc: 0.6514\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5118 - acc: 0.8130 - val_loss: 1.0104 - val_acc: 0.6571\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5266 - acc: 0.7773 - val_loss: 1.0108 - val_acc: 0.6514\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5162 - acc: 0.8015 - val_loss: 1.0034 - val_acc: 0.6571\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5296 - acc: 0.7824 - val_loss: 0.9997 - val_acc: 0.6629\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5053 - acc: 0.7971 - val_loss: 0.9952 - val_acc: 0.6686\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5091 - acc: 0.7900 - val_loss: 0.9989 - val_acc: 0.6800\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5257 - acc: 0.8009 - val_loss: 1.0011 - val_acc: 0.6629\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5137 - acc: 0.7977 - val_loss: 1.0042 - val_acc: 0.6686\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4972 - acc: 0.7996 - val_loss: 1.0096 - val_acc: 0.6514\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5123 - acc: 0.8066 - val_loss: 1.0108 - val_acc: 0.6571\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5098 - acc: 0.8207 - val_loss: 1.0086 - val_acc: 0.6686\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5046 - acc: 0.8022 - val_loss: 1.0072 - val_acc: 0.6686\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5236 - acc: 0.8003 - val_loss: 1.0136 - val_acc: 0.6800\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5287 - acc: 0.7888 - val_loss: 1.0165 - val_acc: 0.6743\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5056 - acc: 0.7945 - val_loss: 1.0256 - val_acc: 0.6686\n",
      "193/193 [==============================] - 0s 62us/step\n",
      "1742/1742 [==============================] - 0s 54us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.4346 - acc: 0.3350 - val_loss: 1.1732 - val_acc: 0.5657\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.2625 - acc: 0.4154 - val_loss: 1.0767 - val_acc: 0.6171\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 1.1966 - acc: 0.4474 - val_loss: 1.0301 - val_acc: 0.6286\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.1340 - acc: 0.4997 - val_loss: 1.0011 - val_acc: 0.6286\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 1.0996 - acc: 0.5227 - val_loss: 0.9786 - val_acc: 0.6171\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0666 - acc: 0.5405 - val_loss: 0.9576 - val_acc: 0.6400\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.0530 - acc: 0.5380 - val_loss: 0.9498 - val_acc: 0.6457\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.0286 - acc: 0.5520 - val_loss: 0.9441 - val_acc: 0.6514\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 1.0113 - acc: 0.5705 - val_loss: 0.9303 - val_acc: 0.6400\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 1.0006 - acc: 0.5756 - val_loss: 0.9167 - val_acc: 0.6514\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9861 - acc: 0.5890 - val_loss: 0.9166 - val_acc: 0.6457\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9847 - acc: 0.5763 - val_loss: 0.9206 - val_acc: 0.6343\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.9794 - acc: 0.5743 - val_loss: 0.9134 - val_acc: 0.6629\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.9566 - acc: 0.5929 - val_loss: 0.9048 - val_acc: 0.6686\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.9596 - acc: 0.5916 - val_loss: 0.9106 - val_acc: 0.6514\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.9578 - acc: 0.5897 - val_loss: 0.9032 - val_acc: 0.6743\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.9197 - acc: 0.6043 - val_loss: 0.9066 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9312 - acc: 0.6018 - val_loss: 0.8992 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9112 - acc: 0.6050 - val_loss: 0.9050 - val_acc: 0.6629\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9221 - acc: 0.6120 - val_loss: 0.9030 - val_acc: 0.6686\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.9285 - acc: 0.6069 - val_loss: 0.8924 - val_acc: 0.6800\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8973 - acc: 0.6248 - val_loss: 0.8914 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9052 - acc: 0.6158 - val_loss: 0.8927 - val_acc: 0.6743\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8773 - acc: 0.6401 - val_loss: 0.8969 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8867 - acc: 0.6311 - val_loss: 0.8875 - val_acc: 0.6800\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8880 - acc: 0.6254 - val_loss: 0.8862 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8836 - acc: 0.6273 - val_loss: 0.8939 - val_acc: 0.6629\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8690 - acc: 0.6445 - val_loss: 0.8990 - val_acc: 0.6686\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8835 - acc: 0.6305 - val_loss: 0.8981 - val_acc: 0.6857\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8554 - acc: 0.6433 - val_loss: 0.8927 - val_acc: 0.6857\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8655 - acc: 0.6439 - val_loss: 0.8906 - val_acc: 0.6914\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.8615 - acc: 0.6465 - val_loss: 0.8876 - val_acc: 0.6857\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8685 - acc: 0.6388 - val_loss: 0.8846 - val_acc: 0.6914\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8392 - acc: 0.6490 - val_loss: 0.8887 - val_acc: 0.6743\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8579 - acc: 0.6477 - val_loss: 0.8833 - val_acc: 0.6800\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8318 - acc: 0.6560 - val_loss: 0.8910 - val_acc: 0.6800\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8378 - acc: 0.6599 - val_loss: 0.8890 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.8434 - acc: 0.6560 - val_loss: 0.8882 - val_acc: 0.6743\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.8259 - acc: 0.6592 - val_loss: 0.8875 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8441 - acc: 0.6369 - val_loss: 0.8951 - val_acc: 0.6743\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8331 - acc: 0.6567 - val_loss: 0.8984 - val_acc: 0.6686\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.8205 - acc: 0.6522 - val_loss: 0.8936 - val_acc: 0.6629\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8054 - acc: 0.6720 - val_loss: 0.8978 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8299 - acc: 0.6675 - val_loss: 0.9005 - val_acc: 0.6686\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8147 - acc: 0.6643 - val_loss: 0.9003 - val_acc: 0.6686\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8208 - acc: 0.6637 - val_loss: 0.8914 - val_acc: 0.6686\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8171 - acc: 0.6669 - val_loss: 0.9007 - val_acc: 0.6629\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7970 - acc: 0.6784 - val_loss: 0.8996 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8021 - acc: 0.6688 - val_loss: 0.9014 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7989 - acc: 0.6707 - val_loss: 0.9074 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8048 - acc: 0.6765 - val_loss: 0.9066 - val_acc: 0.6514\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7810 - acc: 0.6854 - val_loss: 0.9088 - val_acc: 0.6629\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7841 - acc: 0.6854 - val_loss: 0.9115 - val_acc: 0.6400\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7770 - acc: 0.6816 - val_loss: 0.9107 - val_acc: 0.6571\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7796 - acc: 0.6860 - val_loss: 0.9081 - val_acc: 0.6514\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7847 - acc: 0.6765 - val_loss: 0.9063 - val_acc: 0.6514\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7721 - acc: 0.6879 - val_loss: 0.9065 - val_acc: 0.6457\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7753 - acc: 0.6962 - val_loss: 0.9077 - val_acc: 0.6400\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7746 - acc: 0.6867 - val_loss: 0.9124 - val_acc: 0.6400\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7673 - acc: 0.6892 - val_loss: 0.9163 - val_acc: 0.6457\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7593 - acc: 0.6924 - val_loss: 0.9091 - val_acc: 0.6571\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7665 - acc: 0.6930 - val_loss: 0.9120 - val_acc: 0.6457\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7507 - acc: 0.6950 - val_loss: 0.9162 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7474 - acc: 0.6924 - val_loss: 0.9207 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7659 - acc: 0.6886 - val_loss: 0.9193 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7578 - acc: 0.6981 - val_loss: 0.9171 - val_acc: 0.6686\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7609 - acc: 0.6899 - val_loss: 0.9221 - val_acc: 0.6686\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7467 - acc: 0.7052 - val_loss: 0.9194 - val_acc: 0.6686\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7424 - acc: 0.6924 - val_loss: 0.9201 - val_acc: 0.6629\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7451 - acc: 0.6950 - val_loss: 0.9271 - val_acc: 0.6514\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7302 - acc: 0.7020 - val_loss: 0.9206 - val_acc: 0.6629\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7400 - acc: 0.6956 - val_loss: 0.9274 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7263 - acc: 0.6975 - val_loss: 0.9292 - val_acc: 0.6457\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7312 - acc: 0.7039 - val_loss: 0.9248 - val_acc: 0.6571\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7185 - acc: 0.7103 - val_loss: 0.9269 - val_acc: 0.6514\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7164 - acc: 0.7020 - val_loss: 0.9186 - val_acc: 0.6571\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7229 - acc: 0.7135 - val_loss: 0.9247 - val_acc: 0.6514\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7200 - acc: 0.6988 - val_loss: 0.9228 - val_acc: 0.6571\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7302 - acc: 0.7109 - val_loss: 0.9289 - val_acc: 0.6686\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7257 - acc: 0.7160 - val_loss: 0.9274 - val_acc: 0.6686\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7247 - acc: 0.7147 - val_loss: 0.9354 - val_acc: 0.6457\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7160 - acc: 0.7013 - val_loss: 0.9298 - val_acc: 0.6629\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7182 - acc: 0.7084 - val_loss: 0.9265 - val_acc: 0.6629\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7061 - acc: 0.7173 - val_loss: 0.9310 - val_acc: 0.6629\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7114 - acc: 0.7160 - val_loss: 0.9341 - val_acc: 0.6571\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7012 - acc: 0.7211 - val_loss: 0.9397 - val_acc: 0.6629\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7106 - acc: 0.7173 - val_loss: 0.9431 - val_acc: 0.6514\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6858 - acc: 0.7160 - val_loss: 0.9409 - val_acc: 0.6514\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7141 - acc: 0.7160 - val_loss: 0.9347 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7089 - acc: 0.7077 - val_loss: 0.9499 - val_acc: 0.6286\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6915 - acc: 0.7281 - val_loss: 0.9562 - val_acc: 0.6286\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6901 - acc: 0.7205 - val_loss: 0.9629 - val_acc: 0.6514\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6911 - acc: 0.7288 - val_loss: 0.9585 - val_acc: 0.6457\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6832 - acc: 0.7313 - val_loss: 0.9537 - val_acc: 0.6400\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7004 - acc: 0.7128 - val_loss: 0.9506 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6724 - acc: 0.7301 - val_loss: 0.9561 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6871 - acc: 0.7192 - val_loss: 0.9530 - val_acc: 0.6457\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6886 - acc: 0.7205 - val_loss: 0.9578 - val_acc: 0.6514\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6997 - acc: 0.7205 - val_loss: 0.9541 - val_acc: 0.6800\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6834 - acc: 0.7186 - val_loss: 0.9692 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6737 - acc: 0.7307 - val_loss: 0.9651 - val_acc: 0.6457\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6625 - acc: 0.7345 - val_loss: 0.9678 - val_acc: 0.6343\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6759 - acc: 0.7307 - val_loss: 0.9746 - val_acc: 0.6343\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6818 - acc: 0.7250 - val_loss: 0.9735 - val_acc: 0.6514\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6637 - acc: 0.7294 - val_loss: 0.9649 - val_acc: 0.6343\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6604 - acc: 0.7377 - val_loss: 0.9701 - val_acc: 0.6343\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6551 - acc: 0.7377 - val_loss: 0.9608 - val_acc: 0.6400\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6737 - acc: 0.7218 - val_loss: 0.9699 - val_acc: 0.6514\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6558 - acc: 0.7339 - val_loss: 0.9652 - val_acc: 0.6629\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6634 - acc: 0.7332 - val_loss: 0.9733 - val_acc: 0.6686\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6603 - acc: 0.7428 - val_loss: 0.9695 - val_acc: 0.6457\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6771 - acc: 0.7288 - val_loss: 0.9697 - val_acc: 0.6571\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6385 - acc: 0.7460 - val_loss: 0.9675 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6417 - acc: 0.7415 - val_loss: 0.9725 - val_acc: 0.6629\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6463 - acc: 0.7352 - val_loss: 0.9730 - val_acc: 0.6343\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6503 - acc: 0.7294 - val_loss: 0.9757 - val_acc: 0.6571\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6355 - acc: 0.7358 - val_loss: 0.9866 - val_acc: 0.6400\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6512 - acc: 0.7339 - val_loss: 0.9864 - val_acc: 0.6400\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6318 - acc: 0.7511 - val_loss: 0.9912 - val_acc: 0.6286\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6320 - acc: 0.7524 - val_loss: 0.9894 - val_acc: 0.6514\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6533 - acc: 0.7460 - val_loss: 0.9835 - val_acc: 0.6514\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6381 - acc: 0.7428 - val_loss: 0.9970 - val_acc: 0.6171\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6392 - acc: 0.7403 - val_loss: 0.9999 - val_acc: 0.6171\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6522 - acc: 0.7326 - val_loss: 0.9927 - val_acc: 0.6400\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6299 - acc: 0.7447 - val_loss: 0.9952 - val_acc: 0.6400\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6340 - acc: 0.7422 - val_loss: 0.9943 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6233 - acc: 0.7422 - val_loss: 0.9952 - val_acc: 0.6457\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6249 - acc: 0.7435 - val_loss: 0.9944 - val_acc: 0.6514\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.6044 - acc: 0.7530 - val_loss: 0.9994 - val_acc: 0.6457\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6255 - acc: 0.7473 - val_loss: 0.9996 - val_acc: 0.6571\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6201 - acc: 0.7543 - val_loss: 0.9865 - val_acc: 0.6629\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6115 - acc: 0.7620 - val_loss: 1.0019 - val_acc: 0.6343\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6108 - acc: 0.7588 - val_loss: 0.9969 - val_acc: 0.6457\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6226 - acc: 0.7562 - val_loss: 0.9952 - val_acc: 0.6514\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6352 - acc: 0.7473 - val_loss: 1.0042 - val_acc: 0.6400\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6278 - acc: 0.7371 - val_loss: 1.0044 - val_acc: 0.6514\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6101 - acc: 0.7562 - val_loss: 1.0090 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5925 - acc: 0.7632 - val_loss: 1.0143 - val_acc: 0.6229\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5886 - acc: 0.7601 - val_loss: 1.0266 - val_acc: 0.6343\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6131 - acc: 0.7575 - val_loss: 1.0235 - val_acc: 0.6343\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6045 - acc: 0.7562 - val_loss: 1.0174 - val_acc: 0.6343\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5976 - acc: 0.7607 - val_loss: 1.0221 - val_acc: 0.6514\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5965 - acc: 0.7601 - val_loss: 1.0236 - val_acc: 0.6514\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5925 - acc: 0.7575 - val_loss: 1.0321 - val_acc: 0.6400\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5786 - acc: 0.7747 - val_loss: 1.0311 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5994 - acc: 0.7632 - val_loss: 1.0388 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5859 - acc: 0.7607 - val_loss: 1.0503 - val_acc: 0.6400\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5830 - acc: 0.7613 - val_loss: 1.0410 - val_acc: 0.6343\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5822 - acc: 0.7594 - val_loss: 1.0456 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5797 - acc: 0.7658 - val_loss: 1.0429 - val_acc: 0.6514\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5835 - acc: 0.7607 - val_loss: 1.0368 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5593 - acc: 0.7754 - val_loss: 1.0391 - val_acc: 0.6514\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5808 - acc: 0.7754 - val_loss: 1.0489 - val_acc: 0.6571\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5750 - acc: 0.7747 - val_loss: 1.0530 - val_acc: 0.6514\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5837 - acc: 0.7562 - val_loss: 1.0448 - val_acc: 0.6514\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5749 - acc: 0.7671 - val_loss: 1.0563 - val_acc: 0.6229\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5715 - acc: 0.7690 - val_loss: 1.0492 - val_acc: 0.6343\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5817 - acc: 0.7626 - val_loss: 1.0545 - val_acc: 0.6457\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5754 - acc: 0.7601 - val_loss: 1.0508 - val_acc: 0.6514\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5682 - acc: 0.7639 - val_loss: 1.0502 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5517 - acc: 0.7722 - val_loss: 1.0603 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 107us/step - loss: 0.5653 - acc: 0.7862 - val_loss: 1.0576 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5734 - acc: 0.7652 - val_loss: 1.0557 - val_acc: 0.6571\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5684 - acc: 0.7690 - val_loss: 1.0494 - val_acc: 0.6571\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5545 - acc: 0.7747 - val_loss: 1.0588 - val_acc: 0.6571\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5581 - acc: 0.7766 - val_loss: 1.0659 - val_acc: 0.6571\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5505 - acc: 0.7754 - val_loss: 1.0666 - val_acc: 0.6571\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5379 - acc: 0.7875 - val_loss: 1.0617 - val_acc: 0.6571\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5539 - acc: 0.7817 - val_loss: 1.0659 - val_acc: 0.6571\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5538 - acc: 0.7754 - val_loss: 1.0661 - val_acc: 0.6571\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5563 - acc: 0.7652 - val_loss: 1.0779 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5561 - acc: 0.7773 - val_loss: 1.0854 - val_acc: 0.6400\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5559 - acc: 0.7779 - val_loss: 1.0892 - val_acc: 0.6229\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.5601 - acc: 0.7817 - val_loss: 1.0712 - val_acc: 0.6400\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5402 - acc: 0.7792 - val_loss: 1.0698 - val_acc: 0.6343\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5418 - acc: 0.7881 - val_loss: 1.0731 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5402 - acc: 0.7932 - val_loss: 1.0798 - val_acc: 0.6286\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5238 - acc: 0.7913 - val_loss: 1.0882 - val_acc: 0.6457\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5428 - acc: 0.7830 - val_loss: 1.0845 - val_acc: 0.6343\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5310 - acc: 0.7869 - val_loss: 1.0996 - val_acc: 0.6229\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5290 - acc: 0.7760 - val_loss: 1.0891 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5304 - acc: 0.7722 - val_loss: 1.0974 - val_acc: 0.6343\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5309 - acc: 0.7907 - val_loss: 1.0950 - val_acc: 0.6286\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5509 - acc: 0.7824 - val_loss: 1.1011 - val_acc: 0.6343\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5298 - acc: 0.7875 - val_loss: 1.1038 - val_acc: 0.6229\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5124 - acc: 0.7894 - val_loss: 1.1057 - val_acc: 0.6229\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5193 - acc: 0.7875 - val_loss: 1.1128 - val_acc: 0.6286\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.5158 - acc: 0.7913 - val_loss: 1.1037 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.5247 - acc: 0.7964 - val_loss: 1.1001 - val_acc: 0.6514\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4972 - acc: 0.8009 - val_loss: 1.1028 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.4966 - acc: 0.7971 - val_loss: 1.1135 - val_acc: 0.6343\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - ETA: 0s - loss: 0.5006 - acc: 0.799 - 0s 98us/step - loss: 0.5110 - acc: 0.7932 - val_loss: 1.1220 - val_acc: 0.6343\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 119us/step - loss: 0.5227 - acc: 0.7958 - val_loss: 1.1200 - val_acc: 0.6457\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5005 - acc: 0.8034 - val_loss: 1.1316 - val_acc: 0.6229\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5074 - acc: 0.8098 - val_loss: 1.1321 - val_acc: 0.6343\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5212 - acc: 0.7900 - val_loss: 1.1375 - val_acc: 0.6343\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4946 - acc: 0.7920 - val_loss: 1.1349 - val_acc: 0.6400\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5235 - acc: 0.7849 - val_loss: 1.1327 - val_acc: 0.6400\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5098 - acc: 0.7964 - val_loss: 1.1444 - val_acc: 0.6400\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 119us/step - loss: 0.5024 - acc: 0.7945 - val_loss: 1.1403 - val_acc: 0.6400\n",
      "193/193 [==============================] - 0s 120us/step\n",
      "1742/1742 [==============================] - 0s 75us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.4838 - acc: 0.3012 - val_loss: 1.2055 - val_acc: 0.5314\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 1.2538 - acc: 0.4231 - val_loss: 1.0826 - val_acc: 0.5657\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 1.1947 - acc: 0.4537 - val_loss: 1.0424 - val_acc: 0.5829\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.1393 - acc: 0.4818 - val_loss: 1.0179 - val_acc: 0.6000\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.1099 - acc: 0.5080 - val_loss: 0.9955 - val_acc: 0.6171\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 1.0883 - acc: 0.5258 - val_loss: 0.9769 - val_acc: 0.6343\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 1.0657 - acc: 0.5392 - val_loss: 0.9670 - val_acc: 0.6400\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.0375 - acc: 0.5558 - val_loss: 0.9544 - val_acc: 0.6343\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.0174 - acc: 0.5609 - val_loss: 0.9419 - val_acc: 0.6629\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 1.0413 - acc: 0.5444 - val_loss: 0.9375 - val_acc: 0.6629\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.0206 - acc: 0.5686 - val_loss: 0.9354 - val_acc: 0.6686\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.9914 - acc: 0.5731 - val_loss: 0.9302 - val_acc: 0.6800\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.9894 - acc: 0.5846 - val_loss: 0.9280 - val_acc: 0.6686\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9676 - acc: 0.5877 - val_loss: 0.9271 - val_acc: 0.6800\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9661 - acc: 0.5877 - val_loss: 0.9250 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9571 - acc: 0.5909 - val_loss: 0.9196 - val_acc: 0.6743\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.9403 - acc: 0.6082 - val_loss: 0.9214 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.9291 - acc: 0.6120 - val_loss: 0.9198 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.9359 - acc: 0.6037 - val_loss: 0.9156 - val_acc: 0.6686\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.9080 - acc: 0.6248 - val_loss: 0.9167 - val_acc: 0.6514\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9196 - acc: 0.6126 - val_loss: 0.9101 - val_acc: 0.6800\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9113 - acc: 0.6209 - val_loss: 0.9144 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.9056 - acc: 0.6165 - val_loss: 0.9136 - val_acc: 0.6800\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8971 - acc: 0.6420 - val_loss: 0.9110 - val_acc: 0.6629\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8805 - acc: 0.6369 - val_loss: 0.9150 - val_acc: 0.6857\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8841 - acc: 0.6248 - val_loss: 0.9137 - val_acc: 0.6514\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8809 - acc: 0.6362 - val_loss: 0.9105 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8692 - acc: 0.6465 - val_loss: 0.9149 - val_acc: 0.6514\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8728 - acc: 0.6382 - val_loss: 0.9127 - val_acc: 0.6686\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 125us/step - loss: 0.8609 - acc: 0.6407 - val_loss: 0.9092 - val_acc: 0.6743\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8481 - acc: 0.6567 - val_loss: 0.9198 - val_acc: 0.6629\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8603 - acc: 0.6458 - val_loss: 0.9183 - val_acc: 0.6686\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8656 - acc: 0.6388 - val_loss: 0.9115 - val_acc: 0.6686\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8421 - acc: 0.6618 - val_loss: 0.9088 - val_acc: 0.6629\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8469 - acc: 0.6548 - val_loss: 0.9165 - val_acc: 0.6629\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8588 - acc: 0.6586 - val_loss: 0.9171 - val_acc: 0.6743\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8297 - acc: 0.6579 - val_loss: 0.9132 - val_acc: 0.6686\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.8749 - acc: 0.6407 - val_loss: 0.9185 - val_acc: 0.6571\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8347 - acc: 0.6509 - val_loss: 0.9086 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8459 - acc: 0.6465 - val_loss: 0.9118 - val_acc: 0.6629\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8396 - acc: 0.6541 - val_loss: 0.9145 - val_acc: 0.6629\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8212 - acc: 0.6784 - val_loss: 0.9225 - val_acc: 0.6743\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8156 - acc: 0.6656 - val_loss: 0.9162 - val_acc: 0.6629\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.8246 - acc: 0.6662 - val_loss: 0.9176 - val_acc: 0.6514\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8152 - acc: 0.6752 - val_loss: 0.9195 - val_acc: 0.6571\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8165 - acc: 0.6796 - val_loss: 0.9164 - val_acc: 0.6629\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8143 - acc: 0.6726 - val_loss: 0.9234 - val_acc: 0.6629\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8090 - acc: 0.6752 - val_loss: 0.9298 - val_acc: 0.6629\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7984 - acc: 0.6713 - val_loss: 0.9237 - val_acc: 0.6743\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8137 - acc: 0.6579 - val_loss: 0.9253 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7947 - acc: 0.6752 - val_loss: 0.9321 - val_acc: 0.6571\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7926 - acc: 0.6733 - val_loss: 0.9292 - val_acc: 0.6686\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8123 - acc: 0.6669 - val_loss: 0.9290 - val_acc: 0.6686\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7875 - acc: 0.6733 - val_loss: 0.9346 - val_acc: 0.6571\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7925 - acc: 0.6745 - val_loss: 0.9403 - val_acc: 0.6514\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8019 - acc: 0.6765 - val_loss: 0.9323 - val_acc: 0.6514\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7899 - acc: 0.6777 - val_loss: 0.9352 - val_acc: 0.6629\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7860 - acc: 0.6777 - val_loss: 0.9296 - val_acc: 0.6629\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.7730 - acc: 0.6873 - val_loss: 0.9416 - val_acc: 0.6571\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7865 - acc: 0.6637 - val_loss: 0.9344 - val_acc: 0.6571\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7807 - acc: 0.6752 - val_loss: 0.9402 - val_acc: 0.6571\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7855 - acc: 0.6745 - val_loss: 0.9393 - val_acc: 0.6514\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7630 - acc: 0.6835 - val_loss: 0.9382 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7665 - acc: 0.6867 - val_loss: 0.9396 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7665 - acc: 0.7039 - val_loss: 0.9394 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7903 - acc: 0.6726 - val_loss: 0.9356 - val_acc: 0.6457\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7635 - acc: 0.6867 - val_loss: 0.9443 - val_acc: 0.6400\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7469 - acc: 0.7045 - val_loss: 0.9428 - val_acc: 0.6457\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7565 - acc: 0.7064 - val_loss: 0.9420 - val_acc: 0.6571\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.7372 - acc: 0.6981 - val_loss: 0.9479 - val_acc: 0.6457\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7447 - acc: 0.7020 - val_loss: 0.9535 - val_acc: 0.6457\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.7289 - acc: 0.7084 - val_loss: 0.9513 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7359 - acc: 0.6975 - val_loss: 0.9597 - val_acc: 0.6400\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7392 - acc: 0.7033 - val_loss: 0.9572 - val_acc: 0.6286\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7273 - acc: 0.7013 - val_loss: 0.9650 - val_acc: 0.6400\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7383 - acc: 0.6956 - val_loss: 0.9597 - val_acc: 0.6514\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7305 - acc: 0.7013 - val_loss: 0.9683 - val_acc: 0.6457\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7197 - acc: 0.7058 - val_loss: 0.9670 - val_acc: 0.6229\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7498 - acc: 0.6899 - val_loss: 0.9664 - val_acc: 0.6286\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7438 - acc: 0.7039 - val_loss: 0.9635 - val_acc: 0.6400\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7389 - acc: 0.7026 - val_loss: 0.9690 - val_acc: 0.6343\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7086 - acc: 0.7077 - val_loss: 0.9660 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7185 - acc: 0.7224 - val_loss: 0.9705 - val_acc: 0.6286\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7049 - acc: 0.7128 - val_loss: 0.9670 - val_acc: 0.6286\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7165 - acc: 0.7090 - val_loss: 0.9680 - val_acc: 0.6400\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.7079 - acc: 0.7141 - val_loss: 0.9734 - val_acc: 0.6343\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7384 - acc: 0.7096 - val_loss: 0.9736 - val_acc: 0.6229\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7100 - acc: 0.7141 - val_loss: 0.9813 - val_acc: 0.6171\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6982 - acc: 0.7141 - val_loss: 0.9829 - val_acc: 0.6229\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7056 - acc: 0.7135 - val_loss: 0.9902 - val_acc: 0.6229\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7063 - acc: 0.7211 - val_loss: 0.9914 - val_acc: 0.6171\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6984 - acc: 0.7281 - val_loss: 0.9915 - val_acc: 0.6229\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7025 - acc: 0.7179 - val_loss: 0.9911 - val_acc: 0.6171\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6995 - acc: 0.7109 - val_loss: 0.9946 - val_acc: 0.6229\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6799 - acc: 0.7390 - val_loss: 0.9939 - val_acc: 0.6286\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6850 - acc: 0.7141 - val_loss: 0.9881 - val_acc: 0.6343\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6856 - acc: 0.7352 - val_loss: 0.9922 - val_acc: 0.6229\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6990 - acc: 0.7103 - val_loss: 1.0029 - val_acc: 0.6286\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6950 - acc: 0.7237 - val_loss: 0.9975 - val_acc: 0.6343\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6920 - acc: 0.7179 - val_loss: 1.0026 - val_acc: 0.6343\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.6762 - acc: 0.7313 - val_loss: 1.0022 - val_acc: 0.6286\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6698 - acc: 0.7192 - val_loss: 1.0058 - val_acc: 0.6286\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6686 - acc: 0.7339 - val_loss: 1.0072 - val_acc: 0.6286\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6808 - acc: 0.7339 - val_loss: 1.0087 - val_acc: 0.6286\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6436 - acc: 0.7530 - val_loss: 1.0094 - val_acc: 0.6343\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6637 - acc: 0.7447 - val_loss: 1.0131 - val_acc: 0.6171\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6547 - acc: 0.7403 - val_loss: 1.0155 - val_acc: 0.6171\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6603 - acc: 0.7345 - val_loss: 1.0161 - val_acc: 0.6400\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6501 - acc: 0.7301 - val_loss: 1.0183 - val_acc: 0.6229\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6539 - acc: 0.7466 - val_loss: 1.0196 - val_acc: 0.6229\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6702 - acc: 0.7326 - val_loss: 1.0256 - val_acc: 0.6114\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6457 - acc: 0.7390 - val_loss: 1.0283 - val_acc: 0.6114\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6521 - acc: 0.7364 - val_loss: 1.0346 - val_acc: 0.6000\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6574 - acc: 0.7301 - val_loss: 1.0328 - val_acc: 0.6114\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6609 - acc: 0.7396 - val_loss: 1.0244 - val_acc: 0.6171\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6470 - acc: 0.7320 - val_loss: 1.0231 - val_acc: 0.6114\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6448 - acc: 0.7511 - val_loss: 1.0262 - val_acc: 0.6114\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6370 - acc: 0.7537 - val_loss: 1.0422 - val_acc: 0.6171\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6452 - acc: 0.7435 - val_loss: 1.0389 - val_acc: 0.6057\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6384 - acc: 0.7352 - val_loss: 1.0377 - val_acc: 0.6057\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6420 - acc: 0.7358 - val_loss: 1.0437 - val_acc: 0.6057\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6414 - acc: 0.7537 - val_loss: 1.0458 - val_acc: 0.6171\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6251 - acc: 0.7524 - val_loss: 1.0384 - val_acc: 0.6171\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6170 - acc: 0.7505 - val_loss: 1.0397 - val_acc: 0.6114\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6210 - acc: 0.7594 - val_loss: 1.0437 - val_acc: 0.6229\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6203 - acc: 0.7479 - val_loss: 1.0428 - val_acc: 0.6171\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6253 - acc: 0.7575 - val_loss: 1.0560 - val_acc: 0.6000\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6390 - acc: 0.7581 - val_loss: 1.0416 - val_acc: 0.6057\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6251 - acc: 0.7549 - val_loss: 1.0445 - val_acc: 0.6114\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6167 - acc: 0.7543 - val_loss: 1.0533 - val_acc: 0.6000\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6240 - acc: 0.7530 - val_loss: 1.0523 - val_acc: 0.6000\n",
      "Epoch 132/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6158 - acc: 0.7626 - val_loss: 1.0531 - val_acc: 0.6057\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6077 - acc: 0.7645 - val_loss: 1.0557 - val_acc: 0.6057\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6261 - acc: 0.7454 - val_loss: 1.0540 - val_acc: 0.6057\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6297 - acc: 0.7479 - val_loss: 1.0681 - val_acc: 0.6229\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6207 - acc: 0.7447 - val_loss: 1.0640 - val_acc: 0.6057\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6114 - acc: 0.7569 - val_loss: 1.0666 - val_acc: 0.6171\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6066 - acc: 0.7575 - val_loss: 1.0668 - val_acc: 0.6114\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6227 - acc: 0.7505 - val_loss: 1.0680 - val_acc: 0.6057\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6140 - acc: 0.7511 - val_loss: 1.0658 - val_acc: 0.6057\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6114 - acc: 0.7588 - val_loss: 1.0734 - val_acc: 0.6114\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5940 - acc: 0.7613 - val_loss: 1.0773 - val_acc: 0.6171\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5974 - acc: 0.7690 - val_loss: 1.0777 - val_acc: 0.6057\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6106 - acc: 0.7441 - val_loss: 1.0744 - val_acc: 0.6114\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6088 - acc: 0.7594 - val_loss: 1.0764 - val_acc: 0.6057\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5900 - acc: 0.7677 - val_loss: 1.0753 - val_acc: 0.6000\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5798 - acc: 0.7632 - val_loss: 1.0764 - val_acc: 0.6114\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5860 - acc: 0.7664 - val_loss: 1.0769 - val_acc: 0.6171\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5849 - acc: 0.7696 - val_loss: 1.0823 - val_acc: 0.6229\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6037 - acc: 0.7690 - val_loss: 1.0768 - val_acc: 0.5943\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5801 - acc: 0.7805 - val_loss: 1.0863 - val_acc: 0.6057\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5804 - acc: 0.7805 - val_loss: 1.0953 - val_acc: 0.6114\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5642 - acc: 0.7754 - val_loss: 1.1008 - val_acc: 0.6171\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5933 - acc: 0.7626 - val_loss: 1.0951 - val_acc: 0.6057\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5846 - acc: 0.7715 - val_loss: 1.0982 - val_acc: 0.6057\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5638 - acc: 0.7798 - val_loss: 1.0880 - val_acc: 0.6057\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5807 - acc: 0.7703 - val_loss: 1.0932 - val_acc: 0.6171\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5622 - acc: 0.7760 - val_loss: 1.0921 - val_acc: 0.6171\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5977 - acc: 0.7639 - val_loss: 1.0930 - val_acc: 0.6057\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5885 - acc: 0.7715 - val_loss: 1.1054 - val_acc: 0.6171\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.5898 - acc: 0.7690 - val_loss: 1.1077 - val_acc: 0.6114\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5851 - acc: 0.7715 - val_loss: 1.1056 - val_acc: 0.6057\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5688 - acc: 0.7696 - val_loss: 1.1094 - val_acc: 0.6171\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5718 - acc: 0.7613 - val_loss: 1.1049 - val_acc: 0.6171\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5672 - acc: 0.7856 - val_loss: 1.1120 - val_acc: 0.6171\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5707 - acc: 0.7690 - val_loss: 1.1087 - val_acc: 0.6229\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5468 - acc: 0.7907 - val_loss: 1.1098 - val_acc: 0.6171\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5701 - acc: 0.7709 - val_loss: 1.1129 - val_acc: 0.6114\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.5589 - acc: 0.7722 - val_loss: 1.1171 - val_acc: 0.6229\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5285 - acc: 0.8015 - val_loss: 1.1213 - val_acc: 0.6114\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5585 - acc: 0.7779 - val_loss: 1.1163 - val_acc: 0.6229\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5524 - acc: 0.7817 - val_loss: 1.1251 - val_acc: 0.6343\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5624 - acc: 0.7779 - val_loss: 1.1246 - val_acc: 0.6229\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5700 - acc: 0.7779 - val_loss: 1.1234 - val_acc: 0.6229\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5306 - acc: 0.7971 - val_loss: 1.1334 - val_acc: 0.6114\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5343 - acc: 0.7926 - val_loss: 1.1316 - val_acc: 0.6343\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5288 - acc: 0.7958 - val_loss: 1.1304 - val_acc: 0.6114\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5391 - acc: 0.7951 - val_loss: 1.1331 - val_acc: 0.6457\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5451 - acc: 0.7875 - val_loss: 1.1279 - val_acc: 0.6229\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5405 - acc: 0.7798 - val_loss: 1.1288 - val_acc: 0.6229\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5472 - acc: 0.7811 - val_loss: 1.1290 - val_acc: 0.6114\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5347 - acc: 0.7875 - val_loss: 1.1323 - val_acc: 0.6343\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5395 - acc: 0.7907 - val_loss: 1.1332 - val_acc: 0.6171\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5298 - acc: 0.7894 - val_loss: 1.1397 - val_acc: 0.6171\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5300 - acc: 0.7856 - val_loss: 1.1444 - val_acc: 0.6286\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5260 - acc: 0.7951 - val_loss: 1.1511 - val_acc: 0.6400\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5247 - acc: 0.8009 - val_loss: 1.1548 - val_acc: 0.6286\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5357 - acc: 0.7907 - val_loss: 1.1562 - val_acc: 0.6343\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5151 - acc: 0.8111 - val_loss: 1.1520 - val_acc: 0.6286\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5307 - acc: 0.7964 - val_loss: 1.1528 - val_acc: 0.6514\n",
      "Epoch 191/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5435 - acc: 0.7907 - val_loss: 1.1540 - val_acc: 0.6343\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5234 - acc: 0.8041 - val_loss: 1.1648 - val_acc: 0.6229\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5257 - acc: 0.7932 - val_loss: 1.1595 - val_acc: 0.6343\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5403 - acc: 0.7926 - val_loss: 1.1617 - val_acc: 0.6286\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4999 - acc: 0.8117 - val_loss: 1.1725 - val_acc: 0.6343\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5245 - acc: 0.7913 - val_loss: 1.1615 - val_acc: 0.6286\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5137 - acc: 0.8015 - val_loss: 1.1573 - val_acc: 0.6343\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4993 - acc: 0.8066 - val_loss: 1.1659 - val_acc: 0.6343\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5137 - acc: 0.7977 - val_loss: 1.1645 - val_acc: 0.6229\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5073 - acc: 0.7939 - val_loss: 1.1693 - val_acc: 0.6171\n",
      "193/193 [==============================] - 0s 73us/step\n",
      "1742/1742 [==============================] - 0s 58us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.4769 - acc: 0.2821 - val_loss: 1.2244 - val_acc: 0.4229\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 1.2653 - acc: 0.4097 - val_loss: 1.1304 - val_acc: 0.4743\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.2077 - acc: 0.4518 - val_loss: 1.0717 - val_acc: 0.5486\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.1326 - acc: 0.4933 - val_loss: 1.0453 - val_acc: 0.5314\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.1161 - acc: 0.5048 - val_loss: 1.0241 - val_acc: 0.5429\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 1.0805 - acc: 0.5137 - val_loss: 1.0035 - val_acc: 0.5486\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0596 - acc: 0.5163 - val_loss: 0.9945 - val_acc: 0.5657\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0362 - acc: 0.5246 - val_loss: 0.9832 - val_acc: 0.5429\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 1.0144 - acc: 0.5699 - val_loss: 0.9756 - val_acc: 0.5543\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 1.0219 - acc: 0.5597 - val_loss: 0.9753 - val_acc: 0.5429\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9958 - acc: 0.5699 - val_loss: 0.9785 - val_acc: 0.5486\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.9722 - acc: 0.6031 - val_loss: 0.9741 - val_acc: 0.5657\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9781 - acc: 0.5846 - val_loss: 0.9635 - val_acc: 0.5486\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9594 - acc: 0.5897 - val_loss: 0.9589 - val_acc: 0.5829\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.9615 - acc: 0.5999 - val_loss: 0.9595 - val_acc: 0.5600\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.9506 - acc: 0.5858 - val_loss: 0.9606 - val_acc: 0.5600\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9318 - acc: 0.6050 - val_loss: 0.9582 - val_acc: 0.5771\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.9339 - acc: 0.5986 - val_loss: 0.9599 - val_acc: 0.5714\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9333 - acc: 0.6050 - val_loss: 0.9630 - val_acc: 0.5714\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9255 - acc: 0.6031 - val_loss: 0.9607 - val_acc: 0.5714\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9079 - acc: 0.6273 - val_loss: 0.9584 - val_acc: 0.5486\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9182 - acc: 0.6235 - val_loss: 0.9581 - val_acc: 0.5543\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9154 - acc: 0.6197 - val_loss: 0.9560 - val_acc: 0.5829\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9087 - acc: 0.6267 - val_loss: 0.9599 - val_acc: 0.5714\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9016 - acc: 0.6228 - val_loss: 0.9585 - val_acc: 0.5829\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8905 - acc: 0.6318 - val_loss: 0.9637 - val_acc: 0.5829\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8812 - acc: 0.6369 - val_loss: 0.9546 - val_acc: 0.5829\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8760 - acc: 0.6388 - val_loss: 0.9648 - val_acc: 0.5829\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8757 - acc: 0.6439 - val_loss: 0.9647 - val_acc: 0.5886\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8791 - acc: 0.6375 - val_loss: 0.9681 - val_acc: 0.5771\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8794 - acc: 0.6311 - val_loss: 0.9576 - val_acc: 0.5771\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8724 - acc: 0.6573 - val_loss: 0.9609 - val_acc: 0.5714\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8522 - acc: 0.6573 - val_loss: 0.9620 - val_acc: 0.5829\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8494 - acc: 0.6516 - val_loss: 0.9665 - val_acc: 0.5829\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8388 - acc: 0.6509 - val_loss: 0.9725 - val_acc: 0.5886\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8521 - acc: 0.6548 - val_loss: 0.9681 - val_acc: 0.5829\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8345 - acc: 0.6471 - val_loss: 0.9756 - val_acc: 0.5886\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8508 - acc: 0.6535 - val_loss: 0.9710 - val_acc: 0.5829\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8351 - acc: 0.6567 - val_loss: 0.9644 - val_acc: 0.5886\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8305 - acc: 0.6682 - val_loss: 0.9725 - val_acc: 0.5829\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8298 - acc: 0.6656 - val_loss: 0.9743 - val_acc: 0.5886\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8259 - acc: 0.6445 - val_loss: 0.9721 - val_acc: 0.5829\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8091 - acc: 0.6707 - val_loss: 0.9756 - val_acc: 0.5771\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8332 - acc: 0.6554 - val_loss: 0.9811 - val_acc: 0.5886\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8322 - acc: 0.6662 - val_loss: 0.9747 - val_acc: 0.5943\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8246 - acc: 0.6637 - val_loss: 0.9756 - val_acc: 0.6114\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8010 - acc: 0.6771 - val_loss: 0.9759 - val_acc: 0.6000\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8092 - acc: 0.6726 - val_loss: 0.9820 - val_acc: 0.5886\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8110 - acc: 0.6713 - val_loss: 0.9816 - val_acc: 0.6057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7927 - acc: 0.6694 - val_loss: 0.9894 - val_acc: 0.5943\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8180 - acc: 0.6713 - val_loss: 0.9857 - val_acc: 0.5943\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8081 - acc: 0.6765 - val_loss: 0.9897 - val_acc: 0.6057\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7917 - acc: 0.6707 - val_loss: 0.9918 - val_acc: 0.6057\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7753 - acc: 0.6835 - val_loss: 0.9934 - val_acc: 0.5943\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7868 - acc: 0.6899 - val_loss: 0.9946 - val_acc: 0.5943\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7729 - acc: 0.6816 - val_loss: 0.9984 - val_acc: 0.5943\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7992 - acc: 0.6669 - val_loss: 0.9894 - val_acc: 0.5886\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7759 - acc: 0.6854 - val_loss: 0.9936 - val_acc: 0.5943\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7833 - acc: 0.6835 - val_loss: 0.9957 - val_acc: 0.6000\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7727 - acc: 0.6854 - val_loss: 0.9833 - val_acc: 0.6114\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7693 - acc: 0.6924 - val_loss: 0.9950 - val_acc: 0.6057\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7733 - acc: 0.6828 - val_loss: 1.0030 - val_acc: 0.6000\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7728 - acc: 0.6886 - val_loss: 1.0066 - val_acc: 0.5943\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7566 - acc: 0.7039 - val_loss: 1.0047 - val_acc: 0.5943\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7532 - acc: 0.6790 - val_loss: 0.9913 - val_acc: 0.6000\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7438 - acc: 0.6988 - val_loss: 1.0077 - val_acc: 0.5886\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7474 - acc: 0.6918 - val_loss: 1.0070 - val_acc: 0.6000\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7505 - acc: 0.7007 - val_loss: 1.0091 - val_acc: 0.6000\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 113us/step - loss: 0.7498 - acc: 0.6981 - val_loss: 1.0116 - val_acc: 0.5943\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7671 - acc: 0.6879 - val_loss: 1.0010 - val_acc: 0.6057\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7488 - acc: 0.6975 - val_loss: 1.0121 - val_acc: 0.5943\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7463 - acc: 0.7020 - val_loss: 1.0114 - val_acc: 0.5886\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7169 - acc: 0.7109 - val_loss: 1.0161 - val_acc: 0.5943\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7455 - acc: 0.7020 - val_loss: 1.0122 - val_acc: 0.6114\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7297 - acc: 0.7192 - val_loss: 1.0161 - val_acc: 0.5943\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7188 - acc: 0.7243 - val_loss: 1.0150 - val_acc: 0.6000\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7404 - acc: 0.7007 - val_loss: 1.0150 - val_acc: 0.5886\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7201 - acc: 0.7109 - val_loss: 1.0141 - val_acc: 0.6000\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7310 - acc: 0.7026 - val_loss: 1.0219 - val_acc: 0.6000\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7342 - acc: 0.7007 - val_loss: 1.0210 - val_acc: 0.6057\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.7256 - acc: 0.7192 - val_loss: 1.0158 - val_acc: 0.6114\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7322 - acc: 0.6969 - val_loss: 1.0182 - val_acc: 0.5886\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7411 - acc: 0.7052 - val_loss: 1.0241 - val_acc: 0.5886\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7217 - acc: 0.7033 - val_loss: 1.0208 - val_acc: 0.5829\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7099 - acc: 0.7109 - val_loss: 1.0311 - val_acc: 0.5943\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7161 - acc: 0.7198 - val_loss: 1.0198 - val_acc: 0.6057\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7012 - acc: 0.7281 - val_loss: 1.0131 - val_acc: 0.6000\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6943 - acc: 0.7122 - val_loss: 1.0206 - val_acc: 0.6000\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7151 - acc: 0.7147 - val_loss: 1.0280 - val_acc: 0.6000\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6933 - acc: 0.7084 - val_loss: 1.0388 - val_acc: 0.5771\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6986 - acc: 0.7237 - val_loss: 1.0232 - val_acc: 0.6000\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7215 - acc: 0.7116 - val_loss: 1.0283 - val_acc: 0.5886\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6999 - acc: 0.7167 - val_loss: 1.0314 - val_acc: 0.5886\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 144us/step - loss: 0.7124 - acc: 0.7173 - val_loss: 1.0355 - val_acc: 0.5829\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6981 - acc: 0.7147 - val_loss: 1.0397 - val_acc: 0.5829\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6752 - acc: 0.7498 - val_loss: 1.0469 - val_acc: 0.5943\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6820 - acc: 0.7313 - val_loss: 1.0481 - val_acc: 0.6000\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6787 - acc: 0.7384 - val_loss: 1.0431 - val_acc: 0.6000\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6876 - acc: 0.7262 - val_loss: 1.0494 - val_acc: 0.6057\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6800 - acc: 0.7313 - val_loss: 1.0460 - val_acc: 0.5886\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6778 - acc: 0.7435 - val_loss: 1.0333 - val_acc: 0.6114\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6828 - acc: 0.7173 - val_loss: 1.0405 - val_acc: 0.6114\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6742 - acc: 0.7345 - val_loss: 1.0508 - val_acc: 0.6000\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6781 - acc: 0.7307 - val_loss: 1.0564 - val_acc: 0.6000\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6784 - acc: 0.7262 - val_loss: 1.0655 - val_acc: 0.5943\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6679 - acc: 0.7441 - val_loss: 1.0545 - val_acc: 0.6000\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6597 - acc: 0.7441 - val_loss: 1.0619 - val_acc: 0.6000\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6587 - acc: 0.7326 - val_loss: 1.0713 - val_acc: 0.5829\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6695 - acc: 0.7275 - val_loss: 1.0531 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6448 - acc: 0.7415 - val_loss: 1.0591 - val_acc: 0.5943\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6454 - acc: 0.7384 - val_loss: 1.0610 - val_acc: 0.5943\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6506 - acc: 0.7435 - val_loss: 1.0632 - val_acc: 0.5943\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6564 - acc: 0.7345 - val_loss: 1.0690 - val_acc: 0.5829\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6623 - acc: 0.7320 - val_loss: 1.0546 - val_acc: 0.6000\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6489 - acc: 0.7454 - val_loss: 1.0609 - val_acc: 0.5943\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6543 - acc: 0.7294 - val_loss: 1.0669 - val_acc: 0.5829\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6498 - acc: 0.7390 - val_loss: 1.0803 - val_acc: 0.5771\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6400 - acc: 0.7537 - val_loss: 1.0691 - val_acc: 0.5829\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6517 - acc: 0.7473 - val_loss: 1.0728 - val_acc: 0.5943\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6485 - acc: 0.7403 - val_loss: 1.0639 - val_acc: 0.6057\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6388 - acc: 0.7435 - val_loss: 1.0759 - val_acc: 0.6000\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6352 - acc: 0.7492 - val_loss: 1.0811 - val_acc: 0.6000\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6274 - acc: 0.7511 - val_loss: 1.0828 - val_acc: 0.6000\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6304 - acc: 0.7601 - val_loss: 1.0770 - val_acc: 0.5943\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6351 - acc: 0.7428 - val_loss: 1.0916 - val_acc: 0.5886\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6115 - acc: 0.7556 - val_loss: 1.0805 - val_acc: 0.5943\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6114 - acc: 0.7690 - val_loss: 1.0952 - val_acc: 0.5829\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6245 - acc: 0.7518 - val_loss: 1.0860 - val_acc: 0.5943\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6297 - acc: 0.7511 - val_loss: 1.0798 - val_acc: 0.6057\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.6264 - acc: 0.7518 - val_loss: 1.0926 - val_acc: 0.5771\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6192 - acc: 0.7562 - val_loss: 1.0944 - val_acc: 0.5886\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6285 - acc: 0.7486 - val_loss: 1.0938 - val_acc: 0.6114\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6165 - acc: 0.7575 - val_loss: 1.0999 - val_acc: 0.6057\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6167 - acc: 0.7454 - val_loss: 1.1022 - val_acc: 0.5886\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6157 - acc: 0.7543 - val_loss: 1.1103 - val_acc: 0.5771\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6252 - acc: 0.7409 - val_loss: 1.1195 - val_acc: 0.5657\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6012 - acc: 0.7703 - val_loss: 1.0955 - val_acc: 0.5943\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5991 - acc: 0.7594 - val_loss: 1.1048 - val_acc: 0.5943\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.5916 - acc: 0.7703 - val_loss: 1.1153 - val_acc: 0.5886\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5910 - acc: 0.7722 - val_loss: 1.1019 - val_acc: 0.5886\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.5910 - acc: 0.7747 - val_loss: 1.1175 - val_acc: 0.5771\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5940 - acc: 0.7703 - val_loss: 1.1148 - val_acc: 0.6000\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.5946 - acc: 0.7677 - val_loss: 1.1207 - val_acc: 0.5886\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.6091 - acc: 0.7594 - val_loss: 1.1256 - val_acc: 0.5886\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 132us/step - loss: 0.5779 - acc: 0.7703 - val_loss: 1.1167 - val_acc: 0.5943\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 107us/step - loss: 0.5934 - acc: 0.7639 - val_loss: 1.1150 - val_acc: 0.6000\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6050 - acc: 0.7537 - val_loss: 1.1198 - val_acc: 0.5829\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5666 - acc: 0.7779 - val_loss: 1.1151 - val_acc: 0.5886\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5911 - acc: 0.7683 - val_loss: 1.1198 - val_acc: 0.5829\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5720 - acc: 0.7805 - val_loss: 1.1310 - val_acc: 0.5829\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5777 - acc: 0.7709 - val_loss: 1.1290 - val_acc: 0.5886\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5760 - acc: 0.7779 - val_loss: 1.1345 - val_acc: 0.5714\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5566 - acc: 0.7862 - val_loss: 1.1232 - val_acc: 0.5943\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5910 - acc: 0.7664 - val_loss: 1.1311 - val_acc: 0.5886\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5730 - acc: 0.7856 - val_loss: 1.1479 - val_acc: 0.5714\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 131us/step - loss: 0.5790 - acc: 0.7652 - val_loss: 1.1304 - val_acc: 0.5886\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.5611 - acc: 0.7811 - val_loss: 1.1267 - val_acc: 0.5829\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5651 - acc: 0.7779 - val_loss: 1.1347 - val_acc: 0.5829\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5486 - acc: 0.7888 - val_loss: 1.1449 - val_acc: 0.6000\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5514 - acc: 0.7900 - val_loss: 1.1372 - val_acc: 0.5886\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.5715 - acc: 0.7760 - val_loss: 1.1286 - val_acc: 0.6000\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 140us/step - loss: 0.5555 - acc: 0.7811 - val_loss: 1.1388 - val_acc: 0.5829\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5583 - acc: 0.7786 - val_loss: 1.1409 - val_acc: 0.5943\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5732 - acc: 0.7792 - val_loss: 1.1497 - val_acc: 0.5714\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5747 - acc: 0.7735 - val_loss: 1.1525 - val_acc: 0.5829\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5325 - acc: 0.7971 - val_loss: 1.1535 - val_acc: 0.5771\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5296 - acc: 0.7900 - val_loss: 1.1572 - val_acc: 0.5829\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5280 - acc: 0.7894 - val_loss: 1.1534 - val_acc: 0.5829\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5558 - acc: 0.7696 - val_loss: 1.1527 - val_acc: 0.5943\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5353 - acc: 0.8003 - val_loss: 1.1622 - val_acc: 0.5886\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.5336 - acc: 0.7830 - val_loss: 1.1556 - val_acc: 0.5886\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5551 - acc: 0.7862 - val_loss: 1.1636 - val_acc: 0.5943\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5519 - acc: 0.7920 - val_loss: 1.1653 - val_acc: 0.5771\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5458 - acc: 0.7907 - val_loss: 1.1614 - val_acc: 0.5886\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5327 - acc: 0.7945 - val_loss: 1.1636 - val_acc: 0.5886\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5396 - acc: 0.7805 - val_loss: 1.1636 - val_acc: 0.5829\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5440 - acc: 0.8028 - val_loss: 1.1804 - val_acc: 0.5886\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5297 - acc: 0.7824 - val_loss: 1.1747 - val_acc: 0.5943\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5163 - acc: 0.7996 - val_loss: 1.1861 - val_acc: 0.5829\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5172 - acc: 0.8009 - val_loss: 1.1818 - val_acc: 0.5943\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5196 - acc: 0.8015 - val_loss: 1.1759 - val_acc: 0.5886\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5181 - acc: 0.7932 - val_loss: 1.1815 - val_acc: 0.5886\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5539 - acc: 0.7869 - val_loss: 1.1865 - val_acc: 0.5943\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5232 - acc: 0.7869 - val_loss: 1.1741 - val_acc: 0.6057\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5289 - acc: 0.8086 - val_loss: 1.1714 - val_acc: 0.5943\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.5067 - acc: 0.8060 - val_loss: 1.1748 - val_acc: 0.5886\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5153 - acc: 0.7939 - val_loss: 1.1807 - val_acc: 0.5886\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5037 - acc: 0.7977 - val_loss: 1.1732 - val_acc: 0.5886\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5282 - acc: 0.7983 - val_loss: 1.1721 - val_acc: 0.5771\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5015 - acc: 0.8079 - val_loss: 1.1876 - val_acc: 0.5943\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5167 - acc: 0.7990 - val_loss: 1.1959 - val_acc: 0.5829\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5137 - acc: 0.7907 - val_loss: 1.1944 - val_acc: 0.5829\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5060 - acc: 0.8130 - val_loss: 1.1949 - val_acc: 0.5886\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5157 - acc: 0.8060 - val_loss: 1.1994 - val_acc: 0.5829\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5077 - acc: 0.7990 - val_loss: 1.1991 - val_acc: 0.5829\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4912 - acc: 0.8130 - val_loss: 1.1908 - val_acc: 0.5829\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4993 - acc: 0.8015 - val_loss: 1.2137 - val_acc: 0.5657\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5096 - acc: 0.8124 - val_loss: 1.2056 - val_acc: 0.5829\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5059 - acc: 0.8015 - val_loss: 1.2047 - val_acc: 0.5829\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 172us/step - loss: 0.5114 - acc: 0.8105 - val_loss: 1.2041 - val_acc: 0.5886\n",
      "193/193 [==============================] - 0s 98us/step\n",
      "1742/1742 [==============================] - 0s 57us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 7s 5ms/step - loss: 1.4364 - acc: 0.3155 - val_loss: 1.1017 - val_acc: 0.5600\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.1942 - acc: 0.4693 - val_loss: 1.0103 - val_acc: 0.6286\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 1.1174 - acc: 0.5077 - val_loss: 0.9618 - val_acc: 0.6400\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 1.0854 - acc: 0.4930 - val_loss: 0.9418 - val_acc: 0.6857\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 1.0453 - acc: 0.5370 - val_loss: 0.9317 - val_acc: 0.6857\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 1.0207 - acc: 0.5632 - val_loss: 0.9235 - val_acc: 0.6629\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 114us/step - loss: 0.9695 - acc: 0.5862 - val_loss: 0.9054 - val_acc: 0.6800\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 113us/step - loss: 0.9624 - acc: 0.5926 - val_loss: 0.9049 - val_acc: 0.6857\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.9577 - acc: 0.5939 - val_loss: 0.9052 - val_acc: 0.6571\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 124us/step - loss: 0.9417 - acc: 0.6041 - val_loss: 0.8951 - val_acc: 0.6743\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.9486 - acc: 0.5977 - val_loss: 0.8903 - val_acc: 0.6800\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.9087 - acc: 0.6117 - val_loss: 0.8986 - val_acc: 0.6629\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.9266 - acc: 0.5996 - val_loss: 0.8860 - val_acc: 0.6686\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.9043 - acc: 0.6213 - val_loss: 0.8865 - val_acc: 0.6686\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.8899 - acc: 0.6264 - val_loss: 0.8855 - val_acc: 0.6686\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8752 - acc: 0.6456 - val_loss: 0.8948 - val_acc: 0.6686\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8823 - acc: 0.6322 - val_loss: 0.8980 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8603 - acc: 0.6437 - val_loss: 0.8922 - val_acc: 0.6857\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8687 - acc: 0.6526 - val_loss: 0.8867 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.8567 - acc: 0.6564 - val_loss: 0.8908 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.8556 - acc: 0.6398 - val_loss: 0.8932 - val_acc: 0.6686\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8540 - acc: 0.6571 - val_loss: 0.8957 - val_acc: 0.6514\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8610 - acc: 0.6462 - val_loss: 0.8844 - val_acc: 0.6686\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8340 - acc: 0.6456 - val_loss: 0.8867 - val_acc: 0.6743\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8280 - acc: 0.6545 - val_loss: 0.8850 - val_acc: 0.6629\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8185 - acc: 0.6756 - val_loss: 0.8927 - val_acc: 0.6743\n",
      "Epoch 27/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8324 - acc: 0.6692 - val_loss: 0.8943 - val_acc: 0.6571\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.8111 - acc: 0.6679 - val_loss: 0.8987 - val_acc: 0.6514\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8143 - acc: 0.6654 - val_loss: 0.8998 - val_acc: 0.6571\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8021 - acc: 0.6654 - val_loss: 0.9099 - val_acc: 0.6400\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.8083 - acc: 0.6916 - val_loss: 0.9125 - val_acc: 0.6629\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7885 - acc: 0.6858 - val_loss: 0.9053 - val_acc: 0.6686\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7824 - acc: 0.6826 - val_loss: 0.9160 - val_acc: 0.6514\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7772 - acc: 0.6909 - val_loss: 0.9121 - val_acc: 0.6514\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7663 - acc: 0.6865 - val_loss: 0.9168 - val_acc: 0.6514\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7618 - acc: 0.6986 - val_loss: 0.9135 - val_acc: 0.6571\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7615 - acc: 0.6992 - val_loss: 0.9153 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7624 - acc: 0.6999 - val_loss: 0.9190 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7582 - acc: 0.6916 - val_loss: 0.9240 - val_acc: 0.6514\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7522 - acc: 0.6909 - val_loss: 0.9251 - val_acc: 0.6457\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.7433 - acc: 0.6967 - val_loss: 0.9307 - val_acc: 0.6514\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 155us/step - loss: 0.7634 - acc: 0.6782 - val_loss: 0.9316 - val_acc: 0.6457\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.7577 - acc: 0.7037 - val_loss: 0.9278 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7221 - acc: 0.7146 - val_loss: 0.9221 - val_acc: 0.6571\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7294 - acc: 0.6935 - val_loss: 0.9322 - val_acc: 0.6514\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7159 - acc: 0.7120 - val_loss: 0.9444 - val_acc: 0.6457\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7448 - acc: 0.6980 - val_loss: 0.9531 - val_acc: 0.6400\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7209 - acc: 0.7082 - val_loss: 0.9365 - val_acc: 0.6457\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7252 - acc: 0.7139 - val_loss: 0.9330 - val_acc: 0.6457\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7129 - acc: 0.7114 - val_loss: 0.9462 - val_acc: 0.6286\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7152 - acc: 0.7133 - val_loss: 0.9447 - val_acc: 0.6400\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.6943 - acc: 0.7133 - val_loss: 0.9475 - val_acc: 0.6571\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.7112 - acc: 0.7171 - val_loss: 0.9498 - val_acc: 0.6571\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.7013 - acc: 0.7235 - val_loss: 0.9481 - val_acc: 0.6457\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6942 - acc: 0.7209 - val_loss: 0.9440 - val_acc: 0.6457\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6890 - acc: 0.7337 - val_loss: 0.9552 - val_acc: 0.6286\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6902 - acc: 0.7261 - val_loss: 0.9565 - val_acc: 0.6343\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6878 - acc: 0.7324 - val_loss: 0.9581 - val_acc: 0.6400\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6801 - acc: 0.7241 - val_loss: 0.9566 - val_acc: 0.6229\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6822 - acc: 0.7222 - val_loss: 0.9595 - val_acc: 0.6286\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6675 - acc: 0.7458 - val_loss: 0.9725 - val_acc: 0.6286\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6656 - acc: 0.7350 - val_loss: 0.9638 - val_acc: 0.6457\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6695 - acc: 0.7395 - val_loss: 0.9724 - val_acc: 0.6343\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6484 - acc: 0.7350 - val_loss: 0.9559 - val_acc: 0.6629\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6597 - acc: 0.7318 - val_loss: 0.9606 - val_acc: 0.6400\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6533 - acc: 0.7420 - val_loss: 0.9762 - val_acc: 0.6229\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6551 - acc: 0.7490 - val_loss: 0.9842 - val_acc: 0.6286\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6390 - acc: 0.7478 - val_loss: 0.9814 - val_acc: 0.6343\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6568 - acc: 0.7401 - val_loss: 0.9952 - val_acc: 0.6343\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6496 - acc: 0.7490 - val_loss: 0.9871 - val_acc: 0.6171\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6259 - acc: 0.7573 - val_loss: 0.9935 - val_acc: 0.6229\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.6064 - acc: 0.7765 - val_loss: 0.9994 - val_acc: 0.6229\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.6342 - acc: 0.7554 - val_loss: 0.9911 - val_acc: 0.6343\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.6395 - acc: 0.7618 - val_loss: 1.0001 - val_acc: 0.6343\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6223 - acc: 0.7625 - val_loss: 1.0037 - val_acc: 0.6229\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6205 - acc: 0.7484 - val_loss: 1.0144 - val_acc: 0.6400\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6326 - acc: 0.7561 - val_loss: 1.0077 - val_acc: 0.6457\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 110us/step - loss: 0.5900 - acc: 0.7625 - val_loss: 1.0237 - val_acc: 0.6343\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.5849 - acc: 0.7708 - val_loss: 1.0097 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6000 - acc: 0.7663 - val_loss: 1.0312 - val_acc: 0.6400\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 111us/step - loss: 0.5957 - acc: 0.7618 - val_loss: 1.0287 - val_acc: 0.6457\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6074 - acc: 0.7612 - val_loss: 1.0392 - val_acc: 0.6514\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6078 - acc: 0.7637 - val_loss: 1.0367 - val_acc: 0.6400\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5733 - acc: 0.7803 - val_loss: 1.0329 - val_acc: 0.6171\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6065 - acc: 0.7612 - val_loss: 1.0366 - val_acc: 0.6343\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5646 - acc: 0.7829 - val_loss: 1.0338 - val_acc: 0.6286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5734 - acc: 0.7842 - val_loss: 1.0483 - val_acc: 0.6229\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5913 - acc: 0.7663 - val_loss: 1.0406 - val_acc: 0.6229\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.5653 - acc: 0.7880 - val_loss: 1.0403 - val_acc: 0.6286\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5866 - acc: 0.7720 - val_loss: 1.0357 - val_acc: 0.6343\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5520 - acc: 0.7797 - val_loss: 1.0481 - val_acc: 0.6343\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6013 - acc: 0.7784 - val_loss: 1.0525 - val_acc: 0.6229\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5638 - acc: 0.7778 - val_loss: 1.0517 - val_acc: 0.6400\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.5396 - acc: 0.7944 - val_loss: 1.0567 - val_acc: 0.6286\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.5371 - acc: 0.7886 - val_loss: 1.0687 - val_acc: 0.6343\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5596 - acc: 0.7816 - val_loss: 1.0810 - val_acc: 0.6286\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.5585 - acc: 0.7854 - val_loss: 1.0729 - val_acc: 0.6343\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5593 - acc: 0.7905 - val_loss: 1.0815 - val_acc: 0.6286\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5360 - acc: 0.8027 - val_loss: 1.1054 - val_acc: 0.6229\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5583 - acc: 0.7765 - val_loss: 1.0827 - val_acc: 0.6343\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5446 - acc: 0.7854 - val_loss: 1.0905 - val_acc: 0.6286\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.5552 - acc: 0.7867 - val_loss: 1.0740 - val_acc: 0.6400\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5426 - acc: 0.7963 - val_loss: 1.0863 - val_acc: 0.6343\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 125us/step - loss: 0.5601 - acc: 0.7835 - val_loss: 1.0831 - val_acc: 0.6343\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.5576 - acc: 0.7778 - val_loss: 1.0827 - val_acc: 0.6629\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5362 - acc: 0.7957 - val_loss: 1.0925 - val_acc: 0.6571\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5358 - acc: 0.8014 - val_loss: 1.0828 - val_acc: 0.6514\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5307 - acc: 0.8110 - val_loss: 1.1095 - val_acc: 0.6629\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5152 - acc: 0.8103 - val_loss: 1.1080 - val_acc: 0.6400\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5358 - acc: 0.7950 - val_loss: 1.1033 - val_acc: 0.6343\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4883 - acc: 0.8167 - val_loss: 1.1059 - val_acc: 0.6457\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5152 - acc: 0.8072 - val_loss: 1.1292 - val_acc: 0.6400\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5097 - acc: 0.8072 - val_loss: 1.1231 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5053 - acc: 0.7982 - val_loss: 1.1315 - val_acc: 0.6571\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.5090 - acc: 0.8103 - val_loss: 1.1463 - val_acc: 0.6400\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4996 - acc: 0.7989 - val_loss: 1.1347 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.4996 - acc: 0.8084 - val_loss: 1.1328 - val_acc: 0.6514\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5175 - acc: 0.8052 - val_loss: 1.1423 - val_acc: 0.6514\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.4833 - acc: 0.8250 - val_loss: 1.1444 - val_acc: 0.6457\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5035 - acc: 0.8116 - val_loss: 1.1434 - val_acc: 0.6514\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4916 - acc: 0.8199 - val_loss: 1.1601 - val_acc: 0.6514\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4979 - acc: 0.8084 - val_loss: 1.1435 - val_acc: 0.6514\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4672 - acc: 0.8193 - val_loss: 1.1563 - val_acc: 0.6571\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5060 - acc: 0.8123 - val_loss: 1.1610 - val_acc: 0.6629\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4765 - acc: 0.8155 - val_loss: 1.1685 - val_acc: 0.6457\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4778 - acc: 0.8238 - val_loss: 1.1645 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.4780 - acc: 0.8218 - val_loss: 1.1528 - val_acc: 0.6343\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4643 - acc: 0.8314 - val_loss: 1.1641 - val_acc: 0.6400\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4724 - acc: 0.8180 - val_loss: 1.1727 - val_acc: 0.6400\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4732 - acc: 0.8116 - val_loss: 1.1850 - val_acc: 0.6343\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4811 - acc: 0.8161 - val_loss: 1.1770 - val_acc: 0.6343\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4801 - acc: 0.8199 - val_loss: 1.1854 - val_acc: 0.6229\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4681 - acc: 0.8263 - val_loss: 1.1704 - val_acc: 0.6400\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4823 - acc: 0.8059 - val_loss: 1.1771 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.4721 - acc: 0.8174 - val_loss: 1.1725 - val_acc: 0.6400\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4706 - acc: 0.8199 - val_loss: 1.1771 - val_acc: 0.6400\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4457 - acc: 0.8321 - val_loss: 1.2042 - val_acc: 0.6229\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.4743 - acc: 0.8206 - val_loss: 1.1940 - val_acc: 0.6343\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4446 - acc: 0.8352 - val_loss: 1.2004 - val_acc: 0.6400\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4847 - acc: 0.8218 - val_loss: 1.2095 - val_acc: 0.6400\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4593 - acc: 0.8301 - val_loss: 1.2157 - val_acc: 0.6343\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4725 - acc: 0.8206 - val_loss: 1.2053 - val_acc: 0.6286\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.4632 - acc: 0.8276 - val_loss: 1.2079 - val_acc: 0.6457\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4285 - acc: 0.8461 - val_loss: 1.1969 - val_acc: 0.6400\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4302 - acc: 0.8467 - val_loss: 1.2063 - val_acc: 0.6571\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4740 - acc: 0.8193 - val_loss: 1.2390 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4419 - acc: 0.8378 - val_loss: 1.2306 - val_acc: 0.6400\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4375 - acc: 0.8372 - val_loss: 1.2243 - val_acc: 0.6343\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4556 - acc: 0.8346 - val_loss: 1.2280 - val_acc: 0.6286\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4331 - acc: 0.8352 - val_loss: 1.2386 - val_acc: 0.6286\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4342 - acc: 0.8333 - val_loss: 1.2359 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4145 - acc: 0.8455 - val_loss: 1.2455 - val_acc: 0.6457\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4353 - acc: 0.8340 - val_loss: 1.2517 - val_acc: 0.6400\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4434 - acc: 0.8499 - val_loss: 1.2431 - val_acc: 0.6514\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4404 - acc: 0.8416 - val_loss: 1.2495 - val_acc: 0.6571\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4339 - acc: 0.8289 - val_loss: 1.2575 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.3845 - acc: 0.8487 - val_loss: 1.2538 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4104 - acc: 0.8506 - val_loss: 1.2720 - val_acc: 0.6286\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4184 - acc: 0.8570 - val_loss: 1.2779 - val_acc: 0.6514\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3956 - acc: 0.8570 - val_loss: 1.2762 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4157 - acc: 0.8480 - val_loss: 1.2537 - val_acc: 0.6286\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4145 - acc: 0.8563 - val_loss: 1.2671 - val_acc: 0.6457\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4410 - acc: 0.8436 - val_loss: 1.2633 - val_acc: 0.6457\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4204 - acc: 0.8352 - val_loss: 1.2670 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4189 - acc: 0.8474 - val_loss: 1.2864 - val_acc: 0.6514\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3984 - acc: 0.8429 - val_loss: 1.3023 - val_acc: 0.6171\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4272 - acc: 0.8429 - val_loss: 1.2946 - val_acc: 0.6457\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4101 - acc: 0.8467 - val_loss: 1.2813 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.3954 - acc: 0.8525 - val_loss: 1.2748 - val_acc: 0.6343\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3775 - acc: 0.8659 - val_loss: 1.2981 - val_acc: 0.6286\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4174 - acc: 0.8397 - val_loss: 1.2939 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.4012 - acc: 0.8550 - val_loss: 1.2944 - val_acc: 0.6286\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4206 - acc: 0.8506 - val_loss: 1.2776 - val_acc: 0.6286\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3938 - acc: 0.8576 - val_loss: 1.2960 - val_acc: 0.6171\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.3882 - acc: 0.8519 - val_loss: 1.2911 - val_acc: 0.6343\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.3955 - acc: 0.8544 - val_loss: 1.2985 - val_acc: 0.6457\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4036 - acc: 0.8589 - val_loss: 1.3191 - val_acc: 0.6286\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4350 - acc: 0.8397 - val_loss: 1.3047 - val_acc: 0.6057\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.3571 - acc: 0.8633 - val_loss: 1.2896 - val_acc: 0.6400\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3727 - acc: 0.8678 - val_loss: 1.3092 - val_acc: 0.6286\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4148 - acc: 0.8467 - val_loss: 1.3236 - val_acc: 0.6000\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3930 - acc: 0.8608 - val_loss: 1.3068 - val_acc: 0.6457\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3869 - acc: 0.8685 - val_loss: 1.3180 - val_acc: 0.6114\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3624 - acc: 0.8678 - val_loss: 1.3289 - val_acc: 0.6514\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3796 - acc: 0.8646 - val_loss: 1.3238 - val_acc: 0.6400\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4081 - acc: 0.8563 - val_loss: 1.3172 - val_acc: 0.6229\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3860 - acc: 0.8602 - val_loss: 1.3307 - val_acc: 0.6171\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3669 - acc: 0.8697 - val_loss: 1.3352 - val_acc: 0.6286\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.3789 - acc: 0.8614 - val_loss: 1.3217 - val_acc: 0.6229\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3832 - acc: 0.8659 - val_loss: 1.3209 - val_acc: 0.6057\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3801 - acc: 0.8646 - val_loss: 1.3186 - val_acc: 0.6229\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.3829 - acc: 0.8531 - val_loss: 1.3368 - val_acc: 0.6171\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3757 - acc: 0.8557 - val_loss: 1.3432 - val_acc: 0.6171\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3680 - acc: 0.8646 - val_loss: 1.3572 - val_acc: 0.6114\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.3805 - acc: 0.8563 - val_loss: 1.3719 - val_acc: 0.6000\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3902 - acc: 0.8538 - val_loss: 1.3671 - val_acc: 0.6114\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3613 - acc: 0.8736 - val_loss: 1.3536 - val_acc: 0.6229\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3740 - acc: 0.8697 - val_loss: 1.3824 - val_acc: 0.6286\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3772 - acc: 0.8602 - val_loss: 1.3711 - val_acc: 0.6057\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3987 - acc: 0.8621 - val_loss: 1.3618 - val_acc: 0.6229\n",
      "194/194 [==============================] - 0s 83us/step\n",
      "1741/1741 [==============================] - 0s 51us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.3880 - acc: 0.3550 - val_loss: 1.1017 - val_acc: 0.6114\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 1.1827 - acc: 0.4662 - val_loss: 1.0182 - val_acc: 0.6343\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.1011 - acc: 0.5089 - val_loss: 0.9792 - val_acc: 0.6229\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 76us/step - loss: 1.0531 - acc: 0.5466 - val_loss: 0.9403 - val_acc: 0.6114\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 1.0237 - acc: 0.5530 - val_loss: 0.9322 - val_acc: 0.6343\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 1.0159 - acc: 0.5626 - val_loss: 0.9182 - val_acc: 0.6686\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9747 - acc: 0.5868 - val_loss: 0.9165 - val_acc: 0.6514\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.9669 - acc: 0.5894 - val_loss: 0.9035 - val_acc: 0.6343\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9344 - acc: 0.6086 - val_loss: 0.8993 - val_acc: 0.6800\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.9325 - acc: 0.5951 - val_loss: 0.8988 - val_acc: 0.6743\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9416 - acc: 0.5939 - val_loss: 0.8943 - val_acc: 0.6857\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.9252 - acc: 0.6073 - val_loss: 0.8925 - val_acc: 0.6743\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.9044 - acc: 0.6130 - val_loss: 0.8976 - val_acc: 0.6857\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8738 - acc: 0.6481 - val_loss: 0.8899 - val_acc: 0.6571\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8640 - acc: 0.6411 - val_loss: 0.8941 - val_acc: 0.6686\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8765 - acc: 0.6194 - val_loss: 0.8909 - val_acc: 0.6686\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8666 - acc: 0.6398 - val_loss: 0.8829 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8484 - acc: 0.6513 - val_loss: 0.8901 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8581 - acc: 0.6462 - val_loss: 0.8964 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8239 - acc: 0.6699 - val_loss: 0.8907 - val_acc: 0.6514\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8349 - acc: 0.6564 - val_loss: 0.8921 - val_acc: 0.6686\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8334 - acc: 0.6622 - val_loss: 0.8960 - val_acc: 0.6514\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8309 - acc: 0.6539 - val_loss: 0.8948 - val_acc: 0.6514\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8275 - acc: 0.6596 - val_loss: 0.8917 - val_acc: 0.6629\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7916 - acc: 0.6775 - val_loss: 0.8958 - val_acc: 0.6629\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8323 - acc: 0.6488 - val_loss: 0.8970 - val_acc: 0.6514\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8091 - acc: 0.6692 - val_loss: 0.9176 - val_acc: 0.6514\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8078 - acc: 0.6584 - val_loss: 0.9123 - val_acc: 0.6629\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8150 - acc: 0.6833 - val_loss: 0.9049 - val_acc: 0.6571\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7840 - acc: 0.6858 - val_loss: 0.9110 - val_acc: 0.6571\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7722 - acc: 0.6865 - val_loss: 0.9114 - val_acc: 0.6457\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7712 - acc: 0.6954 - val_loss: 0.9197 - val_acc: 0.6629\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7855 - acc: 0.6756 - val_loss: 0.9069 - val_acc: 0.6457\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7619 - acc: 0.6948 - val_loss: 0.9075 - val_acc: 0.6571\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7561 - acc: 0.6916 - val_loss: 0.9112 - val_acc: 0.6514\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7512 - acc: 0.7011 - val_loss: 0.9061 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7607 - acc: 0.6999 - val_loss: 0.9145 - val_acc: 0.6571\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7598 - acc: 0.6890 - val_loss: 0.9163 - val_acc: 0.6571\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7430 - acc: 0.6865 - val_loss: 0.9150 - val_acc: 0.6629\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7545 - acc: 0.6948 - val_loss: 0.9218 - val_acc: 0.6514\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7467 - acc: 0.6999 - val_loss: 0.9200 - val_acc: 0.6686\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7441 - acc: 0.6935 - val_loss: 0.9277 - val_acc: 0.6743\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7283 - acc: 0.7139 - val_loss: 0.9348 - val_acc: 0.6571\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7231 - acc: 0.7107 - val_loss: 0.9264 - val_acc: 0.6743\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7074 - acc: 0.7292 - val_loss: 0.9413 - val_acc: 0.6571\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7202 - acc: 0.7178 - val_loss: 0.9438 - val_acc: 0.6514\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6931 - acc: 0.7216 - val_loss: 0.9366 - val_acc: 0.6629\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7081 - acc: 0.7050 - val_loss: 0.9280 - val_acc: 0.6629\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6990 - acc: 0.7261 - val_loss: 0.9323 - val_acc: 0.6514\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7021 - acc: 0.7280 - val_loss: 0.9339 - val_acc: 0.6800\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7062 - acc: 0.7190 - val_loss: 0.9457 - val_acc: 0.6514\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6778 - acc: 0.7324 - val_loss: 0.9462 - val_acc: 0.6514\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6993 - acc: 0.7152 - val_loss: 0.9549 - val_acc: 0.6457\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6776 - acc: 0.7356 - val_loss: 0.9570 - val_acc: 0.6514\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6803 - acc: 0.7241 - val_loss: 0.9473 - val_acc: 0.6743\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6639 - acc: 0.7388 - val_loss: 0.9513 - val_acc: 0.6571\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6846 - acc: 0.7254 - val_loss: 0.9640 - val_acc: 0.6743\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6436 - acc: 0.7490 - val_loss: 0.9705 - val_acc: 0.6686\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6901 - acc: 0.7248 - val_loss: 0.9699 - val_acc: 0.6629\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6472 - acc: 0.7484 - val_loss: 0.9679 - val_acc: 0.6743\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6388 - acc: 0.7548 - val_loss: 0.9712 - val_acc: 0.6629\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.6567 - acc: 0.7363 - val_loss: 0.9708 - val_acc: 0.6629\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6363 - acc: 0.7458 - val_loss: 0.9737 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6437 - acc: 0.7510 - val_loss: 0.9692 - val_acc: 0.6743\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6207 - acc: 0.7529 - val_loss: 0.9810 - val_acc: 0.6686\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6156 - acc: 0.7618 - val_loss: 0.9861 - val_acc: 0.6514\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6403 - acc: 0.7510 - val_loss: 0.9944 - val_acc: 0.6686\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6276 - acc: 0.7497 - val_loss: 0.9933 - val_acc: 0.6514\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6519 - acc: 0.7484 - val_loss: 0.9970 - val_acc: 0.6514\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6314 - acc: 0.7548 - val_loss: 1.0069 - val_acc: 0.6457\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6225 - acc: 0.7656 - val_loss: 1.0083 - val_acc: 0.6457\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6028 - acc: 0.7669 - val_loss: 1.0041 - val_acc: 0.6571\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6181 - acc: 0.7548 - val_loss: 1.0114 - val_acc: 0.6571\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5937 - acc: 0.7752 - val_loss: 1.0058 - val_acc: 0.6629\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6056 - acc: 0.7708 - val_loss: 1.0227 - val_acc: 0.6571\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5818 - acc: 0.7650 - val_loss: 1.0178 - val_acc: 0.6629\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6026 - acc: 0.7593 - val_loss: 1.0232 - val_acc: 0.6571\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6052 - acc: 0.7746 - val_loss: 1.0366 - val_acc: 0.6457\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6048 - acc: 0.7663 - val_loss: 1.0277 - val_acc: 0.6629\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5914 - acc: 0.7739 - val_loss: 1.0282 - val_acc: 0.6686\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5830 - acc: 0.7784 - val_loss: 1.0350 - val_acc: 0.6743\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5964 - acc: 0.7695 - val_loss: 1.0320 - val_acc: 0.6800\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5684 - acc: 0.7797 - val_loss: 1.0419 - val_acc: 0.6743\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5808 - acc: 0.7746 - val_loss: 1.0463 - val_acc: 0.6629\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5730 - acc: 0.7842 - val_loss: 1.0630 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5676 - acc: 0.7880 - val_loss: 1.0503 - val_acc: 0.6629\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5439 - acc: 0.7899 - val_loss: 1.0425 - val_acc: 0.6514\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5563 - acc: 0.7854 - val_loss: 1.0630 - val_acc: 0.6629\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5828 - acc: 0.7848 - val_loss: 1.0596 - val_acc: 0.6571\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5279 - acc: 0.7931 - val_loss: 1.0664 - val_acc: 0.6400\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5515 - acc: 0.7854 - val_loss: 1.0606 - val_acc: 0.6629\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5447 - acc: 0.7905 - val_loss: 1.0571 - val_acc: 0.6629\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5519 - acc: 0.7905 - val_loss: 1.0710 - val_acc: 0.6629\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5391 - acc: 0.7874 - val_loss: 1.0920 - val_acc: 0.6514\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5201 - acc: 0.8033 - val_loss: 1.0956 - val_acc: 0.6629\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5321 - acc: 0.7976 - val_loss: 1.0918 - val_acc: 0.6629\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5335 - acc: 0.7937 - val_loss: 1.1014 - val_acc: 0.6514\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5456 - acc: 0.7918 - val_loss: 1.0941 - val_acc: 0.6629\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5266 - acc: 0.7995 - val_loss: 1.1032 - val_acc: 0.6686\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5306 - acc: 0.7950 - val_loss: 1.1055 - val_acc: 0.6571\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5403 - acc: 0.7842 - val_loss: 1.1124 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5129 - acc: 0.8110 - val_loss: 1.1191 - val_acc: 0.6457\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5019 - acc: 0.8059 - val_loss: 1.1101 - val_acc: 0.6514\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5211 - acc: 0.8059 - val_loss: 1.1261 - val_acc: 0.6457\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5142 - acc: 0.8206 - val_loss: 1.1203 - val_acc: 0.6514\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5264 - acc: 0.8155 - val_loss: 1.1348 - val_acc: 0.6571\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4876 - acc: 0.8193 - val_loss: 1.1392 - val_acc: 0.6343\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4933 - acc: 0.8238 - val_loss: 1.1373 - val_acc: 0.6514\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5124 - acc: 0.8257 - val_loss: 1.1455 - val_acc: 0.6571\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4935 - acc: 0.8103 - val_loss: 1.1510 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.4874 - acc: 0.8116 - val_loss: 1.1393 - val_acc: 0.6629\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5269 - acc: 0.8078 - val_loss: 1.1551 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5021 - acc: 0.8020 - val_loss: 1.1719 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4587 - acc: 0.8295 - val_loss: 1.1582 - val_acc: 0.6571\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4896 - acc: 0.8148 - val_loss: 1.1649 - val_acc: 0.6514\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4946 - acc: 0.8161 - val_loss: 1.1741 - val_acc: 0.6286\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4860 - acc: 0.8199 - val_loss: 1.1754 - val_acc: 0.6514\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4858 - acc: 0.8129 - val_loss: 1.1741 - val_acc: 0.6286\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4819 - acc: 0.8103 - val_loss: 1.1692 - val_acc: 0.6286\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.4770 - acc: 0.8295 - val_loss: 1.1913 - val_acc: 0.6343\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4743 - acc: 0.8199 - val_loss: 1.1838 - val_acc: 0.6286\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4536 - acc: 0.8333 - val_loss: 1.1883 - val_acc: 0.6343\n",
      "Epoch 123/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4661 - acc: 0.8212 - val_loss: 1.1887 - val_acc: 0.6514\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4839 - acc: 0.8116 - val_loss: 1.1903 - val_acc: 0.6514\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4614 - acc: 0.8289 - val_loss: 1.1951 - val_acc: 0.6571\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4625 - acc: 0.8282 - val_loss: 1.2006 - val_acc: 0.6400\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4431 - acc: 0.8365 - val_loss: 1.2155 - val_acc: 0.6514\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4668 - acc: 0.8269 - val_loss: 1.2199 - val_acc: 0.6400\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4863 - acc: 0.8276 - val_loss: 1.2242 - val_acc: 0.6457\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.4600 - acc: 0.8340 - val_loss: 1.2170 - val_acc: 0.6571\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4344 - acc: 0.8321 - val_loss: 1.2256 - val_acc: 0.6343\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4393 - acc: 0.8372 - val_loss: 1.2241 - val_acc: 0.6571\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4295 - acc: 0.8404 - val_loss: 1.2231 - val_acc: 0.6571\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4223 - acc: 0.8467 - val_loss: 1.2235 - val_acc: 0.6514\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4386 - acc: 0.8372 - val_loss: 1.2347 - val_acc: 0.6400\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4457 - acc: 0.8372 - val_loss: 1.2324 - val_acc: 0.6400\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4412 - acc: 0.8321 - val_loss: 1.2297 - val_acc: 0.6343\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4411 - acc: 0.8352 - val_loss: 1.2552 - val_acc: 0.6343\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4578 - acc: 0.8442 - val_loss: 1.2249 - val_acc: 0.6571\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4436 - acc: 0.8346 - val_loss: 1.2455 - val_acc: 0.6400\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4468 - acc: 0.8359 - val_loss: 1.2492 - val_acc: 0.6514\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4311 - acc: 0.8352 - val_loss: 1.2459 - val_acc: 0.6514\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4222 - acc: 0.8448 - val_loss: 1.2812 - val_acc: 0.6286\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.4168 - acc: 0.8525 - val_loss: 1.2545 - val_acc: 0.6571\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4435 - acc: 0.8314 - val_loss: 1.2338 - val_acc: 0.6743\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4265 - acc: 0.8365 - val_loss: 1.2328 - val_acc: 0.6629\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3952 - acc: 0.8544 - val_loss: 1.2654 - val_acc: 0.6343\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4286 - acc: 0.8397 - val_loss: 1.2558 - val_acc: 0.6571\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4062 - acc: 0.8493 - val_loss: 1.2703 - val_acc: 0.6457\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4245 - acc: 0.8429 - val_loss: 1.2851 - val_acc: 0.6571\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.4066 - acc: 0.8557 - val_loss: 1.2730 - val_acc: 0.6457\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.3973 - acc: 0.8557 - val_loss: 1.2674 - val_acc: 0.6457\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.4091 - acc: 0.8519 - val_loss: 1.2776 - val_acc: 0.6343\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 105us/step - loss: 0.4344 - acc: 0.8365 - val_loss: 1.2751 - val_acc: 0.6343\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4570 - acc: 0.8416 - val_loss: 1.2748 - val_acc: 0.6343\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4088 - acc: 0.8448 - val_loss: 1.2754 - val_acc: 0.6457\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4109 - acc: 0.8512 - val_loss: 1.2907 - val_acc: 0.6571\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4284 - acc: 0.8525 - val_loss: 1.3018 - val_acc: 0.6457\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.4400 - acc: 0.8455 - val_loss: 1.3061 - val_acc: 0.6629\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4128 - acc: 0.8538 - val_loss: 1.2929 - val_acc: 0.6571\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4170 - acc: 0.8525 - val_loss: 1.2731 - val_acc: 0.6571\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4180 - acc: 0.8563 - val_loss: 1.2773 - val_acc: 0.6686\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3859 - acc: 0.8659 - val_loss: 1.2937 - val_acc: 0.6457\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.3875 - acc: 0.8685 - val_loss: 1.2972 - val_acc: 0.6514\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3893 - acc: 0.8640 - val_loss: 1.3005 - val_acc: 0.6514\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3928 - acc: 0.8640 - val_loss: 1.2882 - val_acc: 0.6514\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3754 - acc: 0.8736 - val_loss: 1.2978 - val_acc: 0.6343\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.3951 - acc: 0.8627 - val_loss: 1.3020 - val_acc: 0.6457\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4007 - acc: 0.8544 - val_loss: 1.2889 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.3731 - acc: 0.8621 - val_loss: 1.2958 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4053 - acc: 0.8582 - val_loss: 1.3248 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.3811 - acc: 0.8633 - val_loss: 1.3142 - val_acc: 0.6514\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.3726 - acc: 0.8697 - val_loss: 1.3441 - val_acc: 0.6571\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4039 - acc: 0.8487 - val_loss: 1.3123 - val_acc: 0.6571\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.3668 - acc: 0.8678 - val_loss: 1.3281 - val_acc: 0.6571\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.3872 - acc: 0.8633 - val_loss: 1.3321 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.3800 - acc: 0.8538 - val_loss: 1.3458 - val_acc: 0.6514\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3884 - acc: 0.8633 - val_loss: 1.3418 - val_acc: 0.6514\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.3613 - acc: 0.8633 - val_loss: 1.3434 - val_acc: 0.6286\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.4127 - acc: 0.8589 - val_loss: 1.3521 - val_acc: 0.6514\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.3846 - acc: 0.8608 - val_loss: 1.3295 - val_acc: 0.6343\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3689 - acc: 0.8633 - val_loss: 1.3238 - val_acc: 0.6457\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.3816 - acc: 0.8576 - val_loss: 1.3176 - val_acc: 0.6629\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 0.3316 - acc: 0.8851 - val_loss: 1.3357 - val_acc: 0.6629\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.3765 - acc: 0.8640 - val_loss: 1.3570 - val_acc: 0.6343\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.3689 - acc: 0.8685 - val_loss: 1.3641 - val_acc: 0.6400\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.3824 - acc: 0.8582 - val_loss: 1.3722 - val_acc: 0.6457\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 112us/step - loss: 0.3730 - acc: 0.8640 - val_loss: 1.3512 - val_acc: 0.6686\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.3640 - acc: 0.8729 - val_loss: 1.3809 - val_acc: 0.6571\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3493 - acc: 0.8736 - val_loss: 1.3975 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.3690 - acc: 0.8736 - val_loss: 1.3980 - val_acc: 0.6514\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.3578 - acc: 0.8691 - val_loss: 1.3983 - val_acc: 0.6457\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.3644 - acc: 0.8768 - val_loss: 1.3783 - val_acc: 0.6629\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3742 - acc: 0.8544 - val_loss: 1.3845 - val_acc: 0.6571\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.3576 - acc: 0.8685 - val_loss: 1.4009 - val_acc: 0.6514\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3602 - acc: 0.8729 - val_loss: 1.4057 - val_acc: 0.6571\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3241 - acc: 0.8889 - val_loss: 1.4087 - val_acc: 0.6686\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.3721 - acc: 0.8761 - val_loss: 1.4281 - val_acc: 0.6343\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3644 - acc: 0.8799 - val_loss: 1.4067 - val_acc: 0.6343\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.3564 - acc: 0.8710 - val_loss: 1.3998 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 63us/step\n",
      "1741/1741 [==============================] - 0s 50us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.4999 - acc: 0.3161 - val_loss: 1.1527 - val_acc: 0.5086\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 1.2283 - acc: 0.4413 - val_loss: 1.0393 - val_acc: 0.6171\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 1.1267 - acc: 0.4962 - val_loss: 0.9951 - val_acc: 0.6571\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 1.0829 - acc: 0.5319 - val_loss: 0.9647 - val_acc: 0.6514\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 1.0504 - acc: 0.5466 - val_loss: 0.9408 - val_acc: 0.6686\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 1.0224 - acc: 0.5728 - val_loss: 0.9269 - val_acc: 0.6686\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9875 - acc: 0.5779 - val_loss: 0.9226 - val_acc: 0.6571\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9675 - acc: 0.5856 - val_loss: 0.9182 - val_acc: 0.6457\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9674 - acc: 0.5964 - val_loss: 0.9128 - val_acc: 0.6743\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9596 - acc: 0.5913 - val_loss: 0.9042 - val_acc: 0.6629\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.9302 - acc: 0.6194 - val_loss: 0.9016 - val_acc: 0.6514\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.9207 - acc: 0.6105 - val_loss: 0.9050 - val_acc: 0.6914\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9065 - acc: 0.6092 - val_loss: 0.9075 - val_acc: 0.6629\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8854 - acc: 0.6284 - val_loss: 0.8940 - val_acc: 0.6686\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8839 - acc: 0.6335 - val_loss: 0.8934 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8837 - acc: 0.6354 - val_loss: 0.8924 - val_acc: 0.6914\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8799 - acc: 0.6213 - val_loss: 0.8910 - val_acc: 0.6914\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8730 - acc: 0.6392 - val_loss: 0.8941 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8701 - acc: 0.6398 - val_loss: 0.8981 - val_acc: 0.6629\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.8502 - acc: 0.6552 - val_loss: 0.8969 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8368 - acc: 0.6590 - val_loss: 0.8990 - val_acc: 0.6629\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8399 - acc: 0.6488 - val_loss: 0.9000 - val_acc: 0.6629\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8386 - acc: 0.6616 - val_loss: 0.8942 - val_acc: 0.6457\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8464 - acc: 0.6520 - val_loss: 0.8979 - val_acc: 0.6457\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8155 - acc: 0.6718 - val_loss: 0.9049 - val_acc: 0.6457\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8167 - acc: 0.6648 - val_loss: 0.9120 - val_acc: 0.6457\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8061 - acc: 0.6654 - val_loss: 0.9043 - val_acc: 0.6457\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7993 - acc: 0.6711 - val_loss: 0.9040 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8074 - acc: 0.6603 - val_loss: 0.8993 - val_acc: 0.6743\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7961 - acc: 0.6922 - val_loss: 0.9052 - val_acc: 0.6629\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7882 - acc: 0.6699 - val_loss: 0.9130 - val_acc: 0.6514\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7801 - acc: 0.6820 - val_loss: 0.9076 - val_acc: 0.6571\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7765 - acc: 0.6756 - val_loss: 0.9090 - val_acc: 0.6343\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7520 - acc: 0.6845 - val_loss: 0.9122 - val_acc: 0.6457\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7589 - acc: 0.7050 - val_loss: 0.9165 - val_acc: 0.6400\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.7554 - acc: 0.6986 - val_loss: 0.9402 - val_acc: 0.6343\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7538 - acc: 0.6992 - val_loss: 0.9335 - val_acc: 0.6571\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7581 - acc: 0.6871 - val_loss: 0.9308 - val_acc: 0.6400\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7487 - acc: 0.6826 - val_loss: 0.9319 - val_acc: 0.6229\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7208 - acc: 0.7088 - val_loss: 0.9316 - val_acc: 0.6343\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7293 - acc: 0.7031 - val_loss: 0.9311 - val_acc: 0.6229\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7570 - acc: 0.6941 - val_loss: 0.9349 - val_acc: 0.6286\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7392 - acc: 0.6897 - val_loss: 0.9432 - val_acc: 0.6229\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7234 - acc: 0.7075 - val_loss: 0.9367 - val_acc: 0.6229\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7247 - acc: 0.7095 - val_loss: 0.9355 - val_acc: 0.6171\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7196 - acc: 0.7114 - val_loss: 0.9430 - val_acc: 0.6286\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6968 - acc: 0.7190 - val_loss: 0.9399 - val_acc: 0.6229\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7114 - acc: 0.7120 - val_loss: 0.9488 - val_acc: 0.6400\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 155us/step - loss: 0.7076 - acc: 0.7139 - val_loss: 0.9538 - val_acc: 0.6457\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 129us/step - loss: 0.6683 - acc: 0.7382 - val_loss: 0.9584 - val_acc: 0.6457\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 139us/step - loss: 0.7039 - acc: 0.7069 - val_loss: 0.9698 - val_acc: 0.6400\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.6888 - acc: 0.7305 - val_loss: 0.9730 - val_acc: 0.6229\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6812 - acc: 0.7229 - val_loss: 0.9621 - val_acc: 0.6457\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6895 - acc: 0.7203 - val_loss: 0.9625 - val_acc: 0.6686\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6831 - acc: 0.7222 - val_loss: 0.9646 - val_acc: 0.6400\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6792 - acc: 0.7209 - val_loss: 0.9742 - val_acc: 0.6229\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.6751 - acc: 0.7146 - val_loss: 0.9831 - val_acc: 0.6286\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6774 - acc: 0.7286 - val_loss: 0.9840 - val_acc: 0.6343\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6681 - acc: 0.7401 - val_loss: 0.9800 - val_acc: 0.6629\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6652 - acc: 0.7356 - val_loss: 0.9727 - val_acc: 0.6171\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6738 - acc: 0.7235 - val_loss: 0.9753 - val_acc: 0.6286\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6761 - acc: 0.7337 - val_loss: 0.9926 - val_acc: 0.6286\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6432 - acc: 0.7401 - val_loss: 0.9870 - val_acc: 0.6514\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6438 - acc: 0.7490 - val_loss: 0.9820 - val_acc: 0.6629\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6593 - acc: 0.7420 - val_loss: 0.9843 - val_acc: 0.6400\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6469 - acc: 0.7471 - val_loss: 0.9996 - val_acc: 0.6400\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.6432 - acc: 0.7363 - val_loss: 0.9922 - val_acc: 0.6514\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6391 - acc: 0.7439 - val_loss: 1.0044 - val_acc: 0.6514\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6262 - acc: 0.7593 - val_loss: 1.0036 - val_acc: 0.6514\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6385 - acc: 0.7458 - val_loss: 1.0064 - val_acc: 0.6457\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6153 - acc: 0.7490 - val_loss: 1.0044 - val_acc: 0.6400\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6154 - acc: 0.7414 - val_loss: 1.0055 - val_acc: 0.6400\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6081 - acc: 0.7586 - val_loss: 1.0162 - val_acc: 0.6286\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6045 - acc: 0.7644 - val_loss: 1.0131 - val_acc: 0.6514\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6019 - acc: 0.7529 - val_loss: 1.0313 - val_acc: 0.6400\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6061 - acc: 0.7618 - val_loss: 1.0267 - val_acc: 0.6457\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6069 - acc: 0.7676 - val_loss: 1.0199 - val_acc: 0.6571\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5797 - acc: 0.7752 - val_loss: 1.0295 - val_acc: 0.6457\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5967 - acc: 0.7656 - val_loss: 1.0332 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5815 - acc: 0.7650 - val_loss: 1.0411 - val_acc: 0.6514\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5894 - acc: 0.7701 - val_loss: 1.0496 - val_acc: 0.6571\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6020 - acc: 0.7490 - val_loss: 1.0586 - val_acc: 0.6514\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.6001 - acc: 0.7739 - val_loss: 1.0460 - val_acc: 0.6571\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5654 - acc: 0.7727 - val_loss: 1.0467 - val_acc: 0.6514\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5703 - acc: 0.7669 - val_loss: 1.0591 - val_acc: 0.6457\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5584 - acc: 0.7625 - val_loss: 1.0641 - val_acc: 0.6571\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.5885 - acc: 0.7663 - val_loss: 1.0723 - val_acc: 0.6343\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5816 - acc: 0.7714 - val_loss: 1.0694 - val_acc: 0.6343\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5731 - acc: 0.7810 - val_loss: 1.0710 - val_acc: 0.6457\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5569 - acc: 0.7854 - val_loss: 1.0644 - val_acc: 0.6457\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5432 - acc: 0.7880 - val_loss: 1.0641 - val_acc: 0.6400\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5744 - acc: 0.7797 - val_loss: 1.0584 - val_acc: 0.6629\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5387 - acc: 0.7989 - val_loss: 1.0661 - val_acc: 0.6629\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5596 - acc: 0.7816 - val_loss: 1.0693 - val_acc: 0.6400\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5156 - acc: 0.8008 - val_loss: 1.0653 - val_acc: 0.6400\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5468 - acc: 0.7835 - val_loss: 1.0749 - val_acc: 0.6343\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5427 - acc: 0.7803 - val_loss: 1.0902 - val_acc: 0.6571\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5329 - acc: 0.7976 - val_loss: 1.0822 - val_acc: 0.6457\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.5406 - acc: 0.7835 - val_loss: 1.0999 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5283 - acc: 0.7969 - val_loss: 1.0948 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5404 - acc: 0.7918 - val_loss: 1.0945 - val_acc: 0.6400\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5278 - acc: 0.8033 - val_loss: 1.1087 - val_acc: 0.6457\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.5376 - acc: 0.7842 - val_loss: 1.1076 - val_acc: 0.6571\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5307 - acc: 0.8001 - val_loss: 1.1014 - val_acc: 0.6571\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5013 - acc: 0.8059 - val_loss: 1.0977 - val_acc: 0.6571\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5413 - acc: 0.7925 - val_loss: 1.0996 - val_acc: 0.6457\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5199 - acc: 0.8065 - val_loss: 1.1174 - val_acc: 0.6286\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5253 - acc: 0.7963 - val_loss: 1.1123 - val_acc: 0.6514\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5051 - acc: 0.8078 - val_loss: 1.1175 - val_acc: 0.6400\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5041 - acc: 0.8014 - val_loss: 1.1234 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4964 - acc: 0.8186 - val_loss: 1.1137 - val_acc: 0.6343\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4946 - acc: 0.8167 - val_loss: 1.1125 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5130 - acc: 0.7957 - val_loss: 1.1192 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5155 - acc: 0.7957 - val_loss: 1.1237 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4868 - acc: 0.8040 - val_loss: 1.1240 - val_acc: 0.6571\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4806 - acc: 0.8244 - val_loss: 1.1319 - val_acc: 0.6457\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4956 - acc: 0.8142 - val_loss: 1.1537 - val_acc: 0.6286\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4791 - acc: 0.8186 - val_loss: 1.1455 - val_acc: 0.6343\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4799 - acc: 0.8084 - val_loss: 1.1439 - val_acc: 0.6457\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4770 - acc: 0.8238 - val_loss: 1.1549 - val_acc: 0.6114\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4867 - acc: 0.8135 - val_loss: 1.1533 - val_acc: 0.6343\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4939 - acc: 0.8218 - val_loss: 1.1595 - val_acc: 0.6286\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4707 - acc: 0.8308 - val_loss: 1.1585 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.4796 - acc: 0.8218 - val_loss: 1.1600 - val_acc: 0.6571\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4948 - acc: 0.8059 - val_loss: 1.1490 - val_acc: 0.6457\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4798 - acc: 0.8161 - val_loss: 1.1471 - val_acc: 0.6343\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4372 - acc: 0.8352 - val_loss: 1.1609 - val_acc: 0.6343\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4586 - acc: 0.8155 - val_loss: 1.1624 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.4548 - acc: 0.8282 - val_loss: 1.1950 - val_acc: 0.6229\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4693 - acc: 0.8244 - val_loss: 1.1950 - val_acc: 0.6229\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4727 - acc: 0.8123 - val_loss: 1.1714 - val_acc: 0.6400\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4638 - acc: 0.8276 - val_loss: 1.1869 - val_acc: 0.6400\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4662 - acc: 0.8110 - val_loss: 1.1925 - val_acc: 0.6343\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4886 - acc: 0.8186 - val_loss: 1.2003 - val_acc: 0.6343\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4828 - acc: 0.8078 - val_loss: 1.1819 - val_acc: 0.6343\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.4426 - acc: 0.8423 - val_loss: 1.1688 - val_acc: 0.6343\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4486 - acc: 0.8282 - val_loss: 1.2024 - val_acc: 0.6229\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4358 - acc: 0.8461 - val_loss: 1.1964 - val_acc: 0.6400\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4292 - acc: 0.8448 - val_loss: 1.2053 - val_acc: 0.6286\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4511 - acc: 0.8289 - val_loss: 1.2018 - val_acc: 0.6400\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4340 - acc: 0.8391 - val_loss: 1.2091 - val_acc: 0.6286\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4415 - acc: 0.8474 - val_loss: 1.2114 - val_acc: 0.6286\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4372 - acc: 0.8448 - val_loss: 1.2174 - val_acc: 0.6286\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4566 - acc: 0.8327 - val_loss: 1.2304 - val_acc: 0.6343\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4215 - acc: 0.8436 - val_loss: 1.2335 - val_acc: 0.6400\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4168 - acc: 0.8474 - val_loss: 1.2416 - val_acc: 0.6171\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4349 - acc: 0.8397 - val_loss: 1.2522 - val_acc: 0.6400\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4137 - acc: 0.8410 - val_loss: 1.2588 - val_acc: 0.6171\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4353 - acc: 0.8340 - val_loss: 1.2423 - val_acc: 0.6286\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4327 - acc: 0.8359 - val_loss: 1.2356 - val_acc: 0.6343\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.4421 - acc: 0.8295 - val_loss: 1.2576 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.4061 - acc: 0.8550 - val_loss: 1.2835 - val_acc: 0.6229\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4257 - acc: 0.8372 - val_loss: 1.2746 - val_acc: 0.6229\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4326 - acc: 0.8289 - val_loss: 1.2679 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4201 - acc: 0.8384 - val_loss: 1.2824 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.4063 - acc: 0.8493 - val_loss: 1.2530 - val_acc: 0.6171\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4183 - acc: 0.8531 - val_loss: 1.2547 - val_acc: 0.6286\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4009 - acc: 0.8544 - val_loss: 1.2583 - val_acc: 0.6343\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3771 - acc: 0.8595 - val_loss: 1.2734 - val_acc: 0.6286\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3903 - acc: 0.8461 - val_loss: 1.2782 - val_acc: 0.6286\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4047 - acc: 0.8576 - val_loss: 1.2817 - val_acc: 0.6400\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.4163 - acc: 0.8461 - val_loss: 1.2764 - val_acc: 0.6286\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3758 - acc: 0.8525 - val_loss: 1.2600 - val_acc: 0.6171\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3886 - acc: 0.8576 - val_loss: 1.2884 - val_acc: 0.6286\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4146 - acc: 0.8474 - val_loss: 1.2899 - val_acc: 0.6286\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.3972 - acc: 0.8602 - val_loss: 1.3004 - val_acc: 0.6343\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3896 - acc: 0.8525 - val_loss: 1.2993 - val_acc: 0.6229\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3883 - acc: 0.8531 - val_loss: 1.3024 - val_acc: 0.6229\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3791 - acc: 0.8704 - val_loss: 1.3075 - val_acc: 0.6229\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3869 - acc: 0.8506 - val_loss: 1.3004 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.3826 - acc: 0.8550 - val_loss: 1.3047 - val_acc: 0.6286\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3920 - acc: 0.8487 - val_loss: 1.2933 - val_acc: 0.6400\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.3946 - acc: 0.8531 - val_loss: 1.3119 - val_acc: 0.6286\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3884 - acc: 0.8480 - val_loss: 1.3090 - val_acc: 0.6229\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.4029 - acc: 0.8506 - val_loss: 1.3053 - val_acc: 0.6400\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3774 - acc: 0.8531 - val_loss: 1.3149 - val_acc: 0.6343\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3831 - acc: 0.8659 - val_loss: 1.3143 - val_acc: 0.6343\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3737 - acc: 0.8665 - val_loss: 1.3149 - val_acc: 0.6286\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3702 - acc: 0.8672 - val_loss: 1.3382 - val_acc: 0.6457\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3787 - acc: 0.8557 - val_loss: 1.3330 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.3755 - acc: 0.8665 - val_loss: 1.3535 - val_acc: 0.6286\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.3946 - acc: 0.8544 - val_loss: 1.3267 - val_acc: 0.6343\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.3998 - acc: 0.8550 - val_loss: 1.3175 - val_acc: 0.6343\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3786 - acc: 0.8519 - val_loss: 1.3332 - val_acc: 0.6286\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.3768 - acc: 0.8570 - val_loss: 1.3493 - val_acc: 0.6286\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.3751 - acc: 0.8678 - val_loss: 1.3848 - val_acc: 0.6114\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3661 - acc: 0.8627 - val_loss: 1.3650 - val_acc: 0.6229\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.3585 - acc: 0.8716 - val_loss: 1.3646 - val_acc: 0.6229\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.3726 - acc: 0.8685 - val_loss: 1.3674 - val_acc: 0.6229\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.3513 - acc: 0.8723 - val_loss: 1.3571 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.3720 - acc: 0.8716 - val_loss: 1.3591 - val_acc: 0.6343\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3739 - acc: 0.8755 - val_loss: 1.3701 - val_acc: 0.6457\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3815 - acc: 0.8691 - val_loss: 1.3677 - val_acc: 0.6229\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.3582 - acc: 0.8825 - val_loss: 1.3770 - val_acc: 0.6286\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.3717 - acc: 0.8678 - val_loss: 1.3849 - val_acc: 0.6229\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3565 - acc: 0.8736 - val_loss: 1.3867 - val_acc: 0.5943\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3504 - acc: 0.8755 - val_loss: 1.3992 - val_acc: 0.6400\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3700 - acc: 0.8723 - val_loss: 1.4016 - val_acc: 0.6286\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3493 - acc: 0.8665 - val_loss: 1.3926 - val_acc: 0.6343\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3396 - acc: 0.8736 - val_loss: 1.4017 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 82us/step\n",
      "1741/1741 [==============================] - 0s 55us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.4614 - acc: 0.3155 - val_loss: 1.1038 - val_acc: 0.5829\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.2208 - acc: 0.4566 - val_loss: 1.0117 - val_acc: 0.6343\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 1.1382 - acc: 0.5013 - val_loss: 0.9769 - val_acc: 0.6286\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 1.0900 - acc: 0.5255 - val_loss: 0.9565 - val_acc: 0.6400\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 1.0511 - acc: 0.5517 - val_loss: 0.9316 - val_acc: 0.6629\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 1.0394 - acc: 0.5607 - val_loss: 0.9264 - val_acc: 0.6686\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 1.0051 - acc: 0.5734 - val_loss: 0.9294 - val_acc: 0.6686\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.9699 - acc: 0.5754 - val_loss: 0.9142 - val_acc: 0.6686\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.9739 - acc: 0.5683 - val_loss: 0.9102 - val_acc: 0.6571\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9604 - acc: 0.5856 - val_loss: 0.9065 - val_acc: 0.6914\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9505 - acc: 0.6003 - val_loss: 0.9238 - val_acc: 0.6686\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.9318 - acc: 0.6149 - val_loss: 0.9102 - val_acc: 0.6971\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9298 - acc: 0.6188 - val_loss: 0.9035 - val_acc: 0.6857\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.9128 - acc: 0.6130 - val_loss: 0.9022 - val_acc: 0.6800\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.9125 - acc: 0.6264 - val_loss: 0.8992 - val_acc: 0.7029\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8930 - acc: 0.6258 - val_loss: 0.9004 - val_acc: 0.7029\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.8910 - acc: 0.6335 - val_loss: 0.9034 - val_acc: 0.7029\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8963 - acc: 0.6296 - val_loss: 0.9087 - val_acc: 0.6857\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8637 - acc: 0.6347 - val_loss: 0.9104 - val_acc: 0.6629\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8615 - acc: 0.6398 - val_loss: 0.9095 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8614 - acc: 0.6456 - val_loss: 0.9105 - val_acc: 0.6800\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8703 - acc: 0.6309 - val_loss: 0.9156 - val_acc: 0.6743\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.8446 - acc: 0.6526 - val_loss: 0.9128 - val_acc: 0.6857\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8554 - acc: 0.6475 - val_loss: 0.9101 - val_acc: 0.6914\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8373 - acc: 0.6660 - val_loss: 0.9272 - val_acc: 0.6571\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.8150 - acc: 0.6673 - val_loss: 0.9185 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8202 - acc: 0.6692 - val_loss: 0.9225 - val_acc: 0.6629\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8267 - acc: 0.6667 - val_loss: 0.9343 - val_acc: 0.6514\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8091 - acc: 0.6507 - val_loss: 0.9178 - val_acc: 0.6457\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8154 - acc: 0.6839 - val_loss: 0.9180 - val_acc: 0.6743\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7937 - acc: 0.6788 - val_loss: 0.9344 - val_acc: 0.6686\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7958 - acc: 0.6724 - val_loss: 0.9369 - val_acc: 0.6457\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7713 - acc: 0.6865 - val_loss: 0.9357 - val_acc: 0.6857\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7853 - acc: 0.6865 - val_loss: 0.9322 - val_acc: 0.6629\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7900 - acc: 0.6801 - val_loss: 0.9439 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7834 - acc: 0.6724 - val_loss: 0.9389 - val_acc: 0.6800\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7567 - acc: 0.6903 - val_loss: 0.9501 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7666 - acc: 0.6884 - val_loss: 0.9495 - val_acc: 0.6743\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7862 - acc: 0.6782 - val_loss: 0.9496 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7661 - acc: 0.6833 - val_loss: 0.9496 - val_acc: 0.6514\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7556 - acc: 0.6948 - val_loss: 0.9628 - val_acc: 0.6571\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7455 - acc: 0.7037 - val_loss: 0.9614 - val_acc: 0.6743\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7499 - acc: 0.6916 - val_loss: 0.9682 - val_acc: 0.6629\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7427 - acc: 0.7075 - val_loss: 0.9691 - val_acc: 0.6629\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7325 - acc: 0.7011 - val_loss: 0.9685 - val_acc: 0.6571\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7528 - acc: 0.6903 - val_loss: 0.9750 - val_acc: 0.6743\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7246 - acc: 0.7139 - val_loss: 0.9728 - val_acc: 0.6457\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7340 - acc: 0.7095 - val_loss: 0.9775 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7219 - acc: 0.7267 - val_loss: 0.9777 - val_acc: 0.6743\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7249 - acc: 0.7018 - val_loss: 0.9855 - val_acc: 0.6286\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7118 - acc: 0.7146 - val_loss: 0.9847 - val_acc: 0.6457\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6986 - acc: 0.7203 - val_loss: 0.9776 - val_acc: 0.6343\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7001 - acc: 0.7216 - val_loss: 0.9877 - val_acc: 0.6286\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6858 - acc: 0.7344 - val_loss: 0.9880 - val_acc: 0.6457\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6896 - acc: 0.7178 - val_loss: 1.0005 - val_acc: 0.6514\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6880 - acc: 0.7254 - val_loss: 0.9955 - val_acc: 0.6571\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7065 - acc: 0.7216 - val_loss: 1.0026 - val_acc: 0.6343\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6835 - acc: 0.7318 - val_loss: 1.0091 - val_acc: 0.6457\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6706 - acc: 0.7369 - val_loss: 1.0063 - val_acc: 0.6457\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6966 - acc: 0.7318 - val_loss: 1.0069 - val_acc: 0.6686\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6553 - acc: 0.7318 - val_loss: 1.0119 - val_acc: 0.6686\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6836 - acc: 0.7305 - val_loss: 1.0196 - val_acc: 0.6800\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6627 - acc: 0.7312 - val_loss: 1.0252 - val_acc: 0.6286\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6374 - acc: 0.7446 - val_loss: 1.0190 - val_acc: 0.6400\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6596 - acc: 0.7388 - val_loss: 1.0104 - val_acc: 0.6571\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6516 - acc: 0.7439 - val_loss: 1.0189 - val_acc: 0.6457\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6546 - acc: 0.7350 - val_loss: 1.0134 - val_acc: 0.6514\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6472 - acc: 0.7292 - val_loss: 1.0241 - val_acc: 0.6800\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.6345 - acc: 0.7497 - val_loss: 1.0358 - val_acc: 0.6571\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6499 - acc: 0.7414 - val_loss: 1.0246 - val_acc: 0.6857\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.6444 - acc: 0.7401 - val_loss: 1.0315 - val_acc: 0.6743\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6138 - acc: 0.7701 - val_loss: 1.0298 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6278 - acc: 0.7458 - val_loss: 1.0380 - val_acc: 0.6857\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.6295 - acc: 0.7656 - val_loss: 1.0467 - val_acc: 0.6514\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6177 - acc: 0.7503 - val_loss: 1.0519 - val_acc: 0.6514\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6224 - acc: 0.7471 - val_loss: 1.0403 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6074 - acc: 0.7497 - val_loss: 1.0362 - val_acc: 0.6743\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.6337 - acc: 0.7510 - val_loss: 1.0503 - val_acc: 0.6686\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6008 - acc: 0.7797 - val_loss: 1.0548 - val_acc: 0.6343\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6188 - acc: 0.7580 - val_loss: 1.0569 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5908 - acc: 0.7682 - val_loss: 1.0518 - val_acc: 0.6743\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6052 - acc: 0.7650 - val_loss: 1.0557 - val_acc: 0.6571\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5948 - acc: 0.7593 - val_loss: 1.0576 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5974 - acc: 0.7695 - val_loss: 1.0547 - val_acc: 0.6800\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6114 - acc: 0.7676 - val_loss: 1.0648 - val_acc: 0.6457\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5761 - acc: 0.7739 - val_loss: 1.0653 - val_acc: 0.6457\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5820 - acc: 0.7752 - val_loss: 1.0596 - val_acc: 0.6457\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6155 - acc: 0.7567 - val_loss: 1.0720 - val_acc: 0.6571\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5930 - acc: 0.7599 - val_loss: 1.0657 - val_acc: 0.6686\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5765 - acc: 0.7822 - val_loss: 1.0751 - val_acc: 0.6343\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.5651 - acc: 0.7874 - val_loss: 1.0917 - val_acc: 0.6343\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5801 - acc: 0.7618 - val_loss: 1.0812 - val_acc: 0.6514\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5653 - acc: 0.7746 - val_loss: 1.0830 - val_acc: 0.6514\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5862 - acc: 0.7682 - val_loss: 1.0904 - val_acc: 0.6514\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5674 - acc: 0.7867 - val_loss: 1.1045 - val_acc: 0.6686\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5773 - acc: 0.7733 - val_loss: 1.0985 - val_acc: 0.6343\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5490 - acc: 0.7976 - val_loss: 1.0929 - val_acc: 0.6629\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5390 - acc: 0.7886 - val_loss: 1.0987 - val_acc: 0.6343\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.5567 - acc: 0.7765 - val_loss: 1.1081 - val_acc: 0.6286\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5254 - acc: 0.8046 - val_loss: 1.1133 - val_acc: 0.6286\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5458 - acc: 0.7931 - val_loss: 1.1178 - val_acc: 0.6057\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5619 - acc: 0.7714 - val_loss: 1.1283 - val_acc: 0.6286\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5438 - acc: 0.7950 - val_loss: 1.1258 - val_acc: 0.6286\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5320 - acc: 0.7950 - val_loss: 1.1177 - val_acc: 0.6457\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5235 - acc: 0.7893 - val_loss: 1.1307 - val_acc: 0.6343\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5337 - acc: 0.7957 - val_loss: 1.1439 - val_acc: 0.6457\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5212 - acc: 0.8097 - val_loss: 1.1278 - val_acc: 0.6229\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5247 - acc: 0.8020 - val_loss: 1.1421 - val_acc: 0.6286\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5480 - acc: 0.7931 - val_loss: 1.1268 - val_acc: 0.6571\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5004 - acc: 0.8110 - val_loss: 1.1486 - val_acc: 0.6457\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5018 - acc: 0.8065 - val_loss: 1.1447 - val_acc: 0.6343\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5082 - acc: 0.7957 - val_loss: 1.1567 - val_acc: 0.6400\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5451 - acc: 0.7918 - val_loss: 1.1640 - val_acc: 0.6343\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5095 - acc: 0.7969 - val_loss: 1.1690 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4828 - acc: 0.8129 - val_loss: 1.1560 - val_acc: 0.6343\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5327 - acc: 0.7886 - val_loss: 1.1579 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4833 - acc: 0.8167 - val_loss: 1.1754 - val_acc: 0.6171\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5107 - acc: 0.8014 - val_loss: 1.1814 - val_acc: 0.6343\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5191 - acc: 0.8052 - val_loss: 1.1853 - val_acc: 0.6171\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4956 - acc: 0.8072 - val_loss: 1.1956 - val_acc: 0.6057\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4937 - acc: 0.8161 - val_loss: 1.2063 - val_acc: 0.6171\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4886 - acc: 0.8065 - val_loss: 1.1858 - val_acc: 0.6286\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4937 - acc: 0.8110 - val_loss: 1.1944 - val_acc: 0.6286\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4877 - acc: 0.8065 - val_loss: 1.2050 - val_acc: 0.6343\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4907 - acc: 0.8161 - val_loss: 1.2138 - val_acc: 0.6229\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4936 - acc: 0.8186 - val_loss: 1.2122 - val_acc: 0.6229\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4520 - acc: 0.8301 - val_loss: 1.2236 - val_acc: 0.6171\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4901 - acc: 0.8155 - val_loss: 1.2127 - val_acc: 0.6171\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4703 - acc: 0.8295 - val_loss: 1.2195 - val_acc: 0.6343\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4766 - acc: 0.8263 - val_loss: 1.2194 - val_acc: 0.6343\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4662 - acc: 0.8231 - val_loss: 1.2200 - val_acc: 0.6343\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4577 - acc: 0.8244 - val_loss: 1.2378 - val_acc: 0.6286\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.4742 - acc: 0.8263 - val_loss: 1.2588 - val_acc: 0.6171\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4612 - acc: 0.8180 - val_loss: 1.2554 - val_acc: 0.6171\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4643 - acc: 0.8193 - val_loss: 1.2480 - val_acc: 0.6286\n",
      "Epoch 136/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4750 - acc: 0.8167 - val_loss: 1.2474 - val_acc: 0.6171\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4673 - acc: 0.8123 - val_loss: 1.2594 - val_acc: 0.6171\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4705 - acc: 0.8186 - val_loss: 1.2580 - val_acc: 0.6057\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4724 - acc: 0.8103 - val_loss: 1.2659 - val_acc: 0.6114\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4585 - acc: 0.8282 - val_loss: 1.2672 - val_acc: 0.6114\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.4565 - acc: 0.8193 - val_loss: 1.2571 - val_acc: 0.6114\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4379 - acc: 0.8333 - val_loss: 1.2724 - val_acc: 0.5943\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4380 - acc: 0.8442 - val_loss: 1.2914 - val_acc: 0.5943\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4666 - acc: 0.8321 - val_loss: 1.2879 - val_acc: 0.6057\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4683 - acc: 0.8218 - val_loss: 1.3071 - val_acc: 0.6286\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4367 - acc: 0.8314 - val_loss: 1.3078 - val_acc: 0.6114\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4215 - acc: 0.8404 - val_loss: 1.3036 - val_acc: 0.6171\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4344 - acc: 0.8461 - val_loss: 1.3027 - val_acc: 0.6229\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.4385 - acc: 0.8384 - val_loss: 1.3025 - val_acc: 0.5943\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4178 - acc: 0.8512 - val_loss: 1.2978 - val_acc: 0.5943\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4498 - acc: 0.8359 - val_loss: 1.2987 - val_acc: 0.6171\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4626 - acc: 0.8276 - val_loss: 1.3015 - val_acc: 0.6057\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4826 - acc: 0.8257 - val_loss: 1.3142 - val_acc: 0.6057\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4495 - acc: 0.8238 - val_loss: 1.2965 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4102 - acc: 0.8487 - val_loss: 1.2976 - val_acc: 0.6229\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4167 - acc: 0.8461 - val_loss: 1.3004 - val_acc: 0.6171\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.4358 - acc: 0.8397 - val_loss: 1.3146 - val_acc: 0.6171\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4246 - acc: 0.8416 - val_loss: 1.3342 - val_acc: 0.5943\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4174 - acc: 0.8436 - val_loss: 1.3547 - val_acc: 0.5886\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.4073 - acc: 0.8499 - val_loss: 1.3655 - val_acc: 0.6000\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4274 - acc: 0.8461 - val_loss: 1.3505 - val_acc: 0.6171\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4137 - acc: 0.8442 - val_loss: 1.3473 - val_acc: 0.5943\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4093 - acc: 0.8480 - val_loss: 1.3609 - val_acc: 0.6171\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4099 - acc: 0.8493 - val_loss: 1.3431 - val_acc: 0.5943\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4091 - acc: 0.8550 - val_loss: 1.3681 - val_acc: 0.6171\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.4093 - acc: 0.8448 - val_loss: 1.3689 - val_acc: 0.6171\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4173 - acc: 0.8506 - val_loss: 1.3638 - val_acc: 0.6000\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4198 - acc: 0.8378 - val_loss: 1.3634 - val_acc: 0.6171\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4149 - acc: 0.8372 - val_loss: 1.3567 - val_acc: 0.6171\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3970 - acc: 0.8487 - val_loss: 1.3538 - val_acc: 0.6229\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3603 - acc: 0.8748 - val_loss: 1.3548 - val_acc: 0.6343\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.3975 - acc: 0.8487 - val_loss: 1.3623 - val_acc: 0.6114\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4166 - acc: 0.8397 - val_loss: 1.3536 - val_acc: 0.6229\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3919 - acc: 0.8436 - val_loss: 1.3549 - val_acc: 0.6171\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3862 - acc: 0.8480 - val_loss: 1.3633 - val_acc: 0.6171\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4018 - acc: 0.8480 - val_loss: 1.3865 - val_acc: 0.6114\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4261 - acc: 0.8467 - val_loss: 1.3621 - val_acc: 0.6114\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3983 - acc: 0.8608 - val_loss: 1.3693 - val_acc: 0.6114\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3719 - acc: 0.8646 - val_loss: 1.3796 - val_acc: 0.6229\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.3905 - acc: 0.8672 - val_loss: 1.3784 - val_acc: 0.6343\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3842 - acc: 0.8633 - val_loss: 1.3786 - val_acc: 0.6171\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4107 - acc: 0.8480 - val_loss: 1.3836 - val_acc: 0.6114\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3658 - acc: 0.8653 - val_loss: 1.3865 - val_acc: 0.6229\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3846 - acc: 0.8557 - val_loss: 1.3983 - val_acc: 0.6057\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.3952 - acc: 0.8614 - val_loss: 1.4069 - val_acc: 0.6114\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4017 - acc: 0.8582 - val_loss: 1.3895 - val_acc: 0.5943\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3955 - acc: 0.8570 - val_loss: 1.3967 - val_acc: 0.6114\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.3864 - acc: 0.8557 - val_loss: 1.3878 - val_acc: 0.6114\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3654 - acc: 0.8710 - val_loss: 1.4027 - val_acc: 0.6229\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.3820 - acc: 0.8576 - val_loss: 1.4060 - val_acc: 0.6343\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.3668 - acc: 0.8627 - val_loss: 1.4015 - val_acc: 0.6171\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.3916 - acc: 0.8550 - val_loss: 1.4007 - val_acc: 0.6114\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.3647 - acc: 0.8659 - val_loss: 1.4076 - val_acc: 0.6229\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3829 - acc: 0.8544 - val_loss: 1.4026 - val_acc: 0.6057\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3661 - acc: 0.8672 - val_loss: 1.4251 - val_acc: 0.5829\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3823 - acc: 0.8487 - val_loss: 1.4355 - val_acc: 0.6057\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.3767 - acc: 0.8653 - val_loss: 1.4275 - val_acc: 0.6057\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.3582 - acc: 0.8742 - val_loss: 1.4095 - val_acc: 0.6114\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.3642 - acc: 0.8691 - val_loss: 1.4236 - val_acc: 0.6057\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3620 - acc: 0.8710 - val_loss: 1.4087 - val_acc: 0.5943\n",
      "194/194 [==============================] - 0s 72us/step\n",
      "1741/1741 [==============================] - 0s 52us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 6s 4ms/step - loss: 1.4287 - acc: 0.3378 - val_loss: 1.1030 - val_acc: 0.5771\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 1.2166 - acc: 0.4674 - val_loss: 0.9970 - val_acc: 0.6400\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 1.1257 - acc: 0.5026 - val_loss: 0.9551 - val_acc: 0.6457\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 1.0892 - acc: 0.5128 - val_loss: 0.9372 - val_acc: 0.6457\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 1.0709 - acc: 0.5332 - val_loss: 0.9122 - val_acc: 0.6629\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 1.0364 - acc: 0.5556 - val_loss: 0.9075 - val_acc: 0.6686\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.0194 - acc: 0.5607 - val_loss: 0.8992 - val_acc: 0.6629\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9670 - acc: 0.6022 - val_loss: 0.8914 - val_acc: 0.6743\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.9641 - acc: 0.5945 - val_loss: 0.8962 - val_acc: 0.6743\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 109us/step - loss: 0.9489 - acc: 0.5913 - val_loss: 0.8982 - val_acc: 0.6857\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 113us/step - loss: 0.9478 - acc: 0.5983 - val_loss: 0.8930 - val_acc: 0.6743\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9296 - acc: 0.6169 - val_loss: 0.8869 - val_acc: 0.6800\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9275 - acc: 0.6092 - val_loss: 0.8842 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 128us/step - loss: 0.9111 - acc: 0.6105 - val_loss: 0.8752 - val_acc: 0.6686\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.9147 - acc: 0.6207 - val_loss: 0.8800 - val_acc: 0.6686\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.8739 - acc: 0.6322 - val_loss: 0.8778 - val_acc: 0.6800\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.8972 - acc: 0.6194 - val_loss: 0.8766 - val_acc: 0.6857\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.8699 - acc: 0.6418 - val_loss: 0.8678 - val_acc: 0.6800\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.8592 - acc: 0.6418 - val_loss: 0.8809 - val_acc: 0.6857\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8687 - acc: 0.6347 - val_loss: 0.8729 - val_acc: 0.6914\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.8744 - acc: 0.6398 - val_loss: 0.8711 - val_acc: 0.7029\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8629 - acc: 0.6456 - val_loss: 0.8808 - val_acc: 0.6743\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.8399 - acc: 0.6545 - val_loss: 0.8795 - val_acc: 0.6971\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8415 - acc: 0.6469 - val_loss: 0.8783 - val_acc: 0.6914\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.8425 - acc: 0.6494 - val_loss: 0.8860 - val_acc: 0.6629\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.8153 - acc: 0.6628 - val_loss: 0.8865 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.8128 - acc: 0.6686 - val_loss: 0.8871 - val_acc: 0.6971\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8156 - acc: 0.6635 - val_loss: 0.8896 - val_acc: 0.6800\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7946 - acc: 0.6724 - val_loss: 0.8860 - val_acc: 0.6914\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7958 - acc: 0.6667 - val_loss: 0.8888 - val_acc: 0.6914\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8062 - acc: 0.6788 - val_loss: 0.8885 - val_acc: 0.6800\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.8065 - acc: 0.6711 - val_loss: 0.9022 - val_acc: 0.6514\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.7655 - acc: 0.6992 - val_loss: 0.8937 - val_acc: 0.6686\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8048 - acc: 0.6564 - val_loss: 0.8893 - val_acc: 0.6857\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7784 - acc: 0.6692 - val_loss: 0.8959 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7689 - acc: 0.6750 - val_loss: 0.9029 - val_acc: 0.6743\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7580 - acc: 0.6833 - val_loss: 0.9068 - val_acc: 0.6686\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.7762 - acc: 0.6762 - val_loss: 0.8998 - val_acc: 0.6857\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7604 - acc: 0.6865 - val_loss: 0.9083 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7616 - acc: 0.6775 - val_loss: 0.9057 - val_acc: 0.6571\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7245 - acc: 0.7011 - val_loss: 0.9084 - val_acc: 0.6571\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7636 - acc: 0.6967 - val_loss: 0.9003 - val_acc: 0.6571\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.7450 - acc: 0.6948 - val_loss: 0.9027 - val_acc: 0.6571\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.7294 - acc: 0.7018 - val_loss: 0.9066 - val_acc: 0.6743\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.7222 - acc: 0.7050 - val_loss: 0.9092 - val_acc: 0.6800\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 115us/step - loss: 0.7219 - acc: 0.7152 - val_loss: 0.9246 - val_acc: 0.6514\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.7345 - acc: 0.6954 - val_loss: 0.9259 - val_acc: 0.6686\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.7195 - acc: 0.7088 - val_loss: 0.9115 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7117 - acc: 0.7063 - val_loss: 0.9183 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.7015 - acc: 0.7178 - val_loss: 0.9333 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.6901 - acc: 0.7190 - val_loss: 0.9375 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6995 - acc: 0.7222 - val_loss: 0.9286 - val_acc: 0.6514\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7028 - acc: 0.7088 - val_loss: 0.9315 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6982 - acc: 0.7280 - val_loss: 0.9391 - val_acc: 0.6686\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6941 - acc: 0.7248 - val_loss: 0.9405 - val_acc: 0.6743\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7087 - acc: 0.7190 - val_loss: 0.9461 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6901 - acc: 0.7299 - val_loss: 0.9450 - val_acc: 0.6686\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6723 - acc: 0.7280 - val_loss: 0.9424 - val_acc: 0.6743\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6703 - acc: 0.7280 - val_loss: 0.9378 - val_acc: 0.6571\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6848 - acc: 0.7146 - val_loss: 0.9419 - val_acc: 0.6514\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6618 - acc: 0.7427 - val_loss: 0.9454 - val_acc: 0.6571\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6554 - acc: 0.7331 - val_loss: 0.9613 - val_acc: 0.6457\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.6525 - acc: 0.7337 - val_loss: 0.9615 - val_acc: 0.6514\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.6588 - acc: 0.7420 - val_loss: 0.9642 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6564 - acc: 0.7273 - val_loss: 0.9633 - val_acc: 0.6686\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6374 - acc: 0.7471 - val_loss: 0.9721 - val_acc: 0.6629\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6436 - acc: 0.7331 - val_loss: 0.9597 - val_acc: 0.6686\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6507 - acc: 0.7280 - val_loss: 0.9629 - val_acc: 0.6571\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6417 - acc: 0.7478 - val_loss: 0.9561 - val_acc: 0.6800\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6234 - acc: 0.7478 - val_loss: 0.9600 - val_acc: 0.6686\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6139 - acc: 0.7625 - val_loss: 0.9592 - val_acc: 0.6743\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6292 - acc: 0.7497 - val_loss: 0.9522 - val_acc: 0.6743\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6325 - acc: 0.7490 - val_loss: 0.9668 - val_acc: 0.6629\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6419 - acc: 0.7478 - val_loss: 0.9728 - val_acc: 0.6686\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6178 - acc: 0.7433 - val_loss: 0.9800 - val_acc: 0.6686\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6202 - acc: 0.7599 - val_loss: 0.9814 - val_acc: 0.6629\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.6180 - acc: 0.7535 - val_loss: 0.9810 - val_acc: 0.6571\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5972 - acc: 0.7644 - val_loss: 0.9815 - val_acc: 0.6629\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.6033 - acc: 0.7656 - val_loss: 0.9836 - val_acc: 0.6686\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6192 - acc: 0.7599 - val_loss: 0.9817 - val_acc: 0.6686\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5840 - acc: 0.7676 - val_loss: 0.9830 - val_acc: 0.6686\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5828 - acc: 0.7650 - val_loss: 0.9871 - val_acc: 0.6514\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5945 - acc: 0.7733 - val_loss: 0.9953 - val_acc: 0.6629\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5962 - acc: 0.7656 - val_loss: 0.9942 - val_acc: 0.6686\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5739 - acc: 0.7733 - val_loss: 0.9880 - val_acc: 0.6571\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5750 - acc: 0.7682 - val_loss: 0.9960 - val_acc: 0.6571\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.5809 - acc: 0.7548 - val_loss: 0.9941 - val_acc: 0.6857\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5668 - acc: 0.7822 - val_loss: 0.9970 - val_acc: 0.6743\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5663 - acc: 0.7714 - val_loss: 1.0092 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5890 - acc: 0.7663 - val_loss: 1.0158 - val_acc: 0.6686\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5508 - acc: 0.7727 - val_loss: 1.0096 - val_acc: 0.6629\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.5467 - acc: 0.7771 - val_loss: 1.0198 - val_acc: 0.6514\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5536 - acc: 0.7835 - val_loss: 1.0291 - val_acc: 0.6571\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.5677 - acc: 0.7708 - val_loss: 1.0195 - val_acc: 0.6686\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.5346 - acc: 0.7905 - val_loss: 1.0299 - val_acc: 0.6743\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.5349 - acc: 0.7905 - val_loss: 1.0334 - val_acc: 0.6629\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5375 - acc: 0.7989 - val_loss: 1.0320 - val_acc: 0.6686\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.5489 - acc: 0.7791 - val_loss: 1.0333 - val_acc: 0.6514\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5512 - acc: 0.7867 - val_loss: 1.0363 - val_acc: 0.6629\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5205 - acc: 0.7976 - val_loss: 1.0464 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5098 - acc: 0.8040 - val_loss: 1.0374 - val_acc: 0.6514\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.5162 - acc: 0.8014 - val_loss: 1.0447 - val_acc: 0.6571\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.5384 - acc: 0.7937 - val_loss: 1.0601 - val_acc: 0.6571\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5318 - acc: 0.7957 - val_loss: 1.0582 - val_acc: 0.6514\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5065 - acc: 0.7982 - val_loss: 1.0516 - val_acc: 0.6686\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5011 - acc: 0.8091 - val_loss: 1.0787 - val_acc: 0.6571\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5264 - acc: 0.7976 - val_loss: 1.0802 - val_acc: 0.6571\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.5300 - acc: 0.7893 - val_loss: 1.0991 - val_acc: 0.6571\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.5242 - acc: 0.7937 - val_loss: 1.0945 - val_acc: 0.6457\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5118 - acc: 0.8027 - val_loss: 1.0912 - val_acc: 0.6686\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.5083 - acc: 0.7989 - val_loss: 1.0765 - val_acc: 0.6629\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.5024 - acc: 0.8020 - val_loss: 1.0670 - val_acc: 0.6686\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4929 - acc: 0.8091 - val_loss: 1.0748 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4946 - acc: 0.8116 - val_loss: 1.0926 - val_acc: 0.6343\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4894 - acc: 0.8084 - val_loss: 1.1113 - val_acc: 0.6400\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4847 - acc: 0.8110 - val_loss: 1.1143 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5170 - acc: 0.8084 - val_loss: 1.1012 - val_acc: 0.6571\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4877 - acc: 0.8052 - val_loss: 1.0955 - val_acc: 0.6629\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4903 - acc: 0.8135 - val_loss: 1.1139 - val_acc: 0.6571\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4964 - acc: 0.8186 - val_loss: 1.1216 - val_acc: 0.6457\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5030 - acc: 0.8033 - val_loss: 1.1102 - val_acc: 0.6571\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4896 - acc: 0.8103 - val_loss: 1.1170 - val_acc: 0.6400\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4717 - acc: 0.8155 - val_loss: 1.1323 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4922 - acc: 0.8193 - val_loss: 1.1221 - val_acc: 0.6457\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.4949 - acc: 0.8072 - val_loss: 1.1243 - val_acc: 0.6571\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4668 - acc: 0.8142 - val_loss: 1.1259 - val_acc: 0.6514\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4543 - acc: 0.8167 - val_loss: 1.1238 - val_acc: 0.6571\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4710 - acc: 0.8155 - val_loss: 1.1299 - val_acc: 0.6629\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4642 - acc: 0.8167 - val_loss: 1.1389 - val_acc: 0.6629\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4564 - acc: 0.8116 - val_loss: 1.1281 - val_acc: 0.6629\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4838 - acc: 0.8142 - val_loss: 1.1349 - val_acc: 0.6457\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.4806 - acc: 0.8103 - val_loss: 1.1382 - val_acc: 0.6514\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 104us/step - loss: 0.4300 - acc: 0.8397 - val_loss: 1.1412 - val_acc: 0.6686\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4419 - acc: 0.8238 - val_loss: 1.1441 - val_acc: 0.6571\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4407 - acc: 0.8282 - val_loss: 1.1655 - val_acc: 0.6400\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4467 - acc: 0.8269 - val_loss: 1.1554 - val_acc: 0.6400\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4508 - acc: 0.8206 - val_loss: 1.1600 - val_acc: 0.6514\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.4469 - acc: 0.8352 - val_loss: 1.1598 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4575 - acc: 0.8340 - val_loss: 1.1688 - val_acc: 0.6400\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4514 - acc: 0.8314 - val_loss: 1.1757 - val_acc: 0.6629\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4415 - acc: 0.8321 - val_loss: 1.1760 - val_acc: 0.6629\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4308 - acc: 0.8257 - val_loss: 1.1760 - val_acc: 0.6571\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4415 - acc: 0.8340 - val_loss: 1.1890 - val_acc: 0.6343\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.4529 - acc: 0.8263 - val_loss: 1.1889 - val_acc: 0.6457\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.4343 - acc: 0.8301 - val_loss: 1.2014 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.4437 - acc: 0.8250 - val_loss: 1.2144 - val_acc: 0.6286\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4478 - acc: 0.8391 - val_loss: 1.2089 - val_acc: 0.6286\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4272 - acc: 0.8269 - val_loss: 1.2095 - val_acc: 0.6457\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.4492 - acc: 0.8295 - val_loss: 1.2125 - val_acc: 0.6514\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4253 - acc: 0.8365 - val_loss: 1.2144 - val_acc: 0.6514\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4153 - acc: 0.8391 - val_loss: 1.2237 - val_acc: 0.6457\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.4261 - acc: 0.8384 - val_loss: 1.2242 - val_acc: 0.6514\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.4068 - acc: 0.8404 - val_loss: 1.2249 - val_acc: 0.6286\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.4221 - acc: 0.8429 - val_loss: 1.2334 - val_acc: 0.6514\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4242 - acc: 0.8276 - val_loss: 1.2325 - val_acc: 0.6514\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.4000 - acc: 0.8512 - val_loss: 1.2167 - val_acc: 0.6457\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.4097 - acc: 0.8410 - val_loss: 1.2134 - val_acc: 0.6514\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4055 - acc: 0.8531 - val_loss: 1.2166 - val_acc: 0.6343\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4207 - acc: 0.8487 - val_loss: 1.2321 - val_acc: 0.6457\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4084 - acc: 0.8461 - val_loss: 1.2237 - val_acc: 0.6686\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.4018 - acc: 0.8506 - val_loss: 1.2338 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.3960 - acc: 0.8474 - val_loss: 1.2264 - val_acc: 0.6343\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.4145 - acc: 0.8410 - val_loss: 1.2351 - val_acc: 0.6286\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4185 - acc: 0.8525 - val_loss: 1.2502 - val_acc: 0.6514\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.4138 - acc: 0.8499 - val_loss: 1.2602 - val_acc: 0.6514\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3868 - acc: 0.8544 - val_loss: 1.2627 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4124 - acc: 0.8404 - val_loss: 1.2390 - val_acc: 0.6571\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3818 - acc: 0.8576 - val_loss: 1.2433 - val_acc: 0.6457\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.3892 - acc: 0.8499 - val_loss: 1.2598 - val_acc: 0.6400\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.3740 - acc: 0.8582 - val_loss: 1.2615 - val_acc: 0.6400\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.3777 - acc: 0.8589 - val_loss: 1.2708 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3916 - acc: 0.8582 - val_loss: 1.2951 - val_acc: 0.6400\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.4048 - acc: 0.8423 - val_loss: 1.3039 - val_acc: 0.6229\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.3694 - acc: 0.8685 - val_loss: 1.2864 - val_acc: 0.6571\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3795 - acc: 0.8506 - val_loss: 1.2812 - val_acc: 0.6629\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.3823 - acc: 0.8487 - val_loss: 1.2984 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3950 - acc: 0.8499 - val_loss: 1.3022 - val_acc: 0.6229\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.3826 - acc: 0.8602 - val_loss: 1.3012 - val_acc: 0.6514\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.3709 - acc: 0.8659 - val_loss: 1.2974 - val_acc: 0.6686\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.3997 - acc: 0.8506 - val_loss: 1.3048 - val_acc: 0.6457\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.3851 - acc: 0.8570 - val_loss: 1.3073 - val_acc: 0.6457\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.3842 - acc: 0.8550 - val_loss: 1.3198 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.3689 - acc: 0.8685 - val_loss: 1.3112 - val_acc: 0.6457\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.3902 - acc: 0.8499 - val_loss: 1.3061 - val_acc: 0.6629\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.3583 - acc: 0.8716 - val_loss: 1.3212 - val_acc: 0.6400\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.3966 - acc: 0.8461 - val_loss: 1.3217 - val_acc: 0.6514\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.3602 - acc: 0.8653 - val_loss: 1.3197 - val_acc: 0.6343\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.3631 - acc: 0.8697 - val_loss: 1.3151 - val_acc: 0.6457\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.3783 - acc: 0.8576 - val_loss: 1.3220 - val_acc: 0.6571\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.3851 - acc: 0.8525 - val_loss: 1.3218 - val_acc: 0.6571\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.3629 - acc: 0.8672 - val_loss: 1.3284 - val_acc: 0.6571\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.3597 - acc: 0.8678 - val_loss: 1.3283 - val_acc: 0.6629\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.3457 - acc: 0.8691 - val_loss: 1.3458 - val_acc: 0.6457\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.3313 - acc: 0.8780 - val_loss: 1.3378 - val_acc: 0.6571\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.3726 - acc: 0.8576 - val_loss: 1.3552 - val_acc: 0.6286\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.3480 - acc: 0.8646 - val_loss: 1.3567 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.3970 - acc: 0.8538 - val_loss: 1.3567 - val_acc: 0.6457\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3424 - acc: 0.8729 - val_loss: 1.3721 - val_acc: 0.6457\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.3359 - acc: 0.8742 - val_loss: 1.3604 - val_acc: 0.6400\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.3418 - acc: 0.8742 - val_loss: 1.3513 - val_acc: 0.6571\n",
      "194/194 [==============================] - 0s 83us/step\n",
      "1741/1741 [==============================] - 0s 58us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 6s 4ms/step - loss: 1.4428 - acc: 0.3133 - val_loss: 1.1115 - val_acc: 0.6057\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.2127 - acc: 0.4582 - val_loss: 1.0120 - val_acc: 0.6514\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.1357 - acc: 0.4901 - val_loss: 0.9673 - val_acc: 0.6400\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 1.0532 - acc: 0.5501 - val_loss: 0.9412 - val_acc: 0.6857\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 1.0140 - acc: 0.5724 - val_loss: 0.9227 - val_acc: 0.6743\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 1.0293 - acc: 0.5578 - val_loss: 0.9111 - val_acc: 0.6914\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.0099 - acc: 0.5846 - val_loss: 0.9104 - val_acc: 0.7029\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9936 - acc: 0.5858 - val_loss: 0.9042 - val_acc: 0.6971\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.9726 - acc: 0.5839 - val_loss: 0.9025 - val_acc: 0.6629\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9444 - acc: 0.6018 - val_loss: 0.8984 - val_acc: 0.6914\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.9268 - acc: 0.6031 - val_loss: 0.8973 - val_acc: 0.6857\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.9175 - acc: 0.6184 - val_loss: 0.8982 - val_acc: 0.6800\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9109 - acc: 0.6171 - val_loss: 0.8951 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8778 - acc: 0.6254 - val_loss: 0.8884 - val_acc: 0.6914\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9091 - acc: 0.6158 - val_loss: 0.8967 - val_acc: 0.6914\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8869 - acc: 0.6267 - val_loss: 0.9004 - val_acc: 0.6571\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8843 - acc: 0.6273 - val_loss: 0.8998 - val_acc: 0.6857\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8730 - acc: 0.6362 - val_loss: 0.9049 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8627 - acc: 0.6375 - val_loss: 0.9114 - val_acc: 0.6686\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8769 - acc: 0.6216 - val_loss: 0.9015 - val_acc: 0.6857\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8440 - acc: 0.6567 - val_loss: 0.9021 - val_acc: 0.6514\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8521 - acc: 0.6420 - val_loss: 0.8915 - val_acc: 0.6800\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8374 - acc: 0.6528 - val_loss: 0.9055 - val_acc: 0.6800\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8294 - acc: 0.6675 - val_loss: 0.9040 - val_acc: 0.6914\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8173 - acc: 0.6650 - val_loss: 0.9074 - val_acc: 0.6800\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8261 - acc: 0.6592 - val_loss: 0.9134 - val_acc: 0.6743\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8050 - acc: 0.6650 - val_loss: 0.9148 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8073 - acc: 0.6650 - val_loss: 0.9170 - val_acc: 0.6686\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8059 - acc: 0.6643 - val_loss: 0.9253 - val_acc: 0.6800\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7881 - acc: 0.6618 - val_loss: 0.9223 - val_acc: 0.6857\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7910 - acc: 0.6656 - val_loss: 0.9231 - val_acc: 0.6743\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7994 - acc: 0.6643 - val_loss: 0.9258 - val_acc: 0.6743\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7722 - acc: 0.6886 - val_loss: 0.9392 - val_acc: 0.6800\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7884 - acc: 0.6752 - val_loss: 0.9350 - val_acc: 0.6857\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7867 - acc: 0.6777 - val_loss: 0.9381 - val_acc: 0.6800\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7668 - acc: 0.6886 - val_loss: 0.9411 - val_acc: 0.6800\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7506 - acc: 0.6790 - val_loss: 0.9466 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7531 - acc: 0.6988 - val_loss: 0.9495 - val_acc: 0.6743\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7346 - acc: 0.6937 - val_loss: 0.9659 - val_acc: 0.6629\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7506 - acc: 0.6975 - val_loss: 0.9656 - val_acc: 0.6343\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7316 - acc: 0.7045 - val_loss: 0.9656 - val_acc: 0.6457\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7119 - acc: 0.7084 - val_loss: 0.9639 - val_acc: 0.6457\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7387 - acc: 0.6956 - val_loss: 0.9574 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7316 - acc: 0.6969 - val_loss: 0.9699 - val_acc: 0.6400\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7228 - acc: 0.7135 - val_loss: 0.9790 - val_acc: 0.6514\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7146 - acc: 0.7116 - val_loss: 0.9715 - val_acc: 0.6629\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.7200 - acc: 0.7052 - val_loss: 0.9821 - val_acc: 0.6514\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7251 - acc: 0.7147 - val_loss: 0.9719 - val_acc: 0.6400\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7074 - acc: 0.7122 - val_loss: 0.9844 - val_acc: 0.6286\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6966 - acc: 0.7096 - val_loss: 0.9888 - val_acc: 0.6343\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6962 - acc: 0.7154 - val_loss: 0.9732 - val_acc: 0.6514\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 112us/step - loss: 0.6954 - acc: 0.7186 - val_loss: 0.9725 - val_acc: 0.6457\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6994 - acc: 0.7103 - val_loss: 0.9814 - val_acc: 0.6514\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6782 - acc: 0.7256 - val_loss: 0.9822 - val_acc: 0.6343\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6745 - acc: 0.7307 - val_loss: 0.9796 - val_acc: 0.6571\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6706 - acc: 0.7384 - val_loss: 1.0078 - val_acc: 0.6514\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6675 - acc: 0.7364 - val_loss: 0.9991 - val_acc: 0.6571\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6703 - acc: 0.7422 - val_loss: 1.0147 - val_acc: 0.6343\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6665 - acc: 0.7288 - val_loss: 1.0058 - val_acc: 0.6457\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6730 - acc: 0.7301 - val_loss: 0.9966 - val_acc: 0.6514\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6635 - acc: 0.7390 - val_loss: 0.9985 - val_acc: 0.6343\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6414 - acc: 0.7409 - val_loss: 0.9945 - val_acc: 0.6343\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.6503 - acc: 0.7313 - val_loss: 0.9995 - val_acc: 0.6457\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6595 - acc: 0.7345 - val_loss: 1.0027 - val_acc: 0.6514\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6447 - acc: 0.7530 - val_loss: 0.9994 - val_acc: 0.6629\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6186 - acc: 0.7556 - val_loss: 1.0041 - val_acc: 0.6686\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6280 - acc: 0.7486 - val_loss: 1.0077 - val_acc: 0.6286\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6309 - acc: 0.7498 - val_loss: 1.0154 - val_acc: 0.6514\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6279 - acc: 0.7415 - val_loss: 1.0282 - val_acc: 0.6400\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6157 - acc: 0.7524 - val_loss: 1.0271 - val_acc: 0.6343\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6164 - acc: 0.7441 - val_loss: 1.0293 - val_acc: 0.6457\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.6257 - acc: 0.7447 - val_loss: 1.0369 - val_acc: 0.6400\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6105 - acc: 0.7588 - val_loss: 1.0360 - val_acc: 0.6286\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6162 - acc: 0.7652 - val_loss: 1.0400 - val_acc: 0.6400\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5999 - acc: 0.7588 - val_loss: 1.0478 - val_acc: 0.6514\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6019 - acc: 0.7537 - val_loss: 1.0410 - val_acc: 0.6343\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5890 - acc: 0.7645 - val_loss: 1.0547 - val_acc: 0.6343\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6175 - acc: 0.7390 - val_loss: 1.0558 - val_acc: 0.6514\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6041 - acc: 0.7492 - val_loss: 1.0549 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5799 - acc: 0.7601 - val_loss: 1.0639 - val_acc: 0.6457\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5659 - acc: 0.7735 - val_loss: 1.0700 - val_acc: 0.6343\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5821 - acc: 0.7722 - val_loss: 1.0633 - val_acc: 0.6514\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5959 - acc: 0.7601 - val_loss: 1.0943 - val_acc: 0.6343\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5795 - acc: 0.7798 - val_loss: 1.0758 - val_acc: 0.6514\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5706 - acc: 0.7728 - val_loss: 1.0789 - val_acc: 0.6457\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5666 - acc: 0.7741 - val_loss: 1.0895 - val_acc: 0.6457\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5602 - acc: 0.7805 - val_loss: 1.0828 - val_acc: 0.6343\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5534 - acc: 0.7837 - val_loss: 1.0883 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5394 - acc: 0.7977 - val_loss: 1.0887 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5473 - acc: 0.7824 - val_loss: 1.0975 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5404 - acc: 0.7894 - val_loss: 1.1041 - val_acc: 0.6457\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5546 - acc: 0.7760 - val_loss: 1.1062 - val_acc: 0.6457\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5436 - acc: 0.7971 - val_loss: 1.1040 - val_acc: 0.6343\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.5384 - acc: 0.7932 - val_loss: 1.0983 - val_acc: 0.6286\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5467 - acc: 0.7939 - val_loss: 1.0975 - val_acc: 0.6571\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5510 - acc: 0.7817 - val_loss: 1.1069 - val_acc: 0.6286\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5460 - acc: 0.7862 - val_loss: 1.1065 - val_acc: 0.6057\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5291 - acc: 0.7830 - val_loss: 1.1283 - val_acc: 0.6114\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5307 - acc: 0.7888 - val_loss: 1.1376 - val_acc: 0.6286\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5146 - acc: 0.8079 - val_loss: 1.1346 - val_acc: 0.6171\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5238 - acc: 0.7888 - val_loss: 1.1355 - val_acc: 0.6171\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5061 - acc: 0.8092 - val_loss: 1.1598 - val_acc: 0.6229\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5234 - acc: 0.7983 - val_loss: 1.1440 - val_acc: 0.6286\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5211 - acc: 0.7869 - val_loss: 1.1679 - val_acc: 0.6171\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5135 - acc: 0.7951 - val_loss: 1.1535 - val_acc: 0.6343\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5156 - acc: 0.7951 - val_loss: 1.1532 - val_acc: 0.6400\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5342 - acc: 0.7926 - val_loss: 1.1513 - val_acc: 0.6457\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5133 - acc: 0.7939 - val_loss: 1.1636 - val_acc: 0.6457\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5187 - acc: 0.7971 - val_loss: 1.1530 - val_acc: 0.6457\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5314 - acc: 0.8073 - val_loss: 1.1649 - val_acc: 0.6514\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5037 - acc: 0.8054 - val_loss: 1.1617 - val_acc: 0.6514\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4856 - acc: 0.8200 - val_loss: 1.1670 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5045 - acc: 0.7939 - val_loss: 1.1784 - val_acc: 0.6571\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4710 - acc: 0.8137 - val_loss: 1.1898 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4947 - acc: 0.8009 - val_loss: 1.1760 - val_acc: 0.6400\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4750 - acc: 0.8168 - val_loss: 1.1805 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4941 - acc: 0.8086 - val_loss: 1.1945 - val_acc: 0.6400\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.4695 - acc: 0.8207 - val_loss: 1.2049 - val_acc: 0.6400\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4838 - acc: 0.8092 - val_loss: 1.1977 - val_acc: 0.6286\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4776 - acc: 0.8149 - val_loss: 1.2197 - val_acc: 0.6343\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4566 - acc: 0.8302 - val_loss: 1.2183 - val_acc: 0.6343\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4669 - acc: 0.8283 - val_loss: 1.2124 - val_acc: 0.6229\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4691 - acc: 0.8073 - val_loss: 1.2177 - val_acc: 0.6229\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4703 - acc: 0.8239 - val_loss: 1.2184 - val_acc: 0.6457\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4508 - acc: 0.8245 - val_loss: 1.2371 - val_acc: 0.6457\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4447 - acc: 0.8341 - val_loss: 1.2365 - val_acc: 0.6400\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4491 - acc: 0.8385 - val_loss: 1.2397 - val_acc: 0.6400\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4621 - acc: 0.8162 - val_loss: 1.2428 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4530 - acc: 0.8251 - val_loss: 1.2391 - val_acc: 0.6343\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4381 - acc: 0.8379 - val_loss: 1.2525 - val_acc: 0.6343\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4457 - acc: 0.8322 - val_loss: 1.2409 - val_acc: 0.6229\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4484 - acc: 0.8264 - val_loss: 1.2486 - val_acc: 0.6457\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4596 - acc: 0.8366 - val_loss: 1.2578 - val_acc: 0.6400\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4603 - acc: 0.8239 - val_loss: 1.2701 - val_acc: 0.6229\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4645 - acc: 0.8232 - val_loss: 1.2689 - val_acc: 0.6229\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.4351 - acc: 0.8334 - val_loss: 1.2667 - val_acc: 0.6229\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4334 - acc: 0.8341 - val_loss: 1.2688 - val_acc: 0.6229\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4198 - acc: 0.8392 - val_loss: 1.2906 - val_acc: 0.6286\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.4278 - acc: 0.8277 - val_loss: 1.2891 - val_acc: 0.6343\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4419 - acc: 0.8245 - val_loss: 1.2783 - val_acc: 0.6286\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.4204 - acc: 0.8437 - val_loss: 1.2842 - val_acc: 0.6343\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4272 - acc: 0.8405 - val_loss: 1.3077 - val_acc: 0.6286\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4258 - acc: 0.8379 - val_loss: 1.3236 - val_acc: 0.6514\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4427 - acc: 0.8251 - val_loss: 1.3155 - val_acc: 0.6343\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.4171 - acc: 0.8417 - val_loss: 1.2972 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.4124 - acc: 0.8392 - val_loss: 1.2873 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4329 - acc: 0.8302 - val_loss: 1.2830 - val_acc: 0.6571\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4197 - acc: 0.8385 - val_loss: 1.2950 - val_acc: 0.6400\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4208 - acc: 0.8481 - val_loss: 1.2968 - val_acc: 0.6286\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4402 - acc: 0.8302 - val_loss: 1.3050 - val_acc: 0.6457\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4295 - acc: 0.8366 - val_loss: 1.3011 - val_acc: 0.6229\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.3985 - acc: 0.8551 - val_loss: 1.3278 - val_acc: 0.6400\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.4166 - acc: 0.8366 - val_loss: 1.3174 - val_acc: 0.6343\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.4159 - acc: 0.8392 - val_loss: 1.3209 - val_acc: 0.6343\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.4147 - acc: 0.8539 - val_loss: 1.3227 - val_acc: 0.6457\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 110us/step - loss: 0.4016 - acc: 0.8622 - val_loss: 1.3280 - val_acc: 0.6343\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4193 - acc: 0.8379 - val_loss: 1.3285 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.3866 - acc: 0.8615 - val_loss: 1.3329 - val_acc: 0.6514\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4167 - acc: 0.8494 - val_loss: 1.3453 - val_acc: 0.6343\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.4047 - acc: 0.8558 - val_loss: 1.3275 - val_acc: 0.6229\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4064 - acc: 0.8539 - val_loss: 1.3370 - val_acc: 0.6457\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.3877 - acc: 0.8583 - val_loss: 1.3768 - val_acc: 0.6286\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.3951 - acc: 0.8583 - val_loss: 1.3593 - val_acc: 0.6229\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.4191 - acc: 0.8424 - val_loss: 1.3577 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4222 - acc: 0.8481 - val_loss: 1.3571 - val_acc: 0.6457\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.3817 - acc: 0.8551 - val_loss: 1.3557 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.3995 - acc: 0.8532 - val_loss: 1.3538 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.3748 - acc: 0.8590 - val_loss: 1.3759 - val_acc: 0.6457\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.3887 - acc: 0.8590 - val_loss: 1.3822 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.4068 - acc: 0.8500 - val_loss: 1.3726 - val_acc: 0.6343\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3904 - acc: 0.8564 - val_loss: 1.3696 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4106 - acc: 0.8500 - val_loss: 1.3703 - val_acc: 0.6229\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.3983 - acc: 0.8532 - val_loss: 1.3781 - val_acc: 0.6171\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4115 - acc: 0.8513 - val_loss: 1.3748 - val_acc: 0.6286\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.3835 - acc: 0.8583 - val_loss: 1.3895 - val_acc: 0.6343\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.3754 - acc: 0.8628 - val_loss: 1.3930 - val_acc: 0.6343\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.3653 - acc: 0.8641 - val_loss: 1.3852 - val_acc: 0.6343\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.3740 - acc: 0.8602 - val_loss: 1.3831 - val_acc: 0.6514\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.3643 - acc: 0.8653 - val_loss: 1.3755 - val_acc: 0.6571\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 109us/step - loss: 0.3578 - acc: 0.8743 - val_loss: 1.3914 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.3783 - acc: 0.8526 - val_loss: 1.3913 - val_acc: 0.6457\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.3728 - acc: 0.8622 - val_loss: 1.3937 - val_acc: 0.6514\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3665 - acc: 0.8507 - val_loss: 1.3938 - val_acc: 0.6400\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.3808 - acc: 0.8590 - val_loss: 1.4107 - val_acc: 0.6400\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3808 - acc: 0.8698 - val_loss: 1.4028 - val_acc: 0.6171\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3851 - acc: 0.8615 - val_loss: 1.4229 - val_acc: 0.6457\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3426 - acc: 0.8743 - val_loss: 1.4330 - val_acc: 0.6571\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3904 - acc: 0.8647 - val_loss: 1.4215 - val_acc: 0.6457\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.3511 - acc: 0.8679 - val_loss: 1.4302 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.3799 - acc: 0.8609 - val_loss: 1.4457 - val_acc: 0.6457\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3769 - acc: 0.8653 - val_loss: 1.4372 - val_acc: 0.6229\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.3650 - acc: 0.8705 - val_loss: 1.4589 - val_acc: 0.6286\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.3486 - acc: 0.8717 - val_loss: 1.4494 - val_acc: 0.6229\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3386 - acc: 0.8787 - val_loss: 1.4309 - val_acc: 0.6343\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3711 - acc: 0.8615 - val_loss: 1.4215 - val_acc: 0.6343\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.3626 - acc: 0.8609 - val_loss: 1.4306 - val_acc: 0.6343\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.3758 - acc: 0.8647 - val_loss: 1.4312 - val_acc: 0.6629\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.3489 - acc: 0.8775 - val_loss: 1.4179 - val_acc: 0.6514\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.3517 - acc: 0.8749 - val_loss: 1.4402 - val_acc: 0.6286\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3709 - acc: 0.8730 - val_loss: 1.4292 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 71us/step\n",
      "1742/1742 [==============================] - 0s 55us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 7s 4ms/step - loss: 1.3776 - acc: 0.3459 - val_loss: 1.1439 - val_acc: 0.5486\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 1.2121 - acc: 0.4499 - val_loss: 1.0429 - val_acc: 0.6000\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 1.1261 - acc: 0.5131 - val_loss: 0.9875 - val_acc: 0.6229\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 1.0707 - acc: 0.5399 - val_loss: 0.9550 - val_acc: 0.6286\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 1.0365 - acc: 0.5475 - val_loss: 0.9359 - val_acc: 0.6343\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9914 - acc: 0.5743 - val_loss: 0.9092 - val_acc: 0.6571\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9803 - acc: 0.5724 - val_loss: 0.8936 - val_acc: 0.6400\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 111us/step - loss: 0.9527 - acc: 0.6005 - val_loss: 0.8884 - val_acc: 0.6457\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.9492 - acc: 0.5992 - val_loss: 0.8801 - val_acc: 0.6629\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.9336 - acc: 0.6056 - val_loss: 0.8810 - val_acc: 0.6514\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.9216 - acc: 0.6088 - val_loss: 0.8768 - val_acc: 0.6400\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.9074 - acc: 0.6158 - val_loss: 0.8764 - val_acc: 0.6400\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8987 - acc: 0.6235 - val_loss: 0.8680 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8827 - acc: 0.6343 - val_loss: 0.8620 - val_acc: 0.6514\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8662 - acc: 0.6522 - val_loss: 0.8579 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 122us/step - loss: 0.8734 - acc: 0.6433 - val_loss: 0.8654 - val_acc: 0.6457\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.8644 - acc: 0.6458 - val_loss: 0.8661 - val_acc: 0.6686\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.8486 - acc: 0.6522 - val_loss: 0.8624 - val_acc: 0.6514\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.8411 - acc: 0.6618 - val_loss: 0.8607 - val_acc: 0.6571\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8429 - acc: 0.6509 - val_loss: 0.8712 - val_acc: 0.6400\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8323 - acc: 0.6458 - val_loss: 0.8634 - val_acc: 0.6571\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8296 - acc: 0.6541 - val_loss: 0.8680 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8086 - acc: 0.6643 - val_loss: 0.8674 - val_acc: 0.6457\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.8201 - acc: 0.6675 - val_loss: 0.8626 - val_acc: 0.6457\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8188 - acc: 0.6669 - val_loss: 0.8605 - val_acc: 0.6400\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.8022 - acc: 0.6733 - val_loss: 0.8656 - val_acc: 0.6457\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7876 - acc: 0.6739 - val_loss: 0.8693 - val_acc: 0.6400\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7914 - acc: 0.6765 - val_loss: 0.8759 - val_acc: 0.6400\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7558 - acc: 0.6918 - val_loss: 0.8790 - val_acc: 0.6457\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7966 - acc: 0.6682 - val_loss: 0.8795 - val_acc: 0.6514\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.7541 - acc: 0.7052 - val_loss: 0.8672 - val_acc: 0.6629\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7879 - acc: 0.6796 - val_loss: 0.8786 - val_acc: 0.6514\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.7530 - acc: 0.6879 - val_loss: 0.8758 - val_acc: 0.6457\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.7442 - acc: 0.6981 - val_loss: 0.8716 - val_acc: 0.6571\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7681 - acc: 0.6886 - val_loss: 0.8679 - val_acc: 0.6514\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7541 - acc: 0.6867 - val_loss: 0.8734 - val_acc: 0.6743\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7378 - acc: 0.6950 - val_loss: 0.8764 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7392 - acc: 0.7013 - val_loss: 0.8818 - val_acc: 0.6629\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7550 - acc: 0.6835 - val_loss: 0.8708 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.7271 - acc: 0.7096 - val_loss: 0.8699 - val_acc: 0.6571\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7356 - acc: 0.7077 - val_loss: 0.8695 - val_acc: 0.6457\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7214 - acc: 0.7103 - val_loss: 0.8904 - val_acc: 0.6571\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7085 - acc: 0.7154 - val_loss: 0.8759 - val_acc: 0.6686\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7073 - acc: 0.7052 - val_loss: 0.8918 - val_acc: 0.6743\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7058 - acc: 0.7109 - val_loss: 0.8971 - val_acc: 0.6514\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.7071 - acc: 0.7064 - val_loss: 0.8815 - val_acc: 0.6686\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7067 - acc: 0.7109 - val_loss: 0.8774 - val_acc: 0.6571\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6980 - acc: 0.7230 - val_loss: 0.8839 - val_acc: 0.6571\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6904 - acc: 0.7160 - val_loss: 0.9074 - val_acc: 0.6571\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6831 - acc: 0.7218 - val_loss: 0.9116 - val_acc: 0.6457\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6806 - acc: 0.7301 - val_loss: 0.8939 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6951 - acc: 0.7224 - val_loss: 0.9016 - val_acc: 0.6629\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6841 - acc: 0.7256 - val_loss: 0.8919 - val_acc: 0.6629\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6863 - acc: 0.7269 - val_loss: 0.8915 - val_acc: 0.6457\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6645 - acc: 0.7345 - val_loss: 0.9142 - val_acc: 0.6571\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6548 - acc: 0.7339 - val_loss: 0.9033 - val_acc: 0.6686\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6750 - acc: 0.7179 - val_loss: 0.9007 - val_acc: 0.6629\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6735 - acc: 0.7269 - val_loss: 0.9082 - val_acc: 0.6457\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6444 - acc: 0.7435 - val_loss: 0.8975 - val_acc: 0.6629\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6419 - acc: 0.7613 - val_loss: 0.9145 - val_acc: 0.6400\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6436 - acc: 0.7352 - val_loss: 0.9181 - val_acc: 0.6514\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6504 - acc: 0.7479 - val_loss: 0.9284 - val_acc: 0.6571\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6222 - acc: 0.7479 - val_loss: 0.9344 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6385 - acc: 0.7447 - val_loss: 0.9385 - val_acc: 0.6514\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6316 - acc: 0.7486 - val_loss: 0.9320 - val_acc: 0.6571\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6243 - acc: 0.7422 - val_loss: 0.9351 - val_acc: 0.6571\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6167 - acc: 0.7594 - val_loss: 0.9397 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6123 - acc: 0.7492 - val_loss: 0.9474 - val_acc: 0.6686\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6121 - acc: 0.7549 - val_loss: 0.9405 - val_acc: 0.6514\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6087 - acc: 0.7409 - val_loss: 0.9311 - val_acc: 0.6800\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5926 - acc: 0.7639 - val_loss: 0.9391 - val_acc: 0.6629\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6060 - acc: 0.7549 - val_loss: 0.9435 - val_acc: 0.6571\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5981 - acc: 0.7703 - val_loss: 0.9448 - val_acc: 0.6629\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6000 - acc: 0.7607 - val_loss: 0.9459 - val_acc: 0.6686\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.5826 - acc: 0.7581 - val_loss: 0.9498 - val_acc: 0.6743\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5946 - acc: 0.7549 - val_loss: 0.9588 - val_acc: 0.6629\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5808 - acc: 0.7735 - val_loss: 0.9465 - val_acc: 0.6571\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5815 - acc: 0.7703 - val_loss: 0.9538 - val_acc: 0.6686\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5901 - acc: 0.7703 - val_loss: 0.9437 - val_acc: 0.6629\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5845 - acc: 0.7715 - val_loss: 0.9516 - val_acc: 0.6629\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5646 - acc: 0.7696 - val_loss: 0.9560 - val_acc: 0.6743\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5722 - acc: 0.7779 - val_loss: 0.9670 - val_acc: 0.6686\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.5898 - acc: 0.7556 - val_loss: 0.9713 - val_acc: 0.6571\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5842 - acc: 0.7601 - val_loss: 0.9696 - val_acc: 0.6514\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5474 - acc: 0.7837 - val_loss: 0.9643 - val_acc: 0.6457\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5460 - acc: 0.7862 - val_loss: 0.9676 - val_acc: 0.6686\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5400 - acc: 0.7900 - val_loss: 0.9732 - val_acc: 0.6571\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5333 - acc: 0.7869 - val_loss: 0.9744 - val_acc: 0.6514\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5500 - acc: 0.7690 - val_loss: 0.9818 - val_acc: 0.6457\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5459 - acc: 0.7811 - val_loss: 0.9886 - val_acc: 0.6286\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5452 - acc: 0.8022 - val_loss: 1.0040 - val_acc: 0.6229\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5537 - acc: 0.7856 - val_loss: 1.0038 - val_acc: 0.6286\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5199 - acc: 0.8034 - val_loss: 0.9957 - val_acc: 0.6457\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5223 - acc: 0.7837 - val_loss: 1.0025 - val_acc: 0.6343\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5414 - acc: 0.7881 - val_loss: 0.9999 - val_acc: 0.6286\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5393 - acc: 0.7811 - val_loss: 1.0147 - val_acc: 0.6343\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5215 - acc: 0.7958 - val_loss: 1.0044 - val_acc: 0.6457\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5391 - acc: 0.7779 - val_loss: 0.9957 - val_acc: 0.6400\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5125 - acc: 0.7932 - val_loss: 1.0051 - val_acc: 0.6571\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5197 - acc: 0.7983 - val_loss: 1.0196 - val_acc: 0.6343\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5145 - acc: 0.7951 - val_loss: 1.0119 - val_acc: 0.6400\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5290 - acc: 0.7862 - val_loss: 1.0223 - val_acc: 0.6343\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5042 - acc: 0.8092 - val_loss: 1.0314 - val_acc: 0.6171\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4991 - acc: 0.8060 - val_loss: 1.0168 - val_acc: 0.6457\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4795 - acc: 0.8034 - val_loss: 1.0194 - val_acc: 0.6286\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5265 - acc: 0.8022 - val_loss: 1.0048 - val_acc: 0.6400\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4899 - acc: 0.8073 - val_loss: 1.0326 - val_acc: 0.6400\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4774 - acc: 0.8143 - val_loss: 1.0349 - val_acc: 0.6400\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5104 - acc: 0.8034 - val_loss: 1.0348 - val_acc: 0.6400\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4832 - acc: 0.8041 - val_loss: 1.0438 - val_acc: 0.6343\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.4902 - acc: 0.8156 - val_loss: 1.0450 - val_acc: 0.6286\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4841 - acc: 0.8041 - val_loss: 1.0411 - val_acc: 0.6400\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4753 - acc: 0.8098 - val_loss: 1.0543 - val_acc: 0.6229\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4716 - acc: 0.8207 - val_loss: 1.0533 - val_acc: 0.6343\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4604 - acc: 0.8188 - val_loss: 1.0754 - val_acc: 0.6343\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4705 - acc: 0.8143 - val_loss: 1.0674 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.4801 - acc: 0.8073 - val_loss: 1.0577 - val_acc: 0.6343\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4733 - acc: 0.8181 - val_loss: 1.0832 - val_acc: 0.6229\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4762 - acc: 0.8194 - val_loss: 1.0856 - val_acc: 0.6343\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4672 - acc: 0.8258 - val_loss: 1.0953 - val_acc: 0.6286\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4490 - acc: 0.8251 - val_loss: 1.0816 - val_acc: 0.6400\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4644 - acc: 0.8086 - val_loss: 1.0901 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4548 - acc: 0.8245 - val_loss: 1.1148 - val_acc: 0.6343\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4499 - acc: 0.8302 - val_loss: 1.1156 - val_acc: 0.6114\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4660 - acc: 0.8283 - val_loss: 1.1007 - val_acc: 0.6171\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4465 - acc: 0.8220 - val_loss: 1.1103 - val_acc: 0.6114\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4473 - acc: 0.8385 - val_loss: 1.1120 - val_acc: 0.6057\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4588 - acc: 0.8226 - val_loss: 1.1096 - val_acc: 0.6229\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4500 - acc: 0.8277 - val_loss: 1.1096 - val_acc: 0.6343\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4389 - acc: 0.8341 - val_loss: 1.1148 - val_acc: 0.6400\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.4660 - acc: 0.8251 - val_loss: 1.1340 - val_acc: 0.6229\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.4542 - acc: 0.8220 - val_loss: 1.1238 - val_acc: 0.6057\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4625 - acc: 0.8283 - val_loss: 1.1398 - val_acc: 0.6114\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4401 - acc: 0.8239 - val_loss: 1.1353 - val_acc: 0.6114\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4260 - acc: 0.8315 - val_loss: 1.1298 - val_acc: 0.6114\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.4081 - acc: 0.8558 - val_loss: 1.1161 - val_acc: 0.6171\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4430 - acc: 0.8322 - val_loss: 1.1116 - val_acc: 0.6229\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4132 - acc: 0.8385 - val_loss: 1.1194 - val_acc: 0.6057\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4119 - acc: 0.8347 - val_loss: 1.1300 - val_acc: 0.6171\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4263 - acc: 0.8360 - val_loss: 1.1372 - val_acc: 0.6286\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4273 - acc: 0.8309 - val_loss: 1.1321 - val_acc: 0.6171\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4301 - acc: 0.8302 - val_loss: 1.1270 - val_acc: 0.6114\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.4218 - acc: 0.8296 - val_loss: 1.1381 - val_acc: 0.6057\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4225 - acc: 0.8264 - val_loss: 1.1293 - val_acc: 0.6229\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4295 - acc: 0.8417 - val_loss: 1.1375 - val_acc: 0.6400\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.3969 - acc: 0.8500 - val_loss: 1.1429 - val_acc: 0.6286\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4134 - acc: 0.8437 - val_loss: 1.1471 - val_acc: 0.6229\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4287 - acc: 0.8309 - val_loss: 1.1490 - val_acc: 0.6286\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4038 - acc: 0.8558 - val_loss: 1.1356 - val_acc: 0.6457\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4129 - acc: 0.8437 - val_loss: 1.1643 - val_acc: 0.6114\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.3868 - acc: 0.8500 - val_loss: 1.1573 - val_acc: 0.6229\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4000 - acc: 0.8462 - val_loss: 1.1495 - val_acc: 0.6286\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.4206 - acc: 0.8481 - val_loss: 1.1668 - val_acc: 0.6229\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.3823 - acc: 0.8539 - val_loss: 1.1559 - val_acc: 0.6114\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.4032 - acc: 0.8564 - val_loss: 1.1572 - val_acc: 0.6343\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.3833 - acc: 0.8539 - val_loss: 1.1622 - val_acc: 0.6114\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.3798 - acc: 0.8622 - val_loss: 1.1660 - val_acc: 0.6343\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4154 - acc: 0.8437 - val_loss: 1.1768 - val_acc: 0.6229\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.3670 - acc: 0.8564 - val_loss: 1.2031 - val_acc: 0.6343\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.3754 - acc: 0.8571 - val_loss: 1.2017 - val_acc: 0.6000\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.3949 - acc: 0.8526 - val_loss: 1.1951 - val_acc: 0.6114\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.3847 - acc: 0.8545 - val_loss: 1.1830 - val_acc: 0.6286\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4168 - acc: 0.8456 - val_loss: 1.1957 - val_acc: 0.6114\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.3855 - acc: 0.8468 - val_loss: 1.2136 - val_acc: 0.6057\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3972 - acc: 0.8468 - val_loss: 1.2005 - val_acc: 0.6057\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.3906 - acc: 0.8379 - val_loss: 1.1971 - val_acc: 0.6229\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3729 - acc: 0.8571 - val_loss: 1.2228 - val_acc: 0.6229\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3841 - acc: 0.8558 - val_loss: 1.2199 - val_acc: 0.6171\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.3862 - acc: 0.8609 - val_loss: 1.2153 - val_acc: 0.6114\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3846 - acc: 0.8622 - val_loss: 1.2123 - val_acc: 0.6000\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3961 - acc: 0.8475 - val_loss: 1.1992 - val_acc: 0.6514\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.3737 - acc: 0.8564 - val_loss: 1.2138 - val_acc: 0.6286\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3721 - acc: 0.8500 - val_loss: 1.2200 - val_acc: 0.6171\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.3839 - acc: 0.8564 - val_loss: 1.2038 - val_acc: 0.6171\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3645 - acc: 0.8634 - val_loss: 1.1989 - val_acc: 0.6286\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3793 - acc: 0.8545 - val_loss: 1.2189 - val_acc: 0.6229\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.3698 - acc: 0.8660 - val_loss: 1.2199 - val_acc: 0.6286\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.3560 - acc: 0.8615 - val_loss: 1.2278 - val_acc: 0.6229\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3570 - acc: 0.8692 - val_loss: 1.2284 - val_acc: 0.6400\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.3381 - acc: 0.8705 - val_loss: 1.2359 - val_acc: 0.6114\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.3866 - acc: 0.8545 - val_loss: 1.2479 - val_acc: 0.5943\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.3711 - acc: 0.8558 - val_loss: 1.2276 - val_acc: 0.6286\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3954 - acc: 0.8532 - val_loss: 1.2462 - val_acc: 0.6229\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.3532 - acc: 0.8590 - val_loss: 1.2317 - val_acc: 0.6171\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3748 - acc: 0.8666 - val_loss: 1.2375 - val_acc: 0.6057\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.3593 - acc: 0.8705 - val_loss: 1.2351 - val_acc: 0.6114\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3519 - acc: 0.8730 - val_loss: 1.2393 - val_acc: 0.6171\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3828 - acc: 0.8481 - val_loss: 1.2621 - val_acc: 0.6229\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.3828 - acc: 0.8641 - val_loss: 1.2633 - val_acc: 0.6171\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3654 - acc: 0.8609 - val_loss: 1.2451 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.3453 - acc: 0.8666 - val_loss: 1.2399 - val_acc: 0.6229\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.3560 - acc: 0.8673 - val_loss: 1.2475 - val_acc: 0.6286\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.3542 - acc: 0.8679 - val_loss: 1.2548 - val_acc: 0.6286\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.3247 - acc: 0.8768 - val_loss: 1.2483 - val_acc: 0.6286\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.3624 - acc: 0.8545 - val_loss: 1.2715 - val_acc: 0.6286\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3774 - acc: 0.8602 - val_loss: 1.2706 - val_acc: 0.6343\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.3370 - acc: 0.8749 - val_loss: 1.2675 - val_acc: 0.6514\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.3428 - acc: 0.8730 - val_loss: 1.2876 - val_acc: 0.6229\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.3827 - acc: 0.8494 - val_loss: 1.2977 - val_acc: 0.6400\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.3458 - acc: 0.8749 - val_loss: 1.2957 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 85us/step\n",
      "1742/1742 [==============================] - 0s 57us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 7s 4ms/step - loss: 1.4646 - acc: 0.3267 - val_loss: 1.1355 - val_acc: 0.5486\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 1.1919 - acc: 0.4652 - val_loss: 1.0262 - val_acc: 0.6114\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 1.1423 - acc: 0.5086 - val_loss: 0.9803 - val_acc: 0.6343\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.0761 - acc: 0.5303 - val_loss: 0.9616 - val_acc: 0.6571\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.0489 - acc: 0.5392 - val_loss: 0.9407 - val_acc: 0.6514\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 1.0093 - acc: 0.5718 - val_loss: 0.9232 - val_acc: 0.6800\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 1.0078 - acc: 0.5724 - val_loss: 0.9218 - val_acc: 0.6514\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9635 - acc: 0.5960 - val_loss: 0.9060 - val_acc: 0.6571\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9323 - acc: 0.6043 - val_loss: 0.9097 - val_acc: 0.6629\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9489 - acc: 0.6114 - val_loss: 0.9051 - val_acc: 0.6800\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9430 - acc: 0.6069 - val_loss: 0.9033 - val_acc: 0.6857\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.9230 - acc: 0.6146 - val_loss: 0.9023 - val_acc: 0.6629\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9091 - acc: 0.6139 - val_loss: 0.9012 - val_acc: 0.6629\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9193 - acc: 0.6024 - val_loss: 0.9006 - val_acc: 0.6971\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8950 - acc: 0.6305 - val_loss: 0.8995 - val_acc: 0.6571\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9005 - acc: 0.6401 - val_loss: 0.9006 - val_acc: 0.6800\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8596 - acc: 0.6452 - val_loss: 0.9151 - val_acc: 0.6914\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8681 - acc: 0.6414 - val_loss: 0.9053 - val_acc: 0.6514\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8687 - acc: 0.6337 - val_loss: 0.8978 - val_acc: 0.6571\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.8611 - acc: 0.6522 - val_loss: 0.8952 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8581 - acc: 0.6452 - val_loss: 0.9024 - val_acc: 0.6857\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8616 - acc: 0.6465 - val_loss: 0.9024 - val_acc: 0.6629\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8652 - acc: 0.6477 - val_loss: 0.9065 - val_acc: 0.6514\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8123 - acc: 0.6777 - val_loss: 0.9073 - val_acc: 0.6571\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8489 - acc: 0.6458 - val_loss: 0.9037 - val_acc: 0.6743\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.8289 - acc: 0.6592 - val_loss: 0.9161 - val_acc: 0.6457\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8121 - acc: 0.6682 - val_loss: 0.9140 - val_acc: 0.6514\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8222 - acc: 0.6656 - val_loss: 0.9131 - val_acc: 0.6400\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7980 - acc: 0.6796 - val_loss: 0.9063 - val_acc: 0.6629\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8097 - acc: 0.6918 - val_loss: 0.9135 - val_acc: 0.6400\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7784 - acc: 0.6847 - val_loss: 0.9159 - val_acc: 0.6457\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8004 - acc: 0.6707 - val_loss: 0.9289 - val_acc: 0.6400\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7812 - acc: 0.6899 - val_loss: 0.9219 - val_acc: 0.6514\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.7863 - acc: 0.6899 - val_loss: 0.9272 - val_acc: 0.6514\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 127us/step - loss: 0.7776 - acc: 0.6867 - val_loss: 0.9260 - val_acc: 0.6400\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 111us/step - loss: 0.7765 - acc: 0.6841 - val_loss: 0.9321 - val_acc: 0.6229\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.7754 - acc: 0.6822 - val_loss: 0.9335 - val_acc: 0.6229\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.7541 - acc: 0.6873 - val_loss: 0.9342 - val_acc: 0.6400\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7436 - acc: 0.7026 - val_loss: 0.9427 - val_acc: 0.6457\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7484 - acc: 0.7001 - val_loss: 0.9372 - val_acc: 0.6400\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7372 - acc: 0.7058 - val_loss: 0.9436 - val_acc: 0.6514\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7424 - acc: 0.6981 - val_loss: 0.9451 - val_acc: 0.6286\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7238 - acc: 0.7045 - val_loss: 0.9556 - val_acc: 0.6286\n",
      "Epoch 44/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7328 - acc: 0.7039 - val_loss: 0.9458 - val_acc: 0.6400\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7133 - acc: 0.7173 - val_loss: 0.9468 - val_acc: 0.6514\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7236 - acc: 0.7052 - val_loss: 0.9436 - val_acc: 0.6343\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7371 - acc: 0.6930 - val_loss: 0.9605 - val_acc: 0.6400\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7262 - acc: 0.7001 - val_loss: 0.9587 - val_acc: 0.6400\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6961 - acc: 0.7230 - val_loss: 0.9596 - val_acc: 0.6514\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7197 - acc: 0.7007 - val_loss: 0.9578 - val_acc: 0.6686\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7098 - acc: 0.7147 - val_loss: 0.9496 - val_acc: 0.6400\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7015 - acc: 0.7135 - val_loss: 0.9686 - val_acc: 0.6400\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6965 - acc: 0.7192 - val_loss: 0.9835 - val_acc: 0.6114\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6819 - acc: 0.7154 - val_loss: 0.9722 - val_acc: 0.6286\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6951 - acc: 0.7141 - val_loss: 0.9714 - val_acc: 0.6229\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.6910 - acc: 0.7230 - val_loss: 0.9641 - val_acc: 0.6400\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6795 - acc: 0.7211 - val_loss: 0.9767 - val_acc: 0.6457\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.6651 - acc: 0.7326 - val_loss: 0.9700 - val_acc: 0.6343\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.6726 - acc: 0.7281 - val_loss: 0.9664 - val_acc: 0.6229\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6551 - acc: 0.7384 - val_loss: 0.9757 - val_acc: 0.6171\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6581 - acc: 0.7301 - val_loss: 0.9704 - val_acc: 0.6286\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6617 - acc: 0.7288 - val_loss: 0.9647 - val_acc: 0.6457\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6513 - acc: 0.7326 - val_loss: 0.9651 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6622 - acc: 0.7326 - val_loss: 0.9981 - val_acc: 0.6171\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6434 - acc: 0.7415 - val_loss: 0.9935 - val_acc: 0.6229\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6587 - acc: 0.7396 - val_loss: 0.9832 - val_acc: 0.6457\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6370 - acc: 0.7537 - val_loss: 0.9885 - val_acc: 0.6286\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6160 - acc: 0.7415 - val_loss: 1.0149 - val_acc: 0.6229\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6531 - acc: 0.7294 - val_loss: 1.0157 - val_acc: 0.6343\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6166 - acc: 0.7530 - val_loss: 1.0116 - val_acc: 0.6400\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6212 - acc: 0.7530 - val_loss: 1.0083 - val_acc: 0.6400\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6175 - acc: 0.7524 - val_loss: 1.0059 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6221 - acc: 0.7498 - val_loss: 1.0229 - val_acc: 0.6343\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6082 - acc: 0.7607 - val_loss: 1.0095 - val_acc: 0.6457\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6138 - acc: 0.7645 - val_loss: 1.0170 - val_acc: 0.6457\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5957 - acc: 0.7620 - val_loss: 1.0160 - val_acc: 0.6571\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6039 - acc: 0.7518 - val_loss: 1.0173 - val_acc: 0.6457\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5988 - acc: 0.7639 - val_loss: 1.0472 - val_acc: 0.6571\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6127 - acc: 0.7601 - val_loss: 1.0360 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5957 - acc: 0.7569 - val_loss: 1.0425 - val_acc: 0.6400\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6178 - acc: 0.7498 - val_loss: 1.0395 - val_acc: 0.6400\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5875 - acc: 0.7575 - val_loss: 1.0317 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5827 - acc: 0.7709 - val_loss: 1.0423 - val_acc: 0.6400\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5834 - acc: 0.7709 - val_loss: 1.0571 - val_acc: 0.6286\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.5708 - acc: 0.7709 - val_loss: 1.0608 - val_acc: 0.6343\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5938 - acc: 0.7588 - val_loss: 1.0388 - val_acc: 0.6514\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5694 - acc: 0.7741 - val_loss: 1.0485 - val_acc: 0.6343\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5679 - acc: 0.7620 - val_loss: 1.0652 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.5896 - acc: 0.7626 - val_loss: 1.0763 - val_acc: 0.6400\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.5709 - acc: 0.7709 - val_loss: 1.0700 - val_acc: 0.6514\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.5397 - acc: 0.7894 - val_loss: 1.0683 - val_acc: 0.6286\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.5510 - acc: 0.7786 - val_loss: 1.0835 - val_acc: 0.6229\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5687 - acc: 0.7773 - val_loss: 1.0969 - val_acc: 0.6514\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5541 - acc: 0.7900 - val_loss: 1.0888 - val_acc: 0.6400\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.5487 - acc: 0.7869 - val_loss: 1.0911 - val_acc: 0.6343\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5537 - acc: 0.7786 - val_loss: 1.0803 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5312 - acc: 0.7888 - val_loss: 1.0909 - val_acc: 0.6400\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5349 - acc: 0.7798 - val_loss: 1.0966 - val_acc: 0.6343\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5340 - acc: 0.7894 - val_loss: 1.1011 - val_acc: 0.6343\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5404 - acc: 0.7913 - val_loss: 1.1104 - val_acc: 0.6400\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5169 - acc: 0.7977 - val_loss: 1.1007 - val_acc: 0.6343\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5467 - acc: 0.7856 - val_loss: 1.0962 - val_acc: 0.6343\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.5161 - acc: 0.7900 - val_loss: 1.0908 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.5268 - acc: 0.7971 - val_loss: 1.1051 - val_acc: 0.6343\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5324 - acc: 0.7900 - val_loss: 1.1039 - val_acc: 0.6571\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4996 - acc: 0.8015 - val_loss: 1.1221 - val_acc: 0.6514\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.5195 - acc: 0.8162 - val_loss: 1.1237 - val_acc: 0.6400\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4940 - acc: 0.8015 - val_loss: 1.1202 - val_acc: 0.6400\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5095 - acc: 0.7964 - val_loss: 1.1260 - val_acc: 0.6400\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5120 - acc: 0.7983 - val_loss: 1.1327 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5123 - acc: 0.8009 - val_loss: 1.1352 - val_acc: 0.6514\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5022 - acc: 0.7932 - val_loss: 1.1434 - val_acc: 0.6171\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5121 - acc: 0.8015 - val_loss: 1.1482 - val_acc: 0.6229\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5009 - acc: 0.8034 - val_loss: 1.1446 - val_acc: 0.6514\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.4669 - acc: 0.8149 - val_loss: 1.1574 - val_acc: 0.6286\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 119us/step - loss: 0.4961 - acc: 0.8066 - val_loss: 1.1768 - val_acc: 0.6286\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5016 - acc: 0.8117 - val_loss: 1.1831 - val_acc: 0.6343\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4956 - acc: 0.7977 - val_loss: 1.1696 - val_acc: 0.6171\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4970 - acc: 0.8073 - val_loss: 1.1844 - val_acc: 0.6286\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.5108 - acc: 0.8047 - val_loss: 1.1772 - val_acc: 0.6400\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.4754 - acc: 0.8156 - val_loss: 1.1875 - val_acc: 0.6343\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.4916 - acc: 0.8098 - val_loss: 1.1778 - val_acc: 0.6343\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.4809 - acc: 0.8156 - val_loss: 1.1655 - val_acc: 0.6629\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4545 - acc: 0.8302 - val_loss: 1.1547 - val_acc: 0.6514\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4565 - acc: 0.8296 - val_loss: 1.1746 - val_acc: 0.6514\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.4538 - acc: 0.8334 - val_loss: 1.1981 - val_acc: 0.6514\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4772 - acc: 0.8156 - val_loss: 1.1674 - val_acc: 0.6514\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4719 - acc: 0.8232 - val_loss: 1.1934 - val_acc: 0.6571\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4937 - acc: 0.8066 - val_loss: 1.1728 - val_acc: 0.6686\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.4456 - acc: 0.8328 - val_loss: 1.2073 - val_acc: 0.6571\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4406 - acc: 0.8277 - val_loss: 1.2207 - val_acc: 0.6514\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4598 - acc: 0.8200 - val_loss: 1.2269 - val_acc: 0.6400\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4569 - acc: 0.8411 - val_loss: 1.2284 - val_acc: 0.6400\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4471 - acc: 0.8322 - val_loss: 1.2016 - val_acc: 0.6457\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4320 - acc: 0.8322 - val_loss: 1.2180 - val_acc: 0.6343\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4538 - acc: 0.8360 - val_loss: 1.2158 - val_acc: 0.6343\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.4253 - acc: 0.8239 - val_loss: 1.2234 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.4674 - acc: 0.8073 - val_loss: 1.2174 - val_acc: 0.6343\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4647 - acc: 0.8271 - val_loss: 1.2276 - val_acc: 0.6229\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4288 - acc: 0.8430 - val_loss: 1.2347 - val_acc: 0.6286\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4703 - acc: 0.8188 - val_loss: 1.2158 - val_acc: 0.6457\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4217 - acc: 0.8392 - val_loss: 1.2212 - val_acc: 0.6229\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4391 - acc: 0.8283 - val_loss: 1.2404 - val_acc: 0.6286\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4246 - acc: 0.8373 - val_loss: 1.2705 - val_acc: 0.6286\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4376 - acc: 0.8264 - val_loss: 1.2350 - val_acc: 0.6286\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4359 - acc: 0.8437 - val_loss: 1.2361 - val_acc: 0.6343\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4226 - acc: 0.8417 - val_loss: 1.2429 - val_acc: 0.6400\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4100 - acc: 0.8392 - val_loss: 1.2468 - val_acc: 0.6229\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4373 - acc: 0.8341 - val_loss: 1.2737 - val_acc: 0.6229\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4386 - acc: 0.8302 - val_loss: 1.2615 - val_acc: 0.6229\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4416 - acc: 0.8347 - val_loss: 1.2574 - val_acc: 0.6229\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4069 - acc: 0.8488 - val_loss: 1.2758 - val_acc: 0.6514\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4055 - acc: 0.8526 - val_loss: 1.2795 - val_acc: 0.6171\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4129 - acc: 0.8437 - val_loss: 1.2853 - val_acc: 0.6343\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.4006 - acc: 0.8417 - val_loss: 1.2900 - val_acc: 0.6457\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4091 - acc: 0.8500 - val_loss: 1.2861 - val_acc: 0.6229\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.4103 - acc: 0.8424 - val_loss: 1.2685 - val_acc: 0.6343\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.4136 - acc: 0.8430 - val_loss: 1.2825 - val_acc: 0.6343\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4253 - acc: 0.8296 - val_loss: 1.3155 - val_acc: 0.6400\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.3952 - acc: 0.8551 - val_loss: 1.3048 - val_acc: 0.6229\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.3810 - acc: 0.8526 - val_loss: 1.2843 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 111us/step - loss: 0.4071 - acc: 0.8468 - val_loss: 1.2878 - val_acc: 0.6171\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.3949 - acc: 0.8532 - val_loss: 1.3047 - val_acc: 0.6457\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.3854 - acc: 0.8666 - val_loss: 1.3106 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 144us/step - loss: 0.3891 - acc: 0.8622 - val_loss: 1.2839 - val_acc: 0.6400\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.3769 - acc: 0.8583 - val_loss: 1.2984 - val_acc: 0.6343\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.3776 - acc: 0.8577 - val_loss: 1.3270 - val_acc: 0.6229\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 116us/step - loss: 0.4101 - acc: 0.8577 - val_loss: 1.3261 - val_acc: 0.6286\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.3816 - acc: 0.8602 - val_loss: 1.3345 - val_acc: 0.6114\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.3818 - acc: 0.8596 - val_loss: 1.3331 - val_acc: 0.6057\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4253 - acc: 0.8328 - val_loss: 1.3408 - val_acc: 0.6343\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 116us/step - loss: 0.3926 - acc: 0.8660 - val_loss: 1.3516 - val_acc: 0.6171\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.3695 - acc: 0.8743 - val_loss: 1.3631 - val_acc: 0.6057\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.3932 - acc: 0.8519 - val_loss: 1.3424 - val_acc: 0.6057\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.3995 - acc: 0.8558 - val_loss: 1.3700 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.4186 - acc: 0.8385 - val_loss: 1.3520 - val_acc: 0.6286\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.3833 - acc: 0.8417 - val_loss: 1.3535 - val_acc: 0.6400\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 142us/step - loss: 0.3996 - acc: 0.8551 - val_loss: 1.3426 - val_acc: 0.6286\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 112us/step - loss: 0.3736 - acc: 0.8526 - val_loss: 1.3656 - val_acc: 0.6171\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.3797 - acc: 0.8628 - val_loss: 1.3786 - val_acc: 0.6114\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.3835 - acc: 0.8602 - val_loss: 1.3676 - val_acc: 0.6229\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 138us/step - loss: 0.3644 - acc: 0.8775 - val_loss: 1.3678 - val_acc: 0.6229\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 121us/step - loss: 0.3633 - acc: 0.8666 - val_loss: 1.3711 - val_acc: 0.6286\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 158us/step - loss: 0.3840 - acc: 0.8647 - val_loss: 1.4177 - val_acc: 0.6286\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.3821 - acc: 0.8634 - val_loss: 1.4203 - val_acc: 0.6114\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.3846 - acc: 0.8622 - val_loss: 1.4070 - val_acc: 0.6057\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 126us/step - loss: 0.3886 - acc: 0.8628 - val_loss: 1.3746 - val_acc: 0.6343\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 123us/step - loss: 0.3574 - acc: 0.8705 - val_loss: 1.3951 - val_acc: 0.6229\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.3353 - acc: 0.8736 - val_loss: 1.4264 - val_acc: 0.6114\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 161us/step - loss: 0.3347 - acc: 0.8826 - val_loss: 1.4476 - val_acc: 0.6114\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.3533 - acc: 0.8749 - val_loss: 1.4406 - val_acc: 0.5943\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.3520 - acc: 0.8762 - val_loss: 1.4270 - val_acc: 0.6229\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3703 - acc: 0.8545 - val_loss: 1.4334 - val_acc: 0.6114\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3441 - acc: 0.8743 - val_loss: 1.4441 - val_acc: 0.6171\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.3769 - acc: 0.8641 - val_loss: 1.4580 - val_acc: 0.6114\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.3675 - acc: 0.8577 - val_loss: 1.4311 - val_acc: 0.6114\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.3826 - acc: 0.8596 - val_loss: 1.4417 - val_acc: 0.6229\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.3821 - acc: 0.8622 - val_loss: 1.4530 - val_acc: 0.6000\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.3403 - acc: 0.8762 - val_loss: 1.4425 - val_acc: 0.6229\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.3646 - acc: 0.8724 - val_loss: 1.4141 - val_acc: 0.6400\n",
      "193/193 [==============================] - 0s 205us/step\n",
      "1742/1742 [==============================] - 0s 103us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 7s 5ms/step - loss: 1.4369 - acc: 0.3318 - val_loss: 1.0953 - val_acc: 0.5714\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 118us/step - loss: 1.1834 - acc: 0.4729 - val_loss: 1.0178 - val_acc: 0.5943\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 1.1236 - acc: 0.5073 - val_loss: 0.9698 - val_acc: 0.6457\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.0631 - acc: 0.5501 - val_loss: 0.9433 - val_acc: 0.6229\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.0349 - acc: 0.5584 - val_loss: 0.9446 - val_acc: 0.6286\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.0119 - acc: 0.5622 - val_loss: 0.9315 - val_acc: 0.6229\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9827 - acc: 0.5705 - val_loss: 0.9163 - val_acc: 0.6571\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.9728 - acc: 0.5865 - val_loss: 0.9175 - val_acc: 0.6514\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9593 - acc: 0.5877 - val_loss: 0.9204 - val_acc: 0.6400\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.9598 - acc: 0.5916 - val_loss: 0.9020 - val_acc: 0.6629\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9550 - acc: 0.6088 - val_loss: 0.9110 - val_acc: 0.6400\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.9406 - acc: 0.6063 - val_loss: 0.9065 - val_acc: 0.6686\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8996 - acc: 0.6382 - val_loss: 0.9017 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9096 - acc: 0.6292 - val_loss: 0.9023 - val_acc: 0.6686\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8813 - acc: 0.6420 - val_loss: 0.9185 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8880 - acc: 0.6273 - val_loss: 0.9058 - val_acc: 0.6800\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8709 - acc: 0.6426 - val_loss: 0.9062 - val_acc: 0.6800\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8599 - acc: 0.6522 - val_loss: 0.9113 - val_acc: 0.6629\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 126us/step - loss: 0.8513 - acc: 0.6477 - val_loss: 0.9138 - val_acc: 0.6514\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.8557 - acc: 0.6548 - val_loss: 0.9092 - val_acc: 0.6629\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8482 - acc: 0.6535 - val_loss: 0.9143 - val_acc: 0.6743\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8573 - acc: 0.6388 - val_loss: 0.9215 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8547 - acc: 0.6356 - val_loss: 0.9228 - val_acc: 0.6514\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8319 - acc: 0.6643 - val_loss: 0.9152 - val_acc: 0.6629\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.8187 - acc: 0.6707 - val_loss: 0.9184 - val_acc: 0.6629\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.8114 - acc: 0.6611 - val_loss: 0.9206 - val_acc: 0.6629\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8235 - acc: 0.6586 - val_loss: 0.9247 - val_acc: 0.6343\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8069 - acc: 0.6784 - val_loss: 0.9274 - val_acc: 0.6457\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.8133 - acc: 0.6745 - val_loss: 0.9305 - val_acc: 0.6400\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8058 - acc: 0.6733 - val_loss: 0.9421 - val_acc: 0.6343\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7939 - acc: 0.6701 - val_loss: 0.9325 - val_acc: 0.6514\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7812 - acc: 0.6886 - val_loss: 0.9304 - val_acc: 0.6514\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7678 - acc: 0.6911 - val_loss: 0.9332 - val_acc: 0.6571\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.7788 - acc: 0.6777 - val_loss: 0.9418 - val_acc: 0.6514\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7784 - acc: 0.6911 - val_loss: 0.9377 - val_acc: 0.6629\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.7648 - acc: 0.6892 - val_loss: 0.9471 - val_acc: 0.6514\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.7671 - acc: 0.6918 - val_loss: 0.9522 - val_acc: 0.6457\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7600 - acc: 0.6899 - val_loss: 0.9571 - val_acc: 0.6457\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7658 - acc: 0.6803 - val_loss: 0.9635 - val_acc: 0.6400\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7575 - acc: 0.7020 - val_loss: 0.9601 - val_acc: 0.6343\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7498 - acc: 0.6924 - val_loss: 0.9557 - val_acc: 0.6514\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7328 - acc: 0.7154 - val_loss: 0.9541 - val_acc: 0.6457\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.7130 - acc: 0.7275 - val_loss: 0.9653 - val_acc: 0.6457\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 125us/step - loss: 0.7225 - acc: 0.7058 - val_loss: 0.9748 - val_acc: 0.6286\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 116us/step - loss: 0.7310 - acc: 0.7084 - val_loss: 0.9778 - val_acc: 0.6343\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.7304 - acc: 0.7007 - val_loss: 0.9833 - val_acc: 0.6400\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7117 - acc: 0.7077 - val_loss: 0.9822 - val_acc: 0.6400\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.7090 - acc: 0.7147 - val_loss: 0.9833 - val_acc: 0.6457\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 124us/step - loss: 0.6994 - acc: 0.7339 - val_loss: 0.9698 - val_acc: 0.6514\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.7047 - acc: 0.7173 - val_loss: 0.9779 - val_acc: 0.6286\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 136us/step - loss: 0.6955 - acc: 0.7173 - val_loss: 0.9873 - val_acc: 0.6171\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 122us/step - loss: 0.6712 - acc: 0.7269 - val_loss: 0.9888 - val_acc: 0.6171\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.6885 - acc: 0.7358 - val_loss: 0.9937 - val_acc: 0.6229\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.7096 - acc: 0.7192 - val_loss: 0.9914 - val_acc: 0.6286\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6753 - acc: 0.7275 - val_loss: 1.0043 - val_acc: 0.6457\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6778 - acc: 0.7428 - val_loss: 1.0021 - val_acc: 0.6400\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6904 - acc: 0.7403 - val_loss: 1.0006 - val_acc: 0.6457\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6705 - acc: 0.7403 - val_loss: 1.0029 - val_acc: 0.6343\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6682 - acc: 0.7364 - val_loss: 1.0027 - val_acc: 0.6343\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6556 - acc: 0.7288 - val_loss: 1.0087 - val_acc: 0.6457\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6592 - acc: 0.7454 - val_loss: 1.0176 - val_acc: 0.6343\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6582 - acc: 0.7441 - val_loss: 1.0211 - val_acc: 0.6229\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.6382 - acc: 0.7396 - val_loss: 1.0140 - val_acc: 0.6343\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6455 - acc: 0.7364 - val_loss: 1.0184 - val_acc: 0.6457\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6446 - acc: 0.7543 - val_loss: 1.0308 - val_acc: 0.6343\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6510 - acc: 0.7428 - val_loss: 1.0217 - val_acc: 0.6343\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6326 - acc: 0.7518 - val_loss: 1.0407 - val_acc: 0.6171\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6285 - acc: 0.7524 - val_loss: 1.0303 - val_acc: 0.6171\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 120us/step - loss: 0.6147 - acc: 0.7626 - val_loss: 1.0572 - val_acc: 0.6171\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 138us/step - loss: 0.6305 - acc: 0.7575 - val_loss: 1.0526 - val_acc: 0.6229\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6185 - acc: 0.7620 - val_loss: 1.0482 - val_acc: 0.6171\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5971 - acc: 0.7754 - val_loss: 1.0497 - val_acc: 0.6286\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.6016 - acc: 0.7671 - val_loss: 1.0443 - val_acc: 0.6229\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 124us/step - loss: 0.6286 - acc: 0.7524 - val_loss: 1.0618 - val_acc: 0.6229\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.6023 - acc: 0.7537 - val_loss: 1.0521 - val_acc: 0.6286\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5939 - acc: 0.7722 - val_loss: 1.0706 - val_acc: 0.6057\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.6131 - acc: 0.7626 - val_loss: 1.0756 - val_acc: 0.6286\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 120us/step - loss: 0.6119 - acc: 0.7658 - val_loss: 1.0858 - val_acc: 0.6229\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.5967 - acc: 0.7664 - val_loss: 1.0787 - val_acc: 0.6286\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 120us/step - loss: 0.6081 - acc: 0.7696 - val_loss: 1.0769 - val_acc: 0.6057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.5853 - acc: 0.7811 - val_loss: 1.0818 - val_acc: 0.6114\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 142us/step - loss: 0.5930 - acc: 0.7747 - val_loss: 1.0857 - val_acc: 0.6114\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 109us/step - loss: 0.6059 - acc: 0.7626 - val_loss: 1.0899 - val_acc: 0.6171\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - ETA: 0s - loss: 0.5803 - acc: 0.773 - 0s 129us/step - loss: 0.5823 - acc: 0.7728 - val_loss: 1.0868 - val_acc: 0.6229\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.5789 - acc: 0.7779 - val_loss: 1.0825 - val_acc: 0.6171\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 122us/step - loss: 0.5759 - acc: 0.7849 - val_loss: 1.0849 - val_acc: 0.6343\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 120us/step - loss: 0.5718 - acc: 0.7862 - val_loss: 1.0855 - val_acc: 0.6286\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 146us/step - loss: 0.5816 - acc: 0.7671 - val_loss: 1.0926 - val_acc: 0.6286\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 115us/step - loss: 0.5590 - acc: 0.7824 - val_loss: 1.0962 - val_acc: 0.6229\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5481 - acc: 0.7996 - val_loss: 1.1027 - val_acc: 0.6114\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 123us/step - loss: 0.5408 - acc: 0.7779 - val_loss: 1.1051 - val_acc: 0.6229\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 124us/step - loss: 0.5537 - acc: 0.7990 - val_loss: 1.1195 - val_acc: 0.6229\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 112us/step - loss: 0.5689 - acc: 0.7703 - val_loss: 1.1104 - val_acc: 0.6343\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 112us/step - loss: 0.5568 - acc: 0.7900 - val_loss: 1.1013 - val_acc: 0.6171\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5441 - acc: 0.7990 - val_loss: 1.1188 - val_acc: 0.6400\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5505 - acc: 0.7849 - val_loss: 1.1256 - val_acc: 0.6114\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5485 - acc: 0.7849 - val_loss: 1.1294 - val_acc: 0.6229\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5494 - acc: 0.7862 - val_loss: 1.1218 - val_acc: 0.6286\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5005 - acc: 0.8117 - val_loss: 1.1216 - val_acc: 0.6171\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5551 - acc: 0.7824 - val_loss: 1.1464 - val_acc: 0.6114\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5174 - acc: 0.7964 - val_loss: 1.1487 - val_acc: 0.6114\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5376 - acc: 0.7971 - val_loss: 1.1647 - val_acc: 0.6057\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5463 - acc: 0.7888 - val_loss: 1.1549 - val_acc: 0.6286\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5240 - acc: 0.7977 - val_loss: 1.1568 - val_acc: 0.6229\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.5106 - acc: 0.8009 - val_loss: 1.1584 - val_acc: 0.6229\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5039 - acc: 0.8130 - val_loss: 1.1618 - val_acc: 0.6229\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4887 - acc: 0.8130 - val_loss: 1.1534 - val_acc: 0.6286\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5105 - acc: 0.7996 - val_loss: 1.1637 - val_acc: 0.6114\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5100 - acc: 0.8028 - val_loss: 1.1615 - val_acc: 0.6343\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.4949 - acc: 0.8130 - val_loss: 1.1730 - val_acc: 0.6171\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4799 - acc: 0.8181 - val_loss: 1.1970 - val_acc: 0.6114\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5066 - acc: 0.8015 - val_loss: 1.2105 - val_acc: 0.6000\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5071 - acc: 0.8015 - val_loss: 1.2006 - val_acc: 0.6114\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5205 - acc: 0.8066 - val_loss: 1.1957 - val_acc: 0.6171\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 130us/step - loss: 0.5008 - acc: 0.8028 - val_loss: 1.1899 - val_acc: 0.6229\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 142us/step - loss: 0.4886 - acc: 0.8220 - val_loss: 1.2015 - val_acc: 0.6229\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.4943 - acc: 0.8200 - val_loss: 1.2102 - val_acc: 0.6057\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4914 - acc: 0.8143 - val_loss: 1.2050 - val_acc: 0.6114\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4946 - acc: 0.7990 - val_loss: 1.1985 - val_acc: 0.6114\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4884 - acc: 0.8162 - val_loss: 1.2116 - val_acc: 0.6114\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4766 - acc: 0.8232 - val_loss: 1.2100 - val_acc: 0.6114\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4775 - acc: 0.8213 - val_loss: 1.2009 - val_acc: 0.6114\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 110us/step - loss: 0.4853 - acc: 0.8143 - val_loss: 1.2334 - val_acc: 0.6057\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.4824 - acc: 0.8226 - val_loss: 1.2197 - val_acc: 0.6114\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.4589 - acc: 0.8302 - val_loss: 1.2210 - val_acc: 0.6057\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.4741 - acc: 0.8162 - val_loss: 1.2331 - val_acc: 0.6114\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4482 - acc: 0.8290 - val_loss: 1.2302 - val_acc: 0.6114\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4685 - acc: 0.8264 - val_loss: 1.2413 - val_acc: 0.6057\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4486 - acc: 0.8322 - val_loss: 1.2421 - val_acc: 0.6000\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.4561 - acc: 0.8302 - val_loss: 1.2500 - val_acc: 0.6114\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4701 - acc: 0.8130 - val_loss: 1.2589 - val_acc: 0.6057\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4648 - acc: 0.8277 - val_loss: 1.2554 - val_acc: 0.6171\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.4708 - acc: 0.8117 - val_loss: 1.2383 - val_acc: 0.6114\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.4749 - acc: 0.8207 - val_loss: 1.2539 - val_acc: 0.6229\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.4694 - acc: 0.8258 - val_loss: 1.2609 - val_acc: 0.6114\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.4527 - acc: 0.8424 - val_loss: 1.2565 - val_acc: 0.6057\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.4564 - acc: 0.8258 - val_loss: 1.2500 - val_acc: 0.6171\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4452 - acc: 0.8443 - val_loss: 1.2476 - val_acc: 0.6286\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4345 - acc: 0.8392 - val_loss: 1.2633 - val_acc: 0.6229\n",
      "Epoch 140/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4448 - acc: 0.8354 - val_loss: 1.2779 - val_acc: 0.6114\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4404 - acc: 0.8373 - val_loss: 1.2700 - val_acc: 0.6171\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4468 - acc: 0.8341 - val_loss: 1.2722 - val_acc: 0.6171\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4482 - acc: 0.8379 - val_loss: 1.2822 - val_acc: 0.6057\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4338 - acc: 0.8424 - val_loss: 1.2701 - val_acc: 0.6343\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4322 - acc: 0.8354 - val_loss: 1.2759 - val_acc: 0.6171\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4314 - acc: 0.8385 - val_loss: 1.2866 - val_acc: 0.6114\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4177 - acc: 0.8424 - val_loss: 1.2859 - val_acc: 0.6229\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4413 - acc: 0.8328 - val_loss: 1.2944 - val_acc: 0.6229\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4112 - acc: 0.8360 - val_loss: 1.2848 - val_acc: 0.6343\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4546 - acc: 0.8315 - val_loss: 1.3074 - val_acc: 0.5943\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4359 - acc: 0.8424 - val_loss: 1.3205 - val_acc: 0.6229\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4057 - acc: 0.8571 - val_loss: 1.3194 - val_acc: 0.6286\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4083 - acc: 0.8392 - val_loss: 1.3139 - val_acc: 0.6286\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4299 - acc: 0.8411 - val_loss: 1.3199 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4064 - acc: 0.8596 - val_loss: 1.3204 - val_acc: 0.6229\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4196 - acc: 0.8379 - val_loss: 1.3314 - val_acc: 0.6286\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4041 - acc: 0.8488 - val_loss: 1.3261 - val_acc: 0.6114\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4081 - acc: 0.8500 - val_loss: 1.3385 - val_acc: 0.6171\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.3979 - acc: 0.8647 - val_loss: 1.3328 - val_acc: 0.6057\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.3993 - acc: 0.8519 - val_loss: 1.3305 - val_acc: 0.6229\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.3983 - acc: 0.8590 - val_loss: 1.3455 - val_acc: 0.6286\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4066 - acc: 0.8545 - val_loss: 1.3461 - val_acc: 0.6229\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4003 - acc: 0.8513 - val_loss: 1.3380 - val_acc: 0.6400\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4137 - acc: 0.8500 - val_loss: 1.3496 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4108 - acc: 0.8437 - val_loss: 1.3485 - val_acc: 0.6229\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4056 - acc: 0.8405 - val_loss: 1.3599 - val_acc: 0.6229\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4066 - acc: 0.8373 - val_loss: 1.3458 - val_acc: 0.6229\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.3831 - acc: 0.8634 - val_loss: 1.3655 - val_acc: 0.6229\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3958 - acc: 0.8539 - val_loss: 1.3849 - val_acc: 0.6171\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.3624 - acc: 0.8724 - val_loss: 1.3609 - val_acc: 0.6114\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 110us/step - loss: 0.3927 - acc: 0.8590 - val_loss: 1.3761 - val_acc: 0.5943\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.3726 - acc: 0.8730 - val_loss: 1.3687 - val_acc: 0.6000\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.3787 - acc: 0.8602 - val_loss: 1.3796 - val_acc: 0.6000\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 111us/step - loss: 0.3853 - acc: 0.8551 - val_loss: 1.3942 - val_acc: 0.6229\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.3992 - acc: 0.8449 - val_loss: 1.3946 - val_acc: 0.6057\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.3710 - acc: 0.8653 - val_loss: 1.3812 - val_acc: 0.6229\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.3534 - acc: 0.8736 - val_loss: 1.3738 - val_acc: 0.6057\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.3698 - acc: 0.8634 - val_loss: 1.3726 - val_acc: 0.6114\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.3926 - acc: 0.8609 - val_loss: 1.3873 - val_acc: 0.6057\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3846 - acc: 0.8756 - val_loss: 1.3967 - val_acc: 0.6114\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.3744 - acc: 0.8641 - val_loss: 1.3907 - val_acc: 0.6114\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.3787 - acc: 0.8564 - val_loss: 1.3984 - val_acc: 0.6057\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3716 - acc: 0.8615 - val_loss: 1.4125 - val_acc: 0.6114\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.3552 - acc: 0.8705 - val_loss: 1.4297 - val_acc: 0.5943\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3502 - acc: 0.8634 - val_loss: 1.4237 - val_acc: 0.5943\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3685 - acc: 0.8705 - val_loss: 1.4216 - val_acc: 0.6114\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.3976 - acc: 0.8590 - val_loss: 1.4173 - val_acc: 0.6114\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3658 - acc: 0.8641 - val_loss: 1.4120 - val_acc: 0.6057\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.3695 - acc: 0.8609 - val_loss: 1.4087 - val_acc: 0.6114\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.3600 - acc: 0.8717 - val_loss: 1.4022 - val_acc: 0.6171\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.3486 - acc: 0.8736 - val_loss: 1.4090 - val_acc: 0.5886\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.3712 - acc: 0.8736 - val_loss: 1.4352 - val_acc: 0.5943\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 116us/step - loss: 0.3789 - acc: 0.8590 - val_loss: 1.4400 - val_acc: 0.5943\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.3361 - acc: 0.8781 - val_loss: 1.4474 - val_acc: 0.6114\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.3852 - acc: 0.8653 - val_loss: 1.4337 - val_acc: 0.6000\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.3813 - acc: 0.8749 - val_loss: 1.4534 - val_acc: 0.6114\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.3381 - acc: 0.8743 - val_loss: 1.4484 - val_acc: 0.6000\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.3530 - acc: 0.8685 - val_loss: 1.4472 - val_acc: 0.6057\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 126us/step - loss: 0.3852 - acc: 0.8564 - val_loss: 1.4632 - val_acc: 0.5886\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.3455 - acc: 0.8705 - val_loss: 1.4500 - val_acc: 0.6057\n",
      "193/193 [==============================] - 0s 99us/step\n",
      "1742/1742 [==============================] - 0s 64us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 7s 4ms/step - loss: 1.3998 - acc: 0.3440 - val_loss: 1.1681 - val_acc: 0.4800\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 1.1982 - acc: 0.4633 - val_loss: 1.0726 - val_acc: 0.5143\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.1205 - acc: 0.5112 - val_loss: 1.0417 - val_acc: 0.5486\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.0694 - acc: 0.5386 - val_loss: 1.0122 - val_acc: 0.5371\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 1.0339 - acc: 0.5546 - val_loss: 0.9925 - val_acc: 0.5429\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.0092 - acc: 0.5712 - val_loss: 0.9748 - val_acc: 0.5714\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9967 - acc: 0.5865 - val_loss: 0.9822 - val_acc: 0.5943\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9707 - acc: 0.5935 - val_loss: 0.9752 - val_acc: 0.5543\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9627 - acc: 0.5992 - val_loss: 0.9624 - val_acc: 0.5886\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9583 - acc: 0.5954 - val_loss: 0.9637 - val_acc: 0.5657\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.9288 - acc: 0.6190 - val_loss: 0.9672 - val_acc: 0.5600\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.9190 - acc: 0.6209 - val_loss: 0.9651 - val_acc: 0.5771\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9130 - acc: 0.6158 - val_loss: 0.9770 - val_acc: 0.5714\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9119 - acc: 0.6248 - val_loss: 0.9625 - val_acc: 0.5771\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8997 - acc: 0.6235 - val_loss: 0.9590 - val_acc: 0.5886\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.8967 - acc: 0.6477 - val_loss: 0.9655 - val_acc: 0.5943\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8711 - acc: 0.6343 - val_loss: 0.9851 - val_acc: 0.5714\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.8742 - acc: 0.6362 - val_loss: 0.9680 - val_acc: 0.5886\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8526 - acc: 0.6509 - val_loss: 0.9747 - val_acc: 0.5886\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.8538 - acc: 0.6465 - val_loss: 0.9646 - val_acc: 0.5943\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8466 - acc: 0.6522 - val_loss: 0.9709 - val_acc: 0.6000\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8157 - acc: 0.6809 - val_loss: 0.9710 - val_acc: 0.5943\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8343 - acc: 0.6496 - val_loss: 0.9810 - val_acc: 0.5771\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.8311 - acc: 0.6599 - val_loss: 0.9839 - val_acc: 0.6057\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8215 - acc: 0.6541 - val_loss: 0.9760 - val_acc: 0.6000\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.8197 - acc: 0.6624 - val_loss: 0.9969 - val_acc: 0.5886\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8173 - acc: 0.6726 - val_loss: 0.9845 - val_acc: 0.6057\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.8146 - acc: 0.6650 - val_loss: 0.9838 - val_acc: 0.6114\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.8157 - acc: 0.6637 - val_loss: 1.0010 - val_acc: 0.6114\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.8022 - acc: 0.6662 - val_loss: 0.9960 - val_acc: 0.6171\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7801 - acc: 0.6777 - val_loss: 0.9964 - val_acc: 0.5886\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.7836 - acc: 0.6790 - val_loss: 1.0032 - val_acc: 0.6057\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7576 - acc: 0.6873 - val_loss: 1.0133 - val_acc: 0.6171\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7739 - acc: 0.6733 - val_loss: 0.9959 - val_acc: 0.6057\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.7840 - acc: 0.6828 - val_loss: 1.0017 - val_acc: 0.6171\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7554 - acc: 0.7033 - val_loss: 1.0072 - val_acc: 0.6229\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7657 - acc: 0.6950 - val_loss: 1.0059 - val_acc: 0.6171\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7439 - acc: 0.6937 - val_loss: 1.0210 - val_acc: 0.5943\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7625 - acc: 0.6975 - val_loss: 1.0227 - val_acc: 0.6000\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7261 - acc: 0.7071 - val_loss: 1.0116 - val_acc: 0.5943\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7406 - acc: 0.7045 - val_loss: 1.0258 - val_acc: 0.6000\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.7173 - acc: 0.7064 - val_loss: 1.0322 - val_acc: 0.6057\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7272 - acc: 0.7141 - val_loss: 1.0191 - val_acc: 0.6000\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.7174 - acc: 0.7103 - val_loss: 1.0416 - val_acc: 0.6000\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7286 - acc: 0.7103 - val_loss: 1.0420 - val_acc: 0.6171\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7201 - acc: 0.7243 - val_loss: 1.0282 - val_acc: 0.6114\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7168 - acc: 0.7173 - val_loss: 1.0265 - val_acc: 0.6000\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7308 - acc: 0.7071 - val_loss: 1.0351 - val_acc: 0.5886\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7112 - acc: 0.7179 - val_loss: 1.0336 - val_acc: 0.6057\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6900 - acc: 0.7250 - val_loss: 1.0501 - val_acc: 0.6171\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 109us/step - loss: 0.7120 - acc: 0.7147 - val_loss: 1.0528 - val_acc: 0.5829\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.6892 - acc: 0.7224 - val_loss: 1.0524 - val_acc: 0.5886\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7001 - acc: 0.7269 - val_loss: 1.0642 - val_acc: 0.5886\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.6921 - acc: 0.7186 - val_loss: 1.0626 - val_acc: 0.5829\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6765 - acc: 0.7422 - val_loss: 1.0593 - val_acc: 0.5829\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6664 - acc: 0.7339 - val_loss: 1.0536 - val_acc: 0.5943\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6641 - acc: 0.7409 - val_loss: 1.0655 - val_acc: 0.5886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6722 - acc: 0.7250 - val_loss: 1.0708 - val_acc: 0.5829\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6720 - acc: 0.7250 - val_loss: 1.0709 - val_acc: 0.5943\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.6828 - acc: 0.7224 - val_loss: 1.0711 - val_acc: 0.5886\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6601 - acc: 0.7371 - val_loss: 1.0662 - val_acc: 0.5829\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.6408 - acc: 0.7562 - val_loss: 1.0795 - val_acc: 0.5771\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6649 - acc: 0.7466 - val_loss: 1.0907 - val_acc: 0.5886\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6501 - acc: 0.7409 - val_loss: 1.0867 - val_acc: 0.5886\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6596 - acc: 0.7320 - val_loss: 1.0968 - val_acc: 0.5943\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6467 - acc: 0.7320 - val_loss: 1.1095 - val_acc: 0.5943\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6332 - acc: 0.7479 - val_loss: 1.1031 - val_acc: 0.5829\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.6350 - acc: 0.7492 - val_loss: 1.1070 - val_acc: 0.5771\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6291 - acc: 0.7607 - val_loss: 1.1098 - val_acc: 0.5943\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6360 - acc: 0.7473 - val_loss: 1.1187 - val_acc: 0.5829\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 99us/step - loss: 0.6308 - acc: 0.7530 - val_loss: 1.1158 - val_acc: 0.5886\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.6311 - acc: 0.7518 - val_loss: 1.1292 - val_acc: 0.5829\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.6150 - acc: 0.7594 - val_loss: 1.1068 - val_acc: 0.5714\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 118us/step - loss: 0.5897 - acc: 0.7588 - val_loss: 1.1178 - val_acc: 0.5829\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6078 - acc: 0.7613 - val_loss: 1.1277 - val_acc: 0.5771\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 106us/step - loss: 0.5929 - acc: 0.7773 - val_loss: 1.1394 - val_acc: 0.6000\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5875 - acc: 0.7754 - val_loss: 1.1485 - val_acc: 0.5771\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 105us/step - loss: 0.6278 - acc: 0.7384 - val_loss: 1.1360 - val_acc: 0.5829\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 96us/step - loss: 0.5953 - acc: 0.7601 - val_loss: 1.1405 - val_acc: 0.5943\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5893 - acc: 0.7875 - val_loss: 1.1374 - val_acc: 0.5714\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 108us/step - loss: 0.5848 - acc: 0.7722 - val_loss: 1.1363 - val_acc: 0.5771\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.5800 - acc: 0.7728 - val_loss: 1.1341 - val_acc: 0.5657\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.6032 - acc: 0.7632 - val_loss: 1.1389 - val_acc: 0.5829\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5801 - acc: 0.7754 - val_loss: 1.1491 - val_acc: 0.5771\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5778 - acc: 0.7683 - val_loss: 1.1442 - val_acc: 0.5886\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.5750 - acc: 0.7741 - val_loss: 1.1466 - val_acc: 0.5886\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5545 - acc: 0.7786 - val_loss: 1.1598 - val_acc: 0.5886\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5746 - acc: 0.7626 - val_loss: 1.1502 - val_acc: 0.5771\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5828 - acc: 0.7696 - val_loss: 1.1433 - val_acc: 0.5829\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5741 - acc: 0.7792 - val_loss: 1.1633 - val_acc: 0.5714\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.5474 - acc: 0.7837 - val_loss: 1.1680 - val_acc: 0.5657\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.5509 - acc: 0.7856 - val_loss: 1.1596 - val_acc: 0.5886\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5488 - acc: 0.7779 - val_loss: 1.1438 - val_acc: 0.6000\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 128us/step - loss: 0.5566 - acc: 0.7690 - val_loss: 1.1642 - val_acc: 0.5829\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.5639 - acc: 0.7792 - val_loss: 1.1712 - val_acc: 0.5829\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 136us/step - loss: 0.5538 - acc: 0.7951 - val_loss: 1.1706 - val_acc: 0.5886\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 114us/step - loss: 0.5477 - acc: 0.7792 - val_loss: 1.1750 - val_acc: 0.5886\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 120us/step - loss: 0.5344 - acc: 0.7951 - val_loss: 1.1721 - val_acc: 0.5886\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 107us/step - loss: 0.5311 - acc: 0.7996 - val_loss: 1.2082 - val_acc: 0.5714\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5155 - acc: 0.8066 - val_loss: 1.1914 - val_acc: 0.5829\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5349 - acc: 0.8003 - val_loss: 1.2054 - val_acc: 0.5714\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.5410 - acc: 0.7805 - val_loss: 1.1998 - val_acc: 0.5771\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5262 - acc: 0.7939 - val_loss: 1.1899 - val_acc: 0.5829\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5522 - acc: 0.7856 - val_loss: 1.2056 - val_acc: 0.5829\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5138 - acc: 0.7990 - val_loss: 1.2131 - val_acc: 0.5714\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.5242 - acc: 0.7996 - val_loss: 1.2011 - val_acc: 0.5886\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4968 - acc: 0.8137 - val_loss: 1.2053 - val_acc: 0.5829\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5198 - acc: 0.8079 - val_loss: 1.2125 - val_acc: 0.5829\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5162 - acc: 0.7951 - val_loss: 1.2304 - val_acc: 0.5943\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5159 - acc: 0.8003 - val_loss: 1.2215 - val_acc: 0.6000\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.5261 - acc: 0.8066 - val_loss: 1.2220 - val_acc: 0.5771\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.5247 - acc: 0.8079 - val_loss: 1.2295 - val_acc: 0.5886\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5071 - acc: 0.8066 - val_loss: 1.2426 - val_acc: 0.5829\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.5001 - acc: 0.8003 - val_loss: 1.2506 - val_acc: 0.5771\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4973 - acc: 0.8034 - val_loss: 1.2281 - val_acc: 0.5943\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4933 - acc: 0.8175 - val_loss: 1.2356 - val_acc: 0.5886\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4963 - acc: 0.8022 - val_loss: 1.2341 - val_acc: 0.6000\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 113us/step - loss: 0.4913 - acc: 0.8181 - val_loss: 1.2403 - val_acc: 0.5829\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4620 - acc: 0.8226 - val_loss: 1.2403 - val_acc: 0.5886\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4944 - acc: 0.8009 - val_loss: 1.2418 - val_acc: 0.5943\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4858 - acc: 0.8168 - val_loss: 1.2410 - val_acc: 0.5943\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4927 - acc: 0.8188 - val_loss: 1.2609 - val_acc: 0.5943\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4612 - acc: 0.8354 - val_loss: 1.2699 - val_acc: 0.5886\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4780 - acc: 0.8207 - val_loss: 1.2673 - val_acc: 0.6000\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4881 - acc: 0.8060 - val_loss: 1.2477 - val_acc: 0.5829\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4706 - acc: 0.8251 - val_loss: 1.2495 - val_acc: 0.5714\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.4738 - acc: 0.8232 - val_loss: 1.2602 - val_acc: 0.5886\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4520 - acc: 0.8277 - val_loss: 1.2867 - val_acc: 0.5714\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4645 - acc: 0.8226 - val_loss: 1.2911 - val_acc: 0.5771\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.4534 - acc: 0.8245 - val_loss: 1.2721 - val_acc: 0.5829\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4523 - acc: 0.8290 - val_loss: 1.3100 - val_acc: 0.5657\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4387 - acc: 0.8258 - val_loss: 1.3051 - val_acc: 0.5657\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.4749 - acc: 0.8181 - val_loss: 1.3094 - val_acc: 0.5714\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4730 - acc: 0.8194 - val_loss: 1.3086 - val_acc: 0.5600\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4340 - acc: 0.8456 - val_loss: 1.2883 - val_acc: 0.6000\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 110us/step - loss: 0.4815 - acc: 0.8283 - val_loss: 1.3062 - val_acc: 0.5829\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4626 - acc: 0.8290 - val_loss: 1.3054 - val_acc: 0.5771\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.4554 - acc: 0.8315 - val_loss: 1.3109 - val_acc: 0.5600\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 95us/step - loss: 0.4619 - acc: 0.8315 - val_loss: 1.3172 - val_acc: 0.5771\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4448 - acc: 0.8379 - val_loss: 1.3188 - val_acc: 0.5543\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.4400 - acc: 0.8411 - val_loss: 1.3501 - val_acc: 0.5600\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.4604 - acc: 0.8302 - val_loss: 1.3064 - val_acc: 0.5886\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.4866 - acc: 0.8200 - val_loss: 1.3223 - val_acc: 0.6000\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4513 - acc: 0.8239 - val_loss: 1.3125 - val_acc: 0.6000\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4380 - acc: 0.8462 - val_loss: 1.3184 - val_acc: 0.5943\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4405 - acc: 0.8392 - val_loss: 1.3040 - val_acc: 0.6000\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.4330 - acc: 0.8392 - val_loss: 1.3186 - val_acc: 0.5943\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4470 - acc: 0.8462 - val_loss: 1.3096 - val_acc: 0.6000\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4032 - acc: 0.8539 - val_loss: 1.3358 - val_acc: 0.6057\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.4173 - acc: 0.8564 - val_loss: 1.3542 - val_acc: 0.5886\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.4373 - acc: 0.8379 - val_loss: 1.3465 - val_acc: 0.5771\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4011 - acc: 0.8583 - val_loss: 1.3512 - val_acc: 0.6000\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.4237 - acc: 0.8354 - val_loss: 1.3613 - val_acc: 0.5943\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4127 - acc: 0.8328 - val_loss: 1.3509 - val_acc: 0.6114\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.4260 - acc: 0.8315 - val_loss: 1.3780 - val_acc: 0.6000\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4145 - acc: 0.8373 - val_loss: 1.3708 - val_acc: 0.6000\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4032 - acc: 0.8519 - val_loss: 1.3730 - val_acc: 0.5829\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 103us/step - loss: 0.4172 - acc: 0.8475 - val_loss: 1.3604 - val_acc: 0.5714\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.4009 - acc: 0.8526 - val_loss: 1.3567 - val_acc: 0.5886\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4288 - acc: 0.8449 - val_loss: 1.3852 - val_acc: 0.5771\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4183 - acc: 0.8468 - val_loss: 1.3786 - val_acc: 0.5771\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.4221 - acc: 0.8590 - val_loss: 1.3849 - val_acc: 0.5771\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4030 - acc: 0.8456 - val_loss: 1.3915 - val_acc: 0.5714\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.4181 - acc: 0.8449 - val_loss: 1.4016 - val_acc: 0.5829\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.4056 - acc: 0.8564 - val_loss: 1.3831 - val_acc: 0.5886\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.4046 - acc: 0.8564 - val_loss: 1.3777 - val_acc: 0.6000\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.4122 - acc: 0.8488 - val_loss: 1.3958 - val_acc: 0.5886\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.4152 - acc: 0.8494 - val_loss: 1.3729 - val_acc: 0.5943\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.4227 - acc: 0.8475 - val_loss: 1.3776 - val_acc: 0.6000\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.4025 - acc: 0.8571 - val_loss: 1.3869 - val_acc: 0.5886\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 88us/step - loss: 0.3979 - acc: 0.8449 - val_loss: 1.3824 - val_acc: 0.6057\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4086 - acc: 0.8507 - val_loss: 1.3976 - val_acc: 0.5771\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.4090 - acc: 0.8500 - val_loss: 1.3982 - val_acc: 0.5943\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.4005 - acc: 0.8519 - val_loss: 1.4117 - val_acc: 0.5886\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.3999 - acc: 0.8615 - val_loss: 1.3972 - val_acc: 0.5829\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 107us/step - loss: 0.3931 - acc: 0.8666 - val_loss: 1.4109 - val_acc: 0.5943\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.3663 - acc: 0.8749 - val_loss: 1.4139 - val_acc: 0.5943\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.3882 - acc: 0.8647 - val_loss: 1.4291 - val_acc: 0.5886\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.3907 - acc: 0.8558 - val_loss: 1.4252 - val_acc: 0.5886\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.4023 - acc: 0.8634 - val_loss: 1.4364 - val_acc: 0.5829\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.4127 - acc: 0.8590 - val_loss: 1.4340 - val_acc: 0.5829\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.3968 - acc: 0.8602 - val_loss: 1.4224 - val_acc: 0.5829\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3899 - acc: 0.8653 - val_loss: 1.4430 - val_acc: 0.5829\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.3869 - acc: 0.8571 - val_loss: 1.4430 - val_acc: 0.5829\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.3665 - acc: 0.8641 - val_loss: 1.4264 - val_acc: 0.5829\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3721 - acc: 0.8660 - val_loss: 1.4378 - val_acc: 0.5886\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.3836 - acc: 0.8583 - val_loss: 1.4460 - val_acc: 0.5771\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.3756 - acc: 0.8679 - val_loss: 1.4609 - val_acc: 0.5943\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.3939 - acc: 0.8519 - val_loss: 1.4680 - val_acc: 0.6057\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.3770 - acc: 0.8736 - val_loss: 1.4409 - val_acc: 0.5771\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3769 - acc: 0.8615 - val_loss: 1.4630 - val_acc: 0.5886\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.3731 - acc: 0.8660 - val_loss: 1.4595 - val_acc: 0.6000\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.3663 - acc: 0.8705 - val_loss: 1.4807 - val_acc: 0.5886\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.3674 - acc: 0.8743 - val_loss: 1.4573 - val_acc: 0.5943\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.3866 - acc: 0.8692 - val_loss: 1.4737 - val_acc: 0.5886\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3555 - acc: 0.8660 - val_loss: 1.4879 - val_acc: 0.5829\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.3546 - acc: 0.8724 - val_loss: 1.4495 - val_acc: 0.5943\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.3463 - acc: 0.8832 - val_loss: 1.4546 - val_acc: 0.6000\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.3595 - acc: 0.8711 - val_loss: 1.4543 - val_acc: 0.6000\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.3406 - acc: 0.8787 - val_loss: 1.4538 - val_acc: 0.5829\n",
      "193/193 [==============================] - 0s 72us/step\n",
      "1742/1742 [==============================] - 0s 69us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 7s 4ms/step - loss: 1.5484 - acc: 0.2874 - val_loss: 1.2616 - val_acc: 0.4229\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.3736 - acc: 0.3570 - val_loss: 1.1582 - val_acc: 0.5600\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.3018 - acc: 0.3978 - val_loss: 1.1013 - val_acc: 0.5886\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 1.2227 - acc: 0.4649 - val_loss: 1.0639 - val_acc: 0.5943\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.2085 - acc: 0.4598 - val_loss: 1.0337 - val_acc: 0.5829\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.1613 - acc: 0.4872 - val_loss: 1.0135 - val_acc: 0.6229\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1402 - acc: 0.4802 - val_loss: 0.9950 - val_acc: 0.6114\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1264 - acc: 0.5096 - val_loss: 0.9799 - val_acc: 0.6171\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 1.0870 - acc: 0.5153 - val_loss: 0.9695 - val_acc: 0.6400\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 1.0799 - acc: 0.5211 - val_loss: 0.9617 - val_acc: 0.6514\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0668 - acc: 0.5351 - val_loss: 0.9535 - val_acc: 0.6514\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 1.0557 - acc: 0.5524 - val_loss: 0.9444 - val_acc: 0.6514\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 1.0426 - acc: 0.5415 - val_loss: 0.9388 - val_acc: 0.6686\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.0347 - acc: 0.5562 - val_loss: 0.9384 - val_acc: 0.6457\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0116 - acc: 0.5715 - val_loss: 0.9285 - val_acc: 0.6686\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 1.0106 - acc: 0.5722 - val_loss: 0.9243 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9957 - acc: 0.5677 - val_loss: 0.9155 - val_acc: 0.6800\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 1.0045 - acc: 0.5709 - val_loss: 0.9102 - val_acc: 0.6686\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9859 - acc: 0.5862 - val_loss: 0.9115 - val_acc: 0.6514\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9895 - acc: 0.5734 - val_loss: 0.9091 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9635 - acc: 0.5798 - val_loss: 0.9056 - val_acc: 0.6629\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9544 - acc: 0.6022 - val_loss: 0.9020 - val_acc: 0.6629\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9609 - acc: 0.6047 - val_loss: 0.8979 - val_acc: 0.6686\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9400 - acc: 0.6188 - val_loss: 0.8979 - val_acc: 0.6686\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.9466 - acc: 0.6047 - val_loss: 0.8976 - val_acc: 0.6457\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9355 - acc: 0.6162 - val_loss: 0.8954 - val_acc: 0.6571\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9272 - acc: 0.6169 - val_loss: 0.8865 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9262 - acc: 0.6105 - val_loss: 0.8874 - val_acc: 0.6686\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9416 - acc: 0.6041 - val_loss: 0.8886 - val_acc: 0.6686\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9282 - acc: 0.6277 - val_loss: 0.8886 - val_acc: 0.6686\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.9333 - acc: 0.6098 - val_loss: 0.8868 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8888 - acc: 0.6315 - val_loss: 0.8863 - val_acc: 0.6743\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9092 - acc: 0.6207 - val_loss: 0.8819 - val_acc: 0.6743\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9202 - acc: 0.6149 - val_loss: 0.8757 - val_acc: 0.6971\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9171 - acc: 0.6054 - val_loss: 0.8810 - val_acc: 0.6857\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8879 - acc: 0.6194 - val_loss: 0.8858 - val_acc: 0.6857\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8936 - acc: 0.6373 - val_loss: 0.8869 - val_acc: 0.6800\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8885 - acc: 0.6226 - val_loss: 0.8853 - val_acc: 0.6514\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.8993 - acc: 0.6213 - val_loss: 0.8847 - val_acc: 0.6629\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8924 - acc: 0.6335 - val_loss: 0.8843 - val_acc: 0.6629\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8824 - acc: 0.6290 - val_loss: 0.8859 - val_acc: 0.6514\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8730 - acc: 0.6424 - val_loss: 0.8883 - val_acc: 0.6457\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.9017 - acc: 0.6003 - val_loss: 0.8861 - val_acc: 0.6686\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8563 - acc: 0.6520 - val_loss: 0.8868 - val_acc: 0.6686\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8564 - acc: 0.6462 - val_loss: 0.8864 - val_acc: 0.6629\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8704 - acc: 0.6309 - val_loss: 0.8892 - val_acc: 0.6514\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8776 - acc: 0.6258 - val_loss: 0.8947 - val_acc: 0.6629\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8415 - acc: 0.6520 - val_loss: 0.8891 - val_acc: 0.6743\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8649 - acc: 0.6335 - val_loss: 0.8934 - val_acc: 0.6629\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8589 - acc: 0.6481 - val_loss: 0.8916 - val_acc: 0.6686\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8598 - acc: 0.6475 - val_loss: 0.8929 - val_acc: 0.6629\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8588 - acc: 0.6328 - val_loss: 0.8911 - val_acc: 0.6686\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8531 - acc: 0.6520 - val_loss: 0.8905 - val_acc: 0.6343\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8355 - acc: 0.6437 - val_loss: 0.8963 - val_acc: 0.6457\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8473 - acc: 0.6539 - val_loss: 0.8871 - val_acc: 0.6686\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8464 - acc: 0.6437 - val_loss: 0.8885 - val_acc: 0.6743\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8504 - acc: 0.6596 - val_loss: 0.8943 - val_acc: 0.6514\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8271 - acc: 0.6526 - val_loss: 0.8937 - val_acc: 0.6571\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8291 - acc: 0.6507 - val_loss: 0.8942 - val_acc: 0.6571\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8389 - acc: 0.6584 - val_loss: 0.8920 - val_acc: 0.6629\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8342 - acc: 0.6596 - val_loss: 0.8931 - val_acc: 0.6629\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8364 - acc: 0.6475 - val_loss: 0.8926 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8146 - acc: 0.6641 - val_loss: 0.8888 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8478 - acc: 0.6488 - val_loss: 0.8944 - val_acc: 0.6400\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8332 - acc: 0.6545 - val_loss: 0.8916 - val_acc: 0.6571\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8240 - acc: 0.6590 - val_loss: 0.8889 - val_acc: 0.6800\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8279 - acc: 0.6488 - val_loss: 0.8962 - val_acc: 0.6571\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8243 - acc: 0.6552 - val_loss: 0.8951 - val_acc: 0.6571\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8235 - acc: 0.6571 - val_loss: 0.8926 - val_acc: 0.6571\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8253 - acc: 0.6718 - val_loss: 0.8925 - val_acc: 0.6686\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8160 - acc: 0.6679 - val_loss: 0.8978 - val_acc: 0.6629\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8004 - acc: 0.6775 - val_loss: 0.9064 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8121 - acc: 0.6692 - val_loss: 0.9098 - val_acc: 0.6457\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8044 - acc: 0.6590 - val_loss: 0.9088 - val_acc: 0.6343\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8007 - acc: 0.6782 - val_loss: 0.9069 - val_acc: 0.6400\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8032 - acc: 0.6628 - val_loss: 0.9069 - val_acc: 0.6457\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7924 - acc: 0.6814 - val_loss: 0.9076 - val_acc: 0.6457\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7861 - acc: 0.6782 - val_loss: 0.9036 - val_acc: 0.6629\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7976 - acc: 0.6794 - val_loss: 0.9023 - val_acc: 0.6514\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7861 - acc: 0.6807 - val_loss: 0.9007 - val_acc: 0.6514\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7880 - acc: 0.6839 - val_loss: 0.9034 - val_acc: 0.6514\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7649 - acc: 0.6903 - val_loss: 0.9036 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8030 - acc: 0.6699 - val_loss: 0.9061 - val_acc: 0.6457\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7941 - acc: 0.6782 - val_loss: 0.8965 - val_acc: 0.6629\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7825 - acc: 0.6724 - val_loss: 0.9012 - val_acc: 0.6571\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7772 - acc: 0.6807 - val_loss: 0.9079 - val_acc: 0.6457\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7751 - acc: 0.6928 - val_loss: 0.9079 - val_acc: 0.6514\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7650 - acc: 0.6928 - val_loss: 0.9089 - val_acc: 0.6629\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7741 - acc: 0.6871 - val_loss: 0.9110 - val_acc: 0.6457\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7590 - acc: 0.6941 - val_loss: 0.9098 - val_acc: 0.6514\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7802 - acc: 0.6903 - val_loss: 0.9167 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7727 - acc: 0.6814 - val_loss: 0.9088 - val_acc: 0.6571\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7534 - acc: 0.7037 - val_loss: 0.9109 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7588 - acc: 0.6941 - val_loss: 0.9132 - val_acc: 0.6514\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7736 - acc: 0.6718 - val_loss: 0.9078 - val_acc: 0.6686\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7644 - acc: 0.6897 - val_loss: 0.9088 - val_acc: 0.6571\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7501 - acc: 0.6954 - val_loss: 0.9137 - val_acc: 0.6629\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7661 - acc: 0.6807 - val_loss: 0.9166 - val_acc: 0.6571\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7546 - acc: 0.6852 - val_loss: 0.9258 - val_acc: 0.6343\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7633 - acc: 0.6973 - val_loss: 0.9213 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 56us/step\n",
      "1741/1741 [==============================] - 0s 35us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 7s 4ms/step - loss: 1.4587 - acc: 0.2982 - val_loss: 1.2226 - val_acc: 0.4857\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.3191 - acc: 0.3685 - val_loss: 1.1383 - val_acc: 0.5543\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.2529 - acc: 0.4393 - val_loss: 1.0841 - val_acc: 0.5829\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.1703 - acc: 0.4777 - val_loss: 1.0548 - val_acc: 0.5886\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.1546 - acc: 0.4802 - val_loss: 1.0280 - val_acc: 0.6114\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.1402 - acc: 0.4949 - val_loss: 1.0026 - val_acc: 0.6171\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.0842 - acc: 0.5255 - val_loss: 0.9870 - val_acc: 0.6000\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 1.0838 - acc: 0.5204 - val_loss: 0.9717 - val_acc: 0.6229\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 1.0830 - acc: 0.5185 - val_loss: 0.9644 - val_acc: 0.6229\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0536 - acc: 0.5517 - val_loss: 0.9543 - val_acc: 0.6229\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.0338 - acc: 0.5517 - val_loss: 0.9469 - val_acc: 0.6457\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.0405 - acc: 0.5536 - val_loss: 0.9346 - val_acc: 0.6343\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.0297 - acc: 0.5511 - val_loss: 0.9319 - val_acc: 0.6343\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0192 - acc: 0.5530 - val_loss: 0.9294 - val_acc: 0.6286\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 1.0062 - acc: 0.5511 - val_loss: 0.9260 - val_acc: 0.6400\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.9843 - acc: 0.5830 - val_loss: 0.9213 - val_acc: 0.6514\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9592 - acc: 0.5894 - val_loss: 0.9168 - val_acc: 0.6571\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9706 - acc: 0.5824 - val_loss: 0.9133 - val_acc: 0.6514\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9644 - acc: 0.5881 - val_loss: 0.9095 - val_acc: 0.6629\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9527 - acc: 0.5894 - val_loss: 0.9092 - val_acc: 0.6686\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9710 - acc: 0.5958 - val_loss: 0.9107 - val_acc: 0.6514\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9485 - acc: 0.5990 - val_loss: 0.9096 - val_acc: 0.6629\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9232 - acc: 0.6143 - val_loss: 0.9079 - val_acc: 0.6571\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9308 - acc: 0.6143 - val_loss: 0.9078 - val_acc: 0.6571\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9432 - acc: 0.5983 - val_loss: 0.9049 - val_acc: 0.6686\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9193 - acc: 0.6194 - val_loss: 0.9033 - val_acc: 0.6629\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.9157 - acc: 0.6207 - val_loss: 0.9015 - val_acc: 0.6514\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9046 - acc: 0.6303 - val_loss: 0.8997 - val_acc: 0.6457\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8902 - acc: 0.6347 - val_loss: 0.9007 - val_acc: 0.6571\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9158 - acc: 0.6213 - val_loss: 0.9008 - val_acc: 0.6629\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9028 - acc: 0.6335 - val_loss: 0.9026 - val_acc: 0.6629\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.9056 - acc: 0.6284 - val_loss: 0.8993 - val_acc: 0.6514\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8991 - acc: 0.6264 - val_loss: 0.8989 - val_acc: 0.6457\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8928 - acc: 0.6322 - val_loss: 0.8942 - val_acc: 0.6457\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8994 - acc: 0.6207 - val_loss: 0.8903 - val_acc: 0.6571\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8948 - acc: 0.6328 - val_loss: 0.8911 - val_acc: 0.6457\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8818 - acc: 0.6379 - val_loss: 0.8951 - val_acc: 0.6457\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8748 - acc: 0.6213 - val_loss: 0.9003 - val_acc: 0.6571\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8737 - acc: 0.6309 - val_loss: 0.8917 - val_acc: 0.6629\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8729 - acc: 0.6469 - val_loss: 0.8939 - val_acc: 0.6343\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8509 - acc: 0.6603 - val_loss: 0.8955 - val_acc: 0.6514\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8774 - acc: 0.6296 - val_loss: 0.8932 - val_acc: 0.6514\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8659 - acc: 0.6533 - val_loss: 0.8941 - val_acc: 0.6400\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8755 - acc: 0.6418 - val_loss: 0.8940 - val_acc: 0.6514\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8517 - acc: 0.6552 - val_loss: 0.8950 - val_acc: 0.6629\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8622 - acc: 0.6309 - val_loss: 0.8938 - val_acc: 0.6400\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8536 - acc: 0.6360 - val_loss: 0.8914 - val_acc: 0.6514\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8704 - acc: 0.6379 - val_loss: 0.8940 - val_acc: 0.6400\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8592 - acc: 0.6437 - val_loss: 0.8941 - val_acc: 0.6457\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8506 - acc: 0.6481 - val_loss: 0.8909 - val_acc: 0.6400\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8457 - acc: 0.6564 - val_loss: 0.8949 - val_acc: 0.6457\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8511 - acc: 0.6507 - val_loss: 0.8969 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8338 - acc: 0.6648 - val_loss: 0.8961 - val_acc: 0.6514\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8508 - acc: 0.6577 - val_loss: 0.8969 - val_acc: 0.6571\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8384 - acc: 0.6648 - val_loss: 0.8979 - val_acc: 0.6286\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8297 - acc: 0.6603 - val_loss: 0.8970 - val_acc: 0.6400\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8355 - acc: 0.6488 - val_loss: 0.8972 - val_acc: 0.6571\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8355 - acc: 0.6469 - val_loss: 0.8956 - val_acc: 0.6457\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8312 - acc: 0.6590 - val_loss: 0.8968 - val_acc: 0.6514\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8118 - acc: 0.6711 - val_loss: 0.8994 - val_acc: 0.6514\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8204 - acc: 0.6686 - val_loss: 0.9030 - val_acc: 0.6514\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8054 - acc: 0.6807 - val_loss: 0.9027 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8249 - acc: 0.6679 - val_loss: 0.9033 - val_acc: 0.6286\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8172 - acc: 0.6596 - val_loss: 0.8964 - val_acc: 0.6571\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8180 - acc: 0.6526 - val_loss: 0.8989 - val_acc: 0.6686\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8202 - acc: 0.6667 - val_loss: 0.9031 - val_acc: 0.6457\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8104 - acc: 0.6718 - val_loss: 0.9024 - val_acc: 0.6514\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8073 - acc: 0.6833 - val_loss: 0.9037 - val_acc: 0.6457\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8128 - acc: 0.6699 - val_loss: 0.9075 - val_acc: 0.6457\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7931 - acc: 0.6711 - val_loss: 0.9032 - val_acc: 0.6514\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7911 - acc: 0.6820 - val_loss: 0.8969 - val_acc: 0.6400\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7888 - acc: 0.6814 - val_loss: 0.8996 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7820 - acc: 0.6718 - val_loss: 0.8986 - val_acc: 0.6457\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.8004 - acc: 0.6775 - val_loss: 0.9019 - val_acc: 0.6457\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7742 - acc: 0.6973 - val_loss: 0.9012 - val_acc: 0.6571\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7991 - acc: 0.6833 - val_loss: 0.8978 - val_acc: 0.6457\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7757 - acc: 0.6916 - val_loss: 0.8971 - val_acc: 0.6571\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7587 - acc: 0.6941 - val_loss: 0.9004 - val_acc: 0.6514\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7831 - acc: 0.6845 - val_loss: 0.9035 - val_acc: 0.6571\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7739 - acc: 0.6935 - val_loss: 0.9042 - val_acc: 0.6571\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7882 - acc: 0.6826 - val_loss: 0.9071 - val_acc: 0.6629\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7676 - acc: 0.6980 - val_loss: 0.9094 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7823 - acc: 0.7018 - val_loss: 0.9061 - val_acc: 0.6571\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7635 - acc: 0.6852 - val_loss: 0.9136 - val_acc: 0.6514\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7858 - acc: 0.6801 - val_loss: 0.9102 - val_acc: 0.6514\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7578 - acc: 0.7037 - val_loss: 0.9164 - val_acc: 0.6514\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7754 - acc: 0.6852 - val_loss: 0.9117 - val_acc: 0.6514\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7686 - acc: 0.6788 - val_loss: 0.9133 - val_acc: 0.6514\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.7647 - acc: 0.6948 - val_loss: 0.9151 - val_acc: 0.6514\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7742 - acc: 0.6852 - val_loss: 0.9138 - val_acc: 0.6457\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7676 - acc: 0.6916 - val_loss: 0.9094 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7747 - acc: 0.6794 - val_loss: 0.9130 - val_acc: 0.6457\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7406 - acc: 0.7031 - val_loss: 0.9182 - val_acc: 0.6400\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7619 - acc: 0.6999 - val_loss: 0.9190 - val_acc: 0.6457\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7547 - acc: 0.6954 - val_loss: 0.9214 - val_acc: 0.6514\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.7602 - acc: 0.7005 - val_loss: 0.9240 - val_acc: 0.6571\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7618 - acc: 0.6922 - val_loss: 0.9190 - val_acc: 0.6514\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7431 - acc: 0.7114 - val_loss: 0.9174 - val_acc: 0.6457\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7570 - acc: 0.6999 - val_loss: 0.9174 - val_acc: 0.6514\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7480 - acc: 0.7056 - val_loss: 0.9192 - val_acc: 0.6571\n",
      "194/194 [==============================] - 0s 50us/step\n",
      "1741/1741 [==============================] - 0s 29us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 7s 5ms/step - loss: 1.4985 - acc: 0.2976 - val_loss: 1.2111 - val_acc: 0.5143\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.3311 - acc: 0.3812 - val_loss: 1.1127 - val_acc: 0.6114\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.2505 - acc: 0.4272 - val_loss: 1.0704 - val_acc: 0.6171\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.1824 - acc: 0.4732 - val_loss: 1.0413 - val_acc: 0.6171\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.1454 - acc: 0.4828 - val_loss: 1.0124 - val_acc: 0.6343\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 1.0999 - acc: 0.5160 - val_loss: 0.9918 - val_acc: 0.6457\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.1147 - acc: 0.5115 - val_loss: 0.9784 - val_acc: 0.6400\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.0712 - acc: 0.5236 - val_loss: 0.9703 - val_acc: 0.6571\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0715 - acc: 0.5358 - val_loss: 0.9594 - val_acc: 0.6629\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0415 - acc: 0.5441 - val_loss: 0.9504 - val_acc: 0.6686\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0577 - acc: 0.5338 - val_loss: 0.9469 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0497 - acc: 0.5434 - val_loss: 0.9401 - val_acc: 0.6743\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.0108 - acc: 0.5734 - val_loss: 0.9341 - val_acc: 0.6971\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9997 - acc: 0.5696 - val_loss: 0.9369 - val_acc: 0.6857\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0068 - acc: 0.5702 - val_loss: 0.9338 - val_acc: 0.6800\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0016 - acc: 0.5747 - val_loss: 0.9288 - val_acc: 0.6743\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9893 - acc: 0.5875 - val_loss: 0.9186 - val_acc: 0.6800\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9921 - acc: 0.5645 - val_loss: 0.9147 - val_acc: 0.6914\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9529 - acc: 0.6060 - val_loss: 0.9143 - val_acc: 0.6800\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9717 - acc: 0.5862 - val_loss: 0.9130 - val_acc: 0.6743\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9533 - acc: 0.5900 - val_loss: 0.9117 - val_acc: 0.6857\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.9271 - acc: 0.6156 - val_loss: 0.9130 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.9667 - acc: 0.5907 - val_loss: 0.9119 - val_acc: 0.6743\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9204 - acc: 0.6239 - val_loss: 0.9103 - val_acc: 0.6857\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.9260 - acc: 0.5990 - val_loss: 0.9029 - val_acc: 0.6743\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9493 - acc: 0.6188 - val_loss: 0.9022 - val_acc: 0.6686\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9176 - acc: 0.6143 - val_loss: 0.9030 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9142 - acc: 0.6034 - val_loss: 0.9044 - val_acc: 0.6800\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9175 - acc: 0.6175 - val_loss: 0.9069 - val_acc: 0.6800\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9251 - acc: 0.6271 - val_loss: 0.9059 - val_acc: 0.6629\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8938 - acc: 0.6252 - val_loss: 0.9033 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9074 - acc: 0.6213 - val_loss: 0.9005 - val_acc: 0.6686\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.9142 - acc: 0.6105 - val_loss: 0.9012 - val_acc: 0.6857\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8957 - acc: 0.6322 - val_loss: 0.9038 - val_acc: 0.6800\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8888 - acc: 0.6271 - val_loss: 0.9076 - val_acc: 0.6800\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8936 - acc: 0.6296 - val_loss: 0.9104 - val_acc: 0.6743\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8880 - acc: 0.6258 - val_loss: 0.9090 - val_acc: 0.6686\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8834 - acc: 0.6379 - val_loss: 0.9069 - val_acc: 0.6686\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8755 - acc: 0.6335 - val_loss: 0.9047 - val_acc: 0.6686\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8767 - acc: 0.6309 - val_loss: 0.8994 - val_acc: 0.6800\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8626 - acc: 0.6373 - val_loss: 0.8998 - val_acc: 0.6686\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8776 - acc: 0.6258 - val_loss: 0.9009 - val_acc: 0.6629\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8664 - acc: 0.6443 - val_loss: 0.8998 - val_acc: 0.6686\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8833 - acc: 0.6418 - val_loss: 0.9011 - val_acc: 0.6686\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8694 - acc: 0.6418 - val_loss: 0.8971 - val_acc: 0.6800\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8631 - acc: 0.6552 - val_loss: 0.8966 - val_acc: 0.6800\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8548 - acc: 0.6501 - val_loss: 0.8970 - val_acc: 0.6686\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8520 - acc: 0.6539 - val_loss: 0.9030 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8582 - acc: 0.6360 - val_loss: 0.9030 - val_acc: 0.6629\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8542 - acc: 0.6475 - val_loss: 0.9077 - val_acc: 0.6686\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8517 - acc: 0.6456 - val_loss: 0.9066 - val_acc: 0.6800\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8556 - acc: 0.6539 - val_loss: 0.9046 - val_acc: 0.6743\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8358 - acc: 0.6507 - val_loss: 0.9038 - val_acc: 0.6800\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8280 - acc: 0.6622 - val_loss: 0.9054 - val_acc: 0.6743\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8455 - acc: 0.6481 - val_loss: 0.9064 - val_acc: 0.6743\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8282 - acc: 0.6526 - val_loss: 0.9076 - val_acc: 0.6800\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8428 - acc: 0.6443 - val_loss: 0.9077 - val_acc: 0.6514\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8393 - acc: 0.6533 - val_loss: 0.9053 - val_acc: 0.6686\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8241 - acc: 0.6679 - val_loss: 0.9050 - val_acc: 0.6686\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8161 - acc: 0.6603 - val_loss: 0.9093 - val_acc: 0.6686\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8355 - acc: 0.6622 - val_loss: 0.9075 - val_acc: 0.6743\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8255 - acc: 0.6584 - val_loss: 0.9070 - val_acc: 0.6743\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8336 - acc: 0.6450 - val_loss: 0.9111 - val_acc: 0.6686\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8187 - acc: 0.6584 - val_loss: 0.9098 - val_acc: 0.6629\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8239 - acc: 0.6609 - val_loss: 0.9079 - val_acc: 0.6571\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8172 - acc: 0.6699 - val_loss: 0.9070 - val_acc: 0.6743\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8014 - acc: 0.6731 - val_loss: 0.9118 - val_acc: 0.6629\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8104 - acc: 0.6737 - val_loss: 0.9081 - val_acc: 0.6457\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7974 - acc: 0.6731 - val_loss: 0.9089 - val_acc: 0.6743\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8098 - acc: 0.6584 - val_loss: 0.9151 - val_acc: 0.6686\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8116 - acc: 0.6571 - val_loss: 0.9188 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8101 - acc: 0.6673 - val_loss: 0.9258 - val_acc: 0.6571\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8012 - acc: 0.6609 - val_loss: 0.9226 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7950 - acc: 0.6660 - val_loss: 0.9251 - val_acc: 0.6400\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8056 - acc: 0.6686 - val_loss: 0.9251 - val_acc: 0.6514\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7888 - acc: 0.6916 - val_loss: 0.9147 - val_acc: 0.6514\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7969 - acc: 0.6731 - val_loss: 0.9166 - val_acc: 0.6457\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7895 - acc: 0.6788 - val_loss: 0.9226 - val_acc: 0.6457\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7930 - acc: 0.6737 - val_loss: 0.9206 - val_acc: 0.6571\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7858 - acc: 0.6807 - val_loss: 0.9217 - val_acc: 0.6457\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7920 - acc: 0.6801 - val_loss: 0.9286 - val_acc: 0.6571\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7781 - acc: 0.6845 - val_loss: 0.9210 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7800 - acc: 0.6801 - val_loss: 0.9200 - val_acc: 0.6400\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7690 - acc: 0.6782 - val_loss: 0.9228 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7708 - acc: 0.6794 - val_loss: 0.9193 - val_acc: 0.6571\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7702 - acc: 0.6865 - val_loss: 0.9234 - val_acc: 0.6343\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7760 - acc: 0.6833 - val_loss: 0.9224 - val_acc: 0.6400\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7909 - acc: 0.6724 - val_loss: 0.9273 - val_acc: 0.6343\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7824 - acc: 0.6788 - val_loss: 0.9253 - val_acc: 0.6457\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7548 - acc: 0.6820 - val_loss: 0.9282 - val_acc: 0.6343\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7619 - acc: 0.6820 - val_loss: 0.9275 - val_acc: 0.6514\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7457 - acc: 0.6973 - val_loss: 0.9288 - val_acc: 0.6571\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7709 - acc: 0.6762 - val_loss: 0.9336 - val_acc: 0.6400\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7605 - acc: 0.6890 - val_loss: 0.9294 - val_acc: 0.6400\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7408 - acc: 0.7075 - val_loss: 0.9378 - val_acc: 0.6400\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7489 - acc: 0.6916 - val_loss: 0.9429 - val_acc: 0.6343\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7615 - acc: 0.6909 - val_loss: 0.9404 - val_acc: 0.6229\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7411 - acc: 0.7018 - val_loss: 0.9354 - val_acc: 0.6571\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7417 - acc: 0.7031 - val_loss: 0.9417 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7432 - acc: 0.6973 - val_loss: 0.9420 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 56us/step\n",
      "1741/1741 [==============================] - 0s 46us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.6562 - acc: 0.2931 - val_loss: 1.3534 - val_acc: 0.4743\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.4294 - acc: 0.3819 - val_loss: 1.2057 - val_acc: 0.4800\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.3209 - acc: 0.4087 - val_loss: 1.1237 - val_acc: 0.5486\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.2423 - acc: 0.4291 - val_loss: 1.0708 - val_acc: 0.5771\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.2163 - acc: 0.4655 - val_loss: 1.0386 - val_acc: 0.6114\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.1782 - acc: 0.4840 - val_loss: 1.0128 - val_acc: 0.6229\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1361 - acc: 0.4981 - val_loss: 0.9971 - val_acc: 0.6400\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 1.1091 - acc: 0.5153 - val_loss: 0.9811 - val_acc: 0.6400\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.1261 - acc: 0.5083 - val_loss: 0.9726 - val_acc: 0.6229\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0797 - acc: 0.5281 - val_loss: 0.9671 - val_acc: 0.6343\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0568 - acc: 0.5332 - val_loss: 0.9547 - val_acc: 0.6514\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0564 - acc: 0.5460 - val_loss: 0.9464 - val_acc: 0.6400\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0462 - acc: 0.5345 - val_loss: 0.9441 - val_acc: 0.6286\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0344 - acc: 0.5607 - val_loss: 0.9343 - val_acc: 0.6457\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0153 - acc: 0.5754 - val_loss: 0.9293 - val_acc: 0.6514\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0101 - acc: 0.5613 - val_loss: 0.9256 - val_acc: 0.6629\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9751 - acc: 0.5849 - val_loss: 0.9205 - val_acc: 0.6629\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9975 - acc: 0.5696 - val_loss: 0.9166 - val_acc: 0.6686\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.0117 - acc: 0.5683 - val_loss: 0.9164 - val_acc: 0.6571\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.9797 - acc: 0.5894 - val_loss: 0.9127 - val_acc: 0.6571\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9800 - acc: 0.5658 - val_loss: 0.9066 - val_acc: 0.6571\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.9770 - acc: 0.5849 - val_loss: 0.9032 - val_acc: 0.6571\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9733 - acc: 0.5734 - val_loss: 0.9012 - val_acc: 0.6800\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9522 - acc: 0.5945 - val_loss: 0.9024 - val_acc: 0.6743\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9531 - acc: 0.5932 - val_loss: 0.9009 - val_acc: 0.6686\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9279 - acc: 0.6098 - val_loss: 0.8913 - val_acc: 0.6857\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9577 - acc: 0.5958 - val_loss: 0.8924 - val_acc: 0.6971\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9497 - acc: 0.6003 - val_loss: 0.8945 - val_acc: 0.7029\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9600 - acc: 0.5983 - val_loss: 0.8973 - val_acc: 0.6686\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9268 - acc: 0.5951 - val_loss: 0.8912 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9360 - acc: 0.6028 - val_loss: 0.8919 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9276 - acc: 0.6169 - val_loss: 0.8916 - val_acc: 0.6857\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9349 - acc: 0.6169 - val_loss: 0.8927 - val_acc: 0.6743\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9286 - acc: 0.6137 - val_loss: 0.8914 - val_acc: 0.6629\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.9073 - acc: 0.6309 - val_loss: 0.8893 - val_acc: 0.6800\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.9082 - acc: 0.6066 - val_loss: 0.8887 - val_acc: 0.6914\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9232 - acc: 0.6169 - val_loss: 0.8912 - val_acc: 0.6914\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9152 - acc: 0.6194 - val_loss: 0.8922 - val_acc: 0.6743\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9064 - acc: 0.6175 - val_loss: 0.8886 - val_acc: 0.6971\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9025 - acc: 0.6188 - val_loss: 0.8885 - val_acc: 0.6743\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9042 - acc: 0.6296 - val_loss: 0.8833 - val_acc: 0.6800\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8933 - acc: 0.6220 - val_loss: 0.8877 - val_acc: 0.6686\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9055 - acc: 0.6232 - val_loss: 0.8929 - val_acc: 0.6800\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8967 - acc: 0.6188 - val_loss: 0.8890 - val_acc: 0.6743\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8715 - acc: 0.6284 - val_loss: 0.8903 - val_acc: 0.6800\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8620 - acc: 0.6571 - val_loss: 0.8899 - val_acc: 0.6743\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8872 - acc: 0.6296 - val_loss: 0.8948 - val_acc: 0.6686\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8797 - acc: 0.6379 - val_loss: 0.8907 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8604 - acc: 0.6418 - val_loss: 0.8882 - val_acc: 0.6800\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8768 - acc: 0.6360 - val_loss: 0.8898 - val_acc: 0.6743\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8720 - acc: 0.6341 - val_loss: 0.8913 - val_acc: 0.6743\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8706 - acc: 0.6430 - val_loss: 0.8891 - val_acc: 0.6743\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8653 - acc: 0.6360 - val_loss: 0.8877 - val_acc: 0.6857\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8592 - acc: 0.6328 - val_loss: 0.8874 - val_acc: 0.6800\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8580 - acc: 0.6367 - val_loss: 0.8854 - val_acc: 0.6743\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8466 - acc: 0.6411 - val_loss: 0.8868 - val_acc: 0.6857\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8413 - acc: 0.6571 - val_loss: 0.8882 - val_acc: 0.6800\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8276 - acc: 0.6520 - val_loss: 0.8901 - val_acc: 0.6743\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8462 - acc: 0.6571 - val_loss: 0.8873 - val_acc: 0.6629\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8540 - acc: 0.6424 - val_loss: 0.8890 - val_acc: 0.6514\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8382 - acc: 0.6622 - val_loss: 0.8911 - val_acc: 0.6457\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8240 - acc: 0.6584 - val_loss: 0.8891 - val_acc: 0.6686\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8353 - acc: 0.6533 - val_loss: 0.8936 - val_acc: 0.6686\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.8423 - acc: 0.6584 - val_loss: 0.8956 - val_acc: 0.6571\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8144 - acc: 0.6552 - val_loss: 0.8935 - val_acc: 0.6686\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8236 - acc: 0.6769 - val_loss: 0.8956 - val_acc: 0.6743\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8309 - acc: 0.6545 - val_loss: 0.8941 - val_acc: 0.6800\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8329 - acc: 0.6571 - val_loss: 0.8992 - val_acc: 0.6800\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8273 - acc: 0.6539 - val_loss: 0.8992 - val_acc: 0.6686\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8175 - acc: 0.6635 - val_loss: 0.8990 - val_acc: 0.6686\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8203 - acc: 0.6571 - val_loss: 0.9055 - val_acc: 0.6571\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8170 - acc: 0.6488 - val_loss: 0.9104 - val_acc: 0.6686\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8268 - acc: 0.6590 - val_loss: 0.9069 - val_acc: 0.6743\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8141 - acc: 0.6667 - val_loss: 0.9063 - val_acc: 0.6686\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8048 - acc: 0.6603 - val_loss: 0.9070 - val_acc: 0.6571\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - ETA: 0s - loss: 0.7693 - acc: 0.690 - 0s 72us/step - loss: 0.8061 - acc: 0.6724 - val_loss: 0.9014 - val_acc: 0.6686\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8136 - acc: 0.6814 - val_loss: 0.8992 - val_acc: 0.6800\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8048 - acc: 0.6743 - val_loss: 0.9026 - val_acc: 0.6629\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8169 - acc: 0.6584 - val_loss: 0.8977 - val_acc: 0.6800\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7840 - acc: 0.6845 - val_loss: 0.8939 - val_acc: 0.6743\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8040 - acc: 0.6641 - val_loss: 0.8971 - val_acc: 0.6800\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8067 - acc: 0.6660 - val_loss: 0.8993 - val_acc: 0.6686\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7895 - acc: 0.6743 - val_loss: 0.9051 - val_acc: 0.6743\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7839 - acc: 0.6711 - val_loss: 0.9065 - val_acc: 0.6629\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7944 - acc: 0.6699 - val_loss: 0.9094 - val_acc: 0.6686\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7936 - acc: 0.6711 - val_loss: 0.9113 - val_acc: 0.6800\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7942 - acc: 0.6801 - val_loss: 0.9123 - val_acc: 0.6571\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7928 - acc: 0.6679 - val_loss: 0.9104 - val_acc: 0.6514\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7919 - acc: 0.6788 - val_loss: 0.9093 - val_acc: 0.6686\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7829 - acc: 0.6903 - val_loss: 0.9075 - val_acc: 0.6629\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7706 - acc: 0.6845 - val_loss: 0.9060 - val_acc: 0.6629\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7887 - acc: 0.6654 - val_loss: 0.9080 - val_acc: 0.6629\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.7840 - acc: 0.6782 - val_loss: 0.9078 - val_acc: 0.6571\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7719 - acc: 0.6986 - val_loss: 0.9121 - val_acc: 0.6686\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7656 - acc: 0.6897 - val_loss: 0.9103 - val_acc: 0.6457\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7621 - acc: 0.6737 - val_loss: 0.9121 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7689 - acc: 0.6960 - val_loss: 0.9145 - val_acc: 0.6514\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7727 - acc: 0.6903 - val_loss: 0.9147 - val_acc: 0.6571\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7685 - acc: 0.6890 - val_loss: 0.9124 - val_acc: 0.6686\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7814 - acc: 0.6794 - val_loss: 0.9105 - val_acc: 0.6743\n",
      "194/194 [==============================] - 0s 49us/step\n",
      "1741/1741 [==============================] - 0s 32us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 7s 5ms/step - loss: 1.5437 - acc: 0.2867 - val_loss: 1.2623 - val_acc: 0.4229\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.3562 - acc: 0.3787 - val_loss: 1.1409 - val_acc: 0.5771\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 1.2389 - acc: 0.4419 - val_loss: 1.0868 - val_acc: 0.5600\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 1.2117 - acc: 0.4432 - val_loss: 1.0500 - val_acc: 0.5600\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.1859 - acc: 0.4559 - val_loss: 1.0249 - val_acc: 0.5714\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.1405 - acc: 0.4949 - val_loss: 1.0010 - val_acc: 0.5886\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 1.1113 - acc: 0.5243 - val_loss: 0.9811 - val_acc: 0.6114\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 1.0873 - acc: 0.5364 - val_loss: 0.9686 - val_acc: 0.6400\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0794 - acc: 0.5268 - val_loss: 0.9576 - val_acc: 0.6514\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0645 - acc: 0.5307 - val_loss: 0.9458 - val_acc: 0.6286\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0633 - acc: 0.5441 - val_loss: 0.9393 - val_acc: 0.6571\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0386 - acc: 0.5575 - val_loss: 0.9299 - val_acc: 0.6514\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 1.0397 - acc: 0.5294 - val_loss: 0.9331 - val_acc: 0.6400\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0251 - acc: 0.5626 - val_loss: 0.9244 - val_acc: 0.6514\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9983 - acc: 0.5798 - val_loss: 0.9172 - val_acc: 0.6514\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9964 - acc: 0.5645 - val_loss: 0.9091 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9795 - acc: 0.5798 - val_loss: 0.9044 - val_acc: 0.6686\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.9831 - acc: 0.5843 - val_loss: 0.9036 - val_acc: 0.6686\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9781 - acc: 0.5773 - val_loss: 0.9027 - val_acc: 0.6514\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.9792 - acc: 0.5900 - val_loss: 0.9004 - val_acc: 0.6686\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9776 - acc: 0.6003 - val_loss: 0.9021 - val_acc: 0.6629\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.9559 - acc: 0.5951 - val_loss: 0.9041 - val_acc: 0.6629\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9577 - acc: 0.6015 - val_loss: 0.9006 - val_acc: 0.6571\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9805 - acc: 0.5913 - val_loss: 0.8977 - val_acc: 0.6686\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9464 - acc: 0.5849 - val_loss: 0.8976 - val_acc: 0.6800\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9567 - acc: 0.5964 - val_loss: 0.8928 - val_acc: 0.6800\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9261 - acc: 0.6003 - val_loss: 0.8932 - val_acc: 0.6857\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9289 - acc: 0.6003 - val_loss: 0.8961 - val_acc: 0.6857\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9259 - acc: 0.6111 - val_loss: 0.8908 - val_acc: 0.6800\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9236 - acc: 0.6098 - val_loss: 0.8881 - val_acc: 0.6743\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9111 - acc: 0.6341 - val_loss: 0.8810 - val_acc: 0.6743\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9110 - acc: 0.6188 - val_loss: 0.8865 - val_acc: 0.6800\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9144 - acc: 0.6315 - val_loss: 0.8886 - val_acc: 0.6857\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.9175 - acc: 0.6137 - val_loss: 0.8850 - val_acc: 0.6857\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8920 - acc: 0.6213 - val_loss: 0.8866 - val_acc: 0.6857\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8816 - acc: 0.6284 - val_loss: 0.8871 - val_acc: 0.6800\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 41us/step - loss: 0.9095 - acc: 0.6066 - val_loss: 0.8856 - val_acc: 0.6686\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8908 - acc: 0.6252 - val_loss: 0.8832 - val_acc: 0.6686\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8983 - acc: 0.6341 - val_loss: 0.8810 - val_acc: 0.6743\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8976 - acc: 0.6194 - val_loss: 0.8780 - val_acc: 0.6629\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 40us/step - loss: 0.8856 - acc: 0.6245 - val_loss: 0.8799 - val_acc: 0.6743\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8821 - acc: 0.6175 - val_loss: 0.8790 - val_acc: 0.6743\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8825 - acc: 0.6322 - val_loss: 0.8810 - val_acc: 0.6571\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8898 - acc: 0.6201 - val_loss: 0.8820 - val_acc: 0.6629\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8654 - acc: 0.6271 - val_loss: 0.8803 - val_acc: 0.6686\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8603 - acc: 0.6462 - val_loss: 0.8738 - val_acc: 0.6743\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8709 - acc: 0.6258 - val_loss: 0.8746 - val_acc: 0.6743\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8585 - acc: 0.6360 - val_loss: 0.8787 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8427 - acc: 0.6571 - val_loss: 0.8841 - val_acc: 0.6571\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8647 - acc: 0.6367 - val_loss: 0.8785 - val_acc: 0.6571\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8527 - acc: 0.6513 - val_loss: 0.8786 - val_acc: 0.6629\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8599 - acc: 0.6443 - val_loss: 0.8797 - val_acc: 0.6743\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8459 - acc: 0.6558 - val_loss: 0.8808 - val_acc: 0.6743\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8457 - acc: 0.6564 - val_loss: 0.8757 - val_acc: 0.6686\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8528 - acc: 0.6520 - val_loss: 0.8760 - val_acc: 0.6571\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8541 - acc: 0.6513 - val_loss: 0.8808 - val_acc: 0.6571\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8438 - acc: 0.6488 - val_loss: 0.8820 - val_acc: 0.6629\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8314 - acc: 0.6577 - val_loss: 0.8817 - val_acc: 0.6514\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8477 - acc: 0.6507 - val_loss: 0.8852 - val_acc: 0.6457\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8546 - acc: 0.6418 - val_loss: 0.8822 - val_acc: 0.6400\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8408 - acc: 0.6616 - val_loss: 0.8792 - val_acc: 0.6629\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8279 - acc: 0.6622 - val_loss: 0.8808 - val_acc: 0.6571\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8196 - acc: 0.6654 - val_loss: 0.8838 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8311 - acc: 0.6603 - val_loss: 0.8852 - val_acc: 0.6457\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8198 - acc: 0.6596 - val_loss: 0.8851 - val_acc: 0.6629\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8052 - acc: 0.6699 - val_loss: 0.8904 - val_acc: 0.6571\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8132 - acc: 0.6622 - val_loss: 0.8909 - val_acc: 0.6457\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8040 - acc: 0.6679 - val_loss: 0.8938 - val_acc: 0.6571\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8169 - acc: 0.6648 - val_loss: 0.8875 - val_acc: 0.6514\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8141 - acc: 0.6635 - val_loss: 0.8879 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8133 - acc: 0.6743 - val_loss: 0.8848 - val_acc: 0.6629\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8111 - acc: 0.6750 - val_loss: 0.8893 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8100 - acc: 0.6692 - val_loss: 0.8928 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7957 - acc: 0.6737 - val_loss: 0.8934 - val_acc: 0.6571\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7998 - acc: 0.6673 - val_loss: 0.8959 - val_acc: 0.6514\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8086 - acc: 0.6769 - val_loss: 0.8985 - val_acc: 0.6514\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7944 - acc: 0.6845 - val_loss: 0.8946 - val_acc: 0.6514\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8011 - acc: 0.6814 - val_loss: 0.8949 - val_acc: 0.6571\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7870 - acc: 0.6750 - val_loss: 0.8975 - val_acc: 0.6514\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7920 - acc: 0.6737 - val_loss: 0.8986 - val_acc: 0.6571\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7919 - acc: 0.6705 - val_loss: 0.8971 - val_acc: 0.6571\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7720 - acc: 0.6903 - val_loss: 0.8982 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7879 - acc: 0.6731 - val_loss: 0.9071 - val_acc: 0.6629\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7657 - acc: 0.6833 - val_loss: 0.9071 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7710 - acc: 0.6948 - val_loss: 0.9050 - val_acc: 0.6571\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7834 - acc: 0.6769 - val_loss: 0.9125 - val_acc: 0.6457\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8023 - acc: 0.6648 - val_loss: 0.9127 - val_acc: 0.6514\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7804 - acc: 0.6686 - val_loss: 0.9139 - val_acc: 0.6629\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7771 - acc: 0.6833 - val_loss: 0.9137 - val_acc: 0.6457\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7580 - acc: 0.6833 - val_loss: 0.9121 - val_acc: 0.6343\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7848 - acc: 0.6731 - val_loss: 0.9132 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7549 - acc: 0.6890 - val_loss: 0.9131 - val_acc: 0.6514\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7632 - acc: 0.6890 - val_loss: 0.9140 - val_acc: 0.6571\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7706 - acc: 0.6884 - val_loss: 0.9123 - val_acc: 0.6571\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7592 - acc: 0.6909 - val_loss: 0.9144 - val_acc: 0.6514\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7538 - acc: 0.6775 - val_loss: 0.9159 - val_acc: 0.6571\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7528 - acc: 0.6928 - val_loss: 0.9140 - val_acc: 0.6571\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7483 - acc: 0.6884 - val_loss: 0.9192 - val_acc: 0.6514\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7629 - acc: 0.6871 - val_loss: 0.9213 - val_acc: 0.6571\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.7628 - acc: 0.6877 - val_loss: 0.9209 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 113us/step\n",
      "1741/1741 [==============================] - 0s 46us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5303 - acc: 0.2993 - val_loss: 1.2533 - val_acc: 0.4971\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.3369 - acc: 0.3701 - val_loss: 1.1423 - val_acc: 0.5829\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.2496 - acc: 0.4403 - val_loss: 1.0857 - val_acc: 0.6171\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.2130 - acc: 0.4397 - val_loss: 1.0427 - val_acc: 0.6171\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.1410 - acc: 0.4984 - val_loss: 1.0185 - val_acc: 0.6000\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.1233 - acc: 0.5118 - val_loss: 0.9972 - val_acc: 0.6343\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.1004 - acc: 0.5144 - val_loss: 0.9859 - val_acc: 0.6000\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0836 - acc: 0.5188 - val_loss: 0.9703 - val_acc: 0.6229\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0623 - acc: 0.5227 - val_loss: 0.9625 - val_acc: 0.6400\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0734 - acc: 0.5265 - val_loss: 0.9523 - val_acc: 0.6343\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0455 - acc: 0.5469 - val_loss: 0.9420 - val_acc: 0.6343\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0297 - acc: 0.5475 - val_loss: 0.9328 - val_acc: 0.6457\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0038 - acc: 0.5743 - val_loss: 0.9259 - val_acc: 0.6571\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.0241 - acc: 0.5546 - val_loss: 0.9215 - val_acc: 0.6743\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9886 - acc: 0.5737 - val_loss: 0.9181 - val_acc: 0.6686\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9761 - acc: 0.5801 - val_loss: 0.9186 - val_acc: 0.6571\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9788 - acc: 0.5775 - val_loss: 0.9170 - val_acc: 0.6571\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9767 - acc: 0.5967 - val_loss: 0.9088 - val_acc: 0.6571\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9851 - acc: 0.5763 - val_loss: 0.9117 - val_acc: 0.6457\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9687 - acc: 0.5660 - val_loss: 0.9092 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9655 - acc: 0.5788 - val_loss: 0.9130 - val_acc: 0.6686\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9379 - acc: 0.5960 - val_loss: 0.9102 - val_acc: 0.6514\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9447 - acc: 0.5980 - val_loss: 0.9097 - val_acc: 0.6514\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9437 - acc: 0.5814 - val_loss: 0.9035 - val_acc: 0.6514\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9464 - acc: 0.5973 - val_loss: 0.8999 - val_acc: 0.6514\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9457 - acc: 0.6018 - val_loss: 0.8997 - val_acc: 0.6571\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9268 - acc: 0.6203 - val_loss: 0.8957 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9176 - acc: 0.6139 - val_loss: 0.8922 - val_acc: 0.6629\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.9370 - acc: 0.5999 - val_loss: 0.8934 - val_acc: 0.6629\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9084 - acc: 0.6197 - val_loss: 0.8994 - val_acc: 0.6629\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9192 - acc: 0.6075 - val_loss: 0.8968 - val_acc: 0.6629\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9265 - acc: 0.6184 - val_loss: 0.8971 - val_acc: 0.6800\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9151 - acc: 0.6088 - val_loss: 0.9001 - val_acc: 0.6800\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9155 - acc: 0.6171 - val_loss: 0.9018 - val_acc: 0.6743\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8897 - acc: 0.6311 - val_loss: 0.8963 - val_acc: 0.6686\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8866 - acc: 0.6350 - val_loss: 0.8996 - val_acc: 0.6743\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8865 - acc: 0.6324 - val_loss: 0.9016 - val_acc: 0.6743\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8977 - acc: 0.6260 - val_loss: 0.9018 - val_acc: 0.6857\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8998 - acc: 0.6273 - val_loss: 0.9002 - val_acc: 0.6800\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8909 - acc: 0.6369 - val_loss: 0.9026 - val_acc: 0.6800\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8580 - acc: 0.6535 - val_loss: 0.9056 - val_acc: 0.6800\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8669 - acc: 0.6337 - val_loss: 0.9078 - val_acc: 0.6743\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8774 - acc: 0.6286 - val_loss: 0.9091 - val_acc: 0.6743\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8712 - acc: 0.6305 - val_loss: 0.9063 - val_acc: 0.6800\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8769 - acc: 0.6420 - val_loss: 0.9065 - val_acc: 0.6743\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8644 - acc: 0.6528 - val_loss: 0.9051 - val_acc: 0.6743\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8698 - acc: 0.6324 - val_loss: 0.9052 - val_acc: 0.6971\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8492 - acc: 0.6484 - val_loss: 0.9082 - val_acc: 0.6857\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8523 - acc: 0.6426 - val_loss: 0.9097 - val_acc: 0.6857\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8527 - acc: 0.6548 - val_loss: 0.9074 - val_acc: 0.6857\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8616 - acc: 0.6433 - val_loss: 0.9093 - val_acc: 0.6971\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8465 - acc: 0.6496 - val_loss: 0.9136 - val_acc: 0.6857\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8514 - acc: 0.6624 - val_loss: 0.8997 - val_acc: 0.6914\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8440 - acc: 0.6548 - val_loss: 0.8966 - val_acc: 0.6914\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8438 - acc: 0.6528 - val_loss: 0.9067 - val_acc: 0.6914\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8400 - acc: 0.6401 - val_loss: 0.9061 - val_acc: 0.6914\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8337 - acc: 0.6516 - val_loss: 0.9021 - val_acc: 0.6914\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8259 - acc: 0.6420 - val_loss: 0.9044 - val_acc: 0.6914\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8342 - acc: 0.6586 - val_loss: 0.9106 - val_acc: 0.6914\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8341 - acc: 0.6567 - val_loss: 0.9055 - val_acc: 0.7029\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8333 - acc: 0.6592 - val_loss: 0.9114 - val_acc: 0.6914\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8121 - acc: 0.6758 - val_loss: 0.9102 - val_acc: 0.6971\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8203 - acc: 0.6631 - val_loss: 0.9078 - val_acc: 0.6971\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8026 - acc: 0.6720 - val_loss: 0.9090 - val_acc: 0.6971\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8186 - acc: 0.6675 - val_loss: 0.9141 - val_acc: 0.7029\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8084 - acc: 0.6701 - val_loss: 0.9119 - val_acc: 0.6971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8341 - acc: 0.6554 - val_loss: 0.9173 - val_acc: 0.6914\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8077 - acc: 0.6707 - val_loss: 0.9093 - val_acc: 0.6914\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8067 - acc: 0.6707 - val_loss: 0.9120 - val_acc: 0.7029\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8128 - acc: 0.6675 - val_loss: 0.9134 - val_acc: 0.6914\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8142 - acc: 0.6528 - val_loss: 0.9140 - val_acc: 0.6914\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8037 - acc: 0.6739 - val_loss: 0.9131 - val_acc: 0.6914\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8004 - acc: 0.6707 - val_loss: 0.9246 - val_acc: 0.6857\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7890 - acc: 0.6803 - val_loss: 0.9263 - val_acc: 0.6914\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7837 - acc: 0.6847 - val_loss: 0.9211 - val_acc: 0.6914\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7916 - acc: 0.6618 - val_loss: 0.9225 - val_acc: 0.6914\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7868 - acc: 0.6733 - val_loss: 0.9208 - val_acc: 0.6971\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7888 - acc: 0.6771 - val_loss: 0.9240 - val_acc: 0.6857\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7949 - acc: 0.6854 - val_loss: 0.9243 - val_acc: 0.6857\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7953 - acc: 0.6682 - val_loss: 0.9174 - val_acc: 0.6914\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7728 - acc: 0.7020 - val_loss: 0.9258 - val_acc: 0.6800\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7934 - acc: 0.6803 - val_loss: 0.9317 - val_acc: 0.6800\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7713 - acc: 0.6777 - val_loss: 0.9227 - val_acc: 0.6800\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7824 - acc: 0.6662 - val_loss: 0.9191 - val_acc: 0.6857\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7763 - acc: 0.6784 - val_loss: 0.9265 - val_acc: 0.6857\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7784 - acc: 0.6796 - val_loss: 0.9298 - val_acc: 0.6857\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7848 - acc: 0.6854 - val_loss: 0.9270 - val_acc: 0.6914\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7808 - acc: 0.6745 - val_loss: 0.9224 - val_acc: 0.6857\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7837 - acc: 0.6752 - val_loss: 0.9302 - val_acc: 0.6743\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7711 - acc: 0.6873 - val_loss: 0.9359 - val_acc: 0.6743\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7739 - acc: 0.6847 - val_loss: 0.9339 - val_acc: 0.6914\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7535 - acc: 0.6937 - val_loss: 0.9335 - val_acc: 0.6857\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7681 - acc: 0.6930 - val_loss: 0.9275 - val_acc: 0.6857\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7543 - acc: 0.6956 - val_loss: 0.9307 - val_acc: 0.6857\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7639 - acc: 0.6796 - val_loss: 0.9306 - val_acc: 0.6743\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7572 - acc: 0.6899 - val_loss: 0.9295 - val_acc: 0.6857\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7509 - acc: 0.6835 - val_loss: 0.9370 - val_acc: 0.6857\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7564 - acc: 0.6930 - val_loss: 0.9391 - val_acc: 0.6914\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7325 - acc: 0.7026 - val_loss: 0.9404 - val_acc: 0.6743\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7474 - acc: 0.6905 - val_loss: 0.9324 - val_acc: 0.6800\n",
      "193/193 [==============================] - 0s 47us/step\n",
      "1742/1742 [==============================] - 0s 30us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 7s 4ms/step - loss: 1.5393 - acc: 0.2738 - val_loss: 1.2793 - val_acc: 0.4229\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.3349 - acc: 0.3867 - val_loss: 1.1603 - val_acc: 0.5486\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.2451 - acc: 0.4371 - val_loss: 1.0980 - val_acc: 0.5657\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.1999 - acc: 0.4639 - val_loss: 1.0592 - val_acc: 0.5943\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 1.1931 - acc: 0.4633 - val_loss: 1.0319 - val_acc: 0.6114\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.1403 - acc: 0.4901 - val_loss: 1.0052 - val_acc: 0.6286\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.1211 - acc: 0.5105 - val_loss: 0.9863 - val_acc: 0.6286\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 1.0936 - acc: 0.5163 - val_loss: 0.9749 - val_acc: 0.6571\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 1.0645 - acc: 0.5207 - val_loss: 0.9588 - val_acc: 0.6686\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0531 - acc: 0.5418 - val_loss: 0.9521 - val_acc: 0.6400\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 1.0295 - acc: 0.5546 - val_loss: 0.9440 - val_acc: 0.6343\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0211 - acc: 0.5507 - val_loss: 0.9281 - val_acc: 0.6286\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0153 - acc: 0.5578 - val_loss: 0.9198 - val_acc: 0.6629\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 1.0137 - acc: 0.5654 - val_loss: 0.9170 - val_acc: 0.6571\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.0068 - acc: 0.5737 - val_loss: 0.9087 - val_acc: 0.6800\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0201 - acc: 0.5514 - val_loss: 0.9077 - val_acc: 0.6629\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9916 - acc: 0.5801 - val_loss: 0.9073 - val_acc: 0.6571\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9689 - acc: 0.5833 - val_loss: 0.9005 - val_acc: 0.6571\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9625 - acc: 0.5890 - val_loss: 0.8941 - val_acc: 0.6571\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9604 - acc: 0.5807 - val_loss: 0.8903 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9488 - acc: 0.5935 - val_loss: 0.8876 - val_acc: 0.6571\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9433 - acc: 0.5890 - val_loss: 0.8827 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9393 - acc: 0.6050 - val_loss: 0.8833 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9324 - acc: 0.5992 - val_loss: 0.8818 - val_acc: 0.6743\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9320 - acc: 0.5986 - val_loss: 0.8774 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.9210 - acc: 0.6228 - val_loss: 0.8845 - val_acc: 0.6743\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9262 - acc: 0.5916 - val_loss: 0.8867 - val_acc: 0.6743\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9066 - acc: 0.6286 - val_loss: 0.8791 - val_acc: 0.6800\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9181 - acc: 0.6114 - val_loss: 0.8797 - val_acc: 0.6743\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9070 - acc: 0.6152 - val_loss: 0.8809 - val_acc: 0.6800\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9091 - acc: 0.6228 - val_loss: 0.8806 - val_acc: 0.6857\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.9088 - acc: 0.6248 - val_loss: 0.8781 - val_acc: 0.6743\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8971 - acc: 0.6394 - val_loss: 0.8717 - val_acc: 0.6857\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9087 - acc: 0.6209 - val_loss: 0.8751 - val_acc: 0.6743\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8856 - acc: 0.6235 - val_loss: 0.8771 - val_acc: 0.6686\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8904 - acc: 0.6311 - val_loss: 0.8720 - val_acc: 0.6686\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8747 - acc: 0.6362 - val_loss: 0.8744 - val_acc: 0.6800\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8947 - acc: 0.6311 - val_loss: 0.8728 - val_acc: 0.6743\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8722 - acc: 0.6394 - val_loss: 0.8723 - val_acc: 0.6743\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8606 - acc: 0.6382 - val_loss: 0.8734 - val_acc: 0.6800\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8614 - acc: 0.6439 - val_loss: 0.8657 - val_acc: 0.6743\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8624 - acc: 0.6382 - val_loss: 0.8655 - val_acc: 0.6571\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8610 - acc: 0.6426 - val_loss: 0.8589 - val_acc: 0.6743\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8604 - acc: 0.6592 - val_loss: 0.8587 - val_acc: 0.6743\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8685 - acc: 0.6343 - val_loss: 0.8591 - val_acc: 0.6743\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8395 - acc: 0.6599 - val_loss: 0.8626 - val_acc: 0.6686\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8389 - acc: 0.6643 - val_loss: 0.8639 - val_acc: 0.6514\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8512 - acc: 0.6586 - val_loss: 0.8656 - val_acc: 0.6686\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8559 - acc: 0.6586 - val_loss: 0.8645 - val_acc: 0.6800\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8399 - acc: 0.6509 - val_loss: 0.8714 - val_acc: 0.6686\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8447 - acc: 0.6573 - val_loss: 0.8637 - val_acc: 0.6743\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8351 - acc: 0.6586 - val_loss: 0.8670 - val_acc: 0.6800\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8291 - acc: 0.6707 - val_loss: 0.8650 - val_acc: 0.6800\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8320 - acc: 0.6618 - val_loss: 0.8662 - val_acc: 0.6686\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8050 - acc: 0.6752 - val_loss: 0.8659 - val_acc: 0.6686\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8305 - acc: 0.6688 - val_loss: 0.8653 - val_acc: 0.6629\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8202 - acc: 0.6611 - val_loss: 0.8587 - val_acc: 0.6629\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8192 - acc: 0.6707 - val_loss: 0.8530 - val_acc: 0.6743\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8236 - acc: 0.6624 - val_loss: 0.8554 - val_acc: 0.6743\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8226 - acc: 0.6682 - val_loss: 0.8527 - val_acc: 0.6914\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8146 - acc: 0.6624 - val_loss: 0.8529 - val_acc: 0.6857\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8288 - acc: 0.6528 - val_loss: 0.8571 - val_acc: 0.6629\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8147 - acc: 0.6707 - val_loss: 0.8531 - val_acc: 0.6857\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7890 - acc: 0.6803 - val_loss: 0.8546 - val_acc: 0.6800\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8180 - acc: 0.6599 - val_loss: 0.8584 - val_acc: 0.6686\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8098 - acc: 0.6637 - val_loss: 0.8605 - val_acc: 0.6686\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8012 - acc: 0.6758 - val_loss: 0.8600 - val_acc: 0.6743\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7979 - acc: 0.6656 - val_loss: 0.8595 - val_acc: 0.6629\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8006 - acc: 0.6796 - val_loss: 0.8612 - val_acc: 0.6686\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8057 - acc: 0.6624 - val_loss: 0.8657 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7864 - acc: 0.6879 - val_loss: 0.8660 - val_acc: 0.6629\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8017 - acc: 0.6784 - val_loss: 0.8667 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7816 - acc: 0.6911 - val_loss: 0.8722 - val_acc: 0.6629\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7950 - acc: 0.6733 - val_loss: 0.8659 - val_acc: 0.6686\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7977 - acc: 0.6682 - val_loss: 0.8693 - val_acc: 0.6743\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7836 - acc: 0.6911 - val_loss: 0.8663 - val_acc: 0.6914\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7876 - acc: 0.6739 - val_loss: 0.8745 - val_acc: 0.6686\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7782 - acc: 0.6873 - val_loss: 0.8676 - val_acc: 0.6743\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7782 - acc: 0.6777 - val_loss: 0.8655 - val_acc: 0.6629\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7668 - acc: 0.6905 - val_loss: 0.8696 - val_acc: 0.6514\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7807 - acc: 0.6822 - val_loss: 0.8706 - val_acc: 0.6514\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7756 - acc: 0.6975 - val_loss: 0.8699 - val_acc: 0.6629\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.7603 - acc: 0.6924 - val_loss: 0.8771 - val_acc: 0.6571\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7824 - acc: 0.6867 - val_loss: 0.8768 - val_acc: 0.6514\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7609 - acc: 0.6860 - val_loss: 0.8764 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7824 - acc: 0.6784 - val_loss: 0.8678 - val_acc: 0.6514\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7541 - acc: 0.6981 - val_loss: 0.8708 - val_acc: 0.6514\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.7639 - acc: 0.6803 - val_loss: 0.8715 - val_acc: 0.6571\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7562 - acc: 0.6854 - val_loss: 0.8669 - val_acc: 0.6571\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7425 - acc: 0.7026 - val_loss: 0.8699 - val_acc: 0.6571\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7532 - acc: 0.6924 - val_loss: 0.8720 - val_acc: 0.6571\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7370 - acc: 0.7020 - val_loss: 0.8673 - val_acc: 0.6629\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7608 - acc: 0.6962 - val_loss: 0.8676 - val_acc: 0.6629\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7507 - acc: 0.6886 - val_loss: 0.8722 - val_acc: 0.6514\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7392 - acc: 0.7096 - val_loss: 0.8734 - val_acc: 0.6571\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7604 - acc: 0.6847 - val_loss: 0.8707 - val_acc: 0.6571\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7472 - acc: 0.6962 - val_loss: 0.8733 - val_acc: 0.6629\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7544 - acc: 0.6969 - val_loss: 0.8730 - val_acc: 0.6571\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7250 - acc: 0.7179 - val_loss: 0.8780 - val_acc: 0.6514\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7544 - acc: 0.6899 - val_loss: 0.8816 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 43us/step\n",
      "1742/1742 [==============================] - 0s 30us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.4750 - acc: 0.3031 - val_loss: 1.2390 - val_acc: 0.4686\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 1.3481 - acc: 0.3791 - val_loss: 1.1457 - val_acc: 0.5600\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.2529 - acc: 0.4257 - val_loss: 1.0908 - val_acc: 0.5771\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.2134 - acc: 0.4525 - val_loss: 1.0534 - val_acc: 0.5600\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1654 - acc: 0.4767 - val_loss: 1.0293 - val_acc: 0.5943\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.1387 - acc: 0.4907 - val_loss: 1.0194 - val_acc: 0.6000\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 1.1276 - acc: 0.5195 - val_loss: 0.9963 - val_acc: 0.6171\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.1078 - acc: 0.5073 - val_loss: 0.9784 - val_acc: 0.6171\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 1.0808 - acc: 0.5348 - val_loss: 0.9670 - val_acc: 0.6229\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0514 - acc: 0.5424 - val_loss: 0.9648 - val_acc: 0.6229\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0718 - acc: 0.5220 - val_loss: 0.9561 - val_acc: 0.6171\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0483 - acc: 0.5392 - val_loss: 0.9430 - val_acc: 0.6171\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0129 - acc: 0.5667 - val_loss: 0.9376 - val_acc: 0.6343\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.0112 - acc: 0.5635 - val_loss: 0.9377 - val_acc: 0.6286\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.0207 - acc: 0.5533 - val_loss: 0.9334 - val_acc: 0.6343\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.9915 - acc: 0.5724 - val_loss: 0.9260 - val_acc: 0.6229\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9922 - acc: 0.5820 - val_loss: 0.9211 - val_acc: 0.6400\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9865 - acc: 0.5839 - val_loss: 0.9232 - val_acc: 0.6629\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9935 - acc: 0.5769 - val_loss: 0.9213 - val_acc: 0.6400\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.9724 - acc: 0.5839 - val_loss: 0.9253 - val_acc: 0.6343\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9804 - acc: 0.5967 - val_loss: 0.9245 - val_acc: 0.6400\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9726 - acc: 0.5750 - val_loss: 0.9201 - val_acc: 0.6629\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.9420 - acc: 0.6056 - val_loss: 0.9144 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9481 - acc: 0.6037 - val_loss: 0.9000 - val_acc: 0.6800\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9517 - acc: 0.5916 - val_loss: 0.9086 - val_acc: 0.6629\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9388 - acc: 0.5954 - val_loss: 0.9062 - val_acc: 0.6571\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9316 - acc: 0.6094 - val_loss: 0.9029 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9257 - acc: 0.6056 - val_loss: 0.9112 - val_acc: 0.6629\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.9186 - acc: 0.6101 - val_loss: 0.9058 - val_acc: 0.6800\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9253 - acc: 0.6146 - val_loss: 0.9025 - val_acc: 0.6629\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9118 - acc: 0.6165 - val_loss: 0.9083 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9160 - acc: 0.6190 - val_loss: 0.9045 - val_acc: 0.6686\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9024 - acc: 0.6139 - val_loss: 0.9020 - val_acc: 0.6914\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9045 - acc: 0.6152 - val_loss: 0.9041 - val_acc: 0.6914\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9037 - acc: 0.6248 - val_loss: 0.9052 - val_acc: 0.6800\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8852 - acc: 0.6216 - val_loss: 0.8959 - val_acc: 0.6914\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8654 - acc: 0.6445 - val_loss: 0.8982 - val_acc: 0.6800\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8880 - acc: 0.6388 - val_loss: 0.9046 - val_acc: 0.6857\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8826 - acc: 0.6439 - val_loss: 0.9021 - val_acc: 0.6743\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8679 - acc: 0.6426 - val_loss: 0.9035 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8774 - acc: 0.6331 - val_loss: 0.9049 - val_acc: 0.6743\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8725 - acc: 0.6465 - val_loss: 0.9016 - val_acc: 0.6800\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8707 - acc: 0.6388 - val_loss: 0.9095 - val_acc: 0.6857\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8675 - acc: 0.6439 - val_loss: 0.9092 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8841 - acc: 0.6203 - val_loss: 0.9086 - val_acc: 0.6686\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8547 - acc: 0.6465 - val_loss: 0.9070 - val_acc: 0.6686\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8639 - acc: 0.6496 - val_loss: 0.9031 - val_acc: 0.6571\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8592 - acc: 0.6471 - val_loss: 0.9067 - val_acc: 0.6857\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8466 - acc: 0.6509 - val_loss: 0.9077 - val_acc: 0.6800\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8522 - acc: 0.6356 - val_loss: 0.9089 - val_acc: 0.6686\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8434 - acc: 0.6554 - val_loss: 0.9134 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8490 - acc: 0.6490 - val_loss: 0.9125 - val_acc: 0.6629\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8607 - acc: 0.6375 - val_loss: 0.9087 - val_acc: 0.6629\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8415 - acc: 0.6560 - val_loss: 0.9106 - val_acc: 0.6743\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8572 - acc: 0.6465 - val_loss: 0.9156 - val_acc: 0.6743\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8283 - acc: 0.6688 - val_loss: 0.9061 - val_acc: 0.6800\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8586 - acc: 0.6401 - val_loss: 0.9053 - val_acc: 0.6686\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8580 - acc: 0.6490 - val_loss: 0.9134 - val_acc: 0.6686\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8294 - acc: 0.6567 - val_loss: 0.9131 - val_acc: 0.6857\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 40us/step - loss: 0.8320 - acc: 0.6535 - val_loss: 0.9102 - val_acc: 0.6514\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8289 - acc: 0.6669 - val_loss: 0.9158 - val_acc: 0.6457\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8338 - acc: 0.6503 - val_loss: 0.9117 - val_acc: 0.6514\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8097 - acc: 0.6682 - val_loss: 0.9087 - val_acc: 0.6457\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8383 - acc: 0.6650 - val_loss: 0.9112 - val_acc: 0.6743\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8216 - acc: 0.6656 - val_loss: 0.9144 - val_acc: 0.6514\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8106 - acc: 0.6662 - val_loss: 0.9143 - val_acc: 0.6571\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8059 - acc: 0.6701 - val_loss: 0.9146 - val_acc: 0.6571\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8002 - acc: 0.6688 - val_loss: 0.9053 - val_acc: 0.6686\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7872 - acc: 0.6701 - val_loss: 0.9117 - val_acc: 0.6571\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8093 - acc: 0.6675 - val_loss: 0.9130 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8280 - acc: 0.6477 - val_loss: 0.9133 - val_acc: 0.6286\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7929 - acc: 0.6726 - val_loss: 0.9097 - val_acc: 0.6457\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8057 - acc: 0.6752 - val_loss: 0.9119 - val_acc: 0.6343\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7984 - acc: 0.6733 - val_loss: 0.9156 - val_acc: 0.6457\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8000 - acc: 0.6682 - val_loss: 0.9181 - val_acc: 0.6571\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8054 - acc: 0.6643 - val_loss: 0.9259 - val_acc: 0.6629\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7830 - acc: 0.6847 - val_loss: 0.9218 - val_acc: 0.6629\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7887 - acc: 0.6675 - val_loss: 0.9189 - val_acc: 0.6514\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7978 - acc: 0.6701 - val_loss: 0.9271 - val_acc: 0.6514\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7931 - acc: 0.6784 - val_loss: 0.9308 - val_acc: 0.6514\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7848 - acc: 0.6847 - val_loss: 0.9327 - val_acc: 0.6514\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7889 - acc: 0.6771 - val_loss: 0.9339 - val_acc: 0.6400\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7821 - acc: 0.6790 - val_loss: 0.9305 - val_acc: 0.6457\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7913 - acc: 0.6873 - val_loss: 0.9321 - val_acc: 0.6571\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7943 - acc: 0.6662 - val_loss: 0.9368 - val_acc: 0.6457\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7697 - acc: 0.6975 - val_loss: 0.9390 - val_acc: 0.6514\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7735 - acc: 0.6930 - val_loss: 0.9411 - val_acc: 0.6457\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7752 - acc: 0.6892 - val_loss: 0.9393 - val_acc: 0.6514\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7642 - acc: 0.6924 - val_loss: 0.9378 - val_acc: 0.6514\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7687 - acc: 0.6701 - val_loss: 0.9399 - val_acc: 0.6571\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7659 - acc: 0.6784 - val_loss: 0.9390 - val_acc: 0.6629\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7697 - acc: 0.6905 - val_loss: 0.9399 - val_acc: 0.6514\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7646 - acc: 0.6930 - val_loss: 0.9388 - val_acc: 0.6457\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7581 - acc: 0.6981 - val_loss: 0.9368 - val_acc: 0.6514\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7388 - acc: 0.6988 - val_loss: 0.9384 - val_acc: 0.6514\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7680 - acc: 0.6758 - val_loss: 0.9385 - val_acc: 0.6514\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7633 - acc: 0.6816 - val_loss: 0.9419 - val_acc: 0.6457\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7603 - acc: 0.6924 - val_loss: 0.9336 - val_acc: 0.6400\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7634 - acc: 0.6950 - val_loss: 0.9428 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7557 - acc: 0.6981 - val_loss: 0.9469 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 42us/step\n",
      "1742/1742 [==============================] - 0s 27us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 7s 4ms/step - loss: 1.4833 - acc: 0.2821 - val_loss: 1.2535 - val_acc: 0.4457\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.3471 - acc: 0.3638 - val_loss: 1.1587 - val_acc: 0.5486\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.2497 - acc: 0.4378 - val_loss: 1.1069 - val_acc: 0.5714\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.2164 - acc: 0.4633 - val_loss: 1.0713 - val_acc: 0.5714\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.1937 - acc: 0.4569 - val_loss: 1.0473 - val_acc: 0.5886\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.1366 - acc: 0.4882 - val_loss: 1.0249 - val_acc: 0.5943\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0953 - acc: 0.5214 - val_loss: 1.0090 - val_acc: 0.6000\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0850 - acc: 0.5124 - val_loss: 0.9960 - val_acc: 0.6000\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0610 - acc: 0.5361 - val_loss: 0.9830 - val_acc: 0.6114\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0507 - acc: 0.5514 - val_loss: 0.9710 - val_acc: 0.6400\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.0479 - acc: 0.5495 - val_loss: 0.9579 - val_acc: 0.6343\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0249 - acc: 0.5590 - val_loss: 0.9533 - val_acc: 0.6286\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0009 - acc: 0.5680 - val_loss: 0.9470 - val_acc: 0.6343\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9986 - acc: 0.5826 - val_loss: 0.9427 - val_acc: 0.6400\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.0028 - acc: 0.5724 - val_loss: 0.9381 - val_acc: 0.6400\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0113 - acc: 0.5667 - val_loss: 0.9323 - val_acc: 0.6629\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9880 - acc: 0.5712 - val_loss: 0.9293 - val_acc: 0.6457\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9831 - acc: 0.5763 - val_loss: 0.9256 - val_acc: 0.6400\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9619 - acc: 0.5916 - val_loss: 0.9205 - val_acc: 0.6686\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9546 - acc: 0.6018 - val_loss: 0.9230 - val_acc: 0.6571\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9642 - acc: 0.5922 - val_loss: 0.9219 - val_acc: 0.6800\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9377 - acc: 0.6158 - val_loss: 0.9218 - val_acc: 0.6514\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9446 - acc: 0.5960 - val_loss: 0.9210 - val_acc: 0.6800\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9373 - acc: 0.5948 - val_loss: 0.9166 - val_acc: 0.6571\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9096 - acc: 0.6254 - val_loss: 0.9155 - val_acc: 0.6514\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9367 - acc: 0.6094 - val_loss: 0.9154 - val_acc: 0.6686\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9171 - acc: 0.6152 - val_loss: 0.9136 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9297 - acc: 0.6133 - val_loss: 0.9125 - val_acc: 0.6800\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9184 - acc: 0.6241 - val_loss: 0.9144 - val_acc: 0.6800\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.9365 - acc: 0.6018 - val_loss: 0.9169 - val_acc: 0.6686\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8970 - acc: 0.6350 - val_loss: 0.9147 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9042 - acc: 0.6152 - val_loss: 0.9059 - val_acc: 0.6686\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8975 - acc: 0.6280 - val_loss: 0.9119 - val_acc: 0.6743\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8878 - acc: 0.6273 - val_loss: 0.9083 - val_acc: 0.6800\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8885 - acc: 0.6273 - val_loss: 0.9033 - val_acc: 0.6857\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8993 - acc: 0.6388 - val_loss: 0.9058 - val_acc: 0.6743\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8850 - acc: 0.6509 - val_loss: 0.9065 - val_acc: 0.6686\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8820 - acc: 0.6324 - val_loss: 0.9094 - val_acc: 0.6686\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8878 - acc: 0.6356 - val_loss: 0.9080 - val_acc: 0.6743\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8650 - acc: 0.6445 - val_loss: 0.9070 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8767 - acc: 0.6331 - val_loss: 0.9053 - val_acc: 0.6686\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8669 - acc: 0.6471 - val_loss: 0.9117 - val_acc: 0.6743\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8782 - acc: 0.6350 - val_loss: 0.9080 - val_acc: 0.6686\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8707 - acc: 0.6484 - val_loss: 0.9077 - val_acc: 0.6743\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8727 - acc: 0.6420 - val_loss: 0.9114 - val_acc: 0.6629\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8635 - acc: 0.6522 - val_loss: 0.9088 - val_acc: 0.6571\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8542 - acc: 0.6465 - val_loss: 0.9037 - val_acc: 0.6629\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8514 - acc: 0.6573 - val_loss: 0.9053 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8450 - acc: 0.6688 - val_loss: 0.9070 - val_acc: 0.6686\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8494 - acc: 0.6509 - val_loss: 0.9039 - val_acc: 0.6686\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8495 - acc: 0.6701 - val_loss: 0.9067 - val_acc: 0.6514\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8352 - acc: 0.6554 - val_loss: 0.9092 - val_acc: 0.6686\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8377 - acc: 0.6656 - val_loss: 0.9088 - val_acc: 0.6571\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8453 - acc: 0.6548 - val_loss: 0.9087 - val_acc: 0.6571\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8317 - acc: 0.6733 - val_loss: 0.9100 - val_acc: 0.6629\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8297 - acc: 0.6573 - val_loss: 0.9078 - val_acc: 0.6571\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8320 - acc: 0.6682 - val_loss: 0.9101 - val_acc: 0.6743\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8029 - acc: 0.6713 - val_loss: 0.9169 - val_acc: 0.6571\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8306 - acc: 0.6579 - val_loss: 0.9149 - val_acc: 0.6629\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8201 - acc: 0.6662 - val_loss: 0.9139 - val_acc: 0.6686\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8113 - acc: 0.6682 - val_loss: 0.9140 - val_acc: 0.6743\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8031 - acc: 0.6713 - val_loss: 0.9155 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8209 - acc: 0.6643 - val_loss: 0.9135 - val_acc: 0.6686\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8159 - acc: 0.6605 - val_loss: 0.9207 - val_acc: 0.6629\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8045 - acc: 0.6752 - val_loss: 0.9199 - val_acc: 0.6629\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7935 - acc: 0.6867 - val_loss: 0.9203 - val_acc: 0.6629\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8077 - acc: 0.6758 - val_loss: 0.9209 - val_acc: 0.6629\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.7996 - acc: 0.6758 - val_loss: 0.9259 - val_acc: 0.6743\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8079 - acc: 0.6822 - val_loss: 0.9280 - val_acc: 0.6571\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8023 - acc: 0.6758 - val_loss: 0.9240 - val_acc: 0.6571\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7912 - acc: 0.6694 - val_loss: 0.9264 - val_acc: 0.6686\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7900 - acc: 0.6950 - val_loss: 0.9251 - val_acc: 0.6571\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8026 - acc: 0.6662 - val_loss: 0.9253 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7977 - acc: 0.6733 - val_loss: 0.9260 - val_acc: 0.6457\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.7810 - acc: 0.6796 - val_loss: 0.9321 - val_acc: 0.6571\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7827 - acc: 0.6854 - val_loss: 0.9328 - val_acc: 0.6686\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7808 - acc: 0.6816 - val_loss: 0.9346 - val_acc: 0.6571\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8069 - acc: 0.6886 - val_loss: 0.9354 - val_acc: 0.6629\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7793 - acc: 0.6860 - val_loss: 0.9406 - val_acc: 0.6571\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7661 - acc: 0.6771 - val_loss: 0.9379 - val_acc: 0.6571\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7797 - acc: 0.6828 - val_loss: 0.9395 - val_acc: 0.6343\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7612 - acc: 0.6994 - val_loss: 0.9336 - val_acc: 0.6400\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7784 - acc: 0.6905 - val_loss: 0.9336 - val_acc: 0.6629\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7794 - acc: 0.6860 - val_loss: 0.9374 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7662 - acc: 0.6803 - val_loss: 0.9363 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7746 - acc: 0.6828 - val_loss: 0.9412 - val_acc: 0.6400\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7623 - acc: 0.6962 - val_loss: 0.9464 - val_acc: 0.6629\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7700 - acc: 0.6905 - val_loss: 0.9446 - val_acc: 0.6343\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7767 - acc: 0.6854 - val_loss: 0.9405 - val_acc: 0.6343\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7705 - acc: 0.6835 - val_loss: 0.9415 - val_acc: 0.6286\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7537 - acc: 0.6892 - val_loss: 0.9359 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7669 - acc: 0.6930 - val_loss: 0.9373 - val_acc: 0.6457\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7581 - acc: 0.6994 - val_loss: 0.9362 - val_acc: 0.6514\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7538 - acc: 0.6988 - val_loss: 0.9407 - val_acc: 0.6457\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7559 - acc: 0.6924 - val_loss: 0.9430 - val_acc: 0.6457\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 107us/step - loss: 0.7426 - acc: 0.7090 - val_loss: 0.9520 - val_acc: 0.6343\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.7545 - acc: 0.6969 - val_loss: 0.9473 - val_acc: 0.6457\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7501 - acc: 0.7033 - val_loss: 0.9435 - val_acc: 0.6457\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 112us/step - loss: 0.7402 - acc: 0.7001 - val_loss: 0.9464 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.7528 - acc: 0.6905 - val_loss: 0.9499 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 65us/step\n",
      "1742/1742 [==============================] - 0s 62us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 7s 5ms/step - loss: 1.5425 - acc: 0.2833 - val_loss: 1.3057 - val_acc: 0.3771\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.3684 - acc: 0.3574 - val_loss: 1.2001 - val_acc: 0.4229\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.2503 - acc: 0.4250 - val_loss: 1.1331 - val_acc: 0.5029\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.2067 - acc: 0.4563 - val_loss: 1.0948 - val_acc: 0.5257\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.1640 - acc: 0.4844 - val_loss: 1.0746 - val_acc: 0.5200\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1417 - acc: 0.5048 - val_loss: 1.0602 - val_acc: 0.5257\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1198 - acc: 0.5239 - val_loss: 1.0388 - val_acc: 0.5371\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 1.0902 - acc: 0.5124 - val_loss: 1.0382 - val_acc: 0.5200\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0760 - acc: 0.5341 - val_loss: 1.0212 - val_acc: 0.5371\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0703 - acc: 0.5341 - val_loss: 1.0173 - val_acc: 0.5486\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 1.0573 - acc: 0.5367 - val_loss: 1.0130 - val_acc: 0.5486\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.0132 - acc: 0.5705 - val_loss: 1.0103 - val_acc: 0.5600\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 1.0274 - acc: 0.5539 - val_loss: 1.0031 - val_acc: 0.5600\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0250 - acc: 0.5692 - val_loss: 0.9949 - val_acc: 0.5486\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9888 - acc: 0.5775 - val_loss: 0.9914 - val_acc: 0.5543\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.0026 - acc: 0.5635 - val_loss: 0.9882 - val_acc: 0.5486\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0014 - acc: 0.5750 - val_loss: 0.9856 - val_acc: 0.5543\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9581 - acc: 0.6075 - val_loss: 0.9855 - val_acc: 0.5657\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9621 - acc: 0.5852 - val_loss: 0.9878 - val_acc: 0.5543\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9680 - acc: 0.5929 - val_loss: 0.9797 - val_acc: 0.5771\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9615 - acc: 0.5967 - val_loss: 0.9720 - val_acc: 0.5829\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9605 - acc: 0.5954 - val_loss: 0.9753 - val_acc: 0.5771\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.9487 - acc: 0.5992 - val_loss: 0.9734 - val_acc: 0.5886\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9461 - acc: 0.5986 - val_loss: 0.9762 - val_acc: 0.5943\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9375 - acc: 0.6139 - val_loss: 0.9666 - val_acc: 0.5943\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9332 - acc: 0.6075 - val_loss: 0.9688 - val_acc: 0.5886\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.9293 - acc: 0.6050 - val_loss: 0.9737 - val_acc: 0.5714\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9182 - acc: 0.6292 - val_loss: 0.9658 - val_acc: 0.5714\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9088 - acc: 0.6292 - val_loss: 0.9644 - val_acc: 0.5829\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9102 - acc: 0.6197 - val_loss: 0.9770 - val_acc: 0.5829\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9080 - acc: 0.6241 - val_loss: 0.9679 - val_acc: 0.5886\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9213 - acc: 0.6222 - val_loss: 0.9603 - val_acc: 0.5771\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.9064 - acc: 0.6292 - val_loss: 0.9657 - val_acc: 0.5771\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9105 - acc: 0.6267 - val_loss: 0.9673 - val_acc: 0.5771\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8971 - acc: 0.6184 - val_loss: 0.9721 - val_acc: 0.5657\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8796 - acc: 0.6369 - val_loss: 0.9702 - val_acc: 0.5771\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8862 - acc: 0.6426 - val_loss: 0.9705 - val_acc: 0.5771\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8904 - acc: 0.6241 - val_loss: 0.9671 - val_acc: 0.5771\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8924 - acc: 0.6311 - val_loss: 0.9687 - val_acc: 0.5657\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8725 - acc: 0.6471 - val_loss: 0.9673 - val_acc: 0.5771\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8846 - acc: 0.6452 - val_loss: 0.9719 - val_acc: 0.5714\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8749 - acc: 0.6401 - val_loss: 0.9728 - val_acc: 0.5657\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8759 - acc: 0.6280 - val_loss: 0.9696 - val_acc: 0.5714\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8552 - acc: 0.6528 - val_loss: 0.9771 - val_acc: 0.5714\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8688 - acc: 0.6541 - val_loss: 0.9851 - val_acc: 0.5714\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 39us/step - loss: 0.8529 - acc: 0.6318 - val_loss: 0.9770 - val_acc: 0.5714\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8663 - acc: 0.6420 - val_loss: 0.9774 - val_acc: 0.5600\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8426 - acc: 0.6452 - val_loss: 0.9788 - val_acc: 0.5657\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8542 - acc: 0.6573 - val_loss: 0.9788 - val_acc: 0.5657\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8465 - acc: 0.6535 - val_loss: 0.9821 - val_acc: 0.5714\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8479 - acc: 0.6490 - val_loss: 0.9875 - val_acc: 0.5714\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8398 - acc: 0.6618 - val_loss: 0.9811 - val_acc: 0.5657\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8409 - acc: 0.6624 - val_loss: 0.9815 - val_acc: 0.5714\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8460 - acc: 0.6484 - val_loss: 0.9811 - val_acc: 0.5714\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8280 - acc: 0.6611 - val_loss: 0.9833 - val_acc: 0.5771\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.8253 - acc: 0.6592 - val_loss: 0.9887 - val_acc: 0.5771\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8217 - acc: 0.6694 - val_loss: 0.9912 - val_acc: 0.5829\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8286 - acc: 0.6637 - val_loss: 0.9839 - val_acc: 0.5771\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8184 - acc: 0.6682 - val_loss: 0.9812 - val_acc: 0.5657\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8263 - acc: 0.6707 - val_loss: 0.9866 - val_acc: 0.5829\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8226 - acc: 0.6567 - val_loss: 0.9847 - val_acc: 0.5771\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 41us/step - loss: 0.8309 - acc: 0.6567 - val_loss: 0.9877 - val_acc: 0.5714\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 0.8287 - acc: 0.6516 - val_loss: 0.9907 - val_acc: 0.5657\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8032 - acc: 0.6726 - val_loss: 0.9851 - val_acc: 0.5829\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8225 - acc: 0.6637 - val_loss: 0.9845 - val_acc: 0.5829\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8220 - acc: 0.6618 - val_loss: 0.9837 - val_acc: 0.5829\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7990 - acc: 0.6694 - val_loss: 0.9869 - val_acc: 0.5829\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8165 - acc: 0.6637 - val_loss: 0.9834 - val_acc: 0.5829\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8096 - acc: 0.6841 - val_loss: 0.9849 - val_acc: 0.5829\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8043 - acc: 0.6816 - val_loss: 0.9904 - val_acc: 0.5829\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7963 - acc: 0.6796 - val_loss: 0.9896 - val_acc: 0.5829\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8002 - acc: 0.6720 - val_loss: 1.0044 - val_acc: 0.5771\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7973 - acc: 0.6867 - val_loss: 0.9989 - val_acc: 0.5771\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8036 - acc: 0.6726 - val_loss: 0.9928 - val_acc: 0.5829\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7996 - acc: 0.6758 - val_loss: 1.0009 - val_acc: 0.5829\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7926 - acc: 0.6784 - val_loss: 0.9960 - val_acc: 0.5714\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7869 - acc: 0.6784 - val_loss: 0.9969 - val_acc: 0.6000\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8020 - acc: 0.6739 - val_loss: 0.9911 - val_acc: 0.5943\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7934 - acc: 0.6745 - val_loss: 0.9852 - val_acc: 0.6057\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7803 - acc: 0.6879 - val_loss: 0.9906 - val_acc: 0.5943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7783 - acc: 0.6816 - val_loss: 0.9959 - val_acc: 0.5886\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7972 - acc: 0.6771 - val_loss: 0.9911 - val_acc: 0.5886\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7764 - acc: 0.6777 - val_loss: 0.9980 - val_acc: 0.5943\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7804 - acc: 0.6854 - val_loss: 0.9977 - val_acc: 0.6000\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7699 - acc: 0.6733 - val_loss: 0.9998 - val_acc: 0.6000\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7600 - acc: 0.6962 - val_loss: 1.0026 - val_acc: 0.5943\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7872 - acc: 0.6816 - val_loss: 1.0029 - val_acc: 0.5886\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7667 - acc: 0.6886 - val_loss: 1.0010 - val_acc: 0.6000\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7518 - acc: 0.6835 - val_loss: 1.0052 - val_acc: 0.6057\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7660 - acc: 0.6943 - val_loss: 1.0055 - val_acc: 0.5943\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7615 - acc: 0.6860 - val_loss: 1.0019 - val_acc: 0.6057\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.7570 - acc: 0.6803 - val_loss: 1.0040 - val_acc: 0.6000\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 101us/step - loss: 0.7752 - acc: 0.6777 - val_loss: 1.0047 - val_acc: 0.6000\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7626 - acc: 0.7135 - val_loss: 1.0166 - val_acc: 0.6000\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.7633 - acc: 0.6886 - val_loss: 1.0153 - val_acc: 0.6057\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 149us/step - loss: 0.7480 - acc: 0.7052 - val_loss: 1.0162 - val_acc: 0.5943\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7557 - acc: 0.7001 - val_loss: 1.0190 - val_acc: 0.6057\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 97us/step - loss: 0.7692 - acc: 0.6790 - val_loss: 1.0116 - val_acc: 0.6000\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7654 - acc: 0.6879 - val_loss: 1.0103 - val_acc: 0.5943\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.7534 - acc: 0.6905 - val_loss: 1.0068 - val_acc: 0.6114\n",
      "193/193 [==============================] - 0s 134us/step\n",
      "1742/1742 [==============================] - 0s 40us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.5233 - acc: 0.2944 - val_loss: 1.2257 - val_acc: 0.4400\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.3155 - acc: 0.3972 - val_loss: 1.0900 - val_acc: 0.6400\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1970 - acc: 0.4815 - val_loss: 1.0461 - val_acc: 0.6229\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1614 - acc: 0.4866 - val_loss: 1.0172 - val_acc: 0.6514\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0929 - acc: 0.5249 - val_loss: 0.9860 - val_acc: 0.6229\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0552 - acc: 0.5536 - val_loss: 0.9694 - val_acc: 0.6114\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0688 - acc: 0.5460 - val_loss: 0.9501 - val_acc: 0.6629\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0283 - acc: 0.5568 - val_loss: 0.9425 - val_acc: 0.6457\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0358 - acc: 0.5651 - val_loss: 0.9216 - val_acc: 0.6514\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0046 - acc: 0.5594 - val_loss: 0.9110 - val_acc: 0.6457\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.9873 - acc: 0.5741 - val_loss: 0.9088 - val_acc: 0.6514\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9717 - acc: 0.6034 - val_loss: 0.9011 - val_acc: 0.6400\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9722 - acc: 0.5964 - val_loss: 0.9025 - val_acc: 0.6400\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9588 - acc: 0.5926 - val_loss: 0.8993 - val_acc: 0.6571\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9331 - acc: 0.6003 - val_loss: 0.8954 - val_acc: 0.6743\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9282 - acc: 0.5971 - val_loss: 0.9003 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9201 - acc: 0.6194 - val_loss: 0.8961 - val_acc: 0.6571\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9304 - acc: 0.5977 - val_loss: 0.8906 - val_acc: 0.6800\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9064 - acc: 0.6162 - val_loss: 0.8899 - val_acc: 0.6571\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8953 - acc: 0.6175 - val_loss: 0.8931 - val_acc: 0.6743\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8905 - acc: 0.6392 - val_loss: 0.8819 - val_acc: 0.6914\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8893 - acc: 0.6213 - val_loss: 0.8848 - val_acc: 0.6514\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8967 - acc: 0.6201 - val_loss: 0.8861 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8687 - acc: 0.6290 - val_loss: 0.8822 - val_acc: 0.6686\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8811 - acc: 0.6373 - val_loss: 0.8911 - val_acc: 0.6571\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8712 - acc: 0.6526 - val_loss: 0.8832 - val_acc: 0.6629\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8473 - acc: 0.6584 - val_loss: 0.8830 - val_acc: 0.6743\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8400 - acc: 0.6616 - val_loss: 0.8847 - val_acc: 0.6571\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8549 - acc: 0.6367 - val_loss: 0.8843 - val_acc: 0.6857\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8606 - acc: 0.6437 - val_loss: 0.8898 - val_acc: 0.6571\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8554 - acc: 0.6513 - val_loss: 0.8918 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8341 - acc: 0.6641 - val_loss: 0.8914 - val_acc: 0.6571\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8260 - acc: 0.6628 - val_loss: 0.8959 - val_acc: 0.6571\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8479 - acc: 0.6520 - val_loss: 0.8932 - val_acc: 0.6629\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8392 - acc: 0.6513 - val_loss: 0.8923 - val_acc: 0.6686\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8346 - acc: 0.6526 - val_loss: 0.9023 - val_acc: 0.6743\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8164 - acc: 0.6679 - val_loss: 0.9056 - val_acc: 0.6629\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8261 - acc: 0.6635 - val_loss: 0.9028 - val_acc: 0.6686\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8178 - acc: 0.6648 - val_loss: 0.9084 - val_acc: 0.6571\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8248 - acc: 0.6711 - val_loss: 0.9138 - val_acc: 0.6457\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8181 - acc: 0.6603 - val_loss: 0.9103 - val_acc: 0.6400\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7994 - acc: 0.6711 - val_loss: 0.9144 - val_acc: 0.6629\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7862 - acc: 0.6865 - val_loss: 0.9165 - val_acc: 0.6514\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7771 - acc: 0.6807 - val_loss: 0.9132 - val_acc: 0.6400\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7891 - acc: 0.6705 - val_loss: 0.9159 - val_acc: 0.6457\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7985 - acc: 0.6782 - val_loss: 0.9162 - val_acc: 0.6571\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7912 - acc: 0.6699 - val_loss: 0.9213 - val_acc: 0.6457\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7780 - acc: 0.6858 - val_loss: 0.9262 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7773 - acc: 0.6782 - val_loss: 0.9249 - val_acc: 0.6571\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7921 - acc: 0.6775 - val_loss: 0.9182 - val_acc: 0.6514\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7759 - acc: 0.6814 - val_loss: 0.9162 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7682 - acc: 0.6826 - val_loss: 0.9221 - val_acc: 0.6514\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7549 - acc: 0.7005 - val_loss: 0.9187 - val_acc: 0.6343\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7796 - acc: 0.6890 - val_loss: 0.9228 - val_acc: 0.6343\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7567 - acc: 0.6916 - val_loss: 0.9168 - val_acc: 0.6457\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7375 - acc: 0.6967 - val_loss: 0.9176 - val_acc: 0.6400\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7615 - acc: 0.6839 - val_loss: 0.9264 - val_acc: 0.6457\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7443 - acc: 0.6922 - val_loss: 0.9187 - val_acc: 0.6514\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7498 - acc: 0.6967 - val_loss: 0.9186 - val_acc: 0.6514\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7397 - acc: 0.7011 - val_loss: 0.9261 - val_acc: 0.6343\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7306 - acc: 0.7126 - val_loss: 0.9312 - val_acc: 0.6400\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7405 - acc: 0.7050 - val_loss: 0.9376 - val_acc: 0.6286\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7403 - acc: 0.7024 - val_loss: 0.9322 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7423 - acc: 0.7011 - val_loss: 0.9240 - val_acc: 0.6571\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7355 - acc: 0.7146 - val_loss: 0.9261 - val_acc: 0.6457\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7246 - acc: 0.7095 - val_loss: 0.9303 - val_acc: 0.6514\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7203 - acc: 0.7075 - val_loss: 0.9270 - val_acc: 0.6686\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7077 - acc: 0.7158 - val_loss: 0.9349 - val_acc: 0.6629\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7098 - acc: 0.7235 - val_loss: 0.9383 - val_acc: 0.6571\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7244 - acc: 0.6948 - val_loss: 0.9396 - val_acc: 0.6571\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7145 - acc: 0.7267 - val_loss: 0.9424 - val_acc: 0.6571\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7027 - acc: 0.7165 - val_loss: 0.9441 - val_acc: 0.6400\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7042 - acc: 0.7152 - val_loss: 0.9364 - val_acc: 0.6571\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7132 - acc: 0.7216 - val_loss: 0.9397 - val_acc: 0.6629\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7283 - acc: 0.7171 - val_loss: 0.9476 - val_acc: 0.6457\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7108 - acc: 0.7095 - val_loss: 0.9410 - val_acc: 0.6514\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7093 - acc: 0.7286 - val_loss: 0.9425 - val_acc: 0.6457\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6992 - acc: 0.7267 - val_loss: 0.9424 - val_acc: 0.6343\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6973 - acc: 0.7280 - val_loss: 0.9464 - val_acc: 0.6343\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6986 - acc: 0.7158 - val_loss: 0.9492 - val_acc: 0.6457\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7002 - acc: 0.7101 - val_loss: 0.9480 - val_acc: 0.6400\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6837 - acc: 0.7248 - val_loss: 0.9499 - val_acc: 0.6514\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6732 - acc: 0.7299 - val_loss: 0.9576 - val_acc: 0.6457\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6903 - acc: 0.7401 - val_loss: 0.9632 - val_acc: 0.6571\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6598 - acc: 0.7350 - val_loss: 0.9705 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6678 - acc: 0.7344 - val_loss: 0.9721 - val_acc: 0.6343\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6715 - acc: 0.7439 - val_loss: 0.9676 - val_acc: 0.6343\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6591 - acc: 0.7478 - val_loss: 0.9729 - val_acc: 0.6343\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6819 - acc: 0.7261 - val_loss: 0.9848 - val_acc: 0.6457\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6560 - acc: 0.7529 - val_loss: 0.9922 - val_acc: 0.6457\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6882 - acc: 0.7458 - val_loss: 0.9818 - val_acc: 0.6514\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6566 - acc: 0.7388 - val_loss: 0.9783 - val_acc: 0.6400\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6602 - acc: 0.7407 - val_loss: 0.9834 - val_acc: 0.6457\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6561 - acc: 0.7420 - val_loss: 0.9819 - val_acc: 0.6457\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6642 - acc: 0.7439 - val_loss: 0.9835 - val_acc: 0.6571\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6404 - acc: 0.7414 - val_loss: 0.9843 - val_acc: 0.6571\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6448 - acc: 0.7535 - val_loss: 0.9857 - val_acc: 0.6629\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6394 - acc: 0.7542 - val_loss: 0.9883 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6463 - acc: 0.7395 - val_loss: 0.9793 - val_acc: 0.6686\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6270 - acc: 0.7561 - val_loss: 0.9849 - val_acc: 0.6571\n",
      "194/194 [==============================] - 0s 67us/step\n",
      "1741/1741 [==============================] - 0s 39us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 9s 6ms/step - loss: 1.4531 - acc: 0.3097 - val_loss: 1.1703 - val_acc: 0.4914\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 1.2667 - acc: 0.4246 - val_loss: 1.0763 - val_acc: 0.5657\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 1.1945 - acc: 0.4706 - val_loss: 1.0307 - val_acc: 0.6057\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 1.1162 - acc: 0.4968 - val_loss: 0.9818 - val_acc: 0.6286\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 1.0930 - acc: 0.5211 - val_loss: 0.9624 - val_acc: 0.6343\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 1.0492 - acc: 0.5549 - val_loss: 0.9377 - val_acc: 0.6514\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 1.0060 - acc: 0.5702 - val_loss: 0.9255 - val_acc: 0.6400\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0193 - acc: 0.5581 - val_loss: 0.9205 - val_acc: 0.6686\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.9939 - acc: 0.5773 - val_loss: 0.9119 - val_acc: 0.6686\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.9770 - acc: 0.5881 - val_loss: 0.9075 - val_acc: 0.6514\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.9738 - acc: 0.5862 - val_loss: 0.9043 - val_acc: 0.6686\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9617 - acc: 0.6003 - val_loss: 0.9084 - val_acc: 0.6514\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 107us/step - loss: 0.9362 - acc: 0.6003 - val_loss: 0.9013 - val_acc: 0.6743\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.9391 - acc: 0.6092 - val_loss: 0.8935 - val_acc: 0.6457\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9314 - acc: 0.6264 - val_loss: 0.8969 - val_acc: 0.6514\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9223 - acc: 0.6054 - val_loss: 0.9004 - val_acc: 0.6743\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9008 - acc: 0.6175 - val_loss: 0.8967 - val_acc: 0.6686\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8934 - acc: 0.6207 - val_loss: 0.9064 - val_acc: 0.6743\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8832 - acc: 0.6315 - val_loss: 0.8949 - val_acc: 0.6743\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8993 - acc: 0.6162 - val_loss: 0.8920 - val_acc: 0.6514\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8810 - acc: 0.6258 - val_loss: 0.8894 - val_acc: 0.6686\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8751 - acc: 0.6373 - val_loss: 0.8888 - val_acc: 0.6914\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9032 - acc: 0.6322 - val_loss: 0.8897 - val_acc: 0.6800\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8756 - acc: 0.6252 - val_loss: 0.8893 - val_acc: 0.6800\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8681 - acc: 0.6424 - val_loss: 0.8960 - val_acc: 0.6514\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8284 - acc: 0.6539 - val_loss: 0.8966 - val_acc: 0.6743\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8556 - acc: 0.6443 - val_loss: 0.8987 - val_acc: 0.6629\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8721 - acc: 0.6520 - val_loss: 0.9005 - val_acc: 0.6686\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8455 - acc: 0.6622 - val_loss: 0.9025 - val_acc: 0.6629\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8364 - acc: 0.6558 - val_loss: 0.9021 - val_acc: 0.6514\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8324 - acc: 0.6692 - val_loss: 0.9010 - val_acc: 0.6400\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8163 - acc: 0.6750 - val_loss: 0.8950 - val_acc: 0.6514\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8246 - acc: 0.6526 - val_loss: 0.8958 - val_acc: 0.6514\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8239 - acc: 0.6667 - val_loss: 0.8933 - val_acc: 0.6629\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8124 - acc: 0.6635 - val_loss: 0.8966 - val_acc: 0.6457\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8374 - acc: 0.6731 - val_loss: 0.9036 - val_acc: 0.6514\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8065 - acc: 0.6724 - val_loss: 0.9017 - val_acc: 0.6743\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8057 - acc: 0.6711 - val_loss: 0.8995 - val_acc: 0.6571\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8021 - acc: 0.6660 - val_loss: 0.9141 - val_acc: 0.6400\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7909 - acc: 0.6756 - val_loss: 0.9063 - val_acc: 0.6343\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7909 - acc: 0.6788 - val_loss: 0.9095 - val_acc: 0.6400\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7926 - acc: 0.6852 - val_loss: 0.9054 - val_acc: 0.6514\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7900 - acc: 0.6897 - val_loss: 0.9100 - val_acc: 0.6629\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7811 - acc: 0.6801 - val_loss: 0.9104 - val_acc: 0.6686\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7833 - acc: 0.6833 - val_loss: 0.9047 - val_acc: 0.6629\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7756 - acc: 0.6750 - val_loss: 0.9086 - val_acc: 0.6686\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7838 - acc: 0.6935 - val_loss: 0.9032 - val_acc: 0.6686\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7683 - acc: 0.6871 - val_loss: 0.8985 - val_acc: 0.6686\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7860 - acc: 0.6858 - val_loss: 0.9064 - val_acc: 0.6686\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7559 - acc: 0.6960 - val_loss: 0.9125 - val_acc: 0.6743\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7550 - acc: 0.7056 - val_loss: 0.9044 - val_acc: 0.6686\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7665 - acc: 0.6877 - val_loss: 0.9097 - val_acc: 0.6629\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7525 - acc: 0.6954 - val_loss: 0.9124 - val_acc: 0.6686\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7529 - acc: 0.6916 - val_loss: 0.9116 - val_acc: 0.6629\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7425 - acc: 0.6967 - val_loss: 0.9139 - val_acc: 0.6629\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7560 - acc: 0.6845 - val_loss: 0.9089 - val_acc: 0.6571\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7500 - acc: 0.6999 - val_loss: 0.9097 - val_acc: 0.6686\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7558 - acc: 0.6814 - val_loss: 0.9168 - val_acc: 0.6686\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7410 - acc: 0.6897 - val_loss: 0.9230 - val_acc: 0.6686\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7390 - acc: 0.6967 - val_loss: 0.9157 - val_acc: 0.6686\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7283 - acc: 0.7037 - val_loss: 0.9207 - val_acc: 0.6571\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7199 - acc: 0.7241 - val_loss: 0.9220 - val_acc: 0.6571\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7214 - acc: 0.7152 - val_loss: 0.9192 - val_acc: 0.6457\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7093 - acc: 0.7120 - val_loss: 0.9221 - val_acc: 0.6686\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6939 - acc: 0.7120 - val_loss: 0.9283 - val_acc: 0.6686\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7222 - acc: 0.7082 - val_loss: 0.9325 - val_acc: 0.6457\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6983 - acc: 0.7203 - val_loss: 0.9356 - val_acc: 0.6514\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6969 - acc: 0.7184 - val_loss: 0.9306 - val_acc: 0.6457\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7094 - acc: 0.7056 - val_loss: 0.9291 - val_acc: 0.6571\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7008 - acc: 0.7075 - val_loss: 0.9284 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7148 - acc: 0.7171 - val_loss: 0.9286 - val_acc: 0.6686\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.6929 - acc: 0.7139 - val_loss: 0.9292 - val_acc: 0.6571\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6930 - acc: 0.7235 - val_loss: 0.9325 - val_acc: 0.6343\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6964 - acc: 0.7344 - val_loss: 0.9379 - val_acc: 0.6514\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6705 - acc: 0.7395 - val_loss: 0.9479 - val_acc: 0.6343\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6792 - acc: 0.7356 - val_loss: 0.9484 - val_acc: 0.6343\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6829 - acc: 0.7184 - val_loss: 0.9380 - val_acc: 0.6343\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6800 - acc: 0.7401 - val_loss: 0.9431 - val_acc: 0.6343\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6636 - acc: 0.7299 - val_loss: 0.9452 - val_acc: 0.6343\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6616 - acc: 0.7286 - val_loss: 0.9485 - val_acc: 0.6400\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6704 - acc: 0.7248 - val_loss: 0.9553 - val_acc: 0.6343\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6774 - acc: 0.7305 - val_loss: 0.9512 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6709 - acc: 0.7299 - val_loss: 0.9543 - val_acc: 0.6400\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6621 - acc: 0.7452 - val_loss: 0.9494 - val_acc: 0.6343\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6649 - acc: 0.7337 - val_loss: 0.9566 - val_acc: 0.6400\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6684 - acc: 0.7382 - val_loss: 0.9651 - val_acc: 0.6171\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6526 - acc: 0.7286 - val_loss: 0.9699 - val_acc: 0.6286\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6759 - acc: 0.7273 - val_loss: 0.9717 - val_acc: 0.6229\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6499 - acc: 0.7490 - val_loss: 0.9782 - val_acc: 0.6114\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6588 - acc: 0.7478 - val_loss: 0.9728 - val_acc: 0.6343\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6480 - acc: 0.7382 - val_loss: 0.9762 - val_acc: 0.6286\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6474 - acc: 0.7516 - val_loss: 0.9740 - val_acc: 0.6286\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6295 - acc: 0.7510 - val_loss: 0.9852 - val_acc: 0.6171\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6362 - acc: 0.7535 - val_loss: 0.9907 - val_acc: 0.6171\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6401 - acc: 0.7433 - val_loss: 0.9827 - val_acc: 0.6171\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6474 - acc: 0.7465 - val_loss: 0.9828 - val_acc: 0.6171\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6220 - acc: 0.7644 - val_loss: 0.9999 - val_acc: 0.6400\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6358 - acc: 0.7427 - val_loss: 1.0078 - val_acc: 0.6457\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6280 - acc: 0.7458 - val_loss: 1.0021 - val_acc: 0.6171\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6277 - acc: 0.7471 - val_loss: 1.0088 - val_acc: 0.6114\n",
      "194/194 [==============================] - 0s 62us/step\n",
      "1741/1741 [==============================] - 0s 28us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.4360 - acc: 0.3282 - val_loss: 1.1865 - val_acc: 0.5371\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 1.2585 - acc: 0.4234 - val_loss: 1.0870 - val_acc: 0.6114\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.2072 - acc: 0.4591 - val_loss: 1.0336 - val_acc: 0.6000\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 1.1593 - acc: 0.4891 - val_loss: 1.0047 - val_acc: 0.6229\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0794 - acc: 0.5396 - val_loss: 0.9809 - val_acc: 0.6514\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 1.0768 - acc: 0.5204 - val_loss: 0.9653 - val_acc: 0.6343\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0327 - acc: 0.5453 - val_loss: 0.9462 - val_acc: 0.6571\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0329 - acc: 0.5390 - val_loss: 0.9316 - val_acc: 0.6686\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0087 - acc: 0.5562 - val_loss: 0.9228 - val_acc: 0.6343\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0179 - acc: 0.5421 - val_loss: 0.9161 - val_acc: 0.6343\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.9798 - acc: 0.5811 - val_loss: 0.9185 - val_acc: 0.6571\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9650 - acc: 0.5862 - val_loss: 0.9200 - val_acc: 0.6571\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9603 - acc: 0.5900 - val_loss: 0.9161 - val_acc: 0.6514\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9493 - acc: 0.6079 - val_loss: 0.9064 - val_acc: 0.6857\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9470 - acc: 0.6041 - val_loss: 0.8996 - val_acc: 0.7086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9003 - acc: 0.6315 - val_loss: 0.9056 - val_acc: 0.6914\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9220 - acc: 0.6073 - val_loss: 0.9062 - val_acc: 0.6686\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9265 - acc: 0.6130 - val_loss: 0.9034 - val_acc: 0.6629\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9167 - acc: 0.6047 - val_loss: 0.9063 - val_acc: 0.6514\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9071 - acc: 0.6284 - val_loss: 0.9111 - val_acc: 0.6343\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8992 - acc: 0.6213 - val_loss: 0.9112 - val_acc: 0.6686\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8850 - acc: 0.6373 - val_loss: 0.9072 - val_acc: 0.6571\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8707 - acc: 0.6347 - val_loss: 0.9108 - val_acc: 0.6743\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8587 - acc: 0.6360 - val_loss: 0.9048 - val_acc: 0.6629\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8820 - acc: 0.6430 - val_loss: 0.9106 - val_acc: 0.6800\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8506 - acc: 0.6488 - val_loss: 0.9096 - val_acc: 0.6629\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8591 - acc: 0.6513 - val_loss: 0.9082 - val_acc: 0.6514\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.8509 - acc: 0.6533 - val_loss: 0.9076 - val_acc: 0.6686\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8462 - acc: 0.6469 - val_loss: 0.9095 - val_acc: 0.6400\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8335 - acc: 0.6539 - val_loss: 0.8960 - val_acc: 0.6743\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8320 - acc: 0.6660 - val_loss: 0.9019 - val_acc: 0.6800\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8326 - acc: 0.6679 - val_loss: 0.9114 - val_acc: 0.6686\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8363 - acc: 0.6603 - val_loss: 0.9092 - val_acc: 0.6629\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8227 - acc: 0.6616 - val_loss: 0.9099 - val_acc: 0.6743\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8121 - acc: 0.6673 - val_loss: 0.9097 - val_acc: 0.6629\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8248 - acc: 0.6654 - val_loss: 0.9083 - val_acc: 0.6857\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8239 - acc: 0.6699 - val_loss: 0.9070 - val_acc: 0.6457\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8068 - acc: 0.6628 - val_loss: 0.9108 - val_acc: 0.6743\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8215 - acc: 0.6590 - val_loss: 0.9108 - val_acc: 0.6686\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8147 - acc: 0.6526 - val_loss: 0.9111 - val_acc: 0.6857\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8053 - acc: 0.6558 - val_loss: 0.9145 - val_acc: 0.6571\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8014 - acc: 0.6686 - val_loss: 0.9190 - val_acc: 0.6629\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7975 - acc: 0.6705 - val_loss: 0.9239 - val_acc: 0.6743\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7861 - acc: 0.6718 - val_loss: 0.9201 - val_acc: 0.6571\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7981 - acc: 0.6877 - val_loss: 0.9236 - val_acc: 0.6457\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7827 - acc: 0.6801 - val_loss: 0.9183 - val_acc: 0.6629\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7821 - acc: 0.6737 - val_loss: 0.9232 - val_acc: 0.6629\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7734 - acc: 0.6935 - val_loss: 0.9319 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7659 - acc: 0.6865 - val_loss: 0.9295 - val_acc: 0.6514\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7703 - acc: 0.6782 - val_loss: 0.9265 - val_acc: 0.6514\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7658 - acc: 0.6845 - val_loss: 0.9284 - val_acc: 0.6457\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7552 - acc: 0.6967 - val_loss: 0.9288 - val_acc: 0.6400\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7638 - acc: 0.6814 - val_loss: 0.9295 - val_acc: 0.6457\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7535 - acc: 0.7075 - val_loss: 0.9304 - val_acc: 0.6229\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7662 - acc: 0.6839 - val_loss: 0.9333 - val_acc: 0.6400\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.7601 - acc: 0.6877 - val_loss: 0.9392 - val_acc: 0.6286\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7487 - acc: 0.7120 - val_loss: 0.9483 - val_acc: 0.6400\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7523 - acc: 0.6948 - val_loss: 0.9332 - val_acc: 0.6343\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7333 - acc: 0.6941 - val_loss: 0.9319 - val_acc: 0.6400\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7272 - acc: 0.7075 - val_loss: 0.9422 - val_acc: 0.6229\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7260 - acc: 0.6992 - val_loss: 0.9422 - val_acc: 0.6400\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7303 - acc: 0.6973 - val_loss: 0.9403 - val_acc: 0.6286\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7194 - acc: 0.7146 - val_loss: 0.9394 - val_acc: 0.6400\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7458 - acc: 0.6871 - val_loss: 0.9442 - val_acc: 0.6457\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7214 - acc: 0.7126 - val_loss: 0.9503 - val_acc: 0.6343\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7128 - acc: 0.7043 - val_loss: 0.9528 - val_acc: 0.6457\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7288 - acc: 0.7146 - val_loss: 0.9492 - val_acc: 0.6343\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7192 - acc: 0.7011 - val_loss: 0.9564 - val_acc: 0.6514\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7076 - acc: 0.7088 - val_loss: 0.9638 - val_acc: 0.6171\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7156 - acc: 0.7261 - val_loss: 0.9607 - val_acc: 0.6343\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7033 - acc: 0.7197 - val_loss: 0.9622 - val_acc: 0.6400\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7064 - acc: 0.7184 - val_loss: 0.9619 - val_acc: 0.6171\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6908 - acc: 0.7235 - val_loss: 0.9584 - val_acc: 0.6114\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7069 - acc: 0.7133 - val_loss: 0.9637 - val_acc: 0.6286\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6679 - acc: 0.7363 - val_loss: 0.9762 - val_acc: 0.6229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6877 - acc: 0.7133 - val_loss: 0.9758 - val_acc: 0.6286\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6848 - acc: 0.7229 - val_loss: 0.9832 - val_acc: 0.6171\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6866 - acc: 0.7209 - val_loss: 0.9759 - val_acc: 0.6343\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6837 - acc: 0.7305 - val_loss: 0.9809 - val_acc: 0.6286\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6887 - acc: 0.7209 - val_loss: 0.9765 - val_acc: 0.6286\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6694 - acc: 0.7363 - val_loss: 0.9829 - val_acc: 0.6229\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6775 - acc: 0.7261 - val_loss: 0.9729 - val_acc: 0.6343\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7002 - acc: 0.7120 - val_loss: 0.9741 - val_acc: 0.6171\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6781 - acc: 0.7261 - val_loss: 0.9879 - val_acc: 0.6229\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6765 - acc: 0.7324 - val_loss: 0.9883 - val_acc: 0.6171\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6646 - acc: 0.7305 - val_loss: 0.9855 - val_acc: 0.6343\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6770 - acc: 0.7337 - val_loss: 0.9918 - val_acc: 0.6114\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6800 - acc: 0.7114 - val_loss: 0.9865 - val_acc: 0.6114\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6683 - acc: 0.7337 - val_loss: 0.9848 - val_acc: 0.6229\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6726 - acc: 0.7382 - val_loss: 0.9942 - val_acc: 0.6286\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6572 - acc: 0.7401 - val_loss: 0.9967 - val_acc: 0.6229\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6590 - acc: 0.7299 - val_loss: 0.9977 - val_acc: 0.6171\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6596 - acc: 0.7407 - val_loss: 1.0001 - val_acc: 0.6343\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6612 - acc: 0.7375 - val_loss: 1.0050 - val_acc: 0.6343\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6609 - acc: 0.7254 - val_loss: 1.0108 - val_acc: 0.6286\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6520 - acc: 0.7305 - val_loss: 1.0063 - val_acc: 0.6343\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6388 - acc: 0.7401 - val_loss: 1.0020 - val_acc: 0.6229\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6395 - acc: 0.7471 - val_loss: 0.9994 - val_acc: 0.6171\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6528 - acc: 0.7414 - val_loss: 1.0011 - val_acc: 0.6229\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6333 - acc: 0.7644 - val_loss: 1.0075 - val_acc: 0.6343\n",
      "194/194 [==============================] - 0s 48us/step\n",
      "1741/1741 [==============================] - 0s 29us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 7s 5ms/step - loss: 1.4894 - acc: 0.3040 - val_loss: 1.1911 - val_acc: 0.4857\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.2649 - acc: 0.4125 - val_loss: 1.0829 - val_acc: 0.5486\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1719 - acc: 0.4764 - val_loss: 1.0189 - val_acc: 0.6000\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1406 - acc: 0.4936 - val_loss: 0.9906 - val_acc: 0.6229\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0891 - acc: 0.5313 - val_loss: 0.9687 - val_acc: 0.6457\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0684 - acc: 0.5345 - val_loss: 0.9530 - val_acc: 0.6571\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0423 - acc: 0.5556 - val_loss: 0.9399 - val_acc: 0.6571\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0238 - acc: 0.5747 - val_loss: 0.9242 - val_acc: 0.6914\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0070 - acc: 0.5837 - val_loss: 0.9220 - val_acc: 0.6571\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9960 - acc: 0.5734 - val_loss: 0.9268 - val_acc: 0.6400\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9970 - acc: 0.5683 - val_loss: 0.9261 - val_acc: 0.6514\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9904 - acc: 0.5741 - val_loss: 0.9124 - val_acc: 0.6743\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9855 - acc: 0.5798 - val_loss: 0.9052 - val_acc: 0.6457\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9745 - acc: 0.5837 - val_loss: 0.9109 - val_acc: 0.6571\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9575 - acc: 0.5939 - val_loss: 0.9003 - val_acc: 0.6800\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9509 - acc: 0.5798 - val_loss: 0.9006 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9321 - acc: 0.6034 - val_loss: 0.8991 - val_acc: 0.6800\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9142 - acc: 0.6105 - val_loss: 0.9040 - val_acc: 0.6914\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9101 - acc: 0.6079 - val_loss: 0.8985 - val_acc: 0.6857\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9102 - acc: 0.6149 - val_loss: 0.9008 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9107 - acc: 0.6149 - val_loss: 0.9046 - val_acc: 0.6686\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9016 - acc: 0.6207 - val_loss: 0.9011 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8969 - acc: 0.6239 - val_loss: 0.8953 - val_acc: 0.6914\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8999 - acc: 0.6290 - val_loss: 0.8950 - val_acc: 0.6629\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8927 - acc: 0.6271 - val_loss: 0.8922 - val_acc: 0.6629\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8552 - acc: 0.6437 - val_loss: 0.8957 - val_acc: 0.6629\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8726 - acc: 0.6398 - val_loss: 0.8898 - val_acc: 0.6743\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8587 - acc: 0.6418 - val_loss: 0.8995 - val_acc: 0.6571\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8365 - acc: 0.6558 - val_loss: 0.8972 - val_acc: 0.6629\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8597 - acc: 0.6443 - val_loss: 0.8956 - val_acc: 0.6686\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8537 - acc: 0.6520 - val_loss: 0.9015 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8532 - acc: 0.6513 - val_loss: 0.9000 - val_acc: 0.6629\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8499 - acc: 0.6405 - val_loss: 0.8970 - val_acc: 0.6629\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8505 - acc: 0.6430 - val_loss: 0.8961 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8413 - acc: 0.6456 - val_loss: 0.8974 - val_acc: 0.6857\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8573 - acc: 0.6507 - val_loss: 0.8934 - val_acc: 0.6686\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8139 - acc: 0.6679 - val_loss: 0.8999 - val_acc: 0.6800\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8301 - acc: 0.6679 - val_loss: 0.8980 - val_acc: 0.6743\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8173 - acc: 0.6603 - val_loss: 0.9038 - val_acc: 0.6686\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8187 - acc: 0.6539 - val_loss: 0.9103 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8097 - acc: 0.6654 - val_loss: 0.9062 - val_acc: 0.6743\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8106 - acc: 0.6513 - val_loss: 0.9141 - val_acc: 0.6686\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8263 - acc: 0.6660 - val_loss: 0.9118 - val_acc: 0.6629\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8069 - acc: 0.6667 - val_loss: 0.9210 - val_acc: 0.6743\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7983 - acc: 0.6718 - val_loss: 0.9201 - val_acc: 0.6743\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7881 - acc: 0.6718 - val_loss: 0.9128 - val_acc: 0.6914\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7955 - acc: 0.6635 - val_loss: 0.9162 - val_acc: 0.6800\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7648 - acc: 0.6948 - val_loss: 0.9181 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7950 - acc: 0.6731 - val_loss: 0.9274 - val_acc: 0.6629\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.8040 - acc: 0.6743 - val_loss: 0.9223 - val_acc: 0.6743\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7860 - acc: 0.6852 - val_loss: 0.9205 - val_acc: 0.6743\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7908 - acc: 0.6801 - val_loss: 0.9212 - val_acc: 0.6743\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7723 - acc: 0.6935 - val_loss: 0.9258 - val_acc: 0.6800\n",
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7584 - acc: 0.6890 - val_loss: 0.9382 - val_acc: 0.6743\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7639 - acc: 0.6980 - val_loss: 0.9418 - val_acc: 0.6800\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7584 - acc: 0.7031 - val_loss: 0.9319 - val_acc: 0.6743\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7734 - acc: 0.6794 - val_loss: 0.9400 - val_acc: 0.6686\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 117us/step - loss: 0.7647 - acc: 0.6941 - val_loss: 0.9353 - val_acc: 0.6686\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7418 - acc: 0.7056 - val_loss: 0.9423 - val_acc: 0.6400\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7455 - acc: 0.7101 - val_loss: 0.9369 - val_acc: 0.6571\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.7534 - acc: 0.6897 - val_loss: 0.9402 - val_acc: 0.6629\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.7451 - acc: 0.7107 - val_loss: 0.9416 - val_acc: 0.6571\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7488 - acc: 0.6833 - val_loss: 0.9359 - val_acc: 0.6629\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7490 - acc: 0.7011 - val_loss: 0.9399 - val_acc: 0.6743\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7388 - acc: 0.7082 - val_loss: 0.9419 - val_acc: 0.6629\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7313 - acc: 0.7088 - val_loss: 0.9465 - val_acc: 0.6629\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7155 - acc: 0.7248 - val_loss: 0.9424 - val_acc: 0.6629\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7399 - acc: 0.6935 - val_loss: 0.9465 - val_acc: 0.6514\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.7299 - acc: 0.6909 - val_loss: 0.9513 - val_acc: 0.6686\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.7260 - acc: 0.7069 - val_loss: 0.9494 - val_acc: 0.6571\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.7220 - acc: 0.7114 - val_loss: 0.9539 - val_acc: 0.6571\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7038 - acc: 0.7190 - val_loss: 0.9485 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7096 - acc: 0.7165 - val_loss: 0.9546 - val_acc: 0.6686\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7034 - acc: 0.7190 - val_loss: 0.9570 - val_acc: 0.6686\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 100us/step - loss: 0.7100 - acc: 0.7101 - val_loss: 0.9528 - val_acc: 0.6743\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.7092 - acc: 0.7037 - val_loss: 0.9633 - val_acc: 0.6743\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7092 - acc: 0.7197 - val_loss: 0.9635 - val_acc: 0.6686\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.6878 - acc: 0.7184 - val_loss: 0.9632 - val_acc: 0.6571\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7092 - acc: 0.7152 - val_loss: 0.9701 - val_acc: 0.6686\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6808 - acc: 0.7273 - val_loss: 0.9715 - val_acc: 0.6514\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6905 - acc: 0.7261 - val_loss: 0.9674 - val_acc: 0.6629\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6934 - acc: 0.7235 - val_loss: 0.9684 - val_acc: 0.6629\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.6878 - acc: 0.7184 - val_loss: 0.9734 - val_acc: 0.6571\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7068 - acc: 0.7197 - val_loss: 0.9734 - val_acc: 0.6686\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.6905 - acc: 0.7203 - val_loss: 0.9817 - val_acc: 0.6629\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6950 - acc: 0.7273 - val_loss: 0.9747 - val_acc: 0.6686\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6879 - acc: 0.7190 - val_loss: 0.9764 - val_acc: 0.6686\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.6817 - acc: 0.7286 - val_loss: 0.9740 - val_acc: 0.6743\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6639 - acc: 0.7190 - val_loss: 0.9887 - val_acc: 0.6629\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.6816 - acc: 0.7369 - val_loss: 0.9946 - val_acc: 0.6571\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 101us/step - loss: 0.6590 - acc: 0.7344 - val_loss: 0.9909 - val_acc: 0.6514\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6611 - acc: 0.7331 - val_loss: 0.9926 - val_acc: 0.6629\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6621 - acc: 0.7420 - val_loss: 0.9995 - val_acc: 0.6629\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6572 - acc: 0.7363 - val_loss: 1.0126 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6700 - acc: 0.7318 - val_loss: 1.0092 - val_acc: 0.6629\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6741 - acc: 0.7292 - val_loss: 1.0041 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 93us/step - loss: 0.6622 - acc: 0.7388 - val_loss: 1.0130 - val_acc: 0.6514\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6563 - acc: 0.7388 - val_loss: 1.0112 - val_acc: 0.6400\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6476 - acc: 0.7407 - val_loss: 1.0119 - val_acc: 0.6457\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6492 - acc: 0.7382 - val_loss: 1.0198 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 59us/step\n",
      "1741/1741 [==============================] - 0s 49us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1566/1566 [==============================] - 9s 6ms/step - loss: 1.4897 - acc: 0.2874 - val_loss: 1.1786 - val_acc: 0.5314\n",
      "Epoch 2/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.2509 - acc: 0.4253 - val_loss: 1.0687 - val_acc: 0.5943\n",
      "Epoch 3/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.1799 - acc: 0.4496 - val_loss: 1.0083 - val_acc: 0.6171\n",
      "Epoch 4/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1331 - acc: 0.4821 - val_loss: 0.9864 - val_acc: 0.6286\n",
      "Epoch 5/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0997 - acc: 0.5089 - val_loss: 0.9632 - val_acc: 0.6514\n",
      "Epoch 6/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.0627 - acc: 0.5383 - val_loss: 0.9480 - val_acc: 0.6514\n",
      "Epoch 7/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0390 - acc: 0.5479 - val_loss: 0.9385 - val_acc: 0.6343\n",
      "Epoch 8/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0079 - acc: 0.5594 - val_loss: 0.9300 - val_acc: 0.6686\n",
      "Epoch 9/100\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9957 - acc: 0.5600 - val_loss: 0.9177 - val_acc: 0.6514\n",
      "Epoch 10/100\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9892 - acc: 0.5773 - val_loss: 0.9227 - val_acc: 0.6457\n",
      "Epoch 11/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.9993 - acc: 0.5824 - val_loss: 0.9218 - val_acc: 0.6800\n",
      "Epoch 12/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9762 - acc: 0.5824 - val_loss: 0.9113 - val_acc: 0.6629\n",
      "Epoch 13/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9725 - acc: 0.5805 - val_loss: 0.9062 - val_acc: 0.6800\n",
      "Epoch 14/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9494 - acc: 0.5913 - val_loss: 0.9021 - val_acc: 0.6800\n",
      "Epoch 15/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9409 - acc: 0.5983 - val_loss: 0.9036 - val_acc: 0.6857\n",
      "Epoch 16/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9193 - acc: 0.6130 - val_loss: 0.8958 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9254 - acc: 0.6130 - val_loss: 0.8939 - val_acc: 0.6743\n",
      "Epoch 18/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9264 - acc: 0.6060 - val_loss: 0.8934 - val_acc: 0.6857\n",
      "Epoch 19/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9108 - acc: 0.6169 - val_loss: 0.8964 - val_acc: 0.6686\n",
      "Epoch 20/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8941 - acc: 0.6303 - val_loss: 0.8945 - val_acc: 0.6457\n",
      "Epoch 21/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8942 - acc: 0.6379 - val_loss: 0.8987 - val_acc: 0.6629\n",
      "Epoch 22/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8858 - acc: 0.6354 - val_loss: 0.8958 - val_acc: 0.6686\n",
      "Epoch 23/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8852 - acc: 0.6277 - val_loss: 0.8905 - val_acc: 0.6743\n",
      "Epoch 24/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9111 - acc: 0.6207 - val_loss: 0.8865 - val_acc: 0.6914\n",
      "Epoch 25/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8813 - acc: 0.6296 - val_loss: 0.8882 - val_acc: 0.6914\n",
      "Epoch 26/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8743 - acc: 0.6284 - val_loss: 0.8944 - val_acc: 0.6686\n",
      "Epoch 27/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8761 - acc: 0.6507 - val_loss: 0.8941 - val_acc: 0.6743\n",
      "Epoch 28/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8771 - acc: 0.6373 - val_loss: 0.8883 - val_acc: 0.6800\n",
      "Epoch 29/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8550 - acc: 0.6392 - val_loss: 0.8893 - val_acc: 0.6800\n",
      "Epoch 30/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8486 - acc: 0.6462 - val_loss: 0.8910 - val_acc: 0.6857\n",
      "Epoch 31/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8563 - acc: 0.6398 - val_loss: 0.8872 - val_acc: 0.6629\n",
      "Epoch 32/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8558 - acc: 0.6558 - val_loss: 0.8911 - val_acc: 0.6743\n",
      "Epoch 33/100\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8310 - acc: 0.6571 - val_loss: 0.8797 - val_acc: 0.6800\n",
      "Epoch 34/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8211 - acc: 0.6501 - val_loss: 0.8879 - val_acc: 0.6800\n",
      "Epoch 35/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8402 - acc: 0.6424 - val_loss: 0.8934 - val_acc: 0.7029\n",
      "Epoch 36/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8252 - acc: 0.6571 - val_loss: 0.8910 - val_acc: 0.6857\n",
      "Epoch 37/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8329 - acc: 0.6609 - val_loss: 0.8867 - val_acc: 0.6743\n",
      "Epoch 38/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8253 - acc: 0.6584 - val_loss: 0.8901 - val_acc: 0.6914\n",
      "Epoch 39/100\n",
      "1566/1566 [==============================] - 0s 43us/step - loss: 0.8078 - acc: 0.6533 - val_loss: 0.8971 - val_acc: 0.6800\n",
      "Epoch 40/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8203 - acc: 0.6564 - val_loss: 0.8971 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8063 - acc: 0.6826 - val_loss: 0.9081 - val_acc: 0.6743\n",
      "Epoch 42/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7958 - acc: 0.6788 - val_loss: 0.9030 - val_acc: 0.6686\n",
      "Epoch 43/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7909 - acc: 0.6801 - val_loss: 0.9114 - val_acc: 0.6800\n",
      "Epoch 44/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8073 - acc: 0.6794 - val_loss: 0.9147 - val_acc: 0.6686\n",
      "Epoch 45/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7936 - acc: 0.6679 - val_loss: 0.9281 - val_acc: 0.6686\n",
      "Epoch 46/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7697 - acc: 0.6992 - val_loss: 0.9263 - val_acc: 0.6800\n",
      "Epoch 47/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7806 - acc: 0.6826 - val_loss: 0.9202 - val_acc: 0.6743\n",
      "Epoch 48/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7716 - acc: 0.6845 - val_loss: 0.9152 - val_acc: 0.6629\n",
      "Epoch 49/100\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7646 - acc: 0.6948 - val_loss: 0.9141 - val_acc: 0.6743\n",
      "Epoch 50/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7732 - acc: 0.6743 - val_loss: 0.9131 - val_acc: 0.6629\n",
      "Epoch 51/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7819 - acc: 0.6820 - val_loss: 0.9110 - val_acc: 0.6514\n",
      "Epoch 52/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7839 - acc: 0.6782 - val_loss: 0.9124 - val_acc: 0.6686\n",
      "Epoch 53/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7630 - acc: 0.6833 - val_loss: 0.9181 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7518 - acc: 0.7011 - val_loss: 0.9250 - val_acc: 0.6629\n",
      "Epoch 55/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7653 - acc: 0.6877 - val_loss: 0.9262 - val_acc: 0.6686\n",
      "Epoch 56/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7474 - acc: 0.6980 - val_loss: 0.9141 - val_acc: 0.6743\n",
      "Epoch 57/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7392 - acc: 0.7063 - val_loss: 0.9187 - val_acc: 0.6686\n",
      "Epoch 58/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7571 - acc: 0.6960 - val_loss: 0.9318 - val_acc: 0.6743\n",
      "Epoch 59/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7257 - acc: 0.7114 - val_loss: 0.9283 - val_acc: 0.6686\n",
      "Epoch 60/100\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7488 - acc: 0.7114 - val_loss: 0.9334 - val_acc: 0.6457\n",
      "Epoch 61/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7322 - acc: 0.7011 - val_loss: 0.9338 - val_acc: 0.6343\n",
      "Epoch 62/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7611 - acc: 0.6948 - val_loss: 0.9334 - val_acc: 0.6400\n",
      "Epoch 63/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7288 - acc: 0.7024 - val_loss: 0.9387 - val_acc: 0.6457\n",
      "Epoch 64/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7212 - acc: 0.7005 - val_loss: 0.9417 - val_acc: 0.6514\n",
      "Epoch 65/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7064 - acc: 0.7286 - val_loss: 0.9456 - val_acc: 0.6629\n",
      "Epoch 66/100\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7031 - acc: 0.7069 - val_loss: 0.9433 - val_acc: 0.6571\n",
      "Epoch 67/100\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6856 - acc: 0.7158 - val_loss: 0.9462 - val_acc: 0.6571\n",
      "Epoch 68/100\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7148 - acc: 0.7171 - val_loss: 0.9594 - val_acc: 0.6629\n",
      "Epoch 69/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7065 - acc: 0.7235 - val_loss: 0.9620 - val_acc: 0.6457\n",
      "Epoch 70/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6955 - acc: 0.7229 - val_loss: 0.9683 - val_acc: 0.6343\n",
      "Epoch 71/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7143 - acc: 0.7088 - val_loss: 0.9682 - val_acc: 0.6457\n",
      "Epoch 72/100\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6993 - acc: 0.7133 - val_loss: 0.9645 - val_acc: 0.6571\n",
      "Epoch 73/100\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7013 - acc: 0.7139 - val_loss: 0.9651 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6905 - acc: 0.7165 - val_loss: 0.9649 - val_acc: 0.6514\n",
      "Epoch 75/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6996 - acc: 0.7088 - val_loss: 0.9705 - val_acc: 0.6514\n",
      "Epoch 76/100\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7082 - acc: 0.6980 - val_loss: 0.9676 - val_acc: 0.6457\n",
      "Epoch 77/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6856 - acc: 0.7216 - val_loss: 0.9690 - val_acc: 0.6629\n",
      "Epoch 78/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7006 - acc: 0.7069 - val_loss: 0.9690 - val_acc: 0.6514\n",
      "Epoch 79/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6763 - acc: 0.7209 - val_loss: 0.9670 - val_acc: 0.6457\n",
      "Epoch 80/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6895 - acc: 0.7158 - val_loss: 0.9727 - val_acc: 0.6400\n",
      "Epoch 81/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6715 - acc: 0.7203 - val_loss: 0.9806 - val_acc: 0.6571\n",
      "Epoch 82/100\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6656 - acc: 0.7510 - val_loss: 0.9800 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6769 - acc: 0.7312 - val_loss: 0.9885 - val_acc: 0.6286\n",
      "Epoch 84/100\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6780 - acc: 0.7324 - val_loss: 0.9909 - val_acc: 0.6400\n",
      "Epoch 85/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6648 - acc: 0.7305 - val_loss: 0.9860 - val_acc: 0.6629\n",
      "Epoch 86/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6757 - acc: 0.7350 - val_loss: 0.9971 - val_acc: 0.6343\n",
      "Epoch 87/100\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6422 - acc: 0.7292 - val_loss: 0.9955 - val_acc: 0.6571\n",
      "Epoch 88/100\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6522 - acc: 0.7427 - val_loss: 0.9958 - val_acc: 0.6457\n",
      "Epoch 89/100\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6619 - acc: 0.7267 - val_loss: 0.9919 - val_acc: 0.6400\n",
      "Epoch 90/100\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6559 - acc: 0.7344 - val_loss: 1.0031 - val_acc: 0.6514\n",
      "Epoch 91/100\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6458 - acc: 0.7401 - val_loss: 1.0068 - val_acc: 0.6343\n",
      "Epoch 92/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6462 - acc: 0.7484 - val_loss: 1.0053 - val_acc: 0.6343\n",
      "Epoch 93/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6516 - acc: 0.7414 - val_loss: 1.0000 - val_acc: 0.6514\n",
      "Epoch 94/100\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6339 - acc: 0.7510 - val_loss: 1.0086 - val_acc: 0.6286\n",
      "Epoch 95/100\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6248 - acc: 0.7414 - val_loss: 1.0046 - val_acc: 0.6457\n",
      "Epoch 96/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6548 - acc: 0.7382 - val_loss: 1.0033 - val_acc: 0.6571\n",
      "Epoch 97/100\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6392 - acc: 0.7388 - val_loss: 1.0109 - val_acc: 0.6514\n",
      "Epoch 98/100\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6344 - acc: 0.7471 - val_loss: 1.0189 - val_acc: 0.6629\n",
      "Epoch 99/100\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6148 - acc: 0.7516 - val_loss: 1.0081 - val_acc: 0.6514\n",
      "Epoch 100/100\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6267 - acc: 0.7446 - val_loss: 1.0220 - val_acc: 0.6286\n",
      "194/194 [==============================] - 0s 88us/step\n",
      "1741/1741 [==============================] - 0s 33us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5548 - acc: 0.3191 - val_loss: 1.2296 - val_acc: 0.4514\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.2915 - acc: 0.4123 - val_loss: 1.0799 - val_acc: 0.6400\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.1778 - acc: 0.4665 - val_loss: 1.0200 - val_acc: 0.6286\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0989 - acc: 0.5156 - val_loss: 0.9846 - val_acc: 0.6229\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.0845 - acc: 0.5207 - val_loss: 0.9612 - val_acc: 0.6343\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.0615 - acc: 0.5392 - val_loss: 0.9439 - val_acc: 0.6343\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0497 - acc: 0.5622 - val_loss: 0.9412 - val_acc: 0.6343\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0081 - acc: 0.5846 - val_loss: 0.9310 - val_acc: 0.6457\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0071 - acc: 0.5667 - val_loss: 0.9279 - val_acc: 0.6400\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9918 - acc: 0.5782 - val_loss: 0.9176 - val_acc: 0.6343\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9887 - acc: 0.5884 - val_loss: 0.9123 - val_acc: 0.6686\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9691 - acc: 0.5807 - val_loss: 0.9143 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9722 - acc: 0.5954 - val_loss: 0.9116 - val_acc: 0.6629\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9529 - acc: 0.5897 - val_loss: 0.9066 - val_acc: 0.6571\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9351 - acc: 0.6056 - val_loss: 0.8988 - val_acc: 0.6629\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9354 - acc: 0.6082 - val_loss: 0.9020 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9163 - acc: 0.6094 - val_loss: 0.9005 - val_acc: 0.6629\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8905 - acc: 0.6343 - val_loss: 0.8878 - val_acc: 0.6971\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8986 - acc: 0.6324 - val_loss: 0.8927 - val_acc: 0.6571\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9003 - acc: 0.6350 - val_loss: 0.8953 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9035 - acc: 0.6299 - val_loss: 0.8959 - val_acc: 0.6743\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8989 - acc: 0.6273 - val_loss: 0.9071 - val_acc: 0.6629\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8811 - acc: 0.6439 - val_loss: 0.9027 - val_acc: 0.6743\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8899 - acc: 0.6260 - val_loss: 0.9132 - val_acc: 0.6686\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8825 - acc: 0.6337 - val_loss: 0.9100 - val_acc: 0.6686\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8840 - acc: 0.6260 - val_loss: 0.9067 - val_acc: 0.6686\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8532 - acc: 0.6554 - val_loss: 0.9035 - val_acc: 0.6743\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8549 - acc: 0.6382 - val_loss: 0.9030 - val_acc: 0.6800\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8609 - acc: 0.6337 - val_loss: 0.8970 - val_acc: 0.6914\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8603 - acc: 0.6401 - val_loss: 0.8938 - val_acc: 0.6743\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8560 - acc: 0.6420 - val_loss: 0.8955 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8418 - acc: 0.6426 - val_loss: 0.9037 - val_acc: 0.6571\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8423 - acc: 0.6579 - val_loss: 0.9064 - val_acc: 0.6686\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8302 - acc: 0.6548 - val_loss: 0.9155 - val_acc: 0.6571\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8138 - acc: 0.6777 - val_loss: 0.9127 - val_acc: 0.6629\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8434 - acc: 0.6503 - val_loss: 0.9052 - val_acc: 0.6629\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8098 - acc: 0.6599 - val_loss: 0.9003 - val_acc: 0.6800\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8260 - acc: 0.6631 - val_loss: 0.9047 - val_acc: 0.6686\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8033 - acc: 0.6726 - val_loss: 0.9044 - val_acc: 0.6629\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7946 - acc: 0.6765 - val_loss: 0.9160 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8048 - acc: 0.6752 - val_loss: 0.9130 - val_acc: 0.6629\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7972 - acc: 0.6771 - val_loss: 0.9165 - val_acc: 0.6743\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8014 - acc: 0.6758 - val_loss: 0.9176 - val_acc: 0.6800\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7873 - acc: 0.6707 - val_loss: 0.9185 - val_acc: 0.6743\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7975 - acc: 0.6643 - val_loss: 0.9182 - val_acc: 0.6743\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7940 - acc: 0.6643 - val_loss: 0.9187 - val_acc: 0.6743\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7884 - acc: 0.6905 - val_loss: 0.9319 - val_acc: 0.6800\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7585 - acc: 0.6886 - val_loss: 0.9169 - val_acc: 0.6857\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7769 - acc: 0.6969 - val_loss: 0.9216 - val_acc: 0.6857\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7971 - acc: 0.6694 - val_loss: 0.9260 - val_acc: 0.6686\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7704 - acc: 0.6847 - val_loss: 0.9341 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7737 - acc: 0.6803 - val_loss: 0.9332 - val_acc: 0.6686\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7640 - acc: 0.6822 - val_loss: 0.9314 - val_acc: 0.6743\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7646 - acc: 0.6886 - val_loss: 0.9320 - val_acc: 0.6514\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7466 - acc: 0.6879 - val_loss: 0.9316 - val_acc: 0.6571\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7482 - acc: 0.6943 - val_loss: 0.9369 - val_acc: 0.6457\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7473 - acc: 0.6911 - val_loss: 0.9411 - val_acc: 0.6457\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7516 - acc: 0.6943 - val_loss: 0.9530 - val_acc: 0.6629\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7430 - acc: 0.6937 - val_loss: 0.9469 - val_acc: 0.6686\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7207 - acc: 0.6943 - val_loss: 0.9523 - val_acc: 0.6686\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7339 - acc: 0.7007 - val_loss: 0.9496 - val_acc: 0.6629\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7493 - acc: 0.6835 - val_loss: 0.9511 - val_acc: 0.6571\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6999 - acc: 0.7224 - val_loss: 0.9461 - val_acc: 0.6743\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7297 - acc: 0.7039 - val_loss: 0.9538 - val_acc: 0.6686\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7074 - acc: 0.7160 - val_loss: 0.9503 - val_acc: 0.6743\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7176 - acc: 0.7103 - val_loss: 0.9489 - val_acc: 0.6857\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7199 - acc: 0.7179 - val_loss: 0.9364 - val_acc: 0.6800\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7125 - acc: 0.7058 - val_loss: 0.9477 - val_acc: 0.6457\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7208 - acc: 0.7071 - val_loss: 0.9457 - val_acc: 0.6686\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7028 - acc: 0.7160 - val_loss: 0.9491 - val_acc: 0.6629\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7014 - acc: 0.7128 - val_loss: 0.9562 - val_acc: 0.6514\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7066 - acc: 0.7198 - val_loss: 0.9545 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7039 - acc: 0.7141 - val_loss: 0.9617 - val_acc: 0.6514\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7038 - acc: 0.7039 - val_loss: 0.9580 - val_acc: 0.6571\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6889 - acc: 0.7262 - val_loss: 0.9524 - val_acc: 0.6514\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6738 - acc: 0.7167 - val_loss: 0.9578 - val_acc: 0.6571\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6788 - acc: 0.7250 - val_loss: 0.9701 - val_acc: 0.6743\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6909 - acc: 0.7167 - val_loss: 0.9676 - val_acc: 0.6571\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6793 - acc: 0.7294 - val_loss: 0.9814 - val_acc: 0.6629\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6818 - acc: 0.7262 - val_loss: 0.9888 - val_acc: 0.6629\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6672 - acc: 0.7275 - val_loss: 0.9855 - val_acc: 0.6686\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6693 - acc: 0.7358 - val_loss: 0.9846 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6600 - acc: 0.7250 - val_loss: 0.9966 - val_acc: 0.6686\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6753 - acc: 0.7358 - val_loss: 1.0029 - val_acc: 0.6571\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6621 - acc: 0.7224 - val_loss: 1.0002 - val_acc: 0.6571\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6654 - acc: 0.7288 - val_loss: 0.9970 - val_acc: 0.6571\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6722 - acc: 0.7345 - val_loss: 1.0087 - val_acc: 0.6686\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6484 - acc: 0.7384 - val_loss: 1.0140 - val_acc: 0.6571\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6557 - acc: 0.7390 - val_loss: 1.0091 - val_acc: 0.6457\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6530 - acc: 0.7198 - val_loss: 1.0027 - val_acc: 0.6400\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6458 - acc: 0.7486 - val_loss: 1.0157 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6431 - acc: 0.7428 - val_loss: 1.0211 - val_acc: 0.6457\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6263 - acc: 0.7588 - val_loss: 1.0314 - val_acc: 0.6514\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6436 - acc: 0.7524 - val_loss: 1.0244 - val_acc: 0.6457\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6382 - acc: 0.7486 - val_loss: 1.0177 - val_acc: 0.6343\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6440 - acc: 0.7326 - val_loss: 1.0162 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6376 - acc: 0.7460 - val_loss: 1.0225 - val_acc: 0.6457\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6377 - acc: 0.7384 - val_loss: 1.0210 - val_acc: 0.6514\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6262 - acc: 0.7492 - val_loss: 1.0364 - val_acc: 0.6457\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6383 - acc: 0.7364 - val_loss: 1.0351 - val_acc: 0.6571\n",
      "193/193 [==============================] - 0s 52us/step\n",
      "1742/1742 [==============================] - 0s 33us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 7s 5ms/step - loss: 1.5000 - acc: 0.3127 - val_loss: 1.2079 - val_acc: 0.5086\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.2511 - acc: 0.4346 - val_loss: 1.0823 - val_acc: 0.6171\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.1880 - acc: 0.4722 - val_loss: 1.0227 - val_acc: 0.6343\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.1128 - acc: 0.5080 - val_loss: 0.9836 - val_acc: 0.6457\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 43us/step - loss: 1.0750 - acc: 0.5329 - val_loss: 0.9543 - val_acc: 0.6571\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.0735 - acc: 0.5284 - val_loss: 0.9353 - val_acc: 0.6400\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0418 - acc: 0.5571 - val_loss: 0.9197 - val_acc: 0.6686\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0222 - acc: 0.5609 - val_loss: 0.9131 - val_acc: 0.6800\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9970 - acc: 0.5731 - val_loss: 0.9008 - val_acc: 0.6743\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9730 - acc: 0.5839 - val_loss: 0.8938 - val_acc: 0.6571\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9535 - acc: 0.5865 - val_loss: 0.8947 - val_acc: 0.6571\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9562 - acc: 0.5865 - val_loss: 0.8848 - val_acc: 0.6457\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9322 - acc: 0.6082 - val_loss: 0.8892 - val_acc: 0.6629\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9341 - acc: 0.6037 - val_loss: 0.8925 - val_acc: 0.6514\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9299 - acc: 0.6126 - val_loss: 0.8906 - val_acc: 0.6629\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9111 - acc: 0.6177 - val_loss: 0.8884 - val_acc: 0.6686\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9049 - acc: 0.6216 - val_loss: 0.8797 - val_acc: 0.6686\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9159 - acc: 0.6209 - val_loss: 0.8775 - val_acc: 0.6743\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9106 - acc: 0.6177 - val_loss: 0.8791 - val_acc: 0.6686\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8995 - acc: 0.6222 - val_loss: 0.8770 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8898 - acc: 0.6305 - val_loss: 0.8748 - val_acc: 0.6571\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8725 - acc: 0.6286 - val_loss: 0.8767 - val_acc: 0.6629\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8731 - acc: 0.6375 - val_loss: 0.8767 - val_acc: 0.6571\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8798 - acc: 0.6375 - val_loss: 0.8731 - val_acc: 0.6629\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8689 - acc: 0.6324 - val_loss: 0.8710 - val_acc: 0.6629\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8598 - acc: 0.6445 - val_loss: 0.8694 - val_acc: 0.6743\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8358 - acc: 0.6439 - val_loss: 0.8684 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8452 - acc: 0.6465 - val_loss: 0.8615 - val_acc: 0.6514\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8489 - acc: 0.6445 - val_loss: 0.8621 - val_acc: 0.6514\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8421 - acc: 0.6503 - val_loss: 0.8670 - val_acc: 0.6514\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8371 - acc: 0.6567 - val_loss: 0.8706 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8154 - acc: 0.6631 - val_loss: 0.8684 - val_acc: 0.6343\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8217 - acc: 0.6669 - val_loss: 0.8717 - val_acc: 0.6571\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8334 - acc: 0.6496 - val_loss: 0.8641 - val_acc: 0.6514\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8163 - acc: 0.6618 - val_loss: 0.8584 - val_acc: 0.6571\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8122 - acc: 0.6631 - val_loss: 0.8669 - val_acc: 0.6457\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7903 - acc: 0.6701 - val_loss: 0.8825 - val_acc: 0.6457\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8006 - acc: 0.6643 - val_loss: 0.8666 - val_acc: 0.6629\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7733 - acc: 0.6841 - val_loss: 0.8735 - val_acc: 0.6343\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7911 - acc: 0.6739 - val_loss: 0.8741 - val_acc: 0.6343\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7947 - acc: 0.6816 - val_loss: 0.8772 - val_acc: 0.6514\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 42us/step - loss: 0.7810 - acc: 0.6726 - val_loss: 0.8719 - val_acc: 0.6571\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8026 - acc: 0.6682 - val_loss: 0.8722 - val_acc: 0.6514\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7838 - acc: 0.6803 - val_loss: 0.8822 - val_acc: 0.6629\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7736 - acc: 0.6771 - val_loss: 0.8812 - val_acc: 0.6686\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7637 - acc: 0.6841 - val_loss: 0.8783 - val_acc: 0.6629\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7782 - acc: 0.6739 - val_loss: 0.8760 - val_acc: 0.6457\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7739 - acc: 0.6816 - val_loss: 0.8726 - val_acc: 0.6514\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7436 - acc: 0.7128 - val_loss: 0.8750 - val_acc: 0.6343\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7590 - acc: 0.6899 - val_loss: 0.8752 - val_acc: 0.6400\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7659 - acc: 0.6943 - val_loss: 0.8787 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7609 - acc: 0.6860 - val_loss: 0.8772 - val_acc: 0.6571\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7365 - acc: 0.6937 - val_loss: 0.8791 - val_acc: 0.6514\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7526 - acc: 0.6911 - val_loss: 0.8847 - val_acc: 0.6571\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7377 - acc: 0.6841 - val_loss: 0.8842 - val_acc: 0.6457\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7384 - acc: 0.7077 - val_loss: 0.8828 - val_acc: 0.6514\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7398 - acc: 0.6969 - val_loss: 0.8817 - val_acc: 0.6457\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7342 - acc: 0.7045 - val_loss: 0.8755 - val_acc: 0.6743\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7297 - acc: 0.7116 - val_loss: 0.8717 - val_acc: 0.6457\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7308 - acc: 0.7007 - val_loss: 0.8771 - val_acc: 0.6514\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7249 - acc: 0.7147 - val_loss: 0.8765 - val_acc: 0.6400\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7452 - acc: 0.6975 - val_loss: 0.8755 - val_acc: 0.6457\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7163 - acc: 0.7128 - val_loss: 0.8779 - val_acc: 0.6514\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7111 - acc: 0.7103 - val_loss: 0.8806 - val_acc: 0.6629\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7041 - acc: 0.7077 - val_loss: 0.8856 - val_acc: 0.6629\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7158 - acc: 0.7109 - val_loss: 0.8934 - val_acc: 0.6457\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7106 - acc: 0.7033 - val_loss: 0.8913 - val_acc: 0.6629\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7129 - acc: 0.7071 - val_loss: 0.8941 - val_acc: 0.6571\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6914 - acc: 0.7109 - val_loss: 0.8965 - val_acc: 0.6514\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6859 - acc: 0.7135 - val_loss: 0.8913 - val_acc: 0.6686\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6886 - acc: 0.7224 - val_loss: 0.8967 - val_acc: 0.6571\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6910 - acc: 0.7211 - val_loss: 0.8974 - val_acc: 0.6514\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6935 - acc: 0.7269 - val_loss: 0.8991 - val_acc: 0.6400\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6894 - acc: 0.7160 - val_loss: 0.9004 - val_acc: 0.6629\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6752 - acc: 0.7218 - val_loss: 0.8993 - val_acc: 0.6457\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6945 - acc: 0.7141 - val_loss: 0.8957 - val_acc: 0.6571\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.6662 - acc: 0.7403 - val_loss: 0.8992 - val_acc: 0.6457\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6863 - acc: 0.7160 - val_loss: 0.8968 - val_acc: 0.6457\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6842 - acc: 0.7122 - val_loss: 0.9017 - val_acc: 0.6400\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6664 - acc: 0.7275 - val_loss: 0.8963 - val_acc: 0.6343\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6592 - acc: 0.7307 - val_loss: 0.8973 - val_acc: 0.6514\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6706 - acc: 0.7288 - val_loss: 0.9051 - val_acc: 0.6400\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6866 - acc: 0.7275 - val_loss: 0.9131 - val_acc: 0.6457\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6500 - acc: 0.7288 - val_loss: 0.9106 - val_acc: 0.6514\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6613 - acc: 0.7320 - val_loss: 0.9090 - val_acc: 0.6514\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6705 - acc: 0.7275 - val_loss: 0.9071 - val_acc: 0.6686\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6558 - acc: 0.7275 - val_loss: 0.9017 - val_acc: 0.6457\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6569 - acc: 0.7250 - val_loss: 0.9043 - val_acc: 0.6514\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6343 - acc: 0.7377 - val_loss: 0.9109 - val_acc: 0.6629\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6485 - acc: 0.7396 - val_loss: 0.9145 - val_acc: 0.6686\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6450 - acc: 0.7358 - val_loss: 0.9156 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6203 - acc: 0.7581 - val_loss: 0.9188 - val_acc: 0.6629\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6448 - acc: 0.7371 - val_loss: 0.9194 - val_acc: 0.6571\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6347 - acc: 0.7441 - val_loss: 0.9113 - val_acc: 0.6686\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.6526 - acc: 0.7403 - val_loss: 0.9167 - val_acc: 0.6571\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6382 - acc: 0.7435 - val_loss: 0.9228 - val_acc: 0.6571\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6109 - acc: 0.7441 - val_loss: 0.9192 - val_acc: 0.6686\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6238 - acc: 0.7505 - val_loss: 0.9251 - val_acc: 0.6571\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6081 - acc: 0.7613 - val_loss: 0.9217 - val_acc: 0.6629\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6269 - acc: 0.7479 - val_loss: 0.9263 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 44us/step\n",
      "1742/1742 [==============================] - 0s 31us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5141 - acc: 0.3216 - val_loss: 1.2186 - val_acc: 0.4114\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.2799 - acc: 0.4180 - val_loss: 1.1034 - val_acc: 0.5714\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.1927 - acc: 0.4659 - val_loss: 1.0462 - val_acc: 0.6057\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.1516 - acc: 0.4920 - val_loss: 1.0093 - val_acc: 0.6057\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.1231 - acc: 0.5163 - val_loss: 0.9805 - val_acc: 0.6514\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 1.0675 - acc: 0.5290 - val_loss: 0.9606 - val_acc: 0.6400\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0516 - acc: 0.5520 - val_loss: 0.9422 - val_acc: 0.6743\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0338 - acc: 0.5609 - val_loss: 0.9383 - val_acc: 0.6629\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9876 - acc: 0.5750 - val_loss: 0.9248 - val_acc: 0.6514\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9853 - acc: 0.5846 - val_loss: 0.9194 - val_acc: 0.6343\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9917 - acc: 0.5756 - val_loss: 0.9001 - val_acc: 0.6686\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9627 - acc: 0.5973 - val_loss: 0.8969 - val_acc: 0.6629\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9419 - acc: 0.6043 - val_loss: 0.9030 - val_acc: 0.6800\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9293 - acc: 0.5929 - val_loss: 0.8941 - val_acc: 0.6857\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9341 - acc: 0.6075 - val_loss: 0.8995 - val_acc: 0.6857\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9333 - acc: 0.6069 - val_loss: 0.8937 - val_acc: 0.6857\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9285 - acc: 0.6126 - val_loss: 0.8877 - val_acc: 0.6800\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9183 - acc: 0.6146 - val_loss: 0.8934 - val_acc: 0.6686\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8983 - acc: 0.6362 - val_loss: 0.8969 - val_acc: 0.6914\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8905 - acc: 0.6369 - val_loss: 0.9109 - val_acc: 0.7029\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8740 - acc: 0.6273 - val_loss: 0.8996 - val_acc: 0.6686\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8811 - acc: 0.6311 - val_loss: 0.8877 - val_acc: 0.6800\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8530 - acc: 0.6452 - val_loss: 0.8813 - val_acc: 0.6743\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8792 - acc: 0.6299 - val_loss: 0.8797 - val_acc: 0.6800\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8805 - acc: 0.6337 - val_loss: 0.8863 - val_acc: 0.6571\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8683 - acc: 0.6388 - val_loss: 0.8928 - val_acc: 0.6743\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8481 - acc: 0.6560 - val_loss: 0.8898 - val_acc: 0.6686\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8708 - acc: 0.6414 - val_loss: 0.8924 - val_acc: 0.6857\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8419 - acc: 0.6496 - val_loss: 0.9007 - val_acc: 0.6857\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8354 - acc: 0.6579 - val_loss: 0.9002 - val_acc: 0.6857\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8615 - acc: 0.6394 - val_loss: 0.8945 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8377 - acc: 0.6618 - val_loss: 0.8962 - val_acc: 0.6629\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8331 - acc: 0.6707 - val_loss: 0.8884 - val_acc: 0.6800\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8170 - acc: 0.6733 - val_loss: 0.8900 - val_acc: 0.6571\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8167 - acc: 0.6688 - val_loss: 0.8953 - val_acc: 0.6743\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8258 - acc: 0.6618 - val_loss: 0.8931 - val_acc: 0.6686\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8396 - acc: 0.6516 - val_loss: 0.9104 - val_acc: 0.6400\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8133 - acc: 0.6720 - val_loss: 0.9068 - val_acc: 0.6514\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7995 - acc: 0.6675 - val_loss: 0.9021 - val_acc: 0.6686\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7996 - acc: 0.6784 - val_loss: 0.9013 - val_acc: 0.6629\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8055 - acc: 0.6599 - val_loss: 0.8956 - val_acc: 0.6857\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8052 - acc: 0.6688 - val_loss: 0.9032 - val_acc: 0.6629\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7915 - acc: 0.6758 - val_loss: 0.9125 - val_acc: 0.6571\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7963 - acc: 0.6675 - val_loss: 0.9084 - val_acc: 0.6571\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7898 - acc: 0.6771 - val_loss: 0.9142 - val_acc: 0.6514\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7887 - acc: 0.6822 - val_loss: 0.9127 - val_acc: 0.6743\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7726 - acc: 0.6873 - val_loss: 0.9103 - val_acc: 0.6686\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7611 - acc: 0.6835 - val_loss: 0.9233 - val_acc: 0.6686\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7833 - acc: 0.6803 - val_loss: 0.9146 - val_acc: 0.6457\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7810 - acc: 0.6784 - val_loss: 0.9111 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.7644 - acc: 0.6860 - val_loss: 0.9149 - val_acc: 0.6857\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.7764 - acc: 0.7033 - val_loss: 0.9123 - val_acc: 0.6629\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7639 - acc: 0.6911 - val_loss: 0.9177 - val_acc: 0.6571\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7566 - acc: 0.6892 - val_loss: 0.9172 - val_acc: 0.6800\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7581 - acc: 0.6892 - val_loss: 0.9298 - val_acc: 0.6743\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7504 - acc: 0.6930 - val_loss: 0.9312 - val_acc: 0.6457\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7577 - acc: 0.6924 - val_loss: 0.9349 - val_acc: 0.6400\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7438 - acc: 0.7084 - val_loss: 0.9311 - val_acc: 0.6457\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7570 - acc: 0.6956 - val_loss: 0.9256 - val_acc: 0.6571\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7354 - acc: 0.6994 - val_loss: 0.9226 - val_acc: 0.6743\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7337 - acc: 0.7007 - val_loss: 0.9371 - val_acc: 0.6343\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7304 - acc: 0.7084 - val_loss: 0.9266 - val_acc: 0.6571\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7320 - acc: 0.7186 - val_loss: 0.9284 - val_acc: 0.6571\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7116 - acc: 0.7103 - val_loss: 0.9380 - val_acc: 0.6629\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7261 - acc: 0.7084 - val_loss: 0.9389 - val_acc: 0.6686\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7208 - acc: 0.7141 - val_loss: 0.9459 - val_acc: 0.6514\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7060 - acc: 0.7160 - val_loss: 0.9351 - val_acc: 0.6686\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7310 - acc: 0.7090 - val_loss: 0.9443 - val_acc: 0.6571\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7019 - acc: 0.7179 - val_loss: 0.9390 - val_acc: 0.6629\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7250 - acc: 0.7026 - val_loss: 0.9352 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6831 - acc: 0.7326 - val_loss: 0.9520 - val_acc: 0.6343\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7083 - acc: 0.7243 - val_loss: 0.9560 - val_acc: 0.6571\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7002 - acc: 0.7224 - val_loss: 0.9570 - val_acc: 0.6457\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7104 - acc: 0.7071 - val_loss: 0.9573 - val_acc: 0.6400\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7025 - acc: 0.7135 - val_loss: 0.9564 - val_acc: 0.6571\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6904 - acc: 0.7256 - val_loss: 0.9540 - val_acc: 0.6571\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6891 - acc: 0.7332 - val_loss: 0.9523 - val_acc: 0.6400\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7008 - acc: 0.7250 - val_loss: 0.9542 - val_acc: 0.6743\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6895 - acc: 0.7294 - val_loss: 0.9573 - val_acc: 0.6686\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6801 - acc: 0.7224 - val_loss: 0.9675 - val_acc: 0.6457\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6732 - acc: 0.7288 - val_loss: 0.9589 - val_acc: 0.6457\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6797 - acc: 0.7211 - val_loss: 0.9518 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6769 - acc: 0.7186 - val_loss: 0.9468 - val_acc: 0.6514\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6603 - acc: 0.7320 - val_loss: 0.9612 - val_acc: 0.6571\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6733 - acc: 0.7269 - val_loss: 0.9731 - val_acc: 0.6571\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6863 - acc: 0.7186 - val_loss: 0.9626 - val_acc: 0.6629\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6678 - acc: 0.7332 - val_loss: 0.9735 - val_acc: 0.6457\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6622 - acc: 0.7454 - val_loss: 0.9773 - val_acc: 0.6571\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6395 - acc: 0.7428 - val_loss: 0.9743 - val_acc: 0.6629\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6664 - acc: 0.7384 - val_loss: 0.9788 - val_acc: 0.6514\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6610 - acc: 0.7390 - val_loss: 0.9780 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6488 - acc: 0.7332 - val_loss: 0.9799 - val_acc: 0.6629\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6390 - acc: 0.7396 - val_loss: 0.9796 - val_acc: 0.6629\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6631 - acc: 0.7218 - val_loss: 0.9819 - val_acc: 0.6343\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6447 - acc: 0.7428 - val_loss: 0.9992 - val_acc: 0.6457\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6389 - acc: 0.7313 - val_loss: 0.9870 - val_acc: 0.6457\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6453 - acc: 0.7447 - val_loss: 0.9915 - val_acc: 0.6629\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6411 - acc: 0.7498 - val_loss: 0.9957 - val_acc: 0.6686\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6226 - acc: 0.7537 - val_loss: 0.9994 - val_acc: 0.6400\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6388 - acc: 0.7466 - val_loss: 1.0160 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 41us/step\n",
      "1742/1742 [==============================] - 0s 34us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5060 - acc: 0.3025 - val_loss: 1.1934 - val_acc: 0.4971\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.2623 - acc: 0.4308 - val_loss: 1.0797 - val_acc: 0.5714\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.1986 - acc: 0.4678 - val_loss: 1.0081 - val_acc: 0.6171\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.1246 - acc: 0.5093 - val_loss: 0.9837 - val_acc: 0.6171\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 1.0788 - acc: 0.5367 - val_loss: 0.9743 - val_acc: 0.6171\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0524 - acc: 0.5718 - val_loss: 0.9570 - val_acc: 0.6343\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0592 - acc: 0.5392 - val_loss: 0.9415 - val_acc: 0.6686\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0271 - acc: 0.5514 - val_loss: 0.9492 - val_acc: 0.6743\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.0250 - acc: 0.5488 - val_loss: 0.9402 - val_acc: 0.6686\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.0066 - acc: 0.5558 - val_loss: 0.9252 - val_acc: 0.6743\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9684 - acc: 0.5846 - val_loss: 0.9281 - val_acc: 0.6743\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9792 - acc: 0.5846 - val_loss: 0.9264 - val_acc: 0.6743\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9669 - acc: 0.5890 - val_loss: 0.9184 - val_acc: 0.6800\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9618 - acc: 0.6024 - val_loss: 0.9119 - val_acc: 0.6743\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9626 - acc: 0.5884 - val_loss: 0.9139 - val_acc: 0.6457\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9475 - acc: 0.6063 - val_loss: 0.9070 - val_acc: 0.6857\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9059 - acc: 0.6069 - val_loss: 0.9051 - val_acc: 0.6971\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9071 - acc: 0.6101 - val_loss: 0.9184 - val_acc: 0.6571\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9235 - acc: 0.6146 - val_loss: 0.9074 - val_acc: 0.6629\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.9076 - acc: 0.6139 - val_loss: 0.9159 - val_acc: 0.6629\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.9007 - acc: 0.6216 - val_loss: 0.9118 - val_acc: 0.6629\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9062 - acc: 0.6139 - val_loss: 0.9199 - val_acc: 0.6857\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8799 - acc: 0.6369 - val_loss: 0.9157 - val_acc: 0.6800\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8746 - acc: 0.6433 - val_loss: 0.9066 - val_acc: 0.6800\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8888 - acc: 0.6248 - val_loss: 0.9090 - val_acc: 0.6457\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8639 - acc: 0.6439 - val_loss: 0.9030 - val_acc: 0.6629\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8754 - acc: 0.6414 - val_loss: 0.8967 - val_acc: 0.6743\n",
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8575 - acc: 0.6522 - val_loss: 0.9009 - val_acc: 0.6743\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8713 - acc: 0.6490 - val_loss: 0.9001 - val_acc: 0.6743\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8539 - acc: 0.6509 - val_loss: 0.9037 - val_acc: 0.6629\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8363 - acc: 0.6599 - val_loss: 0.9073 - val_acc: 0.6686\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8409 - acc: 0.6579 - val_loss: 0.9082 - val_acc: 0.6571\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8494 - acc: 0.6490 - val_loss: 0.9165 - val_acc: 0.6400\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8345 - acc: 0.6752 - val_loss: 0.9142 - val_acc: 0.6343\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8264 - acc: 0.6611 - val_loss: 0.9173 - val_acc: 0.6514\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8238 - acc: 0.6688 - val_loss: 0.9160 - val_acc: 0.6629\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8370 - acc: 0.6516 - val_loss: 0.9132 - val_acc: 0.6629\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8349 - acc: 0.6592 - val_loss: 0.9202 - val_acc: 0.6457\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8040 - acc: 0.6611 - val_loss: 0.9213 - val_acc: 0.6514\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8122 - acc: 0.6720 - val_loss: 0.9142 - val_acc: 0.6686\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8149 - acc: 0.6694 - val_loss: 0.9172 - val_acc: 0.6686\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8001 - acc: 0.6899 - val_loss: 0.9298 - val_acc: 0.6629\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8176 - acc: 0.6803 - val_loss: 0.9258 - val_acc: 0.6743\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7955 - acc: 0.6720 - val_loss: 0.9302 - val_acc: 0.6514\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7842 - acc: 0.6975 - val_loss: 0.9414 - val_acc: 0.6400\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7796 - acc: 0.6867 - val_loss: 0.9261 - val_acc: 0.6629\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7790 - acc: 0.6892 - val_loss: 0.9293 - val_acc: 0.6514\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7642 - acc: 0.6924 - val_loss: 0.9311 - val_acc: 0.6400\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7714 - acc: 0.6752 - val_loss: 0.9280 - val_acc: 0.6457\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7720 - acc: 0.6943 - val_loss: 0.9249 - val_acc: 0.6400\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7818 - acc: 0.6739 - val_loss: 0.9318 - val_acc: 0.6571\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7612 - acc: 0.6950 - val_loss: 0.9361 - val_acc: 0.6629\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7550 - acc: 0.6962 - val_loss: 0.9417 - val_acc: 0.6343\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7804 - acc: 0.6796 - val_loss: 0.9372 - val_acc: 0.6514\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 120us/step - loss: 0.7557 - acc: 0.6962 - val_loss: 0.9346 - val_acc: 0.6400\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7577 - acc: 0.6899 - val_loss: 0.9334 - val_acc: 0.6229\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7393 - acc: 0.7045 - val_loss: 0.9312 - val_acc: 0.6286\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7334 - acc: 0.7084 - val_loss: 0.9372 - val_acc: 0.6343\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7382 - acc: 0.7020 - val_loss: 0.9393 - val_acc: 0.6514\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7331 - acc: 0.6988 - val_loss: 0.9406 - val_acc: 0.6457\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7257 - acc: 0.7077 - val_loss: 0.9494 - val_acc: 0.6400\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7423 - acc: 0.6975 - val_loss: 0.9514 - val_acc: 0.6400\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7568 - acc: 0.7001 - val_loss: 0.9468 - val_acc: 0.6286\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7263 - acc: 0.7103 - val_loss: 0.9502 - val_acc: 0.6343\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7580 - acc: 0.6867 - val_loss: 0.9454 - val_acc: 0.6286\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7204 - acc: 0.7250 - val_loss: 0.9531 - val_acc: 0.6286\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7245 - acc: 0.7135 - val_loss: 0.9494 - val_acc: 0.6343\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7002 - acc: 0.7294 - val_loss: 0.9582 - val_acc: 0.6286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7146 - acc: 0.7167 - val_loss: 0.9554 - val_acc: 0.6457\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7064 - acc: 0.7352 - val_loss: 0.9696 - val_acc: 0.6400\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7196 - acc: 0.7064 - val_loss: 0.9692 - val_acc: 0.6286\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7134 - acc: 0.7128 - val_loss: 0.9712 - val_acc: 0.6400\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7080 - acc: 0.7198 - val_loss: 0.9746 - val_acc: 0.6457\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7155 - acc: 0.7173 - val_loss: 0.9720 - val_acc: 0.6286\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7016 - acc: 0.7147 - val_loss: 0.9742 - val_acc: 0.6400\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6956 - acc: 0.7192 - val_loss: 0.9684 - val_acc: 0.6343\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6972 - acc: 0.7147 - val_loss: 0.9765 - val_acc: 0.6629\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6890 - acc: 0.7326 - val_loss: 0.9734 - val_acc: 0.6457\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6866 - acc: 0.7167 - val_loss: 0.9678 - val_acc: 0.6400\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6770 - acc: 0.7109 - val_loss: 0.9731 - val_acc: 0.6343\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.6894 - acc: 0.7237 - val_loss: 0.9778 - val_acc: 0.6400\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6787 - acc: 0.7230 - val_loss: 0.9797 - val_acc: 0.6457\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6836 - acc: 0.7179 - val_loss: 0.9797 - val_acc: 0.6457\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6696 - acc: 0.7262 - val_loss: 0.9766 - val_acc: 0.6343\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6828 - acc: 0.7352 - val_loss: 0.9859 - val_acc: 0.6343\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6928 - acc: 0.7256 - val_loss: 0.9872 - val_acc: 0.6229\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6867 - acc: 0.7320 - val_loss: 0.9905 - val_acc: 0.6114\n",
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6818 - acc: 0.7211 - val_loss: 0.9956 - val_acc: 0.6400\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6621 - acc: 0.7352 - val_loss: 1.0015 - val_acc: 0.6343\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6469 - acc: 0.7396 - val_loss: 0.9993 - val_acc: 0.6286\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6481 - acc: 0.7371 - val_loss: 1.0028 - val_acc: 0.6457\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6461 - acc: 0.7428 - val_loss: 1.0066 - val_acc: 0.6400\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6496 - acc: 0.7352 - val_loss: 0.9993 - val_acc: 0.6400\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6569 - acc: 0.7409 - val_loss: 0.9976 - val_acc: 0.6229\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6370 - acc: 0.7415 - val_loss: 0.9909 - val_acc: 0.6457\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6421 - acc: 0.7422 - val_loss: 1.0101 - val_acc: 0.6686\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6273 - acc: 0.7460 - val_loss: 1.0035 - val_acc: 0.6343\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6416 - acc: 0.7435 - val_loss: 1.0006 - val_acc: 0.6343\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6335 - acc: 0.7403 - val_loss: 1.0039 - val_acc: 0.6457\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6328 - acc: 0.7441 - val_loss: 1.0173 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 51us/step\n",
      "1742/1742 [==============================] - 0s 32us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/100\n",
      "1567/1567 [==============================] - 7s 5ms/step - loss: 1.5230 - acc: 0.2789 - val_loss: 1.2303 - val_acc: 0.4743\n",
      "Epoch 2/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.2981 - acc: 0.3969 - val_loss: 1.1291 - val_acc: 0.4800\n",
      "Epoch 3/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.1865 - acc: 0.4754 - val_loss: 1.0752 - val_acc: 0.5314\n",
      "Epoch 4/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.1225 - acc: 0.4952 - val_loss: 1.0384 - val_acc: 0.5771\n",
      "Epoch 5/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0945 - acc: 0.5144 - val_loss: 1.0150 - val_acc: 0.5429\n",
      "Epoch 6/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0583 - acc: 0.5303 - val_loss: 1.0075 - val_acc: 0.5543\n",
      "Epoch 7/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0342 - acc: 0.5616 - val_loss: 1.0024 - val_acc: 0.5371\n",
      "Epoch 8/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0370 - acc: 0.5463 - val_loss: 0.9860 - val_acc: 0.5371\n",
      "Epoch 9/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.0110 - acc: 0.5724 - val_loss: 0.9862 - val_acc: 0.5314\n",
      "Epoch 10/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.0040 - acc: 0.5705 - val_loss: 0.9778 - val_acc: 0.5486\n",
      "Epoch 11/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9851 - acc: 0.5833 - val_loss: 0.9620 - val_acc: 0.5714\n",
      "Epoch 12/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9796 - acc: 0.5916 - val_loss: 0.9672 - val_acc: 0.5600\n",
      "Epoch 13/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9501 - acc: 0.5929 - val_loss: 0.9639 - val_acc: 0.5657\n",
      "Epoch 14/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9600 - acc: 0.5954 - val_loss: 0.9736 - val_acc: 0.5657\n",
      "Epoch 15/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9598 - acc: 0.5890 - val_loss: 0.9706 - val_acc: 0.5886\n",
      "Epoch 16/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9379 - acc: 0.6069 - val_loss: 0.9606 - val_acc: 0.6057\n",
      "Epoch 17/100\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.9094 - acc: 0.6203 - val_loss: 0.9698 - val_acc: 0.5829\n",
      "Epoch 18/100\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.9333 - acc: 0.6018 - val_loss: 0.9663 - val_acc: 0.5886\n",
      "Epoch 19/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9094 - acc: 0.6133 - val_loss: 0.9583 - val_acc: 0.5886\n",
      "Epoch 20/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9071 - acc: 0.6248 - val_loss: 0.9652 - val_acc: 0.5829\n",
      "Epoch 21/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8869 - acc: 0.6241 - val_loss: 0.9620 - val_acc: 0.5714\n",
      "Epoch 22/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8940 - acc: 0.6292 - val_loss: 0.9671 - val_acc: 0.5886\n",
      "Epoch 23/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8981 - acc: 0.6216 - val_loss: 0.9680 - val_acc: 0.5714\n",
      "Epoch 24/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8805 - acc: 0.6407 - val_loss: 0.9618 - val_acc: 0.5829\n",
      "Epoch 25/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8866 - acc: 0.6299 - val_loss: 0.9638 - val_acc: 0.5886\n",
      "Epoch 26/100\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8670 - acc: 0.6292 - val_loss: 0.9720 - val_acc: 0.5771\n",
      "Epoch 27/100\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8740 - acc: 0.6318 - val_loss: 0.9589 - val_acc: 0.5886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8581 - acc: 0.6254 - val_loss: 0.9686 - val_acc: 0.5829\n",
      "Epoch 29/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8482 - acc: 0.6554 - val_loss: 0.9517 - val_acc: 0.5829\n",
      "Epoch 30/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8698 - acc: 0.6496 - val_loss: 0.9659 - val_acc: 0.5829\n",
      "Epoch 31/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8414 - acc: 0.6573 - val_loss: 0.9714 - val_acc: 0.6000\n",
      "Epoch 32/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8203 - acc: 0.6637 - val_loss: 0.9730 - val_acc: 0.5829\n",
      "Epoch 33/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8387 - acc: 0.6675 - val_loss: 0.9776 - val_acc: 0.5886\n",
      "Epoch 34/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8494 - acc: 0.6401 - val_loss: 0.9676 - val_acc: 0.5771\n",
      "Epoch 35/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8320 - acc: 0.6643 - val_loss: 0.9797 - val_acc: 0.5829\n",
      "Epoch 36/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8306 - acc: 0.6618 - val_loss: 0.9781 - val_acc: 0.5829\n",
      "Epoch 37/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8201 - acc: 0.6682 - val_loss: 0.9866 - val_acc: 0.5829\n",
      "Epoch 38/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8064 - acc: 0.6745 - val_loss: 0.9833 - val_acc: 0.6114\n",
      "Epoch 39/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8100 - acc: 0.6713 - val_loss: 0.9924 - val_acc: 0.6000\n",
      "Epoch 40/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8002 - acc: 0.6643 - val_loss: 0.9854 - val_acc: 0.5943\n",
      "Epoch 41/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8193 - acc: 0.6631 - val_loss: 0.9863 - val_acc: 0.5943\n",
      "Epoch 42/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8030 - acc: 0.6771 - val_loss: 0.9981 - val_acc: 0.6000\n",
      "Epoch 43/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7930 - acc: 0.6726 - val_loss: 0.9911 - val_acc: 0.5714\n",
      "Epoch 44/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7972 - acc: 0.6860 - val_loss: 0.9798 - val_acc: 0.5771\n",
      "Epoch 45/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7898 - acc: 0.6822 - val_loss: 0.9988 - val_acc: 0.5829\n",
      "Epoch 46/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8070 - acc: 0.6688 - val_loss: 0.9810 - val_acc: 0.5771\n",
      "Epoch 47/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7716 - acc: 0.6784 - val_loss: 0.9809 - val_acc: 0.5829\n",
      "Epoch 48/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7801 - acc: 0.6828 - val_loss: 0.9923 - val_acc: 0.5829\n",
      "Epoch 49/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7843 - acc: 0.6899 - val_loss: 0.9971 - val_acc: 0.5771\n",
      "Epoch 50/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7719 - acc: 0.6809 - val_loss: 0.9945 - val_acc: 0.5943\n",
      "Epoch 51/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7709 - acc: 0.6879 - val_loss: 1.0036 - val_acc: 0.5886\n",
      "Epoch 52/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7755 - acc: 0.6988 - val_loss: 0.9953 - val_acc: 0.5771\n",
      "Epoch 53/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7593 - acc: 0.7052 - val_loss: 0.9956 - val_acc: 0.5943\n",
      "Epoch 54/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7576 - acc: 0.7013 - val_loss: 1.0005 - val_acc: 0.6000\n",
      "Epoch 55/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7718 - acc: 0.7020 - val_loss: 1.0123 - val_acc: 0.6000\n",
      "Epoch 56/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7651 - acc: 0.6911 - val_loss: 1.0111 - val_acc: 0.5829\n",
      "Epoch 57/100\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7458 - acc: 0.7013 - val_loss: 1.0151 - val_acc: 0.6000\n",
      "Epoch 58/100\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7496 - acc: 0.6981 - val_loss: 1.0099 - val_acc: 0.6057\n",
      "Epoch 59/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7276 - acc: 0.7071 - val_loss: 1.0069 - val_acc: 0.5943\n",
      "Epoch 60/100\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7476 - acc: 0.6937 - val_loss: 1.0104 - val_acc: 0.5943\n",
      "Epoch 61/100\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7363 - acc: 0.7039 - val_loss: 1.0074 - val_acc: 0.5943\n",
      "Epoch 62/100\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7458 - acc: 0.7001 - val_loss: 1.0307 - val_acc: 0.5943\n",
      "Epoch 63/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7336 - acc: 0.7058 - val_loss: 1.0260 - val_acc: 0.5943\n",
      "Epoch 64/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7270 - acc: 0.7052 - val_loss: 1.0321 - val_acc: 0.5886\n",
      "Epoch 65/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7229 - acc: 0.7001 - val_loss: 1.0286 - val_acc: 0.5943\n",
      "Epoch 66/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7277 - acc: 0.7103 - val_loss: 1.0264 - val_acc: 0.6000\n",
      "Epoch 67/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7364 - acc: 0.7058 - val_loss: 1.0324 - val_acc: 0.5886\n",
      "Epoch 68/100\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7082 - acc: 0.7122 - val_loss: 1.0248 - val_acc: 0.5886\n",
      "Epoch 69/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7175 - acc: 0.7243 - val_loss: 1.0311 - val_acc: 0.6000\n",
      "Epoch 70/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7123 - acc: 0.7039 - val_loss: 1.0386 - val_acc: 0.5829\n",
      "Epoch 71/100\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7130 - acc: 0.7122 - val_loss: 1.0284 - val_acc: 0.5829\n",
      "Epoch 72/100\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7098 - acc: 0.7122 - val_loss: 1.0433 - val_acc: 0.5943\n",
      "Epoch 73/100\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6894 - acc: 0.7224 - val_loss: 1.0441 - val_acc: 0.6000\n",
      "Epoch 74/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6977 - acc: 0.7192 - val_loss: 1.0533 - val_acc: 0.5943\n",
      "Epoch 75/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6894 - acc: 0.7224 - val_loss: 1.0405 - val_acc: 0.6000\n",
      "Epoch 76/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7003 - acc: 0.7198 - val_loss: 1.0453 - val_acc: 0.6057\n",
      "Epoch 77/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7035 - acc: 0.7224 - val_loss: 1.0542 - val_acc: 0.5943\n",
      "Epoch 78/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6879 - acc: 0.7301 - val_loss: 1.0529 - val_acc: 0.6000\n",
      "Epoch 79/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6775 - acc: 0.7198 - val_loss: 1.0467 - val_acc: 0.5943\n",
      "Epoch 80/100\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6989 - acc: 0.7230 - val_loss: 1.0576 - val_acc: 0.5943\n",
      "Epoch 81/100\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6773 - acc: 0.7294 - val_loss: 1.0442 - val_acc: 0.6114\n",
      "Epoch 82/100\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7138 - acc: 0.7058 - val_loss: 1.0509 - val_acc: 0.6057\n",
      "Epoch 83/100\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6706 - acc: 0.7243 - val_loss: 1.0575 - val_acc: 0.5943\n",
      "Epoch 84/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6473 - acc: 0.7332 - val_loss: 1.0416 - val_acc: 0.6171\n",
      "Epoch 85/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6671 - acc: 0.7447 - val_loss: 1.0558 - val_acc: 0.5943\n",
      "Epoch 86/100\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6663 - acc: 0.7294 - val_loss: 1.0555 - val_acc: 0.5943\n",
      "Epoch 87/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6834 - acc: 0.7269 - val_loss: 1.0563 - val_acc: 0.5943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6574 - acc: 0.7511 - val_loss: 1.0597 - val_acc: 0.6057\n",
      "Epoch 89/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6539 - acc: 0.7524 - val_loss: 1.0612 - val_acc: 0.6057\n",
      "Epoch 90/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6607 - acc: 0.7422 - val_loss: 1.0553 - val_acc: 0.6000\n",
      "Epoch 91/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6407 - acc: 0.7435 - val_loss: 1.0503 - val_acc: 0.6171\n",
      "Epoch 92/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6639 - acc: 0.7371 - val_loss: 1.0574 - val_acc: 0.6057\n",
      "Epoch 93/100\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6372 - acc: 0.7486 - val_loss: 1.0658 - val_acc: 0.6057\n",
      "Epoch 94/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6438 - acc: 0.7345 - val_loss: 1.0642 - val_acc: 0.6000\n",
      "Epoch 95/100\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6307 - acc: 0.7620 - val_loss: 1.0691 - val_acc: 0.5943\n",
      "Epoch 96/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6429 - acc: 0.7466 - val_loss: 1.0742 - val_acc: 0.6114\n",
      "Epoch 97/100\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6310 - acc: 0.7498 - val_loss: 1.0737 - val_acc: 0.6057\n",
      "Epoch 98/100\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6420 - acc: 0.7479 - val_loss: 1.0818 - val_acc: 0.6057\n",
      "Epoch 99/100\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6285 - acc: 0.7518 - val_loss: 1.0848 - val_acc: 0.6000\n",
      "Epoch 100/100\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6257 - acc: 0.7466 - val_loss: 1.0922 - val_acc: 0.5886\n",
      "193/193 [==============================] - 0s 55us/step\n",
      "1742/1742 [==============================] - 0s 30us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.4746 - acc: 0.2925 - val_loss: 1.2158 - val_acc: 0.5086\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 1.3303 - acc: 0.3819 - val_loss: 1.1263 - val_acc: 0.5314\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.2598 - acc: 0.4310 - val_loss: 1.0728 - val_acc: 0.5657\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.2157 - acc: 0.4253 - val_loss: 1.0431 - val_acc: 0.5829\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.1424 - acc: 0.5051 - val_loss: 1.0198 - val_acc: 0.5886\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.1188 - acc: 0.4974 - val_loss: 0.9995 - val_acc: 0.6343\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1228 - acc: 0.5019 - val_loss: 0.9858 - val_acc: 0.6171\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0792 - acc: 0.5255 - val_loss: 0.9740 - val_acc: 0.6057\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0592 - acc: 0.5492 - val_loss: 0.9573 - val_acc: 0.6229\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0703 - acc: 0.5313 - val_loss: 0.9503 - val_acc: 0.6400\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 1.0574 - acc: 0.5364 - val_loss: 0.9441 - val_acc: 0.6286\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0396 - acc: 0.5511 - val_loss: 0.9404 - val_acc: 0.6400\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0309 - acc: 0.5498 - val_loss: 0.9319 - val_acc: 0.6514\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.0192 - acc: 0.5626 - val_loss: 0.9241 - val_acc: 0.6400\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 1.0000 - acc: 0.5658 - val_loss: 0.9266 - val_acc: 0.6286\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9993 - acc: 0.5766 - val_loss: 0.9172 - val_acc: 0.6571\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9887 - acc: 0.5792 - val_loss: 0.9093 - val_acc: 0.6514\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.0025 - acc: 0.5773 - val_loss: 0.9111 - val_acc: 0.6343\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9853 - acc: 0.5709 - val_loss: 0.9103 - val_acc: 0.6286\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9734 - acc: 0.5837 - val_loss: 0.9026 - val_acc: 0.6514\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9526 - acc: 0.5843 - val_loss: 0.9015 - val_acc: 0.6457\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9619 - acc: 0.5990 - val_loss: 0.8981 - val_acc: 0.6629\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.9502 - acc: 0.5913 - val_loss: 0.8967 - val_acc: 0.6514\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9311 - acc: 0.6201 - val_loss: 0.8938 - val_acc: 0.6571\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9487 - acc: 0.5920 - val_loss: 0.8952 - val_acc: 0.6629\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9393 - acc: 0.6169 - val_loss: 0.8910 - val_acc: 0.6571\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9418 - acc: 0.5964 - val_loss: 0.8894 - val_acc: 0.6514\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9172 - acc: 0.6232 - val_loss: 0.8848 - val_acc: 0.6514\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9323 - acc: 0.6156 - val_loss: 0.8861 - val_acc: 0.6629\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9457 - acc: 0.6117 - val_loss: 0.8892 - val_acc: 0.6571\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9257 - acc: 0.5990 - val_loss: 0.8827 - val_acc: 0.6686\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9129 - acc: 0.6175 - val_loss: 0.8836 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9047 - acc: 0.6143 - val_loss: 0.8882 - val_acc: 0.6629\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.9145 - acc: 0.6060 - val_loss: 0.8865 - val_acc: 0.6800\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9024 - acc: 0.6188 - val_loss: 0.8815 - val_acc: 0.6800\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9029 - acc: 0.6258 - val_loss: 0.8888 - val_acc: 0.6743\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8899 - acc: 0.6392 - val_loss: 0.8857 - val_acc: 0.6686\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8949 - acc: 0.6290 - val_loss: 0.8848 - val_acc: 0.6800\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8854 - acc: 0.6258 - val_loss: 0.8853 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8859 - acc: 0.6213 - val_loss: 0.8810 - val_acc: 0.6629\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8737 - acc: 0.6475 - val_loss: 0.8854 - val_acc: 0.6571\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8834 - acc: 0.6328 - val_loss: 0.8801 - val_acc: 0.6743\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8610 - acc: 0.6513 - val_loss: 0.8877 - val_acc: 0.6800\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8728 - acc: 0.6354 - val_loss: 0.8904 - val_acc: 0.6514\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8658 - acc: 0.6513 - val_loss: 0.8839 - val_acc: 0.6857\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8692 - acc: 0.6398 - val_loss: 0.8856 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8635 - acc: 0.6437 - val_loss: 0.8860 - val_acc: 0.6914\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8804 - acc: 0.6411 - val_loss: 0.8952 - val_acc: 0.6571\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8637 - acc: 0.6379 - val_loss: 0.8918 - val_acc: 0.6629\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8689 - acc: 0.6501 - val_loss: 0.8843 - val_acc: 0.6914\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8652 - acc: 0.6545 - val_loss: 0.8881 - val_acc: 0.6629\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8540 - acc: 0.6411 - val_loss: 0.8860 - val_acc: 0.6743\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8589 - acc: 0.6437 - val_loss: 0.8874 - val_acc: 0.6629\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8604 - acc: 0.6571 - val_loss: 0.8909 - val_acc: 0.6571\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8449 - acc: 0.6526 - val_loss: 0.8909 - val_acc: 0.6571\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8578 - acc: 0.6443 - val_loss: 0.8867 - val_acc: 0.6514\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8365 - acc: 0.6501 - val_loss: 0.8852 - val_acc: 0.6686\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8400 - acc: 0.6596 - val_loss: 0.8866 - val_acc: 0.6514\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8420 - acc: 0.6469 - val_loss: 0.8864 - val_acc: 0.6514\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8382 - acc: 0.6628 - val_loss: 0.8894 - val_acc: 0.6571\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8158 - acc: 0.6782 - val_loss: 0.8900 - val_acc: 0.6571\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8188 - acc: 0.6596 - val_loss: 0.8898 - val_acc: 0.6514\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8280 - acc: 0.6526 - val_loss: 0.8907 - val_acc: 0.6571\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8292 - acc: 0.6590 - val_loss: 0.8880 - val_acc: 0.6743\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8423 - acc: 0.6488 - val_loss: 0.8837 - val_acc: 0.6743\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8134 - acc: 0.6635 - val_loss: 0.8880 - val_acc: 0.6686\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8181 - acc: 0.6635 - val_loss: 0.8845 - val_acc: 0.6629\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8204 - acc: 0.6648 - val_loss: 0.8874 - val_acc: 0.6686\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8117 - acc: 0.6660 - val_loss: 0.8914 - val_acc: 0.6743\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8130 - acc: 0.6743 - val_loss: 0.8897 - val_acc: 0.6686\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8164 - acc: 0.6660 - val_loss: 0.8910 - val_acc: 0.6629\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8228 - acc: 0.6686 - val_loss: 0.8916 - val_acc: 0.6571\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8119 - acc: 0.6648 - val_loss: 0.8927 - val_acc: 0.6629\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8184 - acc: 0.6711 - val_loss: 0.8959 - val_acc: 0.6571\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8120 - acc: 0.6660 - val_loss: 0.8903 - val_acc: 0.6686\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8052 - acc: 0.6711 - val_loss: 0.8910 - val_acc: 0.6686\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8035 - acc: 0.6762 - val_loss: 0.8968 - val_acc: 0.6571\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7901 - acc: 0.6922 - val_loss: 0.9017 - val_acc: 0.6629\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7934 - acc: 0.6775 - val_loss: 0.8997 - val_acc: 0.6514\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8061 - acc: 0.6775 - val_loss: 0.9046 - val_acc: 0.6343\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7832 - acc: 0.6852 - val_loss: 0.9082 - val_acc: 0.6457\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7927 - acc: 0.6801 - val_loss: 0.8978 - val_acc: 0.6686\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7808 - acc: 0.6865 - val_loss: 0.8995 - val_acc: 0.6514\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7868 - acc: 0.6820 - val_loss: 0.9016 - val_acc: 0.6629\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7661 - acc: 0.6839 - val_loss: 0.9051 - val_acc: 0.6629\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7655 - acc: 0.6833 - val_loss: 0.9056 - val_acc: 0.6629\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7716 - acc: 0.6909 - val_loss: 0.9072 - val_acc: 0.6514\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8052 - acc: 0.6616 - val_loss: 0.9107 - val_acc: 0.6629\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7913 - acc: 0.6801 - val_loss: 0.9120 - val_acc: 0.6514\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7774 - acc: 0.6852 - val_loss: 0.9093 - val_acc: 0.6571\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7774 - acc: 0.6826 - val_loss: 0.9102 - val_acc: 0.6571\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7758 - acc: 0.6890 - val_loss: 0.9110 - val_acc: 0.6686\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7665 - acc: 0.6960 - val_loss: 0.9088 - val_acc: 0.6743\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7671 - acc: 0.6865 - val_loss: 0.9145 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7768 - acc: 0.6852 - val_loss: 0.9174 - val_acc: 0.6571\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7535 - acc: 0.6877 - val_loss: 0.9109 - val_acc: 0.6571\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7798 - acc: 0.6775 - val_loss: 0.9180 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7582 - acc: 0.6884 - val_loss: 0.9179 - val_acc: 0.6571\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7773 - acc: 0.6833 - val_loss: 0.9205 - val_acc: 0.6514\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7662 - acc: 0.6897 - val_loss: 0.9220 - val_acc: 0.6571\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7762 - acc: 0.6865 - val_loss: 0.9202 - val_acc: 0.6343\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7530 - acc: 0.6909 - val_loss: 0.9244 - val_acc: 0.6457\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7602 - acc: 0.6826 - val_loss: 0.9163 - val_acc: 0.6457\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7419 - acc: 0.6992 - val_loss: 0.9148 - val_acc: 0.6457\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7642 - acc: 0.6890 - val_loss: 0.9179 - val_acc: 0.6629\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7640 - acc: 0.6794 - val_loss: 0.9170 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7551 - acc: 0.6973 - val_loss: 0.9194 - val_acc: 0.6629\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7287 - acc: 0.7063 - val_loss: 0.9226 - val_acc: 0.6457\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7254 - acc: 0.7037 - val_loss: 0.9219 - val_acc: 0.6514\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7630 - acc: 0.6980 - val_loss: 0.9261 - val_acc: 0.6514\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7350 - acc: 0.7158 - val_loss: 0.9290 - val_acc: 0.6457\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7312 - acc: 0.7114 - val_loss: 0.9284 - val_acc: 0.6571\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7426 - acc: 0.7088 - val_loss: 0.9234 - val_acc: 0.6514\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7333 - acc: 0.7069 - val_loss: 0.9214 - val_acc: 0.6571\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7138 - acc: 0.7139 - val_loss: 0.9311 - val_acc: 0.6571\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7275 - acc: 0.7146 - val_loss: 0.9313 - val_acc: 0.6571\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7228 - acc: 0.7114 - val_loss: 0.9324 - val_acc: 0.6514\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7061 - acc: 0.7261 - val_loss: 0.9367 - val_acc: 0.6514\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7395 - acc: 0.6935 - val_loss: 0.9392 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7150 - acc: 0.7184 - val_loss: 0.9353 - val_acc: 0.6571\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7251 - acc: 0.7139 - val_loss: 0.9368 - val_acc: 0.6400\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7167 - acc: 0.7158 - val_loss: 0.9439 - val_acc: 0.6400\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7266 - acc: 0.7082 - val_loss: 0.9465 - val_acc: 0.6400\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7050 - acc: 0.7235 - val_loss: 0.9434 - val_acc: 0.6629\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6875 - acc: 0.7267 - val_loss: 0.9362 - val_acc: 0.6571\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7155 - acc: 0.7082 - val_loss: 0.9426 - val_acc: 0.6629\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7222 - acc: 0.7018 - val_loss: 0.9445 - val_acc: 0.6629\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7375 - acc: 0.7056 - val_loss: 0.9506 - val_acc: 0.6629\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7025 - acc: 0.7178 - val_loss: 0.9523 - val_acc: 0.6400\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7110 - acc: 0.7120 - val_loss: 0.9476 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7023 - acc: 0.7158 - val_loss: 0.9455 - val_acc: 0.6571\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7100 - acc: 0.7165 - val_loss: 0.9490 - val_acc: 0.6514\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6899 - acc: 0.7158 - val_loss: 0.9436 - val_acc: 0.6571\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7180 - acc: 0.7088 - val_loss: 0.9384 - val_acc: 0.6743\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6989 - acc: 0.7305 - val_loss: 0.9415 - val_acc: 0.6514\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7049 - acc: 0.7107 - val_loss: 0.9473 - val_acc: 0.6514\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7091 - acc: 0.7261 - val_loss: 0.9514 - val_acc: 0.6514\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6971 - acc: 0.7261 - val_loss: 0.9463 - val_acc: 0.6514\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6949 - acc: 0.7324 - val_loss: 0.9436 - val_acc: 0.6629\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6888 - acc: 0.7171 - val_loss: 0.9427 - val_acc: 0.6686\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6959 - acc: 0.7146 - val_loss: 0.9467 - val_acc: 0.6571\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6926 - acc: 0.7286 - val_loss: 0.9477 - val_acc: 0.6743\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6837 - acc: 0.7229 - val_loss: 0.9498 - val_acc: 0.6629\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6977 - acc: 0.7235 - val_loss: 0.9477 - val_acc: 0.6571\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6881 - acc: 0.7267 - val_loss: 0.9508 - val_acc: 0.6743\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6946 - acc: 0.7184 - val_loss: 0.9513 - val_acc: 0.6629\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6855 - acc: 0.7292 - val_loss: 0.9587 - val_acc: 0.6629\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6968 - acc: 0.7299 - val_loss: 0.9635 - val_acc: 0.6629\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6749 - acc: 0.7209 - val_loss: 0.9608 - val_acc: 0.6514\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6827 - acc: 0.7165 - val_loss: 0.9642 - val_acc: 0.6571\n",
      "194/194 [==============================] - 0s 57us/step\n",
      "1741/1741 [==============================] - 0s 31us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.5089 - acc: 0.2791 - val_loss: 1.2591 - val_acc: 0.4800\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.3392 - acc: 0.3608 - val_loss: 1.1525 - val_acc: 0.5829\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.2672 - acc: 0.4074 - val_loss: 1.0921 - val_acc: 0.5886\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.1996 - acc: 0.4617 - val_loss: 1.0509 - val_acc: 0.6057\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1628 - acc: 0.4853 - val_loss: 1.0238 - val_acc: 0.6286\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1229 - acc: 0.5172 - val_loss: 1.0003 - val_acc: 0.6343\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.1056 - acc: 0.5192 - val_loss: 0.9841 - val_acc: 0.6343\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0811 - acc: 0.5370 - val_loss: 0.9637 - val_acc: 0.6457\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0782 - acc: 0.5236 - val_loss: 0.9579 - val_acc: 0.6286\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0594 - acc: 0.5428 - val_loss: 0.9509 - val_acc: 0.6114\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0411 - acc: 0.5453 - val_loss: 0.9429 - val_acc: 0.6457\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0467 - acc: 0.5453 - val_loss: 0.9335 - val_acc: 0.6514\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0111 - acc: 0.5747 - val_loss: 0.9236 - val_acc: 0.6514\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9931 - acc: 0.5837 - val_loss: 0.9210 - val_acc: 0.6629\n",
      "Epoch 15/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9947 - acc: 0.5664 - val_loss: 0.9156 - val_acc: 0.6629\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9783 - acc: 0.5766 - val_loss: 0.9088 - val_acc: 0.6457\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9955 - acc: 0.5773 - val_loss: 0.9037 - val_acc: 0.6514\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9880 - acc: 0.5868 - val_loss: 0.8997 - val_acc: 0.6571\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9955 - acc: 0.5900 - val_loss: 0.8974 - val_acc: 0.6514\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9666 - acc: 0.5824 - val_loss: 0.8986 - val_acc: 0.6857\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9655 - acc: 0.5830 - val_loss: 0.8953 - val_acc: 0.6743\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9498 - acc: 0.5990 - val_loss: 0.8925 - val_acc: 0.6686\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9560 - acc: 0.6047 - val_loss: 0.8889 - val_acc: 0.6514\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9421 - acc: 0.6086 - val_loss: 0.8864 - val_acc: 0.6686\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9385 - acc: 0.6111 - val_loss: 0.8840 - val_acc: 0.6571\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9263 - acc: 0.6111 - val_loss: 0.8841 - val_acc: 0.6571\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9223 - acc: 0.6117 - val_loss: 0.8806 - val_acc: 0.6686\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9251 - acc: 0.6041 - val_loss: 0.8781 - val_acc: 0.6571\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9140 - acc: 0.6264 - val_loss: 0.8832 - val_acc: 0.6800\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9383 - acc: 0.6258 - val_loss: 0.8885 - val_acc: 0.6743\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8995 - acc: 0.6201 - val_loss: 0.8877 - val_acc: 0.6686\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9096 - acc: 0.6156 - val_loss: 0.8876 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8957 - acc: 0.6245 - val_loss: 0.8807 - val_acc: 0.6800\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9114 - acc: 0.6175 - val_loss: 0.8794 - val_acc: 0.6857\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8924 - acc: 0.6290 - val_loss: 0.8840 - val_acc: 0.6629\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8968 - acc: 0.6073 - val_loss: 0.8795 - val_acc: 0.6800\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8871 - acc: 0.6322 - val_loss: 0.8779 - val_acc: 0.6914\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9046 - acc: 0.6303 - val_loss: 0.8826 - val_acc: 0.6629\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8625 - acc: 0.6526 - val_loss: 0.8776 - val_acc: 0.6629\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8800 - acc: 0.6315 - val_loss: 0.8766 - val_acc: 0.6743\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8792 - acc: 0.6475 - val_loss: 0.8769 - val_acc: 0.6800\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8832 - acc: 0.6360 - val_loss: 0.8802 - val_acc: 0.6514\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8718 - acc: 0.6277 - val_loss: 0.8837 - val_acc: 0.6514\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8793 - acc: 0.6360 - val_loss: 0.8834 - val_acc: 0.6514\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8725 - acc: 0.6373 - val_loss: 0.8802 - val_acc: 0.6571\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8745 - acc: 0.6501 - val_loss: 0.8804 - val_acc: 0.6686\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8629 - acc: 0.6456 - val_loss: 0.8777 - val_acc: 0.6514\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8567 - acc: 0.6386 - val_loss: 0.8758 - val_acc: 0.6571\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8591 - acc: 0.6507 - val_loss: 0.8797 - val_acc: 0.6629\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8437 - acc: 0.6622 - val_loss: 0.8800 - val_acc: 0.6571\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8411 - acc: 0.6501 - val_loss: 0.8811 - val_acc: 0.6571\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8504 - acc: 0.6513 - val_loss: 0.8821 - val_acc: 0.6457\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8698 - acc: 0.6405 - val_loss: 0.8814 - val_acc: 0.6400\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8277 - acc: 0.6456 - val_loss: 0.8765 - val_acc: 0.6571\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8396 - acc: 0.6628 - val_loss: 0.8766 - val_acc: 0.6629\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8335 - acc: 0.6418 - val_loss: 0.8818 - val_acc: 0.6457\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8241 - acc: 0.6571 - val_loss: 0.8878 - val_acc: 0.6571\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8322 - acc: 0.6686 - val_loss: 0.8872 - val_acc: 0.6629\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8142 - acc: 0.6590 - val_loss: 0.8860 - val_acc: 0.6514\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8297 - acc: 0.6609 - val_loss: 0.8854 - val_acc: 0.6743\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8149 - acc: 0.6596 - val_loss: 0.8871 - val_acc: 0.6629\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8260 - acc: 0.6609 - val_loss: 0.8853 - val_acc: 0.6629\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8142 - acc: 0.6756 - val_loss: 0.8824 - val_acc: 0.6514\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7982 - acc: 0.6737 - val_loss: 0.8867 - val_acc: 0.6514\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8187 - acc: 0.6692 - val_loss: 0.8877 - val_acc: 0.6514\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7836 - acc: 0.6750 - val_loss: 0.8878 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8055 - acc: 0.6724 - val_loss: 0.8865 - val_acc: 0.6571\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8021 - acc: 0.6641 - val_loss: 0.8890 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8065 - acc: 0.6801 - val_loss: 0.8887 - val_acc: 0.6514\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7939 - acc: 0.6756 - val_loss: 0.8904 - val_acc: 0.6571\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7892 - acc: 0.6782 - val_loss: 0.8944 - val_acc: 0.6571\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7940 - acc: 0.6871 - val_loss: 0.8911 - val_acc: 0.6571\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7888 - acc: 0.6718 - val_loss: 0.8893 - val_acc: 0.6629\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8057 - acc: 0.6654 - val_loss: 0.8876 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7892 - acc: 0.6877 - val_loss: 0.8921 - val_acc: 0.6629\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7893 - acc: 0.6845 - val_loss: 0.8936 - val_acc: 0.6800\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7867 - acc: 0.6897 - val_loss: 0.8944 - val_acc: 0.6743\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7877 - acc: 0.6794 - val_loss: 0.8930 - val_acc: 0.6686\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8043 - acc: 0.6648 - val_loss: 0.8961 - val_acc: 0.6571\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7804 - acc: 0.6782 - val_loss: 0.8994 - val_acc: 0.6629\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7864 - acc: 0.6743 - val_loss: 0.9028 - val_acc: 0.6629\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7688 - acc: 0.6788 - val_loss: 0.9003 - val_acc: 0.6629\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7964 - acc: 0.6628 - val_loss: 0.8978 - val_acc: 0.6571\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7737 - acc: 0.6871 - val_loss: 0.8938 - val_acc: 0.6686\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7646 - acc: 0.6737 - val_loss: 0.8968 - val_acc: 0.6629\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7813 - acc: 0.6909 - val_loss: 0.9026 - val_acc: 0.6629\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7702 - acc: 0.6922 - val_loss: 0.9042 - val_acc: 0.6743\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7704 - acc: 0.6833 - val_loss: 0.9058 - val_acc: 0.6571\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7614 - acc: 0.6826 - val_loss: 0.9037 - val_acc: 0.6629\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7586 - acc: 0.6788 - val_loss: 0.9054 - val_acc: 0.6629\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7642 - acc: 0.6992 - val_loss: 0.9013 - val_acc: 0.6629\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7565 - acc: 0.6980 - val_loss: 0.9030 - val_acc: 0.6629\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7831 - acc: 0.6801 - val_loss: 0.9025 - val_acc: 0.6629\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7489 - acc: 0.6986 - val_loss: 0.9082 - val_acc: 0.6571\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7511 - acc: 0.6909 - val_loss: 0.9068 - val_acc: 0.6457\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7636 - acc: 0.6948 - val_loss: 0.9083 - val_acc: 0.6743\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7534 - acc: 0.6992 - val_loss: 0.9052 - val_acc: 0.6514\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7481 - acc: 0.7005 - val_loss: 0.9067 - val_acc: 0.6514\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7271 - acc: 0.7190 - val_loss: 0.9099 - val_acc: 0.6629\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7518 - acc: 0.7069 - val_loss: 0.9108 - val_acc: 0.6514\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7345 - acc: 0.7126 - val_loss: 0.9157 - val_acc: 0.6457\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7376 - acc: 0.7114 - val_loss: 0.9176 - val_acc: 0.6457\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7333 - acc: 0.7050 - val_loss: 0.9136 - val_acc: 0.6514\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7589 - acc: 0.6941 - val_loss: 0.9142 - val_acc: 0.6571\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7416 - acc: 0.7005 - val_loss: 0.9159 - val_acc: 0.6686\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7251 - acc: 0.7005 - val_loss: 0.9104 - val_acc: 0.6514\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7380 - acc: 0.7043 - val_loss: 0.9131 - val_acc: 0.6457\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7403 - acc: 0.7114 - val_loss: 0.9161 - val_acc: 0.6514\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7222 - acc: 0.7037 - val_loss: 0.9206 - val_acc: 0.6571\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7316 - acc: 0.7043 - val_loss: 0.9249 - val_acc: 0.6571\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7248 - acc: 0.7120 - val_loss: 0.9180 - val_acc: 0.6571\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7322 - acc: 0.7082 - val_loss: 0.9197 - val_acc: 0.6629\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7055 - acc: 0.7229 - val_loss: 0.9200 - val_acc: 0.6514\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7345 - acc: 0.7203 - val_loss: 0.9259 - val_acc: 0.6400\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7173 - acc: 0.7095 - val_loss: 0.9232 - val_acc: 0.6457\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7054 - acc: 0.7095 - val_loss: 0.9213 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7302 - acc: 0.7075 - val_loss: 0.9198 - val_acc: 0.6514\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7139 - acc: 0.7158 - val_loss: 0.9253 - val_acc: 0.6457\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7140 - acc: 0.7075 - val_loss: 0.9231 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7107 - acc: 0.7171 - val_loss: 0.9239 - val_acc: 0.6571\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6958 - acc: 0.7280 - val_loss: 0.9252 - val_acc: 0.6571\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7188 - acc: 0.7088 - val_loss: 0.9306 - val_acc: 0.6514\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7039 - acc: 0.7209 - val_loss: 0.9336 - val_acc: 0.6514\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7005 - acc: 0.7133 - val_loss: 0.9327 - val_acc: 0.6457\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7013 - acc: 0.7114 - val_loss: 0.9307 - val_acc: 0.6400\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7060 - acc: 0.7075 - val_loss: 0.9285 - val_acc: 0.6400\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7050 - acc: 0.7241 - val_loss: 0.9275 - val_acc: 0.6400\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6968 - acc: 0.7190 - val_loss: 0.9311 - val_acc: 0.6400\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7025 - acc: 0.7095 - val_loss: 0.9466 - val_acc: 0.6457\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6802 - acc: 0.7350 - val_loss: 0.9454 - val_acc: 0.6400\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7115 - acc: 0.7248 - val_loss: 0.9411 - val_acc: 0.6457\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6819 - acc: 0.7273 - val_loss: 0.9359 - val_acc: 0.6514\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6881 - acc: 0.7216 - val_loss: 0.9402 - val_acc: 0.6457\n",
      "Epoch 134/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.6961 - acc: 0.7184 - val_loss: 0.9438 - val_acc: 0.6286\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6658 - acc: 0.7395 - val_loss: 0.9463 - val_acc: 0.6286\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6797 - acc: 0.7216 - val_loss: 0.9425 - val_acc: 0.6343\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6880 - acc: 0.7312 - val_loss: 0.9364 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6975 - acc: 0.7146 - val_loss: 0.9410 - val_acc: 0.6457\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6823 - acc: 0.7350 - val_loss: 0.9383 - val_acc: 0.6514\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6776 - acc: 0.7401 - val_loss: 0.9411 - val_acc: 0.6514\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.6643 - acc: 0.7292 - val_loss: 0.9439 - val_acc: 0.6400\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6997 - acc: 0.7120 - val_loss: 0.9494 - val_acc: 0.6229\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6764 - acc: 0.7267 - val_loss: 0.9492 - val_acc: 0.6286\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6877 - acc: 0.7216 - val_loss: 0.9556 - val_acc: 0.6343\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6839 - acc: 0.7280 - val_loss: 0.9557 - val_acc: 0.6229\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6845 - acc: 0.7241 - val_loss: 0.9525 - val_acc: 0.6571\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6712 - acc: 0.7312 - val_loss: 0.9486 - val_acc: 0.6457\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6712 - acc: 0.7292 - val_loss: 0.9536 - val_acc: 0.6457\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6939 - acc: 0.7222 - val_loss: 0.9514 - val_acc: 0.6400\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6668 - acc: 0.7235 - val_loss: 0.9474 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 47us/step\n",
      "1741/1741 [==============================] - 0s 31us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 7s 5ms/step - loss: 1.5465 - acc: 0.2886 - val_loss: 1.3006 - val_acc: 0.3543\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.3582 - acc: 0.3602 - val_loss: 1.1596 - val_acc: 0.5714\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.2585 - acc: 0.4144 - val_loss: 1.0978 - val_acc: 0.5600\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.2044 - acc: 0.4623 - val_loss: 1.0534 - val_acc: 0.5886\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.1630 - acc: 0.4834 - val_loss: 1.0343 - val_acc: 0.6057\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 1.1266 - acc: 0.4987 - val_loss: 1.0176 - val_acc: 0.6114\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.1200 - acc: 0.5013 - val_loss: 1.0000 - val_acc: 0.6171\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1059 - acc: 0.5019 - val_loss: 0.9854 - val_acc: 0.6343\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0621 - acc: 0.5383 - val_loss: 0.9731 - val_acc: 0.6400\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0496 - acc: 0.5460 - val_loss: 0.9654 - val_acc: 0.6400\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0199 - acc: 0.5536 - val_loss: 0.9621 - val_acc: 0.6343\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.0336 - acc: 0.5421 - val_loss: 0.9521 - val_acc: 0.6343\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 1.0080 - acc: 0.5658 - val_loss: 0.9469 - val_acc: 0.6457\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0078 - acc: 0.5466 - val_loss: 0.9389 - val_acc: 0.6514\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0130 - acc: 0.5485 - val_loss: 0.9336 - val_acc: 0.6571\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9935 - acc: 0.5773 - val_loss: 0.9291 - val_acc: 0.6686\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9809 - acc: 0.5766 - val_loss: 0.9275 - val_acc: 0.6743\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9665 - acc: 0.5881 - val_loss: 0.9247 - val_acc: 0.6800\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9677 - acc: 0.5875 - val_loss: 0.9207 - val_acc: 0.6857\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9753 - acc: 0.5798 - val_loss: 0.9193 - val_acc: 0.6800\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9622 - acc: 0.5945 - val_loss: 0.9219 - val_acc: 0.6457\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9700 - acc: 0.5913 - val_loss: 0.9224 - val_acc: 0.6743\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9467 - acc: 0.6009 - val_loss: 0.9202 - val_acc: 0.6686\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9353 - acc: 0.5913 - val_loss: 0.9127 - val_acc: 0.6629\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9376 - acc: 0.5945 - val_loss: 0.9136 - val_acc: 0.6686\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9376 - acc: 0.5913 - val_loss: 0.9177 - val_acc: 0.6743\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9184 - acc: 0.6092 - val_loss: 0.9161 - val_acc: 0.6629\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9210 - acc: 0.6098 - val_loss: 0.9148 - val_acc: 0.6743\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9106 - acc: 0.6245 - val_loss: 0.9056 - val_acc: 0.6629\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9121 - acc: 0.6252 - val_loss: 0.9040 - val_acc: 0.6686\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9229 - acc: 0.6194 - val_loss: 0.8983 - val_acc: 0.6857\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9146 - acc: 0.6130 - val_loss: 0.9025 - val_acc: 0.6743\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8929 - acc: 0.6226 - val_loss: 0.9049 - val_acc: 0.6743\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8948 - acc: 0.6335 - val_loss: 0.9059 - val_acc: 0.6743\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9125 - acc: 0.6041 - val_loss: 0.9022 - val_acc: 0.6743\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9263 - acc: 0.6054 - val_loss: 0.9025 - val_acc: 0.6743\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8837 - acc: 0.6347 - val_loss: 0.9026 - val_acc: 0.6743\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9010 - acc: 0.6188 - val_loss: 0.9065 - val_acc: 0.6629\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8996 - acc: 0.6271 - val_loss: 0.9033 - val_acc: 0.6571\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8785 - acc: 0.6424 - val_loss: 0.8959 - val_acc: 0.6857\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8653 - acc: 0.6418 - val_loss: 0.8955 - val_acc: 0.6800\n",
      "Epoch 42/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8730 - acc: 0.6405 - val_loss: 0.8975 - val_acc: 0.6629\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8757 - acc: 0.6303 - val_loss: 0.9005 - val_acc: 0.6514\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8702 - acc: 0.6328 - val_loss: 0.9064 - val_acc: 0.6571\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8678 - acc: 0.6392 - val_loss: 0.9057 - val_acc: 0.6571\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8721 - acc: 0.6398 - val_loss: 0.9041 - val_acc: 0.6743\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8638 - acc: 0.6335 - val_loss: 0.9003 - val_acc: 0.6800\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8588 - acc: 0.6315 - val_loss: 0.9028 - val_acc: 0.6571\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8652 - acc: 0.6443 - val_loss: 0.9015 - val_acc: 0.6629\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8487 - acc: 0.6539 - val_loss: 0.9071 - val_acc: 0.6571\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8256 - acc: 0.6673 - val_loss: 0.9048 - val_acc: 0.6457\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8469 - acc: 0.6386 - val_loss: 0.9014 - val_acc: 0.6629\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8359 - acc: 0.6603 - val_loss: 0.9070 - val_acc: 0.6629\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8459 - acc: 0.6641 - val_loss: 0.9083 - val_acc: 0.6629\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8403 - acc: 0.6552 - val_loss: 0.9160 - val_acc: 0.6571\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8370 - acc: 0.6539 - val_loss: 0.9118 - val_acc: 0.6457\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8466 - acc: 0.6405 - val_loss: 0.9092 - val_acc: 0.6571\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8473 - acc: 0.6558 - val_loss: 0.9106 - val_acc: 0.6629\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8308 - acc: 0.6654 - val_loss: 0.9107 - val_acc: 0.6457\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8160 - acc: 0.6558 - val_loss: 0.9096 - val_acc: 0.6457\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8414 - acc: 0.6520 - val_loss: 0.9110 - val_acc: 0.6629\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8322 - acc: 0.6648 - val_loss: 0.9114 - val_acc: 0.6571\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8212 - acc: 0.6622 - val_loss: 0.9079 - val_acc: 0.6514\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8201 - acc: 0.6475 - val_loss: 0.9092 - val_acc: 0.6571\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8221 - acc: 0.6539 - val_loss: 0.9092 - val_acc: 0.6571\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8205 - acc: 0.6616 - val_loss: 0.9170 - val_acc: 0.6457\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8190 - acc: 0.6622 - val_loss: 0.9135 - val_acc: 0.6457\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8326 - acc: 0.6526 - val_loss: 0.9031 - val_acc: 0.6457\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8059 - acc: 0.6622 - val_loss: 0.9048 - val_acc: 0.6514\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8251 - acc: 0.6660 - val_loss: 0.9092 - val_acc: 0.6457\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7981 - acc: 0.6692 - val_loss: 0.9119 - val_acc: 0.6457\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7937 - acc: 0.6788 - val_loss: 0.9124 - val_acc: 0.6514\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7981 - acc: 0.6750 - val_loss: 0.9183 - val_acc: 0.6514\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8157 - acc: 0.6564 - val_loss: 0.9217 - val_acc: 0.6457\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7917 - acc: 0.6788 - val_loss: 0.9228 - val_acc: 0.6457\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7675 - acc: 0.6897 - val_loss: 0.9283 - val_acc: 0.6571\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7934 - acc: 0.6596 - val_loss: 0.9201 - val_acc: 0.6400\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7943 - acc: 0.6782 - val_loss: 0.9146 - val_acc: 0.6514\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7821 - acc: 0.6769 - val_loss: 0.9181 - val_acc: 0.6514\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8044 - acc: 0.6788 - val_loss: 0.9206 - val_acc: 0.6457\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7714 - acc: 0.6667 - val_loss: 0.9167 - val_acc: 0.6457\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7865 - acc: 0.6724 - val_loss: 0.9114 - val_acc: 0.6457\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7781 - acc: 0.6877 - val_loss: 0.9146 - val_acc: 0.6400\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7732 - acc: 0.6782 - val_loss: 0.9166 - val_acc: 0.6400\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8046 - acc: 0.6692 - val_loss: 0.9159 - val_acc: 0.6457\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7610 - acc: 0.6865 - val_loss: 0.9238 - val_acc: 0.6457\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7880 - acc: 0.6756 - val_loss: 0.9231 - val_acc: 0.6343\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7735 - acc: 0.6973 - val_loss: 0.9190 - val_acc: 0.6400\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7914 - acc: 0.6743 - val_loss: 0.9219 - val_acc: 0.6457\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7658 - acc: 0.6782 - val_loss: 0.9247 - val_acc: 0.6457\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7732 - acc: 0.6916 - val_loss: 0.9215 - val_acc: 0.6400\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7668 - acc: 0.6852 - val_loss: 0.9252 - val_acc: 0.6457\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7538 - acc: 0.6839 - val_loss: 0.9220 - val_acc: 0.6457\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7588 - acc: 0.6967 - val_loss: 0.9244 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7640 - acc: 0.6890 - val_loss: 0.9268 - val_acc: 0.6457\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7755 - acc: 0.6884 - val_loss: 0.9275 - val_acc: 0.6457\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7472 - acc: 0.6967 - val_loss: 0.9267 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7483 - acc: 0.7082 - val_loss: 0.9261 - val_acc: 0.6514\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7547 - acc: 0.6890 - val_loss: 0.9274 - val_acc: 0.6400\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7452 - acc: 0.6909 - val_loss: 0.9302 - val_acc: 0.6343\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7537 - acc: 0.6890 - val_loss: 0.9339 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7642 - acc: 0.6890 - val_loss: 0.9340 - val_acc: 0.6457\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7435 - acc: 0.6909 - val_loss: 0.9380 - val_acc: 0.6343\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7461 - acc: 0.6865 - val_loss: 0.9387 - val_acc: 0.6286\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7544 - acc: 0.6922 - val_loss: 0.9367 - val_acc: 0.6343\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.7541 - acc: 0.6948 - val_loss: 0.9357 - val_acc: 0.6514\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7453 - acc: 0.6890 - val_loss: 0.9483 - val_acc: 0.6343\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7444 - acc: 0.7037 - val_loss: 0.9484 - val_acc: 0.6400\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7473 - acc: 0.6967 - val_loss: 0.9401 - val_acc: 0.6457\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7352 - acc: 0.7069 - val_loss: 0.9463 - val_acc: 0.6400\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7548 - acc: 0.6903 - val_loss: 0.9444 - val_acc: 0.6229\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7445 - acc: 0.6794 - val_loss: 0.9452 - val_acc: 0.6400\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7200 - acc: 0.7088 - val_loss: 0.9483 - val_acc: 0.6400\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7289 - acc: 0.7107 - val_loss: 0.9490 - val_acc: 0.6400\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7131 - acc: 0.7209 - val_loss: 0.9568 - val_acc: 0.6514\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7266 - acc: 0.6986 - val_loss: 0.9581 - val_acc: 0.6457\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7180 - acc: 0.7069 - val_loss: 0.9579 - val_acc: 0.6457\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7312 - acc: 0.7037 - val_loss: 0.9574 - val_acc: 0.6343\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7295 - acc: 0.6897 - val_loss: 0.9561 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7234 - acc: 0.7120 - val_loss: 0.9542 - val_acc: 0.6457\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7296 - acc: 0.7126 - val_loss: 0.9489 - val_acc: 0.6400\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7277 - acc: 0.7018 - val_loss: 0.9528 - val_acc: 0.6400\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7061 - acc: 0.7120 - val_loss: 0.9546 - val_acc: 0.6457\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7155 - acc: 0.6922 - val_loss: 0.9515 - val_acc: 0.6400\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7324 - acc: 0.6909 - val_loss: 0.9516 - val_acc: 0.6343\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7179 - acc: 0.7171 - val_loss: 0.9517 - val_acc: 0.6514\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7194 - acc: 0.6992 - val_loss: 0.9549 - val_acc: 0.6400\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7178 - acc: 0.7095 - val_loss: 0.9608 - val_acc: 0.6286\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7039 - acc: 0.7190 - val_loss: 0.9580 - val_acc: 0.6457\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7187 - acc: 0.7152 - val_loss: 0.9624 - val_acc: 0.6457\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7078 - acc: 0.7152 - val_loss: 0.9731 - val_acc: 0.6457\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6900 - acc: 0.7139 - val_loss: 0.9711 - val_acc: 0.6457\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7121 - acc: 0.7082 - val_loss: 0.9655 - val_acc: 0.6514\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7075 - acc: 0.7152 - val_loss: 0.9598 - val_acc: 0.6571\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7015 - acc: 0.7267 - val_loss: 0.9562 - val_acc: 0.6514\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6913 - acc: 0.7222 - val_loss: 0.9579 - val_acc: 0.6343\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7014 - acc: 0.7152 - val_loss: 0.9597 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6774 - acc: 0.7337 - val_loss: 0.9606 - val_acc: 0.6571\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6923 - acc: 0.7248 - val_loss: 0.9639 - val_acc: 0.6514\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6799 - acc: 0.7139 - val_loss: 0.9659 - val_acc: 0.6457\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6968 - acc: 0.7107 - val_loss: 0.9645 - val_acc: 0.6400\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6914 - acc: 0.7261 - val_loss: 0.9641 - val_acc: 0.6629\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6834 - acc: 0.7267 - val_loss: 0.9702 - val_acc: 0.6629\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6919 - acc: 0.7286 - val_loss: 0.9703 - val_acc: 0.6686\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6806 - acc: 0.7222 - val_loss: 0.9744 - val_acc: 0.6629\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7135 - acc: 0.7056 - val_loss: 0.9770 - val_acc: 0.6629\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7011 - acc: 0.7254 - val_loss: 0.9763 - val_acc: 0.6400\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6835 - acc: 0.7222 - val_loss: 0.9800 - val_acc: 0.6400\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6832 - acc: 0.7229 - val_loss: 0.9785 - val_acc: 0.6457\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6676 - acc: 0.7267 - val_loss: 0.9757 - val_acc: 0.6629\n",
      "194/194 [==============================] - 0s 47us/step\n",
      "1741/1741 [==============================] - 0s 33us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.5454 - acc: 0.2950 - val_loss: 1.2673 - val_acc: 0.3771\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.3601 - acc: 0.3455 - val_loss: 1.1542 - val_acc: 0.5086\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.2604 - acc: 0.4444 - val_loss: 1.0874 - val_acc: 0.5657\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.2269 - acc: 0.4298 - val_loss: 1.0533 - val_acc: 0.5600\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.2026 - acc: 0.4502 - val_loss: 1.0331 - val_acc: 0.5657\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.1297 - acc: 0.5064 - val_loss: 1.0129 - val_acc: 0.5714\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.1349 - acc: 0.4962 - val_loss: 0.9925 - val_acc: 0.5829\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1052 - acc: 0.5153 - val_loss: 0.9764 - val_acc: 0.5943\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0691 - acc: 0.5390 - val_loss: 0.9668 - val_acc: 0.6114\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0744 - acc: 0.5153 - val_loss: 0.9620 - val_acc: 0.5943\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0476 - acc: 0.5460 - val_loss: 0.9534 - val_acc: 0.6057\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.0476 - acc: 0.5390 - val_loss: 0.9407 - val_acc: 0.6171\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 1.0258 - acc: 0.5562 - val_loss: 0.9354 - val_acc: 0.6343\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0516 - acc: 0.5466 - val_loss: 0.9300 - val_acc: 0.6457\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0100 - acc: 0.5734 - val_loss: 0.9264 - val_acc: 0.6343\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0047 - acc: 0.5677 - val_loss: 0.9267 - val_acc: 0.6514\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9897 - acc: 0.5702 - val_loss: 0.9167 - val_acc: 0.6400\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9903 - acc: 0.5766 - val_loss: 0.9170 - val_acc: 0.6514\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9889 - acc: 0.5849 - val_loss: 0.9167 - val_acc: 0.6457\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.0042 - acc: 0.5632 - val_loss: 0.9101 - val_acc: 0.6514\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9787 - acc: 0.5837 - val_loss: 0.9068 - val_acc: 0.6457\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9675 - acc: 0.5862 - val_loss: 0.9073 - val_acc: 0.6514\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.9645 - acc: 0.5951 - val_loss: 0.8970 - val_acc: 0.6686\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9539 - acc: 0.5926 - val_loss: 0.8963 - val_acc: 0.6743\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9467 - acc: 0.6015 - val_loss: 0.8965 - val_acc: 0.6629\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9594 - acc: 0.5798 - val_loss: 0.8948 - val_acc: 0.6514\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9318 - acc: 0.5977 - val_loss: 0.8969 - val_acc: 0.6743\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9328 - acc: 0.6124 - val_loss: 0.8999 - val_acc: 0.6743\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9311 - acc: 0.6245 - val_loss: 0.8979 - val_acc: 0.6457\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9310 - acc: 0.6034 - val_loss: 0.8964 - val_acc: 0.6514\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9088 - acc: 0.6181 - val_loss: 0.8972 - val_acc: 0.6686\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9309 - acc: 0.6079 - val_loss: 0.8976 - val_acc: 0.6629\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9364 - acc: 0.5990 - val_loss: 0.8988 - val_acc: 0.6571\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9232 - acc: 0.6245 - val_loss: 0.8970 - val_acc: 0.6686\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9137 - acc: 0.6066 - val_loss: 0.8981 - val_acc: 0.6629\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9181 - acc: 0.6181 - val_loss: 0.8955 - val_acc: 0.6743\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9018 - acc: 0.6162 - val_loss: 0.8925 - val_acc: 0.6743\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8969 - acc: 0.6271 - val_loss: 0.8927 - val_acc: 0.6800\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8937 - acc: 0.6086 - val_loss: 0.8899 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9037 - acc: 0.6201 - val_loss: 0.8876 - val_acc: 0.6686\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8857 - acc: 0.6354 - val_loss: 0.8890 - val_acc: 0.6571\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8869 - acc: 0.6258 - val_loss: 0.8935 - val_acc: 0.6629\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9007 - acc: 0.6328 - val_loss: 0.8896 - val_acc: 0.6686\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8675 - acc: 0.6379 - val_loss: 0.8884 - val_acc: 0.6800\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8810 - acc: 0.6392 - val_loss: 0.8902 - val_acc: 0.6800\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.8753 - acc: 0.6252 - val_loss: 0.8897 - val_acc: 0.6857\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8732 - acc: 0.6341 - val_loss: 0.8918 - val_acc: 0.6800\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8662 - acc: 0.6424 - val_loss: 0.8962 - val_acc: 0.6629\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8649 - acc: 0.6462 - val_loss: 0.8934 - val_acc: 0.6686\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8622 - acc: 0.6418 - val_loss: 0.8941 - val_acc: 0.6571\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8614 - acc: 0.6418 - val_loss: 0.8943 - val_acc: 0.6571\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8674 - acc: 0.6386 - val_loss: 0.8947 - val_acc: 0.6629\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8575 - acc: 0.6405 - val_loss: 0.8974 - val_acc: 0.6629\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8561 - acc: 0.6513 - val_loss: 0.8986 - val_acc: 0.6629\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8563 - acc: 0.6424 - val_loss: 0.8993 - val_acc: 0.6514\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 42us/step - loss: 0.8556 - acc: 0.6437 - val_loss: 0.8973 - val_acc: 0.6400\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8422 - acc: 0.6545 - val_loss: 0.9003 - val_acc: 0.6514\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8320 - acc: 0.6692 - val_loss: 0.8972 - val_acc: 0.6686\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8573 - acc: 0.6411 - val_loss: 0.8913 - val_acc: 0.6629\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8298 - acc: 0.6616 - val_loss: 0.8901 - val_acc: 0.6914\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8463 - acc: 0.6552 - val_loss: 0.8936 - val_acc: 0.6686\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8400 - acc: 0.6405 - val_loss: 0.8946 - val_acc: 0.6629\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8461 - acc: 0.6539 - val_loss: 0.8968 - val_acc: 0.6857\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8521 - acc: 0.6418 - val_loss: 0.8938 - val_acc: 0.6800\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8476 - acc: 0.6526 - val_loss: 0.8952 - val_acc: 0.6686\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8114 - acc: 0.6845 - val_loss: 0.9000 - val_acc: 0.6743\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8261 - acc: 0.6724 - val_loss: 0.8997 - val_acc: 0.6571\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8294 - acc: 0.6648 - val_loss: 0.8966 - val_acc: 0.6629\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8164 - acc: 0.6590 - val_loss: 0.8995 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8144 - acc: 0.6756 - val_loss: 0.9049 - val_acc: 0.6686\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8246 - acc: 0.6558 - val_loss: 0.9018 - val_acc: 0.6629\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8093 - acc: 0.6654 - val_loss: 0.9042 - val_acc: 0.6686\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8264 - acc: 0.6692 - val_loss: 0.9034 - val_acc: 0.6743\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8172 - acc: 0.6590 - val_loss: 0.8988 - val_acc: 0.6629\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8202 - acc: 0.6667 - val_loss: 0.8939 - val_acc: 0.6914\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7994 - acc: 0.6737 - val_loss: 0.8952 - val_acc: 0.6743\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7932 - acc: 0.6622 - val_loss: 0.8979 - val_acc: 0.6743\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7742 - acc: 0.6890 - val_loss: 0.9022 - val_acc: 0.6857\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8117 - acc: 0.6750 - val_loss: 0.9089 - val_acc: 0.6686\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8094 - acc: 0.6481 - val_loss: 0.9060 - val_acc: 0.6514\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7898 - acc: 0.6794 - val_loss: 0.9033 - val_acc: 0.6743\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7823 - acc: 0.6820 - val_loss: 0.9037 - val_acc: 0.6743\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7927 - acc: 0.6571 - val_loss: 0.9036 - val_acc: 0.6571\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7840 - acc: 0.6750 - val_loss: 0.9047 - val_acc: 0.6629\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7943 - acc: 0.6941 - val_loss: 0.9078 - val_acc: 0.6743\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7787 - acc: 0.6916 - val_loss: 0.9070 - val_acc: 0.6800\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7895 - acc: 0.6673 - val_loss: 0.9057 - val_acc: 0.6914\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7906 - acc: 0.6807 - val_loss: 0.9098 - val_acc: 0.6743\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7784 - acc: 0.6820 - val_loss: 0.9112 - val_acc: 0.6686\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7806 - acc: 0.6807 - val_loss: 0.9132 - val_acc: 0.6629\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7799 - acc: 0.6724 - val_loss: 0.9160 - val_acc: 0.6800\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7803 - acc: 0.6884 - val_loss: 0.9163 - val_acc: 0.6629\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7701 - acc: 0.6877 - val_loss: 0.9123 - val_acc: 0.6743\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7558 - acc: 0.6980 - val_loss: 0.9135 - val_acc: 0.6743\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7738 - acc: 0.6897 - val_loss: 0.9183 - val_acc: 0.6800\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7791 - acc: 0.6814 - val_loss: 0.9194 - val_acc: 0.6629\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7726 - acc: 0.6858 - val_loss: 0.9210 - val_acc: 0.6686\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7849 - acc: 0.6801 - val_loss: 0.9217 - val_acc: 0.6571\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7795 - acc: 0.6750 - val_loss: 0.9248 - val_acc: 0.6629\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7666 - acc: 0.6814 - val_loss: 0.9244 - val_acc: 0.6514\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7622 - acc: 0.6941 - val_loss: 0.9193 - val_acc: 0.6629\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7649 - acc: 0.6916 - val_loss: 0.9260 - val_acc: 0.6800\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7573 - acc: 0.6973 - val_loss: 0.9320 - val_acc: 0.6629\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7421 - acc: 0.7056 - val_loss: 0.9261 - val_acc: 0.6686\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7473 - acc: 0.7056 - val_loss: 0.9262 - val_acc: 0.6857\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7502 - acc: 0.6922 - val_loss: 0.9197 - val_acc: 0.7086\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7704 - acc: 0.6980 - val_loss: 0.9219 - val_acc: 0.6914\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7327 - acc: 0.6999 - val_loss: 0.9234 - val_acc: 0.6743\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7246 - acc: 0.7152 - val_loss: 0.9281 - val_acc: 0.6629\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7394 - acc: 0.6935 - val_loss: 0.9316 - val_acc: 0.6629\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7479 - acc: 0.6871 - val_loss: 0.9324 - val_acc: 0.6686\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7380 - acc: 0.7133 - val_loss: 0.9312 - val_acc: 0.6686\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7373 - acc: 0.7063 - val_loss: 0.9337 - val_acc: 0.6571\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7548 - acc: 0.6948 - val_loss: 0.9399 - val_acc: 0.6629\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7503 - acc: 0.6897 - val_loss: 0.9429 - val_acc: 0.6629\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7322 - acc: 0.7018 - val_loss: 0.9438 - val_acc: 0.6571\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7353 - acc: 0.7126 - val_loss: 0.9441 - val_acc: 0.6629\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7432 - acc: 0.6992 - val_loss: 0.9443 - val_acc: 0.6686\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7684 - acc: 0.6801 - val_loss: 0.9396 - val_acc: 0.6686\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7392 - acc: 0.6986 - val_loss: 0.9419 - val_acc: 0.6514\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7180 - acc: 0.7082 - val_loss: 0.9451 - val_acc: 0.6686\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7239 - acc: 0.7126 - val_loss: 0.9491 - val_acc: 0.6571\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7328 - acc: 0.7114 - val_loss: 0.9497 - val_acc: 0.6857\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7253 - acc: 0.7050 - val_loss: 0.9504 - val_acc: 0.6800\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7253 - acc: 0.7095 - val_loss: 0.9544 - val_acc: 0.6686\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7190 - acc: 0.7107 - val_loss: 0.9554 - val_acc: 0.6686\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7225 - acc: 0.7024 - val_loss: 0.9617 - val_acc: 0.6629\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7299 - acc: 0.7056 - val_loss: 0.9727 - val_acc: 0.6400\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7114 - acc: 0.7165 - val_loss: 0.9624 - val_acc: 0.6800\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7190 - acc: 0.7126 - val_loss: 0.9626 - val_acc: 0.6743\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7058 - acc: 0.7216 - val_loss: 0.9670 - val_acc: 0.6857\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7142 - acc: 0.7050 - val_loss: 0.9639 - val_acc: 0.6743\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7042 - acc: 0.7254 - val_loss: 0.9580 - val_acc: 0.6629\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7117 - acc: 0.7088 - val_loss: 0.9607 - val_acc: 0.6514\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7060 - acc: 0.7107 - val_loss: 0.9603 - val_acc: 0.6629\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6979 - acc: 0.7216 - val_loss: 0.9685 - val_acc: 0.6514\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7216 - acc: 0.7069 - val_loss: 0.9749 - val_acc: 0.6571\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7091 - acc: 0.7267 - val_loss: 0.9603 - val_acc: 0.6629\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7130 - acc: 0.7107 - val_loss: 0.9625 - val_acc: 0.6457\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6953 - acc: 0.7165 - val_loss: 0.9650 - val_acc: 0.6686\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6953 - acc: 0.7184 - val_loss: 0.9730 - val_acc: 0.6629\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6910 - acc: 0.7178 - val_loss: 0.9730 - val_acc: 0.6629\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7044 - acc: 0.7146 - val_loss: 0.9721 - val_acc: 0.6514\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6871 - acc: 0.7146 - val_loss: 0.9709 - val_acc: 0.6686\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6836 - acc: 0.7139 - val_loss: 0.9682 - val_acc: 0.6743\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6736 - acc: 0.7356 - val_loss: 0.9644 - val_acc: 0.6571\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6873 - acc: 0.7331 - val_loss: 0.9707 - val_acc: 0.6571\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6780 - acc: 0.7165 - val_loss: 0.9755 - val_acc: 0.6457\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7045 - acc: 0.7209 - val_loss: 0.9727 - val_acc: 0.6514\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6909 - acc: 0.7171 - val_loss: 0.9818 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 84us/step\n",
      "1741/1741 [==============================] - 0s 31us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.6123 - acc: 0.2701 - val_loss: 1.3612 - val_acc: 0.3943\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.4228 - acc: 0.3442 - val_loss: 1.2196 - val_acc: 0.4686\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.3207 - acc: 0.3934 - val_loss: 1.1437 - val_acc: 0.5543\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.2375 - acc: 0.4476 - val_loss: 1.1015 - val_acc: 0.6057\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.2051 - acc: 0.4419 - val_loss: 1.0727 - val_acc: 0.6114\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1849 - acc: 0.4610 - val_loss: 1.0526 - val_acc: 0.6229\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.1735 - acc: 0.4872 - val_loss: 1.0297 - val_acc: 0.6286\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1556 - acc: 0.4808 - val_loss: 1.0130 - val_acc: 0.6286\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.0947 - acc: 0.5236 - val_loss: 0.9966 - val_acc: 0.6571\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0988 - acc: 0.5166 - val_loss: 0.9844 - val_acc: 0.6457\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0705 - acc: 0.5243 - val_loss: 0.9716 - val_acc: 0.6571\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0436 - acc: 0.5485 - val_loss: 0.9621 - val_acc: 0.6686\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0410 - acc: 0.5581 - val_loss: 0.9566 - val_acc: 0.6457\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0494 - acc: 0.5396 - val_loss: 0.9488 - val_acc: 0.6800\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 1.0229 - acc: 0.5594 - val_loss: 0.9407 - val_acc: 0.6800\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 103us/step - loss: 1.0096 - acc: 0.5556 - val_loss: 0.9323 - val_acc: 0.6800\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 1.0074 - acc: 0.5792 - val_loss: 0.9298 - val_acc: 0.6629\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.9954 - acc: 0.5632 - val_loss: 0.9273 - val_acc: 0.6743\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9916 - acc: 0.5779 - val_loss: 0.9229 - val_acc: 0.6857\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9774 - acc: 0.5754 - val_loss: 0.9175 - val_acc: 0.6743\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9822 - acc: 0.5785 - val_loss: 0.9091 - val_acc: 0.6857\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9653 - acc: 0.5779 - val_loss: 0.9066 - val_acc: 0.6743\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.9685 - acc: 0.5856 - val_loss: 0.9032 - val_acc: 0.6800\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9303 - acc: 0.6079 - val_loss: 0.8995 - val_acc: 0.6800\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9483 - acc: 0.5939 - val_loss: 0.8954 - val_acc: 0.6971\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9432 - acc: 0.5951 - val_loss: 0.8933 - val_acc: 0.6686\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9281 - acc: 0.6213 - val_loss: 0.8945 - val_acc: 0.6857\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9354 - acc: 0.5945 - val_loss: 0.8937 - val_acc: 0.6743\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9261 - acc: 0.6175 - val_loss: 0.8918 - val_acc: 0.6629\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9223 - acc: 0.6092 - val_loss: 0.8919 - val_acc: 0.6800\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9331 - acc: 0.6105 - val_loss: 0.8922 - val_acc: 0.6800\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9119 - acc: 0.6252 - val_loss: 0.8896 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.9168 - acc: 0.6258 - val_loss: 0.8887 - val_acc: 0.6686\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9149 - acc: 0.6117 - val_loss: 0.8902 - val_acc: 0.6857\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8971 - acc: 0.6156 - val_loss: 0.8921 - val_acc: 0.6857\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8990 - acc: 0.6162 - val_loss: 0.8915 - val_acc: 0.6743\n",
      "Epoch 37/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.9066 - acc: 0.6130 - val_loss: 0.8913 - val_acc: 0.6743\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9010 - acc: 0.6169 - val_loss: 0.8905 - val_acc: 0.6629\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8864 - acc: 0.6347 - val_loss: 0.8901 - val_acc: 0.6800\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8872 - acc: 0.6239 - val_loss: 0.8882 - val_acc: 0.6800\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8911 - acc: 0.6264 - val_loss: 0.8869 - val_acc: 0.6857\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8932 - acc: 0.6367 - val_loss: 0.8891 - val_acc: 0.6686\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8843 - acc: 0.6379 - val_loss: 0.8925 - val_acc: 0.6686\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8893 - acc: 0.6245 - val_loss: 0.8911 - val_acc: 0.6743\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8756 - acc: 0.6284 - val_loss: 0.8883 - val_acc: 0.6743\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8664 - acc: 0.6501 - val_loss: 0.8853 - val_acc: 0.6800\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8784 - acc: 0.6264 - val_loss: 0.8866 - val_acc: 0.6857\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8563 - acc: 0.6424 - val_loss: 0.8877 - val_acc: 0.6743\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8545 - acc: 0.6424 - val_loss: 0.8843 - val_acc: 0.6800\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8539 - acc: 0.6398 - val_loss: 0.8865 - val_acc: 0.6743\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8780 - acc: 0.6271 - val_loss: 0.8843 - val_acc: 0.6800\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8467 - acc: 0.6660 - val_loss: 0.8860 - val_acc: 0.6743\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8480 - acc: 0.6660 - val_loss: 0.8890 - val_acc: 0.6743\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8433 - acc: 0.6507 - val_loss: 0.8884 - val_acc: 0.6629\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8441 - acc: 0.6577 - val_loss: 0.8879 - val_acc: 0.6629\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8323 - acc: 0.6660 - val_loss: 0.8896 - val_acc: 0.6800\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8167 - acc: 0.6731 - val_loss: 0.8899 - val_acc: 0.6629\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8266 - acc: 0.6513 - val_loss: 0.8885 - val_acc: 0.6686\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8352 - acc: 0.6488 - val_loss: 0.8856 - val_acc: 0.6800\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8371 - acc: 0.6564 - val_loss: 0.8859 - val_acc: 0.6629\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8362 - acc: 0.6564 - val_loss: 0.8865 - val_acc: 0.6686\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8123 - acc: 0.6679 - val_loss: 0.8880 - val_acc: 0.6686\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8217 - acc: 0.6545 - val_loss: 0.8884 - val_acc: 0.6629\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8308 - acc: 0.6552 - val_loss: 0.8893 - val_acc: 0.6800\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8257 - acc: 0.6590 - val_loss: 0.8912 - val_acc: 0.6686\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8166 - acc: 0.6635 - val_loss: 0.8903 - val_acc: 0.6686\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8174 - acc: 0.6488 - val_loss: 0.8868 - val_acc: 0.6857\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8150 - acc: 0.6603 - val_loss: 0.8867 - val_acc: 0.6686\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8138 - acc: 0.6622 - val_loss: 0.8852 - val_acc: 0.6743\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8000 - acc: 0.6794 - val_loss: 0.8865 - val_acc: 0.6629\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8112 - acc: 0.6622 - val_loss: 0.8909 - val_acc: 0.6686\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7993 - acc: 0.6762 - val_loss: 0.8898 - val_acc: 0.6743\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7979 - acc: 0.6807 - val_loss: 0.8967 - val_acc: 0.6743\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7972 - acc: 0.6692 - val_loss: 0.8931 - val_acc: 0.6743\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7889 - acc: 0.6756 - val_loss: 0.8808 - val_acc: 0.6914\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7969 - acc: 0.6782 - val_loss: 0.8834 - val_acc: 0.6857\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7813 - acc: 0.6807 - val_loss: 0.8862 - val_acc: 0.6800\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7799 - acc: 0.6756 - val_loss: 0.8884 - val_acc: 0.6800\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7877 - acc: 0.6743 - val_loss: 0.8916 - val_acc: 0.6743\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7986 - acc: 0.6762 - val_loss: 0.8898 - val_acc: 0.6800\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7876 - acc: 0.6711 - val_loss: 0.8880 - val_acc: 0.6686\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7933 - acc: 0.6648 - val_loss: 0.8862 - val_acc: 0.6743\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7907 - acc: 0.6839 - val_loss: 0.8952 - val_acc: 0.6914\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7721 - acc: 0.6865 - val_loss: 0.8949 - val_acc: 0.6800\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7606 - acc: 0.6935 - val_loss: 0.8966 - val_acc: 0.6743\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7641 - acc: 0.6986 - val_loss: 0.9003 - val_acc: 0.6800\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7886 - acc: 0.6775 - val_loss: 0.8998 - val_acc: 0.6571\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7849 - acc: 0.6814 - val_loss: 0.8993 - val_acc: 0.6743\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7720 - acc: 0.6884 - val_loss: 0.8999 - val_acc: 0.6686\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7690 - acc: 0.6980 - val_loss: 0.9015 - val_acc: 0.6743\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7703 - acc: 0.6897 - val_loss: 0.8980 - val_acc: 0.6800\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7855 - acc: 0.6756 - val_loss: 0.8980 - val_acc: 0.6743\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7580 - acc: 0.6916 - val_loss: 0.9049 - val_acc: 0.6914\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7664 - acc: 0.6801 - val_loss: 0.9041 - val_acc: 0.6800\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7495 - acc: 0.6960 - val_loss: 0.9027 - val_acc: 0.6743\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7650 - acc: 0.6928 - val_loss: 0.9052 - val_acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7463 - acc: 0.7095 - val_loss: 0.9007 - val_acc: 0.6686\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7566 - acc: 0.7043 - val_loss: 0.9051 - val_acc: 0.6686\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7429 - acc: 0.7005 - val_loss: 0.9040 - val_acc: 0.6629\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7547 - acc: 0.6788 - val_loss: 0.9084 - val_acc: 0.6686\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.7566 - acc: 0.6960 - val_loss: 0.9130 - val_acc: 0.6514\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7416 - acc: 0.6986 - val_loss: 0.9102 - val_acc: 0.6686\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7516 - acc: 0.6922 - val_loss: 0.9131 - val_acc: 0.6629\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7676 - acc: 0.6922 - val_loss: 0.9075 - val_acc: 0.6514\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7450 - acc: 0.6973 - val_loss: 0.9046 - val_acc: 0.6514\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7290 - acc: 0.7069 - val_loss: 0.9075 - val_acc: 0.6457\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7441 - acc: 0.6973 - val_loss: 0.9092 - val_acc: 0.6514\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7450 - acc: 0.7018 - val_loss: 0.9109 - val_acc: 0.6686\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7418 - acc: 0.6928 - val_loss: 0.9131 - val_acc: 0.6514\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7365 - acc: 0.7056 - val_loss: 0.9143 - val_acc: 0.6514\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7331 - acc: 0.7043 - val_loss: 0.9152 - val_acc: 0.6457\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7303 - acc: 0.7056 - val_loss: 0.9121 - val_acc: 0.6571\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7179 - acc: 0.7139 - val_loss: 0.9141 - val_acc: 0.6514\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7203 - acc: 0.7031 - val_loss: 0.9130 - val_acc: 0.6400\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7244 - acc: 0.6999 - val_loss: 0.9134 - val_acc: 0.6514\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7031 - acc: 0.7063 - val_loss: 0.9140 - val_acc: 0.6629\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7366 - acc: 0.6922 - val_loss: 0.9140 - val_acc: 0.6743\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7469 - acc: 0.6960 - val_loss: 0.9147 - val_acc: 0.6686\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7381 - acc: 0.7018 - val_loss: 0.9175 - val_acc: 0.6743\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7341 - acc: 0.7056 - val_loss: 0.9212 - val_acc: 0.6629\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7193 - acc: 0.7107 - val_loss: 0.9231 - val_acc: 0.6743\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7168 - acc: 0.6986 - val_loss: 0.9252 - val_acc: 0.6514\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7088 - acc: 0.7178 - val_loss: 0.9259 - val_acc: 0.6571\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7158 - acc: 0.7018 - val_loss: 0.9233 - val_acc: 0.6629\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7123 - acc: 0.7139 - val_loss: 0.9219 - val_acc: 0.6514\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7287 - acc: 0.7069 - val_loss: 0.9285 - val_acc: 0.6400\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7053 - acc: 0.7178 - val_loss: 0.9269 - val_acc: 0.6571\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6836 - acc: 0.7165 - val_loss: 0.9244 - val_acc: 0.6686\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7077 - acc: 0.7139 - val_loss: 0.9265 - val_acc: 0.6686\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 44us/step - loss: 0.7102 - acc: 0.7056 - val_loss: 0.9303 - val_acc: 0.6686\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6952 - acc: 0.7299 - val_loss: 0.9309 - val_acc: 0.6514\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6870 - acc: 0.7363 - val_loss: 0.9282 - val_acc: 0.6686\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7062 - acc: 0.7031 - val_loss: 0.9336 - val_acc: 0.6686\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6831 - acc: 0.7197 - val_loss: 0.9358 - val_acc: 0.6571\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6930 - acc: 0.7152 - val_loss: 0.9313 - val_acc: 0.6514\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6771 - acc: 0.7344 - val_loss: 0.9388 - val_acc: 0.6457\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7159 - acc: 0.7101 - val_loss: 0.9374 - val_acc: 0.6400\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7151 - acc: 0.6999 - val_loss: 0.9364 - val_acc: 0.6514\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6907 - acc: 0.7139 - val_loss: 0.9379 - val_acc: 0.6571\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6865 - acc: 0.7203 - val_loss: 0.9425 - val_acc: 0.6400\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6687 - acc: 0.7261 - val_loss: 0.9476 - val_acc: 0.6343\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6683 - acc: 0.7324 - val_loss: 0.9423 - val_acc: 0.6514\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6743 - acc: 0.7312 - val_loss: 0.9413 - val_acc: 0.6514\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6630 - acc: 0.7299 - val_loss: 0.9441 - val_acc: 0.6514\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6800 - acc: 0.7273 - val_loss: 0.9500 - val_acc: 0.6571\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6894 - acc: 0.7152 - val_loss: 0.9481 - val_acc: 0.6514\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6951 - acc: 0.6992 - val_loss: 0.9509 - val_acc: 0.6400\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6677 - acc: 0.7356 - val_loss: 0.9483 - val_acc: 0.6400\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6664 - acc: 0.7350 - val_loss: 0.9503 - val_acc: 0.6457\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6561 - acc: 0.7292 - val_loss: 0.9507 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 56us/step\n",
      "1741/1741 [==============================] - 0s 43us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 7s 5ms/step - loss: 1.4753 - acc: 0.2974 - val_loss: 1.2487 - val_acc: 0.4629\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.3406 - acc: 0.3931 - val_loss: 1.1528 - val_acc: 0.5143\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.2894 - acc: 0.4008 - val_loss: 1.0970 - val_acc: 0.5543\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.2189 - acc: 0.4486 - val_loss: 1.0545 - val_acc: 0.5600\n",
      "Epoch 5/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.1758 - acc: 0.4646 - val_loss: 1.0310 - val_acc: 0.5829\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1632 - acc: 0.4818 - val_loss: 1.0130 - val_acc: 0.6000\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1172 - acc: 0.5201 - val_loss: 0.9872 - val_acc: 0.6286\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.1080 - acc: 0.5156 - val_loss: 0.9764 - val_acc: 0.6400\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.0818 - acc: 0.5131 - val_loss: 0.9714 - val_acc: 0.6400\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.0568 - acc: 0.5297 - val_loss: 0.9610 - val_acc: 0.6343\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0562 - acc: 0.5361 - val_loss: 0.9551 - val_acc: 0.6229\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0614 - acc: 0.5412 - val_loss: 0.9466 - val_acc: 0.6800\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0250 - acc: 0.5609 - val_loss: 0.9352 - val_acc: 0.6857\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0180 - acc: 0.5469 - val_loss: 0.9298 - val_acc: 0.6800\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.0044 - acc: 0.5756 - val_loss: 0.9247 - val_acc: 0.6686\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0162 - acc: 0.5609 - val_loss: 0.9215 - val_acc: 0.6629\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9931 - acc: 0.5795 - val_loss: 0.9174 - val_acc: 0.6857\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9891 - acc: 0.5692 - val_loss: 0.9152 - val_acc: 0.6857\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9844 - acc: 0.5801 - val_loss: 0.9091 - val_acc: 0.6914\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9640 - acc: 0.5980 - val_loss: 0.9074 - val_acc: 0.6743\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.9654 - acc: 0.5737 - val_loss: 0.9078 - val_acc: 0.6800\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9628 - acc: 0.5986 - val_loss: 0.8976 - val_acc: 0.6857\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9383 - acc: 0.6152 - val_loss: 0.8967 - val_acc: 0.6857\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9614 - acc: 0.6018 - val_loss: 0.9027 - val_acc: 0.6686\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9397 - acc: 0.6107 - val_loss: 0.8984 - val_acc: 0.6857\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9335 - acc: 0.5948 - val_loss: 0.8990 - val_acc: 0.6914\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9285 - acc: 0.5973 - val_loss: 0.8933 - val_acc: 0.6971\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9293 - acc: 0.6069 - val_loss: 0.8959 - val_acc: 0.6743\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9221 - acc: 0.6184 - val_loss: 0.9013 - val_acc: 0.6800\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9133 - acc: 0.6260 - val_loss: 0.9068 - val_acc: 0.6686\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9228 - acc: 0.6152 - val_loss: 0.9012 - val_acc: 0.6800\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9095 - acc: 0.6146 - val_loss: 0.8957 - val_acc: 0.6857\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9005 - acc: 0.6177 - val_loss: 0.8988 - val_acc: 0.6743\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9078 - acc: 0.6158 - val_loss: 0.9000 - val_acc: 0.6686\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9141 - acc: 0.6171 - val_loss: 0.9032 - val_acc: 0.6743\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8888 - acc: 0.6331 - val_loss: 0.9027 - val_acc: 0.6857\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9022 - acc: 0.6088 - val_loss: 0.9026 - val_acc: 0.6800\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.9006 - acc: 0.6209 - val_loss: 0.9023 - val_acc: 0.6800\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8723 - acc: 0.6331 - val_loss: 0.9006 - val_acc: 0.6743\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8674 - acc: 0.6356 - val_loss: 0.9071 - val_acc: 0.6800\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8878 - acc: 0.6273 - val_loss: 0.9010 - val_acc: 0.6800\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8596 - acc: 0.6496 - val_loss: 0.8991 - val_acc: 0.6857\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8987 - acc: 0.6184 - val_loss: 0.8980 - val_acc: 0.6857\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8687 - acc: 0.6471 - val_loss: 0.8991 - val_acc: 0.6857\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8670 - acc: 0.6414 - val_loss: 0.9041 - val_acc: 0.6857\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8667 - acc: 0.6407 - val_loss: 0.9040 - val_acc: 0.6857\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8449 - acc: 0.6548 - val_loss: 0.9076 - val_acc: 0.6800\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8594 - acc: 0.6471 - val_loss: 0.9090 - val_acc: 0.6857\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8394 - acc: 0.6382 - val_loss: 0.9115 - val_acc: 0.6800\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8660 - acc: 0.6465 - val_loss: 0.9122 - val_acc: 0.6743\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8665 - acc: 0.6356 - val_loss: 0.9093 - val_acc: 0.6914\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8462 - acc: 0.6522 - val_loss: 0.9125 - val_acc: 0.6800\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8349 - acc: 0.6477 - val_loss: 0.9076 - val_acc: 0.6914\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8414 - acc: 0.6522 - val_loss: 0.9097 - val_acc: 0.6857\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8399 - acc: 0.6611 - val_loss: 0.9134 - val_acc: 0.6857\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8317 - acc: 0.6618 - val_loss: 0.9161 - val_acc: 0.6914\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8294 - acc: 0.6592 - val_loss: 0.9110 - val_acc: 0.6800\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8208 - acc: 0.6573 - val_loss: 0.9144 - val_acc: 0.6857\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8418 - acc: 0.6599 - val_loss: 0.9190 - val_acc: 0.6800\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8252 - acc: 0.6592 - val_loss: 0.9109 - val_acc: 0.6914\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8312 - acc: 0.6579 - val_loss: 0.9101 - val_acc: 0.6857\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8381 - acc: 0.6535 - val_loss: 0.9152 - val_acc: 0.6914\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8233 - acc: 0.6662 - val_loss: 0.9097 - val_acc: 0.6914\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8111 - acc: 0.6637 - val_loss: 0.9168 - val_acc: 0.6857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8133 - acc: 0.6662 - val_loss: 0.9180 - val_acc: 0.6914\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8156 - acc: 0.6662 - val_loss: 0.9229 - val_acc: 0.6800\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8021 - acc: 0.6579 - val_loss: 0.9224 - val_acc: 0.6743\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8207 - acc: 0.6611 - val_loss: 0.9185 - val_acc: 0.6800\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8125 - acc: 0.6688 - val_loss: 0.9207 - val_acc: 0.6686\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8043 - acc: 0.6637 - val_loss: 0.9259 - val_acc: 0.6800\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8019 - acc: 0.6707 - val_loss: 0.9229 - val_acc: 0.6743\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8098 - acc: 0.6631 - val_loss: 0.9235 - val_acc: 0.6686\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7990 - acc: 0.6707 - val_loss: 0.9307 - val_acc: 0.6743\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8170 - acc: 0.6733 - val_loss: 0.9273 - val_acc: 0.6800\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8046 - acc: 0.6701 - val_loss: 0.9299 - val_acc: 0.6800\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7945 - acc: 0.6835 - val_loss: 0.9299 - val_acc: 0.6914\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7869 - acc: 0.6662 - val_loss: 0.9344 - val_acc: 0.6857\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7864 - acc: 0.6707 - val_loss: 0.9315 - val_acc: 0.6857\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7759 - acc: 0.6847 - val_loss: 0.9332 - val_acc: 0.6857\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7830 - acc: 0.6784 - val_loss: 0.9289 - val_acc: 0.6800\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8070 - acc: 0.6765 - val_loss: 0.9296 - val_acc: 0.6857\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7549 - acc: 0.6981 - val_loss: 0.9379 - val_acc: 0.6743\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7741 - acc: 0.6713 - val_loss: 0.9366 - val_acc: 0.6743\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7904 - acc: 0.6879 - val_loss: 0.9380 - val_acc: 0.6914\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7822 - acc: 0.6784 - val_loss: 0.9415 - val_acc: 0.6800\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7672 - acc: 0.6694 - val_loss: 0.9440 - val_acc: 0.6743\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7653 - acc: 0.6867 - val_loss: 0.9463 - val_acc: 0.6686\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7766 - acc: 0.6847 - val_loss: 0.9417 - val_acc: 0.6800\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7681 - acc: 0.6784 - val_loss: 0.9354 - val_acc: 0.6914\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7672 - acc: 0.6879 - val_loss: 0.9404 - val_acc: 0.6743\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7869 - acc: 0.6682 - val_loss: 0.9435 - val_acc: 0.6743\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7662 - acc: 0.6726 - val_loss: 0.9485 - val_acc: 0.6800\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7581 - acc: 0.7001 - val_loss: 0.9468 - val_acc: 0.6743\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7686 - acc: 0.6899 - val_loss: 0.9427 - val_acc: 0.6629\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7598 - acc: 0.6809 - val_loss: 0.9459 - val_acc: 0.6743\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7504 - acc: 0.6911 - val_loss: 0.9470 - val_acc: 0.6800\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7434 - acc: 0.6981 - val_loss: 0.9482 - val_acc: 0.6743\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7341 - acc: 0.7058 - val_loss: 0.9485 - val_acc: 0.6743\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7527 - acc: 0.6988 - val_loss: 0.9523 - val_acc: 0.6743\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7637 - acc: 0.6707 - val_loss: 0.9494 - val_acc: 0.6857\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7527 - acc: 0.6918 - val_loss: 0.9576 - val_acc: 0.6743\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7418 - acc: 0.7071 - val_loss: 0.9556 - val_acc: 0.6743\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7521 - acc: 0.6956 - val_loss: 0.9525 - val_acc: 0.6800\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7393 - acc: 0.7084 - val_loss: 0.9539 - val_acc: 0.6800\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7329 - acc: 0.6975 - val_loss: 0.9585 - val_acc: 0.6686\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7452 - acc: 0.6943 - val_loss: 0.9573 - val_acc: 0.6743\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7506 - acc: 0.6937 - val_loss: 0.9614 - val_acc: 0.6686\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7311 - acc: 0.6930 - val_loss: 0.9633 - val_acc: 0.6743\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7289 - acc: 0.7013 - val_loss: 0.9676 - val_acc: 0.6743\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7112 - acc: 0.7020 - val_loss: 0.9742 - val_acc: 0.6800\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7366 - acc: 0.7135 - val_loss: 0.9710 - val_acc: 0.6629\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7336 - acc: 0.7013 - val_loss: 0.9740 - val_acc: 0.6686\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7172 - acc: 0.7096 - val_loss: 0.9711 - val_acc: 0.6629\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7302 - acc: 0.7045 - val_loss: 0.9842 - val_acc: 0.6571\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7303 - acc: 0.6988 - val_loss: 0.9749 - val_acc: 0.6743\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7375 - acc: 0.6962 - val_loss: 0.9724 - val_acc: 0.6686\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7181 - acc: 0.7058 - val_loss: 0.9744 - val_acc: 0.6686\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7275 - acc: 0.7052 - val_loss: 0.9716 - val_acc: 0.6629\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7258 - acc: 0.7116 - val_loss: 0.9739 - val_acc: 0.6629\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7264 - acc: 0.7064 - val_loss: 0.9724 - val_acc: 0.6629\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7288 - acc: 0.6981 - val_loss: 0.9721 - val_acc: 0.6514\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.7059 - acc: 0.7141 - val_loss: 0.9757 - val_acc: 0.6514\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7204 - acc: 0.7052 - val_loss: 0.9740 - val_acc: 0.6514\n",
      "Epoch 124/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7149 - acc: 0.7001 - val_loss: 0.9788 - val_acc: 0.6457\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6911 - acc: 0.7294 - val_loss: 0.9901 - val_acc: 0.6514\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6906 - acc: 0.7186 - val_loss: 0.9899 - val_acc: 0.6457\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7142 - acc: 0.7077 - val_loss: 0.9816 - val_acc: 0.6514\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6920 - acc: 0.7205 - val_loss: 0.9964 - val_acc: 0.6629\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7094 - acc: 0.7090 - val_loss: 0.9901 - val_acc: 0.6686\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7174 - acc: 0.7122 - val_loss: 0.9884 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6900 - acc: 0.7205 - val_loss: 0.9873 - val_acc: 0.6571\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7054 - acc: 0.7326 - val_loss: 0.9906 - val_acc: 0.6629\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6902 - acc: 0.7211 - val_loss: 0.9923 - val_acc: 0.6686\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6919 - acc: 0.7071 - val_loss: 0.9915 - val_acc: 0.6457\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6836 - acc: 0.7167 - val_loss: 0.9991 - val_acc: 0.6571\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6876 - acc: 0.7179 - val_loss: 0.9970 - val_acc: 0.6514\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7032 - acc: 0.7160 - val_loss: 0.9891 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6820 - acc: 0.7090 - val_loss: 0.9898 - val_acc: 0.6400\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6815 - acc: 0.7198 - val_loss: 0.9914 - val_acc: 0.6457\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6929 - acc: 0.7128 - val_loss: 0.9990 - val_acc: 0.6514\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6839 - acc: 0.7064 - val_loss: 0.9961 - val_acc: 0.6457\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6821 - acc: 0.7186 - val_loss: 0.9934 - val_acc: 0.6514\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6795 - acc: 0.7147 - val_loss: 1.0079 - val_acc: 0.6571\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6872 - acc: 0.7250 - val_loss: 1.0029 - val_acc: 0.6457\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6773 - acc: 0.7128 - val_loss: 1.0095 - val_acc: 0.6343\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6759 - acc: 0.7294 - val_loss: 1.0128 - val_acc: 0.6457\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6921 - acc: 0.7307 - val_loss: 1.0167 - val_acc: 0.6514\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6821 - acc: 0.7160 - val_loss: 1.0111 - val_acc: 0.6514\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6630 - acc: 0.7332 - val_loss: 1.0156 - val_acc: 0.6400\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6640 - acc: 0.7256 - val_loss: 1.0193 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 51us/step\n",
      "1742/1742 [==============================] - 0s 30us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5757 - acc: 0.2795 - val_loss: 1.3046 - val_acc: 0.4571\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.3832 - acc: 0.3631 - val_loss: 1.1846 - val_acc: 0.5029\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.2705 - acc: 0.4365 - val_loss: 1.1044 - val_acc: 0.5429\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.2096 - acc: 0.4608 - val_loss: 1.0643 - val_acc: 0.5886\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.1551 - acc: 0.4888 - val_loss: 1.0371 - val_acc: 0.6114\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.1610 - acc: 0.4793 - val_loss: 1.0121 - val_acc: 0.6286\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1088 - acc: 0.5156 - val_loss: 0.9953 - val_acc: 0.6114\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1019 - acc: 0.5265 - val_loss: 0.9826 - val_acc: 0.6400\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0548 - acc: 0.5444 - val_loss: 0.9659 - val_acc: 0.6571\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0464 - acc: 0.5329 - val_loss: 0.9570 - val_acc: 0.6571\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 1.0571 - acc: 0.5367 - val_loss: 0.9514 - val_acc: 0.6686\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 1.0411 - acc: 0.5514 - val_loss: 0.9423 - val_acc: 0.6686\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0252 - acc: 0.5501 - val_loss: 0.9316 - val_acc: 0.6743\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9932 - acc: 0.5769 - val_loss: 0.9288 - val_acc: 0.6457\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9995 - acc: 0.5680 - val_loss: 0.9189 - val_acc: 0.6571\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9743 - acc: 0.5807 - val_loss: 0.9142 - val_acc: 0.6457\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9805 - acc: 0.5890 - val_loss: 0.9094 - val_acc: 0.6686\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9726 - acc: 0.5788 - val_loss: 0.9048 - val_acc: 0.6629\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9617 - acc: 0.5801 - val_loss: 0.8994 - val_acc: 0.6571\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9616 - acc: 0.5890 - val_loss: 0.8938 - val_acc: 0.6514\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9525 - acc: 0.6094 - val_loss: 0.8940 - val_acc: 0.6686\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9488 - acc: 0.6050 - val_loss: 0.8896 - val_acc: 0.6629\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9375 - acc: 0.6024 - val_loss: 0.8852 - val_acc: 0.6571\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9412 - acc: 0.5986 - val_loss: 0.8801 - val_acc: 0.6571\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9396 - acc: 0.6082 - val_loss: 0.8823 - val_acc: 0.6571\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9432 - acc: 0.5967 - val_loss: 0.8787 - val_acc: 0.6686\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9103 - acc: 0.6158 - val_loss: 0.8815 - val_acc: 0.6514\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9149 - acc: 0.6177 - val_loss: 0.8811 - val_acc: 0.6629\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9253 - acc: 0.6197 - val_loss: 0.8798 - val_acc: 0.6514\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9088 - acc: 0.6056 - val_loss: 0.8785 - val_acc: 0.6457\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9129 - acc: 0.5948 - val_loss: 0.8701 - val_acc: 0.6571\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8974 - acc: 0.6216 - val_loss: 0.8687 - val_acc: 0.6743\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8894 - acc: 0.6120 - val_loss: 0.8724 - val_acc: 0.6629\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.8963 - acc: 0.6324 - val_loss: 0.8685 - val_acc: 0.6743\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8768 - acc: 0.6311 - val_loss: 0.8721 - val_acc: 0.6686\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8942 - acc: 0.6331 - val_loss: 0.8712 - val_acc: 0.6686\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8948 - acc: 0.6171 - val_loss: 0.8721 - val_acc: 0.6686\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.8811 - acc: 0.6465 - val_loss: 0.8712 - val_acc: 0.6686\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8736 - acc: 0.6305 - val_loss: 0.8680 - val_acc: 0.6743\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8738 - acc: 0.6343 - val_loss: 0.8656 - val_acc: 0.6800\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8724 - acc: 0.6426 - val_loss: 0.8702 - val_acc: 0.6743\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8518 - acc: 0.6452 - val_loss: 0.8710 - val_acc: 0.6743\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8577 - acc: 0.6286 - val_loss: 0.8726 - val_acc: 0.6686\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8663 - acc: 0.6324 - val_loss: 0.8730 - val_acc: 0.6629\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.8683 - acc: 0.6439 - val_loss: 0.8685 - val_acc: 0.6743\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8443 - acc: 0.6618 - val_loss: 0.8658 - val_acc: 0.6629\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8615 - acc: 0.6311 - val_loss: 0.8681 - val_acc: 0.6743\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8542 - acc: 0.6426 - val_loss: 0.8649 - val_acc: 0.6800\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8421 - acc: 0.6503 - val_loss: 0.8624 - val_acc: 0.6686\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8421 - acc: 0.6445 - val_loss: 0.8598 - val_acc: 0.6800\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8393 - acc: 0.6573 - val_loss: 0.8596 - val_acc: 0.6743\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8155 - acc: 0.6739 - val_loss: 0.8641 - val_acc: 0.6743\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8400 - acc: 0.6484 - val_loss: 0.8611 - val_acc: 0.6571\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8365 - acc: 0.6586 - val_loss: 0.8587 - val_acc: 0.6743\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8254 - acc: 0.6599 - val_loss: 0.8566 - val_acc: 0.6800\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.8277 - acc: 0.6548 - val_loss: 0.8551 - val_acc: 0.6743\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8412 - acc: 0.6401 - val_loss: 0.8530 - val_acc: 0.6800\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8289 - acc: 0.6586 - val_loss: 0.8500 - val_acc: 0.6857\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8253 - acc: 0.6548 - val_loss: 0.8520 - val_acc: 0.6686\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8069 - acc: 0.6771 - val_loss: 0.8584 - val_acc: 0.6686\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8236 - acc: 0.6701 - val_loss: 0.8533 - val_acc: 0.6743\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8073 - acc: 0.6631 - val_loss: 0.8547 - val_acc: 0.6857\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8103 - acc: 0.6656 - val_loss: 0.8566 - val_acc: 0.6857\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7894 - acc: 0.6726 - val_loss: 0.8563 - val_acc: 0.6686\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7899 - acc: 0.6790 - val_loss: 0.8609 - val_acc: 0.6629\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7918 - acc: 0.6784 - val_loss: 0.8552 - val_acc: 0.6686\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8151 - acc: 0.6637 - val_loss: 0.8603 - val_acc: 0.6743\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8279 - acc: 0.6631 - val_loss: 0.8615 - val_acc: 0.6686\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7874 - acc: 0.6752 - val_loss: 0.8605 - val_acc: 0.6686\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7936 - acc: 0.6682 - val_loss: 0.8601 - val_acc: 0.6743\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7958 - acc: 0.6643 - val_loss: 0.8591 - val_acc: 0.6743\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7831 - acc: 0.6905 - val_loss: 0.8585 - val_acc: 0.6686\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7958 - acc: 0.6733 - val_loss: 0.8623 - val_acc: 0.6686\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7794 - acc: 0.6726 - val_loss: 0.8619 - val_acc: 0.6686\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7786 - acc: 0.6752 - val_loss: 0.8550 - val_acc: 0.6743\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7757 - acc: 0.6758 - val_loss: 0.8553 - val_acc: 0.6800\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7744 - acc: 0.6707 - val_loss: 0.8600 - val_acc: 0.6686\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7736 - acc: 0.6828 - val_loss: 0.8608 - val_acc: 0.6629\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7786 - acc: 0.6841 - val_loss: 0.8601 - val_acc: 0.6629\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7600 - acc: 0.6905 - val_loss: 0.8544 - val_acc: 0.6629\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7768 - acc: 0.6950 - val_loss: 0.8583 - val_acc: 0.6686\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7602 - acc: 0.6854 - val_loss: 0.8574 - val_acc: 0.6571\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7656 - acc: 0.6905 - val_loss: 0.8557 - val_acc: 0.6629\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7684 - acc: 0.6765 - val_loss: 0.8571 - val_acc: 0.6743\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7551 - acc: 0.6918 - val_loss: 0.8577 - val_acc: 0.6743\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7551 - acc: 0.7039 - val_loss: 0.8640 - val_acc: 0.6686\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7641 - acc: 0.6860 - val_loss: 0.8649 - val_acc: 0.6571\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7546 - acc: 0.6975 - val_loss: 0.8617 - val_acc: 0.6629\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7484 - acc: 0.6879 - val_loss: 0.8623 - val_acc: 0.6571\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7430 - acc: 0.6956 - val_loss: 0.8605 - val_acc: 0.6743\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7433 - acc: 0.6899 - val_loss: 0.8589 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7553 - acc: 0.6841 - val_loss: 0.8549 - val_acc: 0.6629\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7550 - acc: 0.7007 - val_loss: 0.8546 - val_acc: 0.6686\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7537 - acc: 0.6937 - val_loss: 0.8572 - val_acc: 0.6686\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7455 - acc: 0.6911 - val_loss: 0.8605 - val_acc: 0.6629\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7394 - acc: 0.6950 - val_loss: 0.8599 - val_acc: 0.6686\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.7379 - acc: 0.6937 - val_loss: 0.8629 - val_acc: 0.6686\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7345 - acc: 0.6937 - val_loss: 0.8666 - val_acc: 0.6629\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7275 - acc: 0.6962 - val_loss: 0.8647 - val_acc: 0.6743\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7251 - acc: 0.7007 - val_loss: 0.8653 - val_acc: 0.6686\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7445 - acc: 0.6943 - val_loss: 0.8647 - val_acc: 0.6629\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7528 - acc: 0.6892 - val_loss: 0.8664 - val_acc: 0.6457\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7183 - acc: 0.7064 - val_loss: 0.8640 - val_acc: 0.6629\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7227 - acc: 0.6962 - val_loss: 0.8641 - val_acc: 0.6743\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7272 - acc: 0.7090 - val_loss: 0.8708 - val_acc: 0.6629\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7140 - acc: 0.6943 - val_loss: 0.8664 - val_acc: 0.6629\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7073 - acc: 0.7192 - val_loss: 0.8658 - val_acc: 0.6686\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7174 - acc: 0.7147 - val_loss: 0.8675 - val_acc: 0.6629\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7302 - acc: 0.7071 - val_loss: 0.8635 - val_acc: 0.6686\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7232 - acc: 0.7013 - val_loss: 0.8643 - val_acc: 0.6686\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7130 - acc: 0.7109 - val_loss: 0.8677 - val_acc: 0.6686\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7130 - acc: 0.7001 - val_loss: 0.8666 - val_acc: 0.6686\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7312 - acc: 0.7001 - val_loss: 0.8711 - val_acc: 0.6571\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7026 - acc: 0.7147 - val_loss: 0.8717 - val_acc: 0.6629\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7117 - acc: 0.7071 - val_loss: 0.8709 - val_acc: 0.6686\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7117 - acc: 0.7160 - val_loss: 0.8743 - val_acc: 0.6629\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7124 - acc: 0.7141 - val_loss: 0.8785 - val_acc: 0.6571\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7003 - acc: 0.7141 - val_loss: 0.8808 - val_acc: 0.6571\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6981 - acc: 0.7275 - val_loss: 0.8788 - val_acc: 0.6686\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6964 - acc: 0.7160 - val_loss: 0.8741 - val_acc: 0.6629\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7051 - acc: 0.7128 - val_loss: 0.8733 - val_acc: 0.6629\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6872 - acc: 0.7147 - val_loss: 0.8766 - val_acc: 0.6743\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7079 - acc: 0.7154 - val_loss: 0.8739 - val_acc: 0.6686\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6911 - acc: 0.7269 - val_loss: 0.8774 - val_acc: 0.6514\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6989 - acc: 0.7103 - val_loss: 0.8762 - val_acc: 0.6571\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6853 - acc: 0.7154 - val_loss: 0.8756 - val_acc: 0.6629\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7025 - acc: 0.7090 - val_loss: 0.8812 - val_acc: 0.6629\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6838 - acc: 0.7262 - val_loss: 0.8830 - val_acc: 0.6514\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6860 - acc: 0.7237 - val_loss: 0.8933 - val_acc: 0.6571\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6977 - acc: 0.7167 - val_loss: 0.8808 - val_acc: 0.6571\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6762 - acc: 0.7339 - val_loss: 0.8791 - val_acc: 0.6629\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6819 - acc: 0.7307 - val_loss: 0.8820 - val_acc: 0.6686\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7035 - acc: 0.7256 - val_loss: 0.8773 - val_acc: 0.6571\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6707 - acc: 0.7435 - val_loss: 0.8749 - val_acc: 0.6571\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6809 - acc: 0.7192 - val_loss: 0.8749 - val_acc: 0.6571\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6739 - acc: 0.7269 - val_loss: 0.8768 - val_acc: 0.6686\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6630 - acc: 0.7313 - val_loss: 0.8761 - val_acc: 0.6800\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 44us/step - loss: 0.6824 - acc: 0.7198 - val_loss: 0.8824 - val_acc: 0.6629\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6539 - acc: 0.7428 - val_loss: 0.8860 - val_acc: 0.6743\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6662 - acc: 0.7307 - val_loss: 0.8878 - val_acc: 0.6800\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6706 - acc: 0.7320 - val_loss: 0.8930 - val_acc: 0.6743\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6660 - acc: 0.7288 - val_loss: 0.8941 - val_acc: 0.6629\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6681 - acc: 0.7269 - val_loss: 0.8915 - val_acc: 0.6743\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6610 - acc: 0.7294 - val_loss: 0.8898 - val_acc: 0.6857\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6692 - acc: 0.7396 - val_loss: 0.8919 - val_acc: 0.6686\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6618 - acc: 0.7307 - val_loss: 0.8929 - val_acc: 0.6629\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6476 - acc: 0.7313 - val_loss: 0.8907 - val_acc: 0.6571\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6402 - acc: 0.7428 - val_loss: 0.8952 - val_acc: 0.6629\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6482 - acc: 0.7396 - val_loss: 0.9039 - val_acc: 0.6571\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6775 - acc: 0.7160 - val_loss: 0.9067 - val_acc: 0.6571\n",
      "193/193 [==============================] - 0s 79us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742/1742 [==============================] - 0s 30us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5245 - acc: 0.2814 - val_loss: 1.2466 - val_acc: 0.4457\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.3438 - acc: 0.3772 - val_loss: 1.1570 - val_acc: 0.5429\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 1.2805 - acc: 0.4027 - val_loss: 1.0969 - val_acc: 0.5886\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.2223 - acc: 0.4410 - val_loss: 1.0610 - val_acc: 0.6057\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.1809 - acc: 0.4710 - val_loss: 1.0357 - val_acc: 0.5886\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.1481 - acc: 0.4888 - val_loss: 1.0174 - val_acc: 0.6000\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.1240 - acc: 0.5016 - val_loss: 0.9967 - val_acc: 0.6400\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.1062 - acc: 0.5054 - val_loss: 0.9809 - val_acc: 0.6400\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0837 - acc: 0.5297 - val_loss: 0.9665 - val_acc: 0.6457\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0745 - acc: 0.5310 - val_loss: 0.9621 - val_acc: 0.6457\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0491 - acc: 0.5469 - val_loss: 0.9542 - val_acc: 0.6571\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0441 - acc: 0.5539 - val_loss: 0.9468 - val_acc: 0.6400\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0238 - acc: 0.5482 - val_loss: 0.9386 - val_acc: 0.6800\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0407 - acc: 0.5501 - val_loss: 0.9318 - val_acc: 0.6514\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9982 - acc: 0.5743 - val_loss: 0.9229 - val_acc: 0.6571\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9723 - acc: 0.5807 - val_loss: 0.9228 - val_acc: 0.6457\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9813 - acc: 0.5858 - val_loss: 0.9136 - val_acc: 0.6514\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9967 - acc: 0.5680 - val_loss: 0.9119 - val_acc: 0.6629\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9786 - acc: 0.5743 - val_loss: 0.9107 - val_acc: 0.6686\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9472 - acc: 0.5973 - val_loss: 0.9092 - val_acc: 0.6686\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9505 - acc: 0.5941 - val_loss: 0.9073 - val_acc: 0.6571\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9609 - acc: 0.5807 - val_loss: 0.9021 - val_acc: 0.6514\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9503 - acc: 0.6005 - val_loss: 0.9026 - val_acc: 0.6571\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9497 - acc: 0.6158 - val_loss: 0.9006 - val_acc: 0.6629\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9299 - acc: 0.6120 - val_loss: 0.8975 - val_acc: 0.6686\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9594 - acc: 0.6024 - val_loss: 0.8905 - val_acc: 0.6686\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9332 - acc: 0.5909 - val_loss: 0.8901 - val_acc: 0.6743\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9247 - acc: 0.6120 - val_loss: 0.8895 - val_acc: 0.6743\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9220 - acc: 0.6177 - val_loss: 0.8936 - val_acc: 0.6686\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9067 - acc: 0.6152 - val_loss: 0.8890 - val_acc: 0.6629\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9053 - acc: 0.6222 - val_loss: 0.8890 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9108 - acc: 0.6222 - val_loss: 0.8862 - val_acc: 0.6743\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9012 - acc: 0.6235 - val_loss: 0.8863 - val_acc: 0.6743\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8956 - acc: 0.6235 - val_loss: 0.8914 - val_acc: 0.6686\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8829 - acc: 0.6362 - val_loss: 0.8976 - val_acc: 0.6629\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9060 - acc: 0.6171 - val_loss: 0.8902 - val_acc: 0.6686\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8926 - acc: 0.6273 - val_loss: 0.8865 - val_acc: 0.6686\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8822 - acc: 0.6337 - val_loss: 0.8876 - val_acc: 0.6743\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8915 - acc: 0.6190 - val_loss: 0.8837 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8748 - acc: 0.6318 - val_loss: 0.8789 - val_acc: 0.6686\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8714 - acc: 0.6235 - val_loss: 0.8835 - val_acc: 0.6629\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8840 - acc: 0.6197 - val_loss: 0.8810 - val_acc: 0.6686\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8634 - acc: 0.6426 - val_loss: 0.8874 - val_acc: 0.6743\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8646 - acc: 0.6222 - val_loss: 0.8876 - val_acc: 0.6800\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8705 - acc: 0.6292 - val_loss: 0.8829 - val_acc: 0.6800\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8727 - acc: 0.6318 - val_loss: 0.8915 - val_acc: 0.6857\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8658 - acc: 0.6535 - val_loss: 0.8981 - val_acc: 0.6800\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8730 - acc: 0.6292 - val_loss: 0.8921 - val_acc: 0.6743\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8499 - acc: 0.6401 - val_loss: 0.8909 - val_acc: 0.6800\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8467 - acc: 0.6599 - val_loss: 0.8917 - val_acc: 0.6743\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8525 - acc: 0.6522 - val_loss: 0.8934 - val_acc: 0.6743\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8373 - acc: 0.6611 - val_loss: 0.8940 - val_acc: 0.6571\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8415 - acc: 0.6426 - val_loss: 0.8890 - val_acc: 0.6743\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8453 - acc: 0.6420 - val_loss: 0.8877 - val_acc: 0.6857\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8598 - acc: 0.6567 - val_loss: 0.8875 - val_acc: 0.6743\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8436 - acc: 0.6465 - val_loss: 0.8866 - val_acc: 0.6800\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8591 - acc: 0.6503 - val_loss: 0.8911 - val_acc: 0.6629\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8404 - acc: 0.6484 - val_loss: 0.8951 - val_acc: 0.6514\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8300 - acc: 0.6535 - val_loss: 0.8943 - val_acc: 0.6629\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8409 - acc: 0.6490 - val_loss: 0.8924 - val_acc: 0.6629\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8189 - acc: 0.6573 - val_loss: 0.8968 - val_acc: 0.6629\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8217 - acc: 0.6554 - val_loss: 0.8941 - val_acc: 0.6571\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8275 - acc: 0.6567 - val_loss: 0.8946 - val_acc: 0.6514\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8038 - acc: 0.6752 - val_loss: 0.8953 - val_acc: 0.6629\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7959 - acc: 0.6618 - val_loss: 0.8960 - val_acc: 0.6571\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8261 - acc: 0.6509 - val_loss: 0.9003 - val_acc: 0.6571\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8193 - acc: 0.6739 - val_loss: 0.8963 - val_acc: 0.6686\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8008 - acc: 0.6733 - val_loss: 0.8952 - val_acc: 0.6629\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8159 - acc: 0.6465 - val_loss: 0.9009 - val_acc: 0.6457\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7976 - acc: 0.6847 - val_loss: 0.9035 - val_acc: 0.6457\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7998 - acc: 0.6790 - val_loss: 0.9011 - val_acc: 0.6457\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7942 - acc: 0.6688 - val_loss: 0.9016 - val_acc: 0.6457\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7943 - acc: 0.6771 - val_loss: 0.9072 - val_acc: 0.6629\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8003 - acc: 0.6733 - val_loss: 0.9038 - val_acc: 0.6629\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8119 - acc: 0.6745 - val_loss: 0.9052 - val_acc: 0.6514\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8073 - acc: 0.6726 - val_loss: 0.8994 - val_acc: 0.6514\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7943 - acc: 0.6720 - val_loss: 0.9014 - val_acc: 0.6571\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7795 - acc: 0.6809 - val_loss: 0.9019 - val_acc: 0.6514\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8027 - acc: 0.6733 - val_loss: 0.8973 - val_acc: 0.6457\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7796 - acc: 0.6892 - val_loss: 0.9019 - val_acc: 0.6400\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7848 - acc: 0.6847 - val_loss: 0.8995 - val_acc: 0.6457\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7860 - acc: 0.6860 - val_loss: 0.9042 - val_acc: 0.6457\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7676 - acc: 0.6956 - val_loss: 0.9011 - val_acc: 0.6514\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7742 - acc: 0.6841 - val_loss: 0.9081 - val_acc: 0.6457\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7731 - acc: 0.6809 - val_loss: 0.9130 - val_acc: 0.6457\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7657 - acc: 0.6771 - val_loss: 0.9114 - val_acc: 0.6514\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7588 - acc: 0.7007 - val_loss: 0.9093 - val_acc: 0.6514\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7706 - acc: 0.6924 - val_loss: 0.9122 - val_acc: 0.6571\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7586 - acc: 0.6899 - val_loss: 0.9122 - val_acc: 0.6514\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7696 - acc: 0.6758 - val_loss: 0.9257 - val_acc: 0.6400\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7668 - acc: 0.6918 - val_loss: 0.9240 - val_acc: 0.6457\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7639 - acc: 0.6873 - val_loss: 0.9111 - val_acc: 0.6514\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7681 - acc: 0.6950 - val_loss: 0.9111 - val_acc: 0.6457\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7466 - acc: 0.7052 - val_loss: 0.9203 - val_acc: 0.6343\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7586 - acc: 0.6771 - val_loss: 0.9183 - val_acc: 0.6457\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7382 - acc: 0.7039 - val_loss: 0.9155 - val_acc: 0.6514\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7522 - acc: 0.6924 - val_loss: 0.9219 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7509 - acc: 0.6969 - val_loss: 0.9214 - val_acc: 0.6457\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7511 - acc: 0.6930 - val_loss: 0.9229 - val_acc: 0.6457\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7455 - acc: 0.7020 - val_loss: 0.9250 - val_acc: 0.6400\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7466 - acc: 0.6924 - val_loss: 0.9257 - val_acc: 0.6400\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7417 - acc: 0.7064 - val_loss: 0.9316 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7386 - acc: 0.7007 - val_loss: 0.9306 - val_acc: 0.6457\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7340 - acc: 0.7039 - val_loss: 0.9249 - val_acc: 0.6400\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7383 - acc: 0.7071 - val_loss: 0.9224 - val_acc: 0.6514\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7292 - acc: 0.6950 - val_loss: 0.9277 - val_acc: 0.6457\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7487 - acc: 0.6924 - val_loss: 0.9261 - val_acc: 0.6457\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7434 - acc: 0.6899 - val_loss: 0.9277 - val_acc: 0.6457\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7399 - acc: 0.7033 - val_loss: 0.9284 - val_acc: 0.6457\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7294 - acc: 0.7077 - val_loss: 0.9366 - val_acc: 0.6400\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7320 - acc: 0.6943 - val_loss: 0.9348 - val_acc: 0.6457\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7313 - acc: 0.6956 - val_loss: 0.9392 - val_acc: 0.6400\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7233 - acc: 0.7224 - val_loss: 0.9442 - val_acc: 0.6343\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7168 - acc: 0.7128 - val_loss: 0.9396 - val_acc: 0.6400\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7026 - acc: 0.7141 - val_loss: 0.9401 - val_acc: 0.6343\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7309 - acc: 0.6981 - val_loss: 0.9414 - val_acc: 0.6286\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7140 - acc: 0.7135 - val_loss: 0.9437 - val_acc: 0.6343\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7241 - acc: 0.7154 - val_loss: 0.9401 - val_acc: 0.6514\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7192 - acc: 0.7052 - val_loss: 0.9408 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7125 - acc: 0.7135 - val_loss: 0.9517 - val_acc: 0.6400\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7002 - acc: 0.7154 - val_loss: 0.9552 - val_acc: 0.6514\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7165 - acc: 0.7058 - val_loss: 0.9495 - val_acc: 0.6457\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7118 - acc: 0.7071 - val_loss: 0.9522 - val_acc: 0.6457\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6997 - acc: 0.7192 - val_loss: 0.9464 - val_acc: 0.6400\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7204 - acc: 0.7052 - val_loss: 0.9543 - val_acc: 0.6343\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7079 - acc: 0.7160 - val_loss: 0.9504 - val_acc: 0.6400\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6900 - acc: 0.7218 - val_loss: 0.9561 - val_acc: 0.6286\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7113 - acc: 0.7205 - val_loss: 0.9590 - val_acc: 0.6343\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7067 - acc: 0.7077 - val_loss: 0.9590 - val_acc: 0.6343\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7037 - acc: 0.7237 - val_loss: 0.9586 - val_acc: 0.6400\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6964 - acc: 0.7281 - val_loss: 0.9586 - val_acc: 0.6343\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7080 - acc: 0.7320 - val_loss: 0.9582 - val_acc: 0.6343\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7059 - acc: 0.7167 - val_loss: 0.9606 - val_acc: 0.6343\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7014 - acc: 0.7045 - val_loss: 0.9566 - val_acc: 0.6343\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6827 - acc: 0.7205 - val_loss: 0.9549 - val_acc: 0.6400\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7040 - acc: 0.7084 - val_loss: 0.9595 - val_acc: 0.6400\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7048 - acc: 0.7077 - val_loss: 0.9567 - val_acc: 0.6343\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6631 - acc: 0.7250 - val_loss: 0.9536 - val_acc: 0.6400\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7004 - acc: 0.7218 - val_loss: 0.9574 - val_acc: 0.6457\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6866 - acc: 0.7275 - val_loss: 0.9623 - val_acc: 0.6400\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6822 - acc: 0.7160 - val_loss: 0.9617 - val_acc: 0.6343\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6710 - acc: 0.7301 - val_loss: 0.9549 - val_acc: 0.6400\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.6807 - acc: 0.7250 - val_loss: 0.9590 - val_acc: 0.6400\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6821 - acc: 0.7243 - val_loss: 0.9672 - val_acc: 0.6457\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6837 - acc: 0.7275 - val_loss: 0.9650 - val_acc: 0.6571\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6583 - acc: 0.7281 - val_loss: 0.9671 - val_acc: 0.6400\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6783 - acc: 0.7205 - val_loss: 0.9660 - val_acc: 0.6400\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6622 - acc: 0.7301 - val_loss: 0.9703 - val_acc: 0.6514\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6808 - acc: 0.7332 - val_loss: 0.9753 - val_acc: 0.6400\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6631 - acc: 0.7384 - val_loss: 0.9714 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 49us/step\n",
      "1742/1742 [==============================] - 0s 33us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5771 - acc: 0.2897 - val_loss: 1.3217 - val_acc: 0.4571\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.3905 - acc: 0.3644 - val_loss: 1.1845 - val_acc: 0.5200\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.2808 - acc: 0.4308 - val_loss: 1.1110 - val_acc: 0.5314\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.2134 - acc: 0.4525 - val_loss: 1.0676 - val_acc: 0.5600\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1824 - acc: 0.4773 - val_loss: 1.0442 - val_acc: 0.5771\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.1577 - acc: 0.5003 - val_loss: 1.0241 - val_acc: 0.6057\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1289 - acc: 0.5003 - val_loss: 1.0042 - val_acc: 0.6057\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1109 - acc: 0.5182 - val_loss: 0.9917 - val_acc: 0.6057\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0975 - acc: 0.5278 - val_loss: 0.9787 - val_acc: 0.6114\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0710 - acc: 0.5246 - val_loss: 0.9715 - val_acc: 0.6114\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0387 - acc: 0.5444 - val_loss: 0.9616 - val_acc: 0.6229\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.0191 - acc: 0.5578 - val_loss: 0.9553 - val_acc: 0.6343\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0386 - acc: 0.5609 - val_loss: 0.9495 - val_acc: 0.6400\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0295 - acc: 0.5571 - val_loss: 0.9458 - val_acc: 0.6514\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0204 - acc: 0.5597 - val_loss: 0.9398 - val_acc: 0.6286\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0214 - acc: 0.5654 - val_loss: 0.9309 - val_acc: 0.6229\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9933 - acc: 0.5731 - val_loss: 0.9291 - val_acc: 0.6229\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9888 - acc: 0.5801 - val_loss: 0.9274 - val_acc: 0.6286\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9795 - acc: 0.5877 - val_loss: 0.9252 - val_acc: 0.6286\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9804 - acc: 0.5692 - val_loss: 0.9221 - val_acc: 0.6286\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9678 - acc: 0.5763 - val_loss: 0.9197 - val_acc: 0.6457\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9799 - acc: 0.5922 - val_loss: 0.9205 - val_acc: 0.6457\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9588 - acc: 0.5935 - val_loss: 0.9174 - val_acc: 0.6457\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9504 - acc: 0.6133 - val_loss: 0.9174 - val_acc: 0.6400\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9409 - acc: 0.6005 - val_loss: 0.9164 - val_acc: 0.6457\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9490 - acc: 0.5807 - val_loss: 0.9112 - val_acc: 0.6514\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9298 - acc: 0.6248 - val_loss: 0.9094 - val_acc: 0.6457\n",
      "Epoch 28/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9302 - acc: 0.6063 - val_loss: 0.9116 - val_acc: 0.6457\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.9393 - acc: 0.6018 - val_loss: 0.9121 - val_acc: 0.6514\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9016 - acc: 0.6101 - val_loss: 0.9151 - val_acc: 0.6571\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9234 - acc: 0.5973 - val_loss: 0.9153 - val_acc: 0.6457\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9022 - acc: 0.6209 - val_loss: 0.9170 - val_acc: 0.6514\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9055 - acc: 0.6209 - val_loss: 0.9174 - val_acc: 0.6571\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9038 - acc: 0.6120 - val_loss: 0.9188 - val_acc: 0.6457\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9022 - acc: 0.6177 - val_loss: 0.9250 - val_acc: 0.6400\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8888 - acc: 0.6152 - val_loss: 0.9190 - val_acc: 0.6514\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8824 - acc: 0.6305 - val_loss: 0.9156 - val_acc: 0.6571\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8918 - acc: 0.6292 - val_loss: 0.9175 - val_acc: 0.6571\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8842 - acc: 0.6401 - val_loss: 0.9190 - val_acc: 0.6514\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8774 - acc: 0.6375 - val_loss: 0.9139 - val_acc: 0.6571\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8821 - acc: 0.6203 - val_loss: 0.9160 - val_acc: 0.6514\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8762 - acc: 0.6452 - val_loss: 0.9223 - val_acc: 0.6743\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8710 - acc: 0.6477 - val_loss: 0.9213 - val_acc: 0.6514\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8772 - acc: 0.6350 - val_loss: 0.9189 - val_acc: 0.6571\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8683 - acc: 0.6394 - val_loss: 0.9169 - val_acc: 0.6571\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8670 - acc: 0.6535 - val_loss: 0.9223 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8513 - acc: 0.6528 - val_loss: 0.9192 - val_acc: 0.6457\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8662 - acc: 0.6331 - val_loss: 0.9185 - val_acc: 0.6571\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8759 - acc: 0.6554 - val_loss: 0.9125 - val_acc: 0.6571\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8479 - acc: 0.6528 - val_loss: 0.9225 - val_acc: 0.6457\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8597 - acc: 0.6465 - val_loss: 0.9216 - val_acc: 0.6629\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8441 - acc: 0.6554 - val_loss: 0.9179 - val_acc: 0.6571\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8495 - acc: 0.6420 - val_loss: 0.9122 - val_acc: 0.6457\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8344 - acc: 0.6637 - val_loss: 0.9123 - val_acc: 0.6571\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8429 - acc: 0.6554 - val_loss: 0.9179 - val_acc: 0.6629\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8528 - acc: 0.6394 - val_loss: 0.9201 - val_acc: 0.6400\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8563 - acc: 0.6477 - val_loss: 0.9255 - val_acc: 0.6514\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8368 - acc: 0.6624 - val_loss: 0.9197 - val_acc: 0.6514\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8266 - acc: 0.6471 - val_loss: 0.9214 - val_acc: 0.6514\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8175 - acc: 0.6522 - val_loss: 0.9204 - val_acc: 0.6571\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8194 - acc: 0.6599 - val_loss: 0.9203 - val_acc: 0.6571\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8263 - acc: 0.6637 - val_loss: 0.9269 - val_acc: 0.6629\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8278 - acc: 0.6675 - val_loss: 0.9312 - val_acc: 0.6514\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8154 - acc: 0.6739 - val_loss: 0.9351 - val_acc: 0.6457\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8243 - acc: 0.6528 - val_loss: 0.9325 - val_acc: 0.6571\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8348 - acc: 0.6586 - val_loss: 0.9261 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8080 - acc: 0.6624 - val_loss: 0.9261 - val_acc: 0.6571\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8256 - acc: 0.6765 - val_loss: 0.9353 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8066 - acc: 0.6822 - val_loss: 0.9363 - val_acc: 0.6571\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7947 - acc: 0.6713 - val_loss: 0.9320 - val_acc: 0.6629\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8068 - acc: 0.6745 - val_loss: 0.9342 - val_acc: 0.6514\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7895 - acc: 0.6803 - val_loss: 0.9346 - val_acc: 0.6571\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7936 - acc: 0.6739 - val_loss: 0.9346 - val_acc: 0.6514\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8154 - acc: 0.6656 - val_loss: 0.9348 - val_acc: 0.6571\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8048 - acc: 0.6637 - val_loss: 0.9380 - val_acc: 0.6571\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7942 - acc: 0.6854 - val_loss: 0.9337 - val_acc: 0.6743\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8041 - acc: 0.6720 - val_loss: 0.9391 - val_acc: 0.6629\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7874 - acc: 0.6777 - val_loss: 0.9365 - val_acc: 0.6686\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7827 - acc: 0.6777 - val_loss: 0.9357 - val_acc: 0.6514\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7867 - acc: 0.6803 - val_loss: 0.9401 - val_acc: 0.6571\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7760 - acc: 0.6841 - val_loss: 0.9397 - val_acc: 0.6400\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7927 - acc: 0.6847 - val_loss: 0.9387 - val_acc: 0.6457\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7806 - acc: 0.6854 - val_loss: 0.9446 - val_acc: 0.6343\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7656 - acc: 0.6873 - val_loss: 0.9441 - val_acc: 0.6400\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7822 - acc: 0.6847 - val_loss: 0.9398 - val_acc: 0.6343\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7704 - acc: 0.6860 - val_loss: 0.9404 - val_acc: 0.6457\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7749 - acc: 0.6956 - val_loss: 0.9399 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7738 - acc: 0.6879 - val_loss: 0.9417 - val_acc: 0.6571\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7848 - acc: 0.6873 - val_loss: 0.9423 - val_acc: 0.6629\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7597 - acc: 0.6911 - val_loss: 0.9415 - val_acc: 0.6629\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7759 - acc: 0.6854 - val_loss: 0.9470 - val_acc: 0.6457\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7575 - acc: 0.6892 - val_loss: 0.9527 - val_acc: 0.6343\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7554 - acc: 0.6860 - val_loss: 0.9461 - val_acc: 0.6457\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7521 - acc: 0.6892 - val_loss: 0.9509 - val_acc: 0.6514\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7556 - acc: 0.6956 - val_loss: 0.9486 - val_acc: 0.6514\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7608 - acc: 0.6854 - val_loss: 0.9520 - val_acc: 0.6514\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7399 - acc: 0.6981 - val_loss: 0.9493 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7516 - acc: 0.6975 - val_loss: 0.9488 - val_acc: 0.6400\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7667 - acc: 0.6899 - val_loss: 0.9551 - val_acc: 0.6400\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7562 - acc: 0.6975 - val_loss: 0.9568 - val_acc: 0.6343\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7419 - acc: 0.6975 - val_loss: 0.9580 - val_acc: 0.6343\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7360 - acc: 0.7090 - val_loss: 0.9616 - val_acc: 0.6400\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7468 - acc: 0.6956 - val_loss: 0.9587 - val_acc: 0.6286\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7564 - acc: 0.6899 - val_loss: 0.9613 - val_acc: 0.6400\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7517 - acc: 0.6924 - val_loss: 0.9579 - val_acc: 0.6514\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7458 - acc: 0.6956 - val_loss: 0.9624 - val_acc: 0.6286\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7456 - acc: 0.7033 - val_loss: 0.9676 - val_acc: 0.6229\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7469 - acc: 0.6994 - val_loss: 0.9647 - val_acc: 0.6457\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7287 - acc: 0.7128 - val_loss: 0.9704 - val_acc: 0.6286\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7462 - acc: 0.6969 - val_loss: 0.9739 - val_acc: 0.6343\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7166 - acc: 0.7071 - val_loss: 0.9808 - val_acc: 0.6286\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7330 - acc: 0.7077 - val_loss: 0.9762 - val_acc: 0.6286\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7434 - acc: 0.7128 - val_loss: 0.9709 - val_acc: 0.6286\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7463 - acc: 0.6886 - val_loss: 0.9702 - val_acc: 0.6343\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7150 - acc: 0.7039 - val_loss: 0.9687 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7329 - acc: 0.7007 - val_loss: 0.9676 - val_acc: 0.6400\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7276 - acc: 0.7109 - val_loss: 0.9729 - val_acc: 0.6400\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7338 - acc: 0.7020 - val_loss: 0.9683 - val_acc: 0.6400\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7181 - acc: 0.7160 - val_loss: 0.9742 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7150 - acc: 0.7052 - val_loss: 0.9748 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7315 - acc: 0.6899 - val_loss: 0.9748 - val_acc: 0.6343\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7106 - acc: 0.7147 - val_loss: 0.9784 - val_acc: 0.6286\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7208 - acc: 0.7103 - val_loss: 0.9741 - val_acc: 0.6286\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7207 - acc: 0.7001 - val_loss: 0.9777 - val_acc: 0.6229\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7118 - acc: 0.7256 - val_loss: 0.9834 - val_acc: 0.6171\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7090 - acc: 0.7179 - val_loss: 0.9834 - val_acc: 0.6229\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7183 - acc: 0.7198 - val_loss: 0.9871 - val_acc: 0.6229\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7310 - acc: 0.7013 - val_loss: 0.9874 - val_acc: 0.6400\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7056 - acc: 0.7198 - val_loss: 0.9821 - val_acc: 0.6286\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7081 - acc: 0.7173 - val_loss: 0.9746 - val_acc: 0.6286\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7118 - acc: 0.7160 - val_loss: 0.9790 - val_acc: 0.6171\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6858 - acc: 0.7179 - val_loss: 0.9774 - val_acc: 0.6343\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7081 - acc: 0.7160 - val_loss: 0.9781 - val_acc: 0.6229\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6959 - acc: 0.7198 - val_loss: 0.9770 - val_acc: 0.6343\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7071 - acc: 0.7109 - val_loss: 0.9813 - val_acc: 0.6286\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7058 - acc: 0.7154 - val_loss: 0.9822 - val_acc: 0.6343\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6833 - acc: 0.7192 - val_loss: 0.9869 - val_acc: 0.6286\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6839 - acc: 0.7332 - val_loss: 0.9973 - val_acc: 0.6229\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6935 - acc: 0.7243 - val_loss: 0.9960 - val_acc: 0.6114\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6759 - acc: 0.7275 - val_loss: 0.9923 - val_acc: 0.6057\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6762 - acc: 0.7339 - val_loss: 0.9997 - val_acc: 0.6229\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6832 - acc: 0.7352 - val_loss: 1.0002 - val_acc: 0.6229\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6944 - acc: 0.7281 - val_loss: 0.9930 - val_acc: 0.6171\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6931 - acc: 0.7211 - val_loss: 0.9926 - val_acc: 0.6229\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7104 - acc: 0.7026 - val_loss: 0.9896 - val_acc: 0.6171\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6889 - acc: 0.7307 - val_loss: 0.9891 - val_acc: 0.6286\n",
      "Epoch 147/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6709 - acc: 0.7364 - val_loss: 0.9917 - val_acc: 0.6286\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6874 - acc: 0.7364 - val_loss: 0.9968 - val_acc: 0.6343\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6772 - acc: 0.7352 - val_loss: 0.9970 - val_acc: 0.6343\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6825 - acc: 0.7256 - val_loss: 0.9970 - val_acc: 0.6286\n",
      "193/193 [==============================] - 0s 69us/step\n",
      "1742/1742 [==============================] - 0s 30us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5741 - acc: 0.3031 - val_loss: 1.2995 - val_acc: 0.4171\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.3954 - acc: 0.3720 - val_loss: 1.1657 - val_acc: 0.4571\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 1.2641 - acc: 0.4333 - val_loss: 1.1035 - val_acc: 0.5543\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.2333 - acc: 0.4569 - val_loss: 1.0727 - val_acc: 0.5657\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.1836 - acc: 0.4773 - val_loss: 1.0551 - val_acc: 0.5429\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.1689 - acc: 0.4837 - val_loss: 1.0403 - val_acc: 0.5371\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.1180 - acc: 0.5118 - val_loss: 1.0249 - val_acc: 0.5771\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 1.1076 - acc: 0.5214 - val_loss: 1.0115 - val_acc: 0.5600\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.1080 - acc: 0.5354 - val_loss: 1.0015 - val_acc: 0.5714\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0849 - acc: 0.5310 - val_loss: 0.9936 - val_acc: 0.5714\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0518 - acc: 0.5392 - val_loss: 0.9903 - val_acc: 0.5714\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0682 - acc: 0.5437 - val_loss: 0.9849 - val_acc: 0.5543\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0321 - acc: 0.5635 - val_loss: 0.9850 - val_acc: 0.5543\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0218 - acc: 0.5590 - val_loss: 0.9796 - val_acc: 0.5829\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.0112 - acc: 0.5654 - val_loss: 0.9749 - val_acc: 0.5886\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9989 - acc: 0.5724 - val_loss: 0.9715 - val_acc: 0.5886\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9968 - acc: 0.5807 - val_loss: 0.9647 - val_acc: 0.5829\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9899 - acc: 0.6037 - val_loss: 0.9762 - val_acc: 0.5771\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9935 - acc: 0.5731 - val_loss: 0.9741 - val_acc: 0.5829\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9937 - acc: 0.5692 - val_loss: 0.9689 - val_acc: 0.5829\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9615 - acc: 0.6011 - val_loss: 0.9707 - val_acc: 0.5714\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9647 - acc: 0.5980 - val_loss: 0.9694 - val_acc: 0.5714\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9595 - acc: 0.5954 - val_loss: 0.9592 - val_acc: 0.5886\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9419 - acc: 0.6043 - val_loss: 0.9629 - val_acc: 0.5829\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9531 - acc: 0.5992 - val_loss: 0.9599 - val_acc: 0.5886\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9660 - acc: 0.5941 - val_loss: 0.9537 - val_acc: 0.5771\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.9433 - acc: 0.6133 - val_loss: 0.9554 - val_acc: 0.5829\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.9416 - acc: 0.5929 - val_loss: 0.9571 - val_acc: 0.5829\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.9140 - acc: 0.6177 - val_loss: 0.9504 - val_acc: 0.5714\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9011 - acc: 0.6318 - val_loss: 0.9493 - val_acc: 0.5943\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9203 - acc: 0.5929 - val_loss: 0.9536 - val_acc: 0.5886\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9096 - acc: 0.6177 - val_loss: 0.9610 - val_acc: 0.5886\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9217 - acc: 0.6203 - val_loss: 0.9580 - val_acc: 0.5600\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9038 - acc: 0.6171 - val_loss: 0.9531 - val_acc: 0.5543\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9112 - acc: 0.6299 - val_loss: 0.9553 - val_acc: 0.5714\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8948 - acc: 0.6292 - val_loss: 0.9496 - val_acc: 0.5771\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8898 - acc: 0.6382 - val_loss: 0.9495 - val_acc: 0.5657\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8739 - acc: 0.6394 - val_loss: 0.9551 - val_acc: 0.5771\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8825 - acc: 0.6177 - val_loss: 0.9562 - val_acc: 0.5771\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8969 - acc: 0.6299 - val_loss: 0.9540 - val_acc: 0.5771\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8777 - acc: 0.6426 - val_loss: 0.9580 - val_acc: 0.5829\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8902 - acc: 0.6235 - val_loss: 0.9669 - val_acc: 0.5829\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8557 - acc: 0.6503 - val_loss: 0.9658 - val_acc: 0.5829\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8812 - acc: 0.6343 - val_loss: 0.9626 - val_acc: 0.5771\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8846 - acc: 0.6567 - val_loss: 0.9673 - val_acc: 0.5771\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8735 - acc: 0.6407 - val_loss: 0.9696 - val_acc: 0.5886\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8775 - acc: 0.6477 - val_loss: 0.9604 - val_acc: 0.5943\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8435 - acc: 0.6656 - val_loss: 0.9617 - val_acc: 0.5943\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8422 - acc: 0.6516 - val_loss: 0.9621 - val_acc: 0.5886\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8655 - acc: 0.6445 - val_loss: 0.9675 - val_acc: 0.5886\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8479 - acc: 0.6445 - val_loss: 0.9630 - val_acc: 0.5943\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8580 - acc: 0.6496 - val_loss: 0.9615 - val_acc: 0.5943\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8477 - acc: 0.6465 - val_loss: 0.9628 - val_acc: 0.5943\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8601 - acc: 0.6324 - val_loss: 0.9689 - val_acc: 0.5886\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8465 - acc: 0.6637 - val_loss: 0.9644 - val_acc: 0.5943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8324 - acc: 0.6682 - val_loss: 0.9671 - val_acc: 0.6000\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8488 - acc: 0.6573 - val_loss: 0.9672 - val_acc: 0.5943\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8422 - acc: 0.6362 - val_loss: 0.9715 - val_acc: 0.5943\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8297 - acc: 0.6790 - val_loss: 0.9704 - val_acc: 0.5943\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8325 - acc: 0.6528 - val_loss: 0.9674 - val_acc: 0.5771\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8153 - acc: 0.6682 - val_loss: 0.9672 - val_acc: 0.5886\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8044 - acc: 0.6701 - val_loss: 0.9680 - val_acc: 0.5943\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8275 - acc: 0.6675 - val_loss: 0.9720 - val_acc: 0.6000\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8106 - acc: 0.6739 - val_loss: 0.9765 - val_acc: 0.6000\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8229 - acc: 0.6675 - val_loss: 0.9751 - val_acc: 0.5943\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8092 - acc: 0.6688 - val_loss: 0.9776 - val_acc: 0.5943\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8129 - acc: 0.6554 - val_loss: 0.9778 - val_acc: 0.5886\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8214 - acc: 0.6586 - val_loss: 0.9743 - val_acc: 0.5829\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7893 - acc: 0.6937 - val_loss: 0.9777 - val_acc: 0.5886\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8213 - acc: 0.6752 - val_loss: 0.9723 - val_acc: 0.5829\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8135 - acc: 0.6733 - val_loss: 0.9741 - val_acc: 0.5886\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8145 - acc: 0.6541 - val_loss: 0.9817 - val_acc: 0.5886\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7833 - acc: 0.6809 - val_loss: 0.9854 - val_acc: 0.5829\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8141 - acc: 0.6656 - val_loss: 0.9818 - val_acc: 0.5829\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8053 - acc: 0.6675 - val_loss: 0.9802 - val_acc: 0.5829\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7905 - acc: 0.6771 - val_loss: 0.9838 - val_acc: 0.5943\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7949 - acc: 0.6784 - val_loss: 0.9807 - val_acc: 0.5886\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8025 - acc: 0.6777 - val_loss: 0.9812 - val_acc: 0.5943\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7772 - acc: 0.6752 - val_loss: 0.9821 - val_acc: 0.5943\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7912 - acc: 0.6726 - val_loss: 0.9836 - val_acc: 0.5943\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8033 - acc: 0.6713 - val_loss: 0.9832 - val_acc: 0.5886\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7939 - acc: 0.6899 - val_loss: 0.9897 - val_acc: 0.5943\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7894 - acc: 0.6809 - val_loss: 0.9858 - val_acc: 0.6000\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7825 - acc: 0.6777 - val_loss: 0.9879 - val_acc: 0.5943\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7692 - acc: 0.6943 - val_loss: 0.9948 - val_acc: 0.5771\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7754 - acc: 0.6784 - val_loss: 0.9919 - val_acc: 0.5886\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7573 - acc: 0.6905 - val_loss: 0.9913 - val_acc: 0.5886\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7727 - acc: 0.6873 - val_loss: 0.9967 - val_acc: 0.5771\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7757 - acc: 0.6841 - val_loss: 0.9967 - val_acc: 0.5829\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7576 - acc: 0.6873 - val_loss: 0.9890 - val_acc: 0.5886\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7618 - acc: 0.6803 - val_loss: 0.9915 - val_acc: 0.5943\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7802 - acc: 0.6790 - val_loss: 0.9973 - val_acc: 0.6057\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7679 - acc: 0.6892 - val_loss: 0.9925 - val_acc: 0.6057\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7666 - acc: 0.6930 - val_loss: 0.9938 - val_acc: 0.5886\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7514 - acc: 0.7064 - val_loss: 0.9967 - val_acc: 0.6000\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7453 - acc: 0.7033 - val_loss: 1.0009 - val_acc: 0.5886\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7509 - acc: 0.6886 - val_loss: 1.0048 - val_acc: 0.6000\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7569 - acc: 0.6828 - val_loss: 1.0016 - val_acc: 0.6000\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7479 - acc: 0.7013 - val_loss: 0.9960 - val_acc: 0.6000\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7394 - acc: 0.7084 - val_loss: 0.9994 - val_acc: 0.6171\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7430 - acc: 0.7077 - val_loss: 1.0013 - val_acc: 0.6171\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7551 - acc: 0.6924 - val_loss: 0.9998 - val_acc: 0.5943\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7493 - acc: 0.6892 - val_loss: 1.0046 - val_acc: 0.5886\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7632 - acc: 0.6847 - val_loss: 1.0025 - val_acc: 0.5943\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7483 - acc: 0.6937 - val_loss: 1.0105 - val_acc: 0.5943\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7397 - acc: 0.6937 - val_loss: 1.0075 - val_acc: 0.5943\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7471 - acc: 0.7090 - val_loss: 1.0031 - val_acc: 0.6057\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7303 - acc: 0.7109 - val_loss: 1.0054 - val_acc: 0.6114\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7435 - acc: 0.6924 - val_loss: 1.0001 - val_acc: 0.6114\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7536 - acc: 0.6994 - val_loss: 1.0066 - val_acc: 0.6057\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7201 - acc: 0.7007 - val_loss: 1.0093 - val_acc: 0.6000\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7285 - acc: 0.7192 - val_loss: 1.0023 - val_acc: 0.6057\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7518 - acc: 0.7039 - val_loss: 1.0083 - val_acc: 0.6057\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7251 - acc: 0.6988 - val_loss: 1.0129 - val_acc: 0.6057\n",
      "Epoch 115/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.7396 - acc: 0.7033 - val_loss: 1.0095 - val_acc: 0.6057\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7280 - acc: 0.7058 - val_loss: 1.0088 - val_acc: 0.6114\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7055 - acc: 0.7224 - val_loss: 1.0111 - val_acc: 0.6057\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7287 - acc: 0.7173 - val_loss: 1.0183 - val_acc: 0.6057\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7248 - acc: 0.6930 - val_loss: 1.0072 - val_acc: 0.6057\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7090 - acc: 0.7160 - val_loss: 1.0033 - val_acc: 0.6114\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7156 - acc: 0.7352 - val_loss: 1.0069 - val_acc: 0.6057\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7091 - acc: 0.7039 - val_loss: 1.0080 - val_acc: 0.6057\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.7250 - acc: 0.7045 - val_loss: 1.0104 - val_acc: 0.6057\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.7182 - acc: 0.7122 - val_loss: 1.0150 - val_acc: 0.6114\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7309 - acc: 0.7026 - val_loss: 1.0197 - val_acc: 0.6057\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7086 - acc: 0.7141 - val_loss: 1.0161 - val_acc: 0.6114\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7438 - acc: 0.7109 - val_loss: 1.0021 - val_acc: 0.6000\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6974 - acc: 0.7230 - val_loss: 1.0114 - val_acc: 0.6057\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6963 - acc: 0.7281 - val_loss: 1.0263 - val_acc: 0.6000\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7068 - acc: 0.7224 - val_loss: 1.0165 - val_acc: 0.5943\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7130 - acc: 0.7173 - val_loss: 1.0206 - val_acc: 0.6057\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6972 - acc: 0.7173 - val_loss: 1.0257 - val_acc: 0.6057\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6894 - acc: 0.7281 - val_loss: 1.0263 - val_acc: 0.6000\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6985 - acc: 0.7186 - val_loss: 1.0255 - val_acc: 0.6057\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7203 - acc: 0.7090 - val_loss: 1.0261 - val_acc: 0.5886\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7001 - acc: 0.7192 - val_loss: 1.0354 - val_acc: 0.5943\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6997 - acc: 0.7109 - val_loss: 1.0335 - val_acc: 0.5943\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6983 - acc: 0.7230 - val_loss: 1.0269 - val_acc: 0.6000\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6730 - acc: 0.7339 - val_loss: 1.0243 - val_acc: 0.6114\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7058 - acc: 0.7103 - val_loss: 1.0365 - val_acc: 0.6000\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6723 - acc: 0.7364 - val_loss: 1.0351 - val_acc: 0.5943\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6896 - acc: 0.7345 - val_loss: 1.0323 - val_acc: 0.6114\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6820 - acc: 0.7288 - val_loss: 1.0344 - val_acc: 0.6057\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6917 - acc: 0.7275 - val_loss: 1.0358 - val_acc: 0.6114\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6763 - acc: 0.7403 - val_loss: 1.0372 - val_acc: 0.6114\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6845 - acc: 0.7275 - val_loss: 1.0376 - val_acc: 0.6057\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6749 - acc: 0.7250 - val_loss: 1.0365 - val_acc: 0.6171\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6682 - acc: 0.7307 - val_loss: 1.0375 - val_acc: 0.6000\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6756 - acc: 0.7345 - val_loss: 1.0374 - val_acc: 0.6057\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6654 - acc: 0.7275 - val_loss: 1.0416 - val_acc: 0.6000\n",
      "193/193 [==============================] - 0s 46us/step\n",
      "1742/1742 [==============================] - 0s 28us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.4671 - acc: 0.3046 - val_loss: 1.2011 - val_acc: 0.4914\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 1.2940 - acc: 0.4151 - val_loss: 1.0947 - val_acc: 0.5886\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.1921 - acc: 0.4502 - val_loss: 1.0283 - val_acc: 0.6229\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.1492 - acc: 0.4815 - val_loss: 0.9831 - val_acc: 0.6457\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1277 - acc: 0.4751 - val_loss: 0.9527 - val_acc: 0.6686\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.0585 - acc: 0.5396 - val_loss: 0.9459 - val_acc: 0.6571\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0519 - acc: 0.5421 - val_loss: 0.9276 - val_acc: 0.6457\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0265 - acc: 0.5600 - val_loss: 0.9191 - val_acc: 0.6514\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 1.0036 - acc: 0.5709 - val_loss: 0.9074 - val_acc: 0.6686\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.9896 - acc: 0.5792 - val_loss: 0.8920 - val_acc: 0.6800\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9865 - acc: 0.5792 - val_loss: 0.8870 - val_acc: 0.6743\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9719 - acc: 0.5747 - val_loss: 0.8817 - val_acc: 0.6743\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9547 - acc: 0.6060 - val_loss: 0.8816 - val_acc: 0.6571\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9440 - acc: 0.5983 - val_loss: 0.8806 - val_acc: 0.6629\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.9450 - acc: 0.6079 - val_loss: 0.8750 - val_acc: 0.6914\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9299 - acc: 0.6124 - val_loss: 0.8762 - val_acc: 0.6743\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9119 - acc: 0.6022 - val_loss: 0.8802 - val_acc: 0.6629\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9137 - acc: 0.6175 - val_loss: 0.8843 - val_acc: 0.6629\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8891 - acc: 0.6341 - val_loss: 0.8763 - val_acc: 0.6514\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9088 - acc: 0.6315 - val_loss: 0.8784 - val_acc: 0.6457\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8976 - acc: 0.6162 - val_loss: 0.8806 - val_acc: 0.6457\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8929 - acc: 0.6194 - val_loss: 0.8847 - val_acc: 0.6514\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8791 - acc: 0.6309 - val_loss: 0.8800 - val_acc: 0.6686\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8733 - acc: 0.6328 - val_loss: 0.8863 - val_acc: 0.6629\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8812 - acc: 0.6322 - val_loss: 0.8822 - val_acc: 0.6743\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8728 - acc: 0.6469 - val_loss: 0.8892 - val_acc: 0.6629\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8523 - acc: 0.6526 - val_loss: 0.8986 - val_acc: 0.6686\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8788 - acc: 0.6315 - val_loss: 0.8913 - val_acc: 0.6514\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8297 - acc: 0.6584 - val_loss: 0.9048 - val_acc: 0.6571\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8602 - acc: 0.6558 - val_loss: 0.8909 - val_acc: 0.6686\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8452 - acc: 0.6481 - val_loss: 0.8930 - val_acc: 0.6686\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8479 - acc: 0.6526 - val_loss: 0.8961 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8240 - acc: 0.6667 - val_loss: 0.8955 - val_acc: 0.6514\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8329 - acc: 0.6533 - val_loss: 0.8935 - val_acc: 0.6571\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8254 - acc: 0.6692 - val_loss: 0.8934 - val_acc: 0.6686\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8180 - acc: 0.6686 - val_loss: 0.9001 - val_acc: 0.6457\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8167 - acc: 0.6628 - val_loss: 0.9028 - val_acc: 0.6400\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8311 - acc: 0.6571 - val_loss: 0.9155 - val_acc: 0.6457\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8037 - acc: 0.6737 - val_loss: 0.9048 - val_acc: 0.6629\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7939 - acc: 0.6903 - val_loss: 0.9030 - val_acc: 0.6400\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7854 - acc: 0.6769 - val_loss: 0.9019 - val_acc: 0.6514\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7975 - acc: 0.6788 - val_loss: 0.8991 - val_acc: 0.6400\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8141 - acc: 0.6750 - val_loss: 0.8952 - val_acc: 0.6571\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8073 - acc: 0.6756 - val_loss: 0.9063 - val_acc: 0.6457\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7829 - acc: 0.6877 - val_loss: 0.9092 - val_acc: 0.6514\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7721 - acc: 0.6801 - val_loss: 0.9103 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7987 - acc: 0.6871 - val_loss: 0.9108 - val_acc: 0.6457\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7960 - acc: 0.6775 - val_loss: 0.9107 - val_acc: 0.6629\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7791 - acc: 0.6820 - val_loss: 0.9131 - val_acc: 0.6571\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7802 - acc: 0.6922 - val_loss: 0.9194 - val_acc: 0.6514\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7710 - acc: 0.6909 - val_loss: 0.9228 - val_acc: 0.6400\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7686 - acc: 0.6871 - val_loss: 0.9204 - val_acc: 0.6629\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7459 - acc: 0.7024 - val_loss: 0.9159 - val_acc: 0.6629\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7624 - acc: 0.7011 - val_loss: 0.9158 - val_acc: 0.6629\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7690 - acc: 0.6935 - val_loss: 0.9274 - val_acc: 0.6457\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7582 - acc: 0.7043 - val_loss: 0.9203 - val_acc: 0.6457\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7679 - acc: 0.6884 - val_loss: 0.9144 - val_acc: 0.6571\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7557 - acc: 0.7005 - val_loss: 0.9262 - val_acc: 0.6514\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7440 - acc: 0.6948 - val_loss: 0.9192 - val_acc: 0.6629\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7381 - acc: 0.7063 - val_loss: 0.9469 - val_acc: 0.6514\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7490 - acc: 0.6916 - val_loss: 0.9416 - val_acc: 0.6514\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7304 - acc: 0.7037 - val_loss: 0.9277 - val_acc: 0.6457\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7361 - acc: 0.7050 - val_loss: 0.9278 - val_acc: 0.6514\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7401 - acc: 0.6967 - val_loss: 0.9404 - val_acc: 0.6457\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7018 - acc: 0.7197 - val_loss: 0.9414 - val_acc: 0.6457\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7220 - acc: 0.7037 - val_loss: 0.9408 - val_acc: 0.6457\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7179 - acc: 0.7120 - val_loss: 0.9467 - val_acc: 0.6514\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7198 - acc: 0.7254 - val_loss: 0.9496 - val_acc: 0.6457\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7095 - acc: 0.7101 - val_loss: 0.9540 - val_acc: 0.6400\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7072 - acc: 0.7184 - val_loss: 0.9451 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7170 - acc: 0.7088 - val_loss: 0.9496 - val_acc: 0.6514\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7132 - acc: 0.7261 - val_loss: 0.9476 - val_acc: 0.6457\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7117 - acc: 0.7203 - val_loss: 0.9542 - val_acc: 0.6571\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7009 - acc: 0.7037 - val_loss: 0.9592 - val_acc: 0.6343\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7123 - acc: 0.7318 - val_loss: 0.9657 - val_acc: 0.6400\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6927 - acc: 0.7305 - val_loss: 0.9597 - val_acc: 0.6514\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7144 - acc: 0.7146 - val_loss: 0.9557 - val_acc: 0.6400\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7051 - acc: 0.7152 - val_loss: 0.9546 - val_acc: 0.6400\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6902 - acc: 0.7209 - val_loss: 0.9483 - val_acc: 0.6457\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7129 - acc: 0.7152 - val_loss: 0.9569 - val_acc: 0.6571\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6700 - acc: 0.7350 - val_loss: 0.9594 - val_acc: 0.6571\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6836 - acc: 0.7209 - val_loss: 0.9684 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6755 - acc: 0.7292 - val_loss: 0.9669 - val_acc: 0.6629\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6731 - acc: 0.7241 - val_loss: 0.9563 - val_acc: 0.6686\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6769 - acc: 0.7350 - val_loss: 0.9616 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6665 - acc: 0.7427 - val_loss: 0.9605 - val_acc: 0.6514\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6790 - acc: 0.7344 - val_loss: 0.9640 - val_acc: 0.6400\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6829 - acc: 0.7395 - val_loss: 0.9643 - val_acc: 0.6514\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6804 - acc: 0.7331 - val_loss: 0.9726 - val_acc: 0.6457\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.6542 - acc: 0.7420 - val_loss: 0.9734 - val_acc: 0.6457\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6726 - acc: 0.7375 - val_loss: 0.9721 - val_acc: 0.6571\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6510 - acc: 0.7433 - val_loss: 0.9795 - val_acc: 0.6514\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6686 - acc: 0.7216 - val_loss: 0.9694 - val_acc: 0.6629\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6540 - acc: 0.7369 - val_loss: 0.9718 - val_acc: 0.6571\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6392 - acc: 0.7542 - val_loss: 0.9800 - val_acc: 0.6571\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6469 - acc: 0.7522 - val_loss: 0.9884 - val_acc: 0.6514\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6657 - acc: 0.7478 - val_loss: 0.9862 - val_acc: 0.6400\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6518 - acc: 0.7484 - val_loss: 0.9858 - val_acc: 0.6400\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6327 - acc: 0.7490 - val_loss: 0.9852 - val_acc: 0.6400\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6465 - acc: 0.7484 - val_loss: 0.9909 - val_acc: 0.6343\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6398 - acc: 0.7439 - val_loss: 0.9795 - val_acc: 0.6343\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6387 - acc: 0.7420 - val_loss: 0.9833 - val_acc: 0.6343\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6253 - acc: 0.7522 - val_loss: 0.9873 - val_acc: 0.6457\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6305 - acc: 0.7567 - val_loss: 0.9780 - val_acc: 0.6514\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6194 - acc: 0.7573 - val_loss: 0.9840 - val_acc: 0.6514\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6000 - acc: 0.7682 - val_loss: 0.9889 - val_acc: 0.6457\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6224 - acc: 0.7542 - val_loss: 0.9867 - val_acc: 0.6457\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6095 - acc: 0.7676 - val_loss: 0.9898 - val_acc: 0.6571\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6075 - acc: 0.7580 - val_loss: 0.9917 - val_acc: 0.6571\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6041 - acc: 0.7580 - val_loss: 1.0052 - val_acc: 0.6514\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5972 - acc: 0.7573 - val_loss: 1.0026 - val_acc: 0.6457\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6047 - acc: 0.7644 - val_loss: 0.9963 - val_acc: 0.6457\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6077 - acc: 0.7561 - val_loss: 0.9879 - val_acc: 0.6571\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5983 - acc: 0.7733 - val_loss: 0.9909 - val_acc: 0.6629\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6177 - acc: 0.7554 - val_loss: 0.9961 - val_acc: 0.6514\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.5909 - acc: 0.7727 - val_loss: 1.0076 - val_acc: 0.6400\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6081 - acc: 0.7644 - val_loss: 1.0113 - val_acc: 0.6514\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6010 - acc: 0.7714 - val_loss: 1.0164 - val_acc: 0.6343\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.6014 - acc: 0.7688 - val_loss: 1.0151 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6017 - acc: 0.7676 - val_loss: 1.0136 - val_acc: 0.6514\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5894 - acc: 0.7682 - val_loss: 1.0180 - val_acc: 0.6400\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5908 - acc: 0.7733 - val_loss: 1.0158 - val_acc: 0.6400\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5924 - acc: 0.7631 - val_loss: 1.0138 - val_acc: 0.6343\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6017 - acc: 0.7701 - val_loss: 1.0191 - val_acc: 0.6514\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5884 - acc: 0.7848 - val_loss: 1.0150 - val_acc: 0.6571\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5659 - acc: 0.7874 - val_loss: 1.0265 - val_acc: 0.6400\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5800 - acc: 0.7829 - val_loss: 1.0284 - val_acc: 0.6400\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5817 - acc: 0.7746 - val_loss: 1.0286 - val_acc: 0.6457\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5785 - acc: 0.7727 - val_loss: 1.0260 - val_acc: 0.6571\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5800 - acc: 0.7688 - val_loss: 1.0276 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5565 - acc: 0.7771 - val_loss: 1.0200 - val_acc: 0.6629\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5675 - acc: 0.7771 - val_loss: 1.0398 - val_acc: 0.6457\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5452 - acc: 0.7842 - val_loss: 1.0355 - val_acc: 0.6457\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5465 - acc: 0.7861 - val_loss: 1.0382 - val_acc: 0.6571\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5562 - acc: 0.7893 - val_loss: 1.0447 - val_acc: 0.6629\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5664 - acc: 0.7746 - val_loss: 1.0551 - val_acc: 0.6571\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5540 - acc: 0.7880 - val_loss: 1.0603 - val_acc: 0.6457\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5705 - acc: 0.7842 - val_loss: 1.0453 - val_acc: 0.6629\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5361 - acc: 0.7867 - val_loss: 1.0507 - val_acc: 0.6514\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5863 - acc: 0.7797 - val_loss: 1.0518 - val_acc: 0.6514\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5227 - acc: 0.7969 - val_loss: 1.0636 - val_acc: 0.6343\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5578 - acc: 0.7797 - val_loss: 1.0510 - val_acc: 0.6400\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5466 - acc: 0.7848 - val_loss: 1.0584 - val_acc: 0.6457\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5505 - acc: 0.7822 - val_loss: 1.0616 - val_acc: 0.6571\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5689 - acc: 0.7752 - val_loss: 1.0556 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5282 - acc: 0.7969 - val_loss: 1.0619 - val_acc: 0.6400\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5217 - acc: 0.7880 - val_loss: 1.0542 - val_acc: 0.6457\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5524 - acc: 0.7905 - val_loss: 1.0662 - val_acc: 0.6400\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5291 - acc: 0.7886 - val_loss: 1.0769 - val_acc: 0.6514\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5402 - acc: 0.7886 - val_loss: 1.0801 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 53us/step\n",
      "1741/1741 [==============================] - 0s 33us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.5630 - acc: 0.2861 - val_loss: 1.2166 - val_acc: 0.4457\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.2987 - acc: 0.4080 - val_loss: 1.0864 - val_acc: 0.5829\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 1.1786 - acc: 0.4636 - val_loss: 1.0260 - val_acc: 0.5943\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.1487 - acc: 0.4879 - val_loss: 0.9983 - val_acc: 0.6057\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0982 - acc: 0.5140 - val_loss: 0.9702 - val_acc: 0.6000\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0871 - acc: 0.5243 - val_loss: 0.9495 - val_acc: 0.6229\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0273 - acc: 0.5639 - val_loss: 0.9392 - val_acc: 0.6114\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0085 - acc: 0.5664 - val_loss: 0.9317 - val_acc: 0.6400\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9798 - acc: 0.5868 - val_loss: 0.9216 - val_acc: 0.6457\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.9824 - acc: 0.5741 - val_loss: 0.9144 - val_acc: 0.6514\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9820 - acc: 0.5747 - val_loss: 0.9168 - val_acc: 0.6629\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9510 - acc: 0.5983 - val_loss: 0.9137 - val_acc: 0.6857\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9512 - acc: 0.6015 - val_loss: 0.9161 - val_acc: 0.6571\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9412 - acc: 0.5977 - val_loss: 0.9018 - val_acc: 0.6686\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9466 - acc: 0.5977 - val_loss: 0.8996 - val_acc: 0.6571\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9397 - acc: 0.6047 - val_loss: 0.8986 - val_acc: 0.6686\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9146 - acc: 0.6117 - val_loss: 0.8960 - val_acc: 0.6629\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9238 - acc: 0.6041 - val_loss: 0.9026 - val_acc: 0.6571\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8871 - acc: 0.6284 - val_loss: 0.9007 - val_acc: 0.6686\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8902 - acc: 0.6271 - val_loss: 0.8959 - val_acc: 0.6457\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8754 - acc: 0.6405 - val_loss: 0.8872 - val_acc: 0.6686\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8616 - acc: 0.6488 - val_loss: 0.8932 - val_acc: 0.6571\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8793 - acc: 0.6469 - val_loss: 0.8879 - val_acc: 0.6629\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8865 - acc: 0.6258 - val_loss: 0.8908 - val_acc: 0.6457\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8776 - acc: 0.6418 - val_loss: 0.8957 - val_acc: 0.6514\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8498 - acc: 0.6603 - val_loss: 0.8932 - val_acc: 0.6686\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8645 - acc: 0.6488 - val_loss: 0.9151 - val_acc: 0.6629\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8533 - acc: 0.6462 - val_loss: 0.9014 - val_acc: 0.6629\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8453 - acc: 0.6654 - val_loss: 0.8958 - val_acc: 0.6629\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8317 - acc: 0.6616 - val_loss: 0.8967 - val_acc: 0.6400\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8432 - acc: 0.6603 - val_loss: 0.8959 - val_acc: 0.6686\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8297 - acc: 0.6705 - val_loss: 0.8959 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8315 - acc: 0.6686 - val_loss: 0.8934 - val_acc: 0.6457\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8283 - acc: 0.6673 - val_loss: 0.8981 - val_acc: 0.6457\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8227 - acc: 0.6648 - val_loss: 0.9009 - val_acc: 0.6514\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8034 - acc: 0.6794 - val_loss: 0.9023 - val_acc: 0.6629\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8128 - acc: 0.6679 - val_loss: 0.9034 - val_acc: 0.6571\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7982 - acc: 0.6826 - val_loss: 0.9028 - val_acc: 0.6457\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8043 - acc: 0.6762 - val_loss: 0.9147 - val_acc: 0.6514\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8019 - acc: 0.6839 - val_loss: 0.9166 - val_acc: 0.6400\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7727 - acc: 0.6839 - val_loss: 0.9039 - val_acc: 0.6457\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7929 - acc: 0.6718 - val_loss: 0.8976 - val_acc: 0.6686\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7824 - acc: 0.6884 - val_loss: 0.8971 - val_acc: 0.6629\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7895 - acc: 0.6807 - val_loss: 0.8973 - val_acc: 0.6457\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7748 - acc: 0.6839 - val_loss: 0.8991 - val_acc: 0.6514\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7981 - acc: 0.6628 - val_loss: 0.9053 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7733 - acc: 0.6858 - val_loss: 0.9055 - val_acc: 0.6514\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7749 - acc: 0.6986 - val_loss: 0.9039 - val_acc: 0.6514\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.7626 - acc: 0.6928 - val_loss: 0.9072 - val_acc: 0.6571\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7637 - acc: 0.6909 - val_loss: 0.9162 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7732 - acc: 0.6897 - val_loss: 0.9190 - val_acc: 0.6457\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7499 - acc: 0.6865 - val_loss: 0.9197 - val_acc: 0.6457\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7563 - acc: 0.6967 - val_loss: 0.9114 - val_acc: 0.6514\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7441 - acc: 0.6941 - val_loss: 0.9124 - val_acc: 0.6686\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7335 - acc: 0.7101 - val_loss: 0.9171 - val_acc: 0.6686\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7479 - acc: 0.7037 - val_loss: 0.9108 - val_acc: 0.6629\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7184 - acc: 0.7235 - val_loss: 0.9149 - val_acc: 0.6571\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7259 - acc: 0.7101 - val_loss: 0.9279 - val_acc: 0.6514\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7386 - acc: 0.6922 - val_loss: 0.9358 - val_acc: 0.6514\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7292 - acc: 0.7126 - val_loss: 0.9286 - val_acc: 0.6686\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7383 - acc: 0.6852 - val_loss: 0.9224 - val_acc: 0.6629\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7365 - acc: 0.7069 - val_loss: 0.9202 - val_acc: 0.6514\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7238 - acc: 0.6928 - val_loss: 0.9214 - val_acc: 0.6629\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7265 - acc: 0.7069 - val_loss: 0.9284 - val_acc: 0.6514\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7233 - acc: 0.7005 - val_loss: 0.9286 - val_acc: 0.6514\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7049 - acc: 0.7095 - val_loss: 0.9317 - val_acc: 0.6571\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7185 - acc: 0.7126 - val_loss: 0.9261 - val_acc: 0.6686\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7268 - acc: 0.7031 - val_loss: 0.9332 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6978 - acc: 0.7254 - val_loss: 0.9368 - val_acc: 0.6457\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6786 - acc: 0.7337 - val_loss: 0.9419 - val_acc: 0.6457\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6841 - acc: 0.7299 - val_loss: 0.9579 - val_acc: 0.6343\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7026 - acc: 0.7209 - val_loss: 0.9512 - val_acc: 0.6457\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6830 - acc: 0.7369 - val_loss: 0.9542 - val_acc: 0.6400\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6941 - acc: 0.7318 - val_loss: 0.9570 - val_acc: 0.6457\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6766 - acc: 0.7324 - val_loss: 0.9626 - val_acc: 0.6286\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6793 - acc: 0.7395 - val_loss: 0.9562 - val_acc: 0.6400\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6772 - acc: 0.7241 - val_loss: 0.9536 - val_acc: 0.6457\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6598 - acc: 0.7216 - val_loss: 0.9503 - val_acc: 0.6514\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6673 - acc: 0.7299 - val_loss: 0.9539 - val_acc: 0.6514\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6862 - acc: 0.7280 - val_loss: 0.9581 - val_acc: 0.6514\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6780 - acc: 0.7369 - val_loss: 0.9596 - val_acc: 0.6686\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6654 - acc: 0.7375 - val_loss: 0.9632 - val_acc: 0.6629\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6598 - acc: 0.7554 - val_loss: 0.9739 - val_acc: 0.6457\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6593 - acc: 0.7350 - val_loss: 0.9731 - val_acc: 0.6514\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6466 - acc: 0.7395 - val_loss: 0.9759 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6704 - acc: 0.7350 - val_loss: 0.9744 - val_acc: 0.6571\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6609 - acc: 0.7356 - val_loss: 0.9823 - val_acc: 0.6629\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6471 - acc: 0.7331 - val_loss: 0.9840 - val_acc: 0.6457\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6467 - acc: 0.7478 - val_loss: 0.9756 - val_acc: 0.6629\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6330 - acc: 0.7599 - val_loss: 0.9815 - val_acc: 0.6514\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6377 - acc: 0.7484 - val_loss: 0.9751 - val_acc: 0.6400\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6504 - acc: 0.7299 - val_loss: 0.9769 - val_acc: 0.6571\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6198 - acc: 0.7663 - val_loss: 0.9788 - val_acc: 0.6514\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6212 - acc: 0.7503 - val_loss: 0.9844 - val_acc: 0.6229\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6286 - acc: 0.7465 - val_loss: 0.9969 - val_acc: 0.6171\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6343 - acc: 0.7510 - val_loss: 0.9961 - val_acc: 0.6400\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6079 - acc: 0.7637 - val_loss: 1.0032 - val_acc: 0.6457\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6243 - acc: 0.7490 - val_loss: 1.0011 - val_acc: 0.6400\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6136 - acc: 0.7503 - val_loss: 1.0050 - val_acc: 0.6343\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6198 - acc: 0.7573 - val_loss: 1.0000 - val_acc: 0.6343\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6193 - acc: 0.7567 - val_loss: 1.0009 - val_acc: 0.6457\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6063 - acc: 0.7586 - val_loss: 1.0068 - val_acc: 0.6286\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6124 - acc: 0.7516 - val_loss: 1.0130 - val_acc: 0.6286\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5935 - acc: 0.7803 - val_loss: 1.0102 - val_acc: 0.6514\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6121 - acc: 0.7567 - val_loss: 1.0100 - val_acc: 0.6400\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6037 - acc: 0.7618 - val_loss: 1.0235 - val_acc: 0.6400\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6111 - acc: 0.7599 - val_loss: 1.0214 - val_acc: 0.6514\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6001 - acc: 0.7701 - val_loss: 1.0186 - val_acc: 0.6400\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5979 - acc: 0.7727 - val_loss: 1.0142 - val_acc: 0.6571\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5960 - acc: 0.7618 - val_loss: 1.0189 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5983 - acc: 0.7612 - val_loss: 1.0349 - val_acc: 0.6457\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5990 - acc: 0.7618 - val_loss: 1.0337 - val_acc: 0.6457\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5881 - acc: 0.7612 - val_loss: 1.0291 - val_acc: 0.6400\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5735 - acc: 0.7810 - val_loss: 1.0341 - val_acc: 0.6286\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5792 - acc: 0.7771 - val_loss: 1.0308 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5857 - acc: 0.7778 - val_loss: 1.0476 - val_acc: 0.6343\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5995 - acc: 0.7644 - val_loss: 1.0361 - val_acc: 0.6286\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5657 - acc: 0.7791 - val_loss: 1.0336 - val_acc: 0.6343\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5848 - acc: 0.7650 - val_loss: 1.0378 - val_acc: 0.6286\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5711 - acc: 0.7778 - val_loss: 1.0442 - val_acc: 0.6400\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5803 - acc: 0.7682 - val_loss: 1.0450 - val_acc: 0.6514\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5843 - acc: 0.7593 - val_loss: 1.0498 - val_acc: 0.6400\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5613 - acc: 0.7803 - val_loss: 1.0485 - val_acc: 0.6286\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5588 - acc: 0.7912 - val_loss: 1.0462 - val_acc: 0.6343\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5502 - acc: 0.7816 - val_loss: 1.0419 - val_acc: 0.6286\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5510 - acc: 0.7854 - val_loss: 1.0449 - val_acc: 0.6171\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5805 - acc: 0.7650 - val_loss: 1.0498 - val_acc: 0.6457\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5608 - acc: 0.7867 - val_loss: 1.0547 - val_acc: 0.6343\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5391 - acc: 0.7905 - val_loss: 1.0476 - val_acc: 0.6514\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5671 - acc: 0.7803 - val_loss: 1.0482 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5539 - acc: 0.7835 - val_loss: 1.0526 - val_acc: 0.6457\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5698 - acc: 0.7771 - val_loss: 1.0564 - val_acc: 0.6286\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5345 - acc: 0.7976 - val_loss: 1.0511 - val_acc: 0.6400\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5553 - acc: 0.7829 - val_loss: 1.0531 - val_acc: 0.6457\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5476 - acc: 0.7867 - val_loss: 1.0438 - val_acc: 0.6514\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5328 - acc: 0.7995 - val_loss: 1.0498 - val_acc: 0.6457\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5252 - acc: 0.7989 - val_loss: 1.0632 - val_acc: 0.6514\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5333 - acc: 0.7931 - val_loss: 1.0663 - val_acc: 0.6457\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5562 - acc: 0.7848 - val_loss: 1.0702 - val_acc: 0.6400\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5423 - acc: 0.7976 - val_loss: 1.0582 - val_acc: 0.6514\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5316 - acc: 0.7950 - val_loss: 1.0590 - val_acc: 0.6457\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5455 - acc: 0.7931 - val_loss: 1.0641 - val_acc: 0.6343\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5322 - acc: 0.7880 - val_loss: 1.0690 - val_acc: 0.6457\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5231 - acc: 0.7893 - val_loss: 1.0666 - val_acc: 0.6457\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5396 - acc: 0.7905 - val_loss: 1.0721 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5067 - acc: 0.7893 - val_loss: 1.0810 - val_acc: 0.6457\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5370 - acc: 0.7880 - val_loss: 1.0749 - val_acc: 0.6343\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5198 - acc: 0.8046 - val_loss: 1.0851 - val_acc: 0.6457\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5452 - acc: 0.7867 - val_loss: 1.0919 - val_acc: 0.6514\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5331 - acc: 0.7778 - val_loss: 1.0839 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 47us/step\n",
      "1741/1741 [==============================] - 0s 31us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 7s 5ms/step - loss: 1.5607 - acc: 0.2950 - val_loss: 1.2228 - val_acc: 0.4229\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.2931 - acc: 0.4049 - val_loss: 1.0836 - val_acc: 0.5771\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1782 - acc: 0.4738 - val_loss: 1.0238 - val_acc: 0.6229\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1462 - acc: 0.4955 - val_loss: 0.9927 - val_acc: 0.5886\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0911 - acc: 0.5217 - val_loss: 0.9735 - val_acc: 0.6400\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0532 - acc: 0.5479 - val_loss: 0.9630 - val_acc: 0.6343\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.0434 - acc: 0.5511 - val_loss: 0.9475 - val_acc: 0.6571\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0352 - acc: 0.5626 - val_loss: 0.9403 - val_acc: 0.6857\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0161 - acc: 0.5734 - val_loss: 0.9301 - val_acc: 0.6686\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9951 - acc: 0.5766 - val_loss: 0.9196 - val_acc: 0.6857\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9678 - acc: 0.5805 - val_loss: 0.9149 - val_acc: 0.6686\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9776 - acc: 0.5785 - val_loss: 0.9098 - val_acc: 0.6571\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9547 - acc: 0.5881 - val_loss: 0.9106 - val_acc: 0.6571\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9690 - acc: 0.5875 - val_loss: 0.9067 - val_acc: 0.6629\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9480 - acc: 0.5990 - val_loss: 0.9002 - val_acc: 0.6686\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9253 - acc: 0.6245 - val_loss: 0.9010 - val_acc: 0.6800\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9320 - acc: 0.5996 - val_loss: 0.8958 - val_acc: 0.6743\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9043 - acc: 0.6239 - val_loss: 0.8910 - val_acc: 0.6800\n",
      "Epoch 19/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9017 - acc: 0.6188 - val_loss: 0.8915 - val_acc: 0.6743\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8976 - acc: 0.6347 - val_loss: 0.9002 - val_acc: 0.6686\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9048 - acc: 0.6130 - val_loss: 0.9015 - val_acc: 0.6857\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9077 - acc: 0.6284 - val_loss: 0.8959 - val_acc: 0.6629\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8779 - acc: 0.6354 - val_loss: 0.8940 - val_acc: 0.6914\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8979 - acc: 0.6290 - val_loss: 0.8995 - val_acc: 0.6571\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8805 - acc: 0.6411 - val_loss: 0.9006 - val_acc: 0.6686\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8665 - acc: 0.6360 - val_loss: 0.9030 - val_acc: 0.6686\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8638 - acc: 0.6456 - val_loss: 0.9021 - val_acc: 0.6686\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8659 - acc: 0.6360 - val_loss: 0.8947 - val_acc: 0.6743\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8490 - acc: 0.6398 - val_loss: 0.8990 - val_acc: 0.6629\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8279 - acc: 0.6635 - val_loss: 0.9020 - val_acc: 0.6629\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8364 - acc: 0.6513 - val_loss: 0.9000 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8303 - acc: 0.6552 - val_loss: 0.9001 - val_acc: 0.6629\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8399 - acc: 0.6386 - val_loss: 0.9039 - val_acc: 0.6686\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8253 - acc: 0.6718 - val_loss: 0.9058 - val_acc: 0.6514\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8135 - acc: 0.6705 - val_loss: 0.9065 - val_acc: 0.6457\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8204 - acc: 0.6590 - val_loss: 0.9029 - val_acc: 0.6571\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8155 - acc: 0.6660 - val_loss: 0.9050 - val_acc: 0.6686\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8059 - acc: 0.6628 - val_loss: 0.9010 - val_acc: 0.6686\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8177 - acc: 0.6590 - val_loss: 0.8987 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8015 - acc: 0.6609 - val_loss: 0.9069 - val_acc: 0.6686\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8111 - acc: 0.6590 - val_loss: 0.9072 - val_acc: 0.6629\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7972 - acc: 0.6660 - val_loss: 0.9121 - val_acc: 0.6629\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7883 - acc: 0.6788 - val_loss: 0.9125 - val_acc: 0.6629\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7960 - acc: 0.6833 - val_loss: 0.9213 - val_acc: 0.6686\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7840 - acc: 0.6762 - val_loss: 0.9269 - val_acc: 0.6514\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7841 - acc: 0.6833 - val_loss: 0.9220 - val_acc: 0.6514\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7925 - acc: 0.6762 - val_loss: 0.9119 - val_acc: 0.6514\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7610 - acc: 0.6890 - val_loss: 0.9112 - val_acc: 0.6743\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7750 - acc: 0.6762 - val_loss: 0.9110 - val_acc: 0.6800\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7713 - acc: 0.6814 - val_loss: 0.9191 - val_acc: 0.6514\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7667 - acc: 0.7005 - val_loss: 0.9216 - val_acc: 0.6686\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7584 - acc: 0.6954 - val_loss: 0.9294 - val_acc: 0.6457\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7543 - acc: 0.6884 - val_loss: 0.9304 - val_acc: 0.6514\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7717 - acc: 0.6801 - val_loss: 0.9265 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7500 - acc: 0.6941 - val_loss: 0.9348 - val_acc: 0.6400\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7528 - acc: 0.6858 - val_loss: 0.9325 - val_acc: 0.6514\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7381 - acc: 0.6986 - val_loss: 0.9358 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7533 - acc: 0.6935 - val_loss: 0.9343 - val_acc: 0.6343\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7486 - acc: 0.7018 - val_loss: 0.9432 - val_acc: 0.6286\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7346 - acc: 0.6948 - val_loss: 0.9395 - val_acc: 0.6400\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7338 - acc: 0.7005 - val_loss: 0.9363 - val_acc: 0.6457\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7469 - acc: 0.6973 - val_loss: 0.9488 - val_acc: 0.6457\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7311 - acc: 0.6948 - val_loss: 0.9501 - val_acc: 0.6571\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7246 - acc: 0.7050 - val_loss: 0.9506 - val_acc: 0.6629\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7104 - acc: 0.7095 - val_loss: 0.9548 - val_acc: 0.6686\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6981 - acc: 0.7292 - val_loss: 0.9589 - val_acc: 0.6571\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7326 - acc: 0.7114 - val_loss: 0.9613 - val_acc: 0.6571\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7245 - acc: 0.7037 - val_loss: 0.9614 - val_acc: 0.6571\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7139 - acc: 0.7120 - val_loss: 0.9668 - val_acc: 0.6514\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7090 - acc: 0.7011 - val_loss: 0.9607 - val_acc: 0.6343\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7029 - acc: 0.7241 - val_loss: 0.9624 - val_acc: 0.6457\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7027 - acc: 0.7171 - val_loss: 0.9602 - val_acc: 0.6400\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7018 - acc: 0.7120 - val_loss: 0.9559 - val_acc: 0.6229\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7057 - acc: 0.7158 - val_loss: 0.9586 - val_acc: 0.6343\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7005 - acc: 0.7037 - val_loss: 0.9560 - val_acc: 0.6343\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6923 - acc: 0.7414 - val_loss: 0.9587 - val_acc: 0.6400\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6934 - acc: 0.7184 - val_loss: 0.9680 - val_acc: 0.6343\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6787 - acc: 0.7337 - val_loss: 0.9697 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6944 - acc: 0.7299 - val_loss: 0.9694 - val_acc: 0.6457\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6708 - acc: 0.7350 - val_loss: 0.9607 - val_acc: 0.6514\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6937 - acc: 0.7382 - val_loss: 0.9613 - val_acc: 0.6400\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6648 - acc: 0.7337 - val_loss: 0.9667 - val_acc: 0.6400\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6928 - acc: 0.7165 - val_loss: 0.9678 - val_acc: 0.6457\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6565 - acc: 0.7331 - val_loss: 0.9749 - val_acc: 0.6343\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6548 - acc: 0.7369 - val_loss: 0.9740 - val_acc: 0.6457\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6588 - acc: 0.7414 - val_loss: 0.9722 - val_acc: 0.6629\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6810 - acc: 0.7273 - val_loss: 0.9715 - val_acc: 0.6400\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6614 - acc: 0.7369 - val_loss: 0.9834 - val_acc: 0.6343\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6471 - acc: 0.7407 - val_loss: 0.9884 - val_acc: 0.6457\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6523 - acc: 0.7497 - val_loss: 0.9898 - val_acc: 0.6457\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6646 - acc: 0.7375 - val_loss: 0.9789 - val_acc: 0.6343\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6565 - acc: 0.7458 - val_loss: 0.9860 - val_acc: 0.6343\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6470 - acc: 0.7478 - val_loss: 0.9904 - val_acc: 0.6400\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6419 - acc: 0.7299 - val_loss: 0.9970 - val_acc: 0.6629\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6489 - acc: 0.7414 - val_loss: 1.0013 - val_acc: 0.6457\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6683 - acc: 0.7305 - val_loss: 0.9975 - val_acc: 0.6629\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6463 - acc: 0.7344 - val_loss: 0.9929 - val_acc: 0.6629\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6286 - acc: 0.7567 - val_loss: 0.9954 - val_acc: 0.6457\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6309 - acc: 0.7471 - val_loss: 1.0122 - val_acc: 0.6457\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6449 - acc: 0.7452 - val_loss: 1.0094 - val_acc: 0.6514\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6176 - acc: 0.7452 - val_loss: 1.0128 - val_acc: 0.6343\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6209 - acc: 0.7586 - val_loss: 1.0200 - val_acc: 0.6343\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6174 - acc: 0.7542 - val_loss: 1.0187 - val_acc: 0.6457\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6200 - acc: 0.7452 - val_loss: 1.0314 - val_acc: 0.6400\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6204 - acc: 0.7554 - val_loss: 1.0366 - val_acc: 0.6343\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6102 - acc: 0.7682 - val_loss: 1.0275 - val_acc: 0.6457\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6181 - acc: 0.7516 - val_loss: 1.0232 - val_acc: 0.6286\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6187 - acc: 0.7656 - val_loss: 1.0315 - val_acc: 0.6229\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5990 - acc: 0.7580 - val_loss: 1.0340 - val_acc: 0.6343\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6040 - acc: 0.7618 - val_loss: 1.0351 - val_acc: 0.6229\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6019 - acc: 0.7561 - val_loss: 1.0471 - val_acc: 0.6343\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5859 - acc: 0.7746 - val_loss: 1.0407 - val_acc: 0.6286\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6036 - acc: 0.7567 - val_loss: 1.0497 - val_acc: 0.6343\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5956 - acc: 0.7714 - val_loss: 1.0569 - val_acc: 0.6343\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5713 - acc: 0.7822 - val_loss: 1.0557 - val_acc: 0.6286\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6125 - acc: 0.7567 - val_loss: 1.0644 - val_acc: 0.6171\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5965 - acc: 0.7669 - val_loss: 1.0757 - val_acc: 0.6343\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6096 - acc: 0.7605 - val_loss: 1.0612 - val_acc: 0.6343\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5862 - acc: 0.7733 - val_loss: 1.0706 - val_acc: 0.6343\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5856 - acc: 0.7669 - val_loss: 1.0754 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5724 - acc: 0.7727 - val_loss: 1.0723 - val_acc: 0.6343\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5862 - acc: 0.7663 - val_loss: 1.0734 - val_acc: 0.6343\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5665 - acc: 0.7861 - val_loss: 1.0733 - val_acc: 0.6286\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5659 - acc: 0.7778 - val_loss: 1.0739 - val_acc: 0.6286\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5852 - acc: 0.7669 - val_loss: 1.0816 - val_acc: 0.6286\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5615 - acc: 0.7778 - val_loss: 1.0781 - val_acc: 0.6171\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5667 - acc: 0.7765 - val_loss: 1.0764 - val_acc: 0.6343\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5759 - acc: 0.7886 - val_loss: 1.0822 - val_acc: 0.6343\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5764 - acc: 0.7727 - val_loss: 1.0888 - val_acc: 0.6343\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5492 - acc: 0.7944 - val_loss: 1.0921 - val_acc: 0.6286\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5764 - acc: 0.7784 - val_loss: 1.0880 - val_acc: 0.6286\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5644 - acc: 0.7739 - val_loss: 1.0954 - val_acc: 0.6286\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5575 - acc: 0.7739 - val_loss: 1.0980 - val_acc: 0.6171\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5407 - acc: 0.7918 - val_loss: 1.1008 - val_acc: 0.6114\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5316 - acc: 0.7969 - val_loss: 1.0963 - val_acc: 0.6171\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5530 - acc: 0.7810 - val_loss: 1.0932 - val_acc: 0.6171\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5310 - acc: 0.7931 - val_loss: 1.0975 - val_acc: 0.6286\n",
      "Epoch 138/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5626 - acc: 0.7797 - val_loss: 1.1022 - val_acc: 0.6286\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5420 - acc: 0.7912 - val_loss: 1.1061 - val_acc: 0.6229\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5656 - acc: 0.7720 - val_loss: 1.1177 - val_acc: 0.6171\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.5559 - acc: 0.7905 - val_loss: 1.1122 - val_acc: 0.6229\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5375 - acc: 0.7912 - val_loss: 1.1058 - val_acc: 0.6400\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5499 - acc: 0.7816 - val_loss: 1.1097 - val_acc: 0.6286\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5439 - acc: 0.7886 - val_loss: 1.1143 - val_acc: 0.6286\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5371 - acc: 0.7822 - val_loss: 1.1048 - val_acc: 0.6286\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5515 - acc: 0.7727 - val_loss: 1.1078 - val_acc: 0.6343\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5500 - acc: 0.7797 - val_loss: 1.1229 - val_acc: 0.6229\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5029 - acc: 0.7969 - val_loss: 1.1335 - val_acc: 0.6229\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5162 - acc: 0.8020 - val_loss: 1.1343 - val_acc: 0.6229\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5127 - acc: 0.8040 - val_loss: 1.1299 - val_acc: 0.6171\n",
      "194/194 [==============================] - 0s 51us/step\n",
      "1741/1741 [==============================] - 0s 41us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.6870 - acc: 0.2797 - val_loss: 1.3076 - val_acc: 0.3257\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.3626 - acc: 0.3627 - val_loss: 1.0968 - val_acc: 0.6114\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.2008 - acc: 0.4617 - val_loss: 1.0283 - val_acc: 0.6171\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1505 - acc: 0.5026 - val_loss: 1.0067 - val_acc: 0.5829\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1220 - acc: 0.5109 - val_loss: 0.9887 - val_acc: 0.6229\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1014 - acc: 0.5204 - val_loss: 0.9721 - val_acc: 0.6286\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0617 - acc: 0.5338 - val_loss: 0.9617 - val_acc: 0.6514\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0309 - acc: 0.5626 - val_loss: 0.9505 - val_acc: 0.6629\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0364 - acc: 0.5556 - val_loss: 0.9397 - val_acc: 0.6457\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0146 - acc: 0.5760 - val_loss: 0.9354 - val_acc: 0.6400\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0078 - acc: 0.5715 - val_loss: 0.9272 - val_acc: 0.6686\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.0019 - acc: 0.5658 - val_loss: 0.9186 - val_acc: 0.6743\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9671 - acc: 0.5881 - val_loss: 0.9035 - val_acc: 0.6800\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9622 - acc: 0.5926 - val_loss: 0.9062 - val_acc: 0.6686\n",
      "Epoch 15/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9406 - acc: 0.5920 - val_loss: 0.9012 - val_acc: 0.6857\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9431 - acc: 0.5983 - val_loss: 0.9113 - val_acc: 0.6743\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9250 - acc: 0.6054 - val_loss: 0.9057 - val_acc: 0.6857\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9291 - acc: 0.6175 - val_loss: 0.9043 - val_acc: 0.6800\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9356 - acc: 0.6105 - val_loss: 0.9066 - val_acc: 0.6629\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9063 - acc: 0.6130 - val_loss: 0.9038 - val_acc: 0.6743\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9202 - acc: 0.6060 - val_loss: 0.9008 - val_acc: 0.6800\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8995 - acc: 0.6143 - val_loss: 0.9012 - val_acc: 0.6800\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8977 - acc: 0.6245 - val_loss: 0.8969 - val_acc: 0.6743\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9010 - acc: 0.6169 - val_loss: 0.8931 - val_acc: 0.6800\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8885 - acc: 0.6367 - val_loss: 0.9001 - val_acc: 0.6629\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8695 - acc: 0.6437 - val_loss: 0.8938 - val_acc: 0.6686\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8829 - acc: 0.6226 - val_loss: 0.8927 - val_acc: 0.6743\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8816 - acc: 0.6290 - val_loss: 0.8959 - val_acc: 0.6571\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8782 - acc: 0.6277 - val_loss: 0.9021 - val_acc: 0.6629\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8454 - acc: 0.6411 - val_loss: 0.9028 - val_acc: 0.6629\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8582 - acc: 0.6379 - val_loss: 0.9017 - val_acc: 0.6571\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8379 - acc: 0.6609 - val_loss: 0.9102 - val_acc: 0.6629\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8566 - acc: 0.6564 - val_loss: 0.9059 - val_acc: 0.6457\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8435 - acc: 0.6590 - val_loss: 0.9084 - val_acc: 0.6514\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8346 - acc: 0.6724 - val_loss: 0.9140 - val_acc: 0.6743\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8348 - acc: 0.6635 - val_loss: 0.9115 - val_acc: 0.6743\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8357 - acc: 0.6507 - val_loss: 0.9053 - val_acc: 0.6800\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8339 - acc: 0.6501 - val_loss: 0.9101 - val_acc: 0.6629\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8262 - acc: 0.6660 - val_loss: 0.9036 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8011 - acc: 0.6762 - val_loss: 0.9134 - val_acc: 0.6629\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8193 - acc: 0.6635 - val_loss: 0.8992 - val_acc: 0.6800\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8140 - acc: 0.6679 - val_loss: 0.9123 - val_acc: 0.6629\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8151 - acc: 0.6667 - val_loss: 0.9077 - val_acc: 0.6629\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7905 - acc: 0.6839 - val_loss: 0.9192 - val_acc: 0.6571\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8064 - acc: 0.6699 - val_loss: 0.9082 - val_acc: 0.6914\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7997 - acc: 0.6845 - val_loss: 0.9143 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7924 - acc: 0.6705 - val_loss: 0.9127 - val_acc: 0.6686\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7772 - acc: 0.6801 - val_loss: 0.9107 - val_acc: 0.6800\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7795 - acc: 0.6826 - val_loss: 0.9178 - val_acc: 0.6686\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7706 - acc: 0.6909 - val_loss: 0.9207 - val_acc: 0.6457\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7598 - acc: 0.6948 - val_loss: 0.9212 - val_acc: 0.6514\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7632 - acc: 0.6973 - val_loss: 0.9206 - val_acc: 0.6686\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7668 - acc: 0.6833 - val_loss: 0.9269 - val_acc: 0.6686\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7702 - acc: 0.6724 - val_loss: 0.9345 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7593 - acc: 0.6922 - val_loss: 0.9383 - val_acc: 0.6571\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7602 - acc: 0.6922 - val_loss: 0.9395 - val_acc: 0.6571\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7469 - acc: 0.7031 - val_loss: 0.9292 - val_acc: 0.6686\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7571 - acc: 0.6890 - val_loss: 0.9352 - val_acc: 0.6686\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7554 - acc: 0.6967 - val_loss: 0.9348 - val_acc: 0.6629\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7292 - acc: 0.7050 - val_loss: 0.9399 - val_acc: 0.6800\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7445 - acc: 0.6948 - val_loss: 0.9411 - val_acc: 0.6857\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7493 - acc: 0.6928 - val_loss: 0.9422 - val_acc: 0.6514\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7581 - acc: 0.6871 - val_loss: 0.9432 - val_acc: 0.6743\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7381 - acc: 0.6980 - val_loss: 0.9462 - val_acc: 0.6629\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7313 - acc: 0.7011 - val_loss: 0.9568 - val_acc: 0.6629\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7211 - acc: 0.7318 - val_loss: 0.9646 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7306 - acc: 0.7056 - val_loss: 0.9589 - val_acc: 0.6400\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7172 - acc: 0.7043 - val_loss: 0.9567 - val_acc: 0.6629\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7128 - acc: 0.7261 - val_loss: 0.9625 - val_acc: 0.6686\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7230 - acc: 0.7031 - val_loss: 0.9683 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7119 - acc: 0.7069 - val_loss: 0.9709 - val_acc: 0.6514\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7142 - acc: 0.7024 - val_loss: 0.9693 - val_acc: 0.6457\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7123 - acc: 0.7063 - val_loss: 0.9617 - val_acc: 0.6629\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7216 - acc: 0.7126 - val_loss: 0.9628 - val_acc: 0.6743\n",
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6962 - acc: 0.7171 - val_loss: 0.9731 - val_acc: 0.6743\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6883 - acc: 0.7203 - val_loss: 0.9758 - val_acc: 0.6514\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6942 - acc: 0.7241 - val_loss: 0.9716 - val_acc: 0.6457\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6963 - acc: 0.7267 - val_loss: 0.9732 - val_acc: 0.6514\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7033 - acc: 0.7273 - val_loss: 0.9747 - val_acc: 0.6571\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6793 - acc: 0.7273 - val_loss: 0.9737 - val_acc: 0.6514\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6883 - acc: 0.7229 - val_loss: 0.9749 - val_acc: 0.6457\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6847 - acc: 0.7305 - val_loss: 0.9846 - val_acc: 0.6400\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6861 - acc: 0.7229 - val_loss: 0.9851 - val_acc: 0.6286\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6713 - acc: 0.7324 - val_loss: 0.9789 - val_acc: 0.6514\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6855 - acc: 0.7375 - val_loss: 0.9851 - val_acc: 0.6400\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6626 - acc: 0.7318 - val_loss: 0.9847 - val_acc: 0.6514\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6762 - acc: 0.7267 - val_loss: 0.9877 - val_acc: 0.6571\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6639 - acc: 0.7331 - val_loss: 0.9875 - val_acc: 0.6629\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6505 - acc: 0.7363 - val_loss: 0.9968 - val_acc: 0.6400\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6635 - acc: 0.7395 - val_loss: 1.0007 - val_acc: 0.6343\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6512 - acc: 0.7331 - val_loss: 1.0143 - val_acc: 0.6400\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6649 - acc: 0.7369 - val_loss: 1.0071 - val_acc: 0.6457\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6530 - acc: 0.7344 - val_loss: 1.0123 - val_acc: 0.6343\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6608 - acc: 0.7324 - val_loss: 1.0136 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6479 - acc: 0.7490 - val_loss: 1.0171 - val_acc: 0.6400\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6321 - acc: 0.7414 - val_loss: 1.0230 - val_acc: 0.6229\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6675 - acc: 0.7344 - val_loss: 1.0240 - val_acc: 0.6343\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6352 - acc: 0.7522 - val_loss: 1.0155 - val_acc: 0.6457\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6640 - acc: 0.7222 - val_loss: 1.0185 - val_acc: 0.6457\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6285 - acc: 0.7593 - val_loss: 1.0234 - val_acc: 0.6457\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6499 - acc: 0.7375 - val_loss: 1.0291 - val_acc: 0.6514\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6305 - acc: 0.7388 - val_loss: 1.0311 - val_acc: 0.6343\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6398 - acc: 0.7497 - val_loss: 1.0353 - val_acc: 0.6457\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6161 - acc: 0.7618 - val_loss: 1.0369 - val_acc: 0.6400\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6408 - acc: 0.7439 - val_loss: 1.0330 - val_acc: 0.6400\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6331 - acc: 0.7586 - val_loss: 1.0345 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6351 - acc: 0.7420 - val_loss: 1.0350 - val_acc: 0.6343\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6329 - acc: 0.7478 - val_loss: 1.0404 - val_acc: 0.6400\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6231 - acc: 0.7446 - val_loss: 1.0400 - val_acc: 0.6343\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6276 - acc: 0.7567 - val_loss: 1.0442 - val_acc: 0.6343\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6124 - acc: 0.7522 - val_loss: 1.0552 - val_acc: 0.6229\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6397 - acc: 0.7510 - val_loss: 1.0492 - val_acc: 0.6000\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6153 - acc: 0.7586 - val_loss: 1.0446 - val_acc: 0.6114\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6181 - acc: 0.7593 - val_loss: 1.0424 - val_acc: 0.6286\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6006 - acc: 0.7567 - val_loss: 1.0361 - val_acc: 0.6171\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5936 - acc: 0.7676 - val_loss: 1.0346 - val_acc: 0.6229\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6030 - acc: 0.7605 - val_loss: 1.0408 - val_acc: 0.6229\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6000 - acc: 0.7618 - val_loss: 1.0481 - val_acc: 0.6229\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5996 - acc: 0.7625 - val_loss: 1.0534 - val_acc: 0.6286\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6050 - acc: 0.7650 - val_loss: 1.0520 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5931 - acc: 0.7682 - val_loss: 1.0543 - val_acc: 0.6171\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6048 - acc: 0.7727 - val_loss: 1.0632 - val_acc: 0.6229\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5925 - acc: 0.7720 - val_loss: 1.0609 - val_acc: 0.6571\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.5824 - acc: 0.7822 - val_loss: 1.0597 - val_acc: 0.6514\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5757 - acc: 0.7637 - val_loss: 1.0639 - val_acc: 0.6457\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5719 - acc: 0.7797 - val_loss: 1.0681 - val_acc: 0.6229\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5654 - acc: 0.7771 - val_loss: 1.0676 - val_acc: 0.6114\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5850 - acc: 0.7637 - val_loss: 1.0790 - val_acc: 0.6114\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5814 - acc: 0.7637 - val_loss: 1.0838 - val_acc: 0.6286\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.5784 - acc: 0.7618 - val_loss: 1.0804 - val_acc: 0.6286\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5643 - acc: 0.7925 - val_loss: 1.0827 - val_acc: 0.6286\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5811 - acc: 0.7759 - val_loss: 1.0871 - val_acc: 0.6171\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5662 - acc: 0.7784 - val_loss: 1.0916 - val_acc: 0.6229\n",
      "Epoch 134/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5490 - acc: 0.7886 - val_loss: 1.0849 - val_acc: 0.6286\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5739 - acc: 0.7784 - val_loss: 1.0930 - val_acc: 0.6171\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5578 - acc: 0.7784 - val_loss: 1.0945 - val_acc: 0.6229\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5539 - acc: 0.7899 - val_loss: 1.0983 - val_acc: 0.6229\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5689 - acc: 0.7688 - val_loss: 1.0968 - val_acc: 0.6400\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5580 - acc: 0.7835 - val_loss: 1.1092 - val_acc: 0.6286\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5371 - acc: 0.7931 - val_loss: 1.1080 - val_acc: 0.6057\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5434 - acc: 0.7867 - val_loss: 1.1012 - val_acc: 0.6000\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.5683 - acc: 0.7765 - val_loss: 1.0998 - val_acc: 0.5943\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5420 - acc: 0.7925 - val_loss: 1.1146 - val_acc: 0.6000\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5349 - acc: 0.8008 - val_loss: 1.1163 - val_acc: 0.6000\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5270 - acc: 0.7944 - val_loss: 1.1118 - val_acc: 0.6229\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5478 - acc: 0.7950 - val_loss: 1.1207 - val_acc: 0.6171\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5463 - acc: 0.7797 - val_loss: 1.1206 - val_acc: 0.6171\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5411 - acc: 0.7829 - val_loss: 1.1142 - val_acc: 0.6171\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5347 - acc: 0.7969 - val_loss: 1.1070 - val_acc: 0.6057\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5241 - acc: 0.7918 - val_loss: 1.1024 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 46us/step\n",
      "1741/1741 [==============================] - 0s 33us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.5438 - acc: 0.2989 - val_loss: 1.2233 - val_acc: 0.5086\n",
      "Epoch 2/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.2964 - acc: 0.4029 - val_loss: 1.0876 - val_acc: 0.5657\n",
      "Epoch 3/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.2007 - acc: 0.4630 - val_loss: 1.0232 - val_acc: 0.5829\n",
      "Epoch 4/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.1402 - acc: 0.4962 - val_loss: 0.9922 - val_acc: 0.6286\n",
      "Epoch 5/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1158 - acc: 0.5243 - val_loss: 0.9612 - val_acc: 0.6514\n",
      "Epoch 6/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0775 - acc: 0.5300 - val_loss: 0.9397 - val_acc: 0.6514\n",
      "Epoch 7/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0402 - acc: 0.5607 - val_loss: 0.9329 - val_acc: 0.6629\n",
      "Epoch 8/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0369 - acc: 0.5639 - val_loss: 0.9236 - val_acc: 0.6800\n",
      "Epoch 9/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0188 - acc: 0.5390 - val_loss: 0.9090 - val_acc: 0.6800\n",
      "Epoch 10/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9981 - acc: 0.5709 - val_loss: 0.9047 - val_acc: 0.6800\n",
      "Epoch 11/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0003 - acc: 0.5754 - val_loss: 0.8983 - val_acc: 0.6971\n",
      "Epoch 12/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9858 - acc: 0.5747 - val_loss: 0.8915 - val_acc: 0.6971\n",
      "Epoch 13/150\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9631 - acc: 0.5875 - val_loss: 0.8923 - val_acc: 0.7029\n",
      "Epoch 14/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9553 - acc: 0.5964 - val_loss: 0.8906 - val_acc: 0.7086\n",
      "Epoch 15/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9518 - acc: 0.5830 - val_loss: 0.8867 - val_acc: 0.7086\n",
      "Epoch 16/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9200 - acc: 0.6137 - val_loss: 0.8889 - val_acc: 0.6686\n",
      "Epoch 17/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9260 - acc: 0.6156 - val_loss: 0.8918 - val_acc: 0.6914\n",
      "Epoch 18/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9320 - acc: 0.6028 - val_loss: 0.8855 - val_acc: 0.6800\n",
      "Epoch 19/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9102 - acc: 0.6226 - val_loss: 0.8869 - val_acc: 0.6971\n",
      "Epoch 20/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8991 - acc: 0.6188 - val_loss: 0.8863 - val_acc: 0.6914\n",
      "Epoch 21/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9012 - acc: 0.6188 - val_loss: 0.8879 - val_acc: 0.6857\n",
      "Epoch 22/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9083 - acc: 0.6207 - val_loss: 0.8813 - val_acc: 0.6743\n",
      "Epoch 23/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8810 - acc: 0.6360 - val_loss: 0.8737 - val_acc: 0.7029\n",
      "Epoch 24/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8798 - acc: 0.6277 - val_loss: 0.8771 - val_acc: 0.6857\n",
      "Epoch 25/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8615 - acc: 0.6418 - val_loss: 0.8796 - val_acc: 0.7143\n",
      "Epoch 26/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8792 - acc: 0.6430 - val_loss: 0.8897 - val_acc: 0.6971\n",
      "Epoch 27/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8719 - acc: 0.6443 - val_loss: 0.8866 - val_acc: 0.6743\n",
      "Epoch 28/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8710 - acc: 0.6303 - val_loss: 0.8825 - val_acc: 0.6686\n",
      "Epoch 29/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8511 - acc: 0.6481 - val_loss: 0.8838 - val_acc: 0.6800\n",
      "Epoch 30/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8593 - acc: 0.6271 - val_loss: 0.8826 - val_acc: 0.6686\n",
      "Epoch 31/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8569 - acc: 0.6494 - val_loss: 0.8838 - val_acc: 0.6686\n",
      "Epoch 32/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8575 - acc: 0.6437 - val_loss: 0.8862 - val_acc: 0.6629\n",
      "Epoch 33/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8456 - acc: 0.6418 - val_loss: 0.8776 - val_acc: 0.6914\n",
      "Epoch 34/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8438 - acc: 0.6526 - val_loss: 0.8811 - val_acc: 0.6743\n",
      "Epoch 35/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8391 - acc: 0.6475 - val_loss: 0.8880 - val_acc: 0.6686\n",
      "Epoch 36/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8401 - acc: 0.6558 - val_loss: 0.8848 - val_acc: 0.6857\n",
      "Epoch 37/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8289 - acc: 0.6609 - val_loss: 0.8880 - val_acc: 0.6914\n",
      "Epoch 38/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8235 - acc: 0.6654 - val_loss: 0.8906 - val_acc: 0.6800\n",
      "Epoch 39/150\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8045 - acc: 0.6820 - val_loss: 0.8954 - val_acc: 0.6743\n",
      "Epoch 40/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8143 - acc: 0.6654 - val_loss: 0.8956 - val_acc: 0.6629\n",
      "Epoch 41/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8076 - acc: 0.6699 - val_loss: 0.8906 - val_acc: 0.6857\n",
      "Epoch 42/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8149 - acc: 0.6699 - val_loss: 0.8876 - val_acc: 0.6971\n",
      "Epoch 43/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8090 - acc: 0.6769 - val_loss: 0.8905 - val_acc: 0.6857\n",
      "Epoch 44/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8044 - acc: 0.6750 - val_loss: 0.9036 - val_acc: 0.6514\n",
      "Epoch 45/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8101 - acc: 0.6628 - val_loss: 0.8990 - val_acc: 0.6629\n",
      "Epoch 46/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7991 - acc: 0.6807 - val_loss: 0.9001 - val_acc: 0.6629\n",
      "Epoch 47/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7738 - acc: 0.6928 - val_loss: 0.8994 - val_acc: 0.6629\n",
      "Epoch 48/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7854 - acc: 0.6826 - val_loss: 0.8913 - val_acc: 0.6629\n",
      "Epoch 49/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7814 - acc: 0.6750 - val_loss: 0.8994 - val_acc: 0.6686\n",
      "Epoch 50/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7895 - acc: 0.6833 - val_loss: 0.8979 - val_acc: 0.6457\n",
      "Epoch 51/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7707 - acc: 0.6884 - val_loss: 0.9096 - val_acc: 0.6571\n",
      "Epoch 52/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7705 - acc: 0.6890 - val_loss: 0.8972 - val_acc: 0.6800\n",
      "Epoch 53/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7834 - acc: 0.6794 - val_loss: 0.9005 - val_acc: 0.6571\n",
      "Epoch 54/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7628 - acc: 0.6839 - val_loss: 0.9011 - val_acc: 0.6686\n",
      "Epoch 55/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7769 - acc: 0.6916 - val_loss: 0.9011 - val_acc: 0.6686\n",
      "Epoch 56/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7715 - acc: 0.6928 - val_loss: 0.9072 - val_acc: 0.6571\n",
      "Epoch 57/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7485 - acc: 0.7107 - val_loss: 0.9169 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7550 - acc: 0.6973 - val_loss: 0.9097 - val_acc: 0.6629\n",
      "Epoch 59/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7661 - acc: 0.6897 - val_loss: 0.9065 - val_acc: 0.6686\n",
      "Epoch 60/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7149 - acc: 0.7056 - val_loss: 0.9097 - val_acc: 0.6343\n",
      "Epoch 61/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7470 - acc: 0.6871 - val_loss: 0.9226 - val_acc: 0.6629\n",
      "Epoch 62/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7539 - acc: 0.6877 - val_loss: 0.9125 - val_acc: 0.6457\n",
      "Epoch 63/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7402 - acc: 0.7037 - val_loss: 0.9152 - val_acc: 0.6629\n",
      "Epoch 64/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7456 - acc: 0.7056 - val_loss: 0.9110 - val_acc: 0.6857\n",
      "Epoch 65/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7348 - acc: 0.6999 - val_loss: 0.9117 - val_acc: 0.6800\n",
      "Epoch 66/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7393 - acc: 0.6973 - val_loss: 0.9108 - val_acc: 0.6629\n",
      "Epoch 67/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7220 - acc: 0.7069 - val_loss: 0.9149 - val_acc: 0.6686\n",
      "Epoch 68/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7385 - acc: 0.6782 - val_loss: 0.9260 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7158 - acc: 0.6967 - val_loss: 0.9223 - val_acc: 0.6629\n",
      "Epoch 70/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6978 - acc: 0.7197 - val_loss: 0.9225 - val_acc: 0.6686\n",
      "Epoch 71/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6951 - acc: 0.7254 - val_loss: 0.9385 - val_acc: 0.6343\n",
      "Epoch 72/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7201 - acc: 0.7248 - val_loss: 0.9299 - val_acc: 0.6400\n",
      "Epoch 73/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6877 - acc: 0.7229 - val_loss: 0.9314 - val_acc: 0.6400\n",
      "Epoch 74/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7059 - acc: 0.7178 - val_loss: 0.9327 - val_acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6834 - acc: 0.7305 - val_loss: 0.9290 - val_acc: 0.6400\n",
      "Epoch 76/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7145 - acc: 0.7165 - val_loss: 0.9375 - val_acc: 0.6343\n",
      "Epoch 77/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6723 - acc: 0.7286 - val_loss: 0.9261 - val_acc: 0.6571\n",
      "Epoch 78/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7134 - acc: 0.7063 - val_loss: 0.9196 - val_acc: 0.6743\n",
      "Epoch 79/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7127 - acc: 0.7101 - val_loss: 0.9258 - val_acc: 0.6571\n",
      "Epoch 80/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6711 - acc: 0.7337 - val_loss: 0.9237 - val_acc: 0.6629\n",
      "Epoch 81/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6806 - acc: 0.7171 - val_loss: 0.9314 - val_acc: 0.6514\n",
      "Epoch 82/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6682 - acc: 0.7292 - val_loss: 0.9293 - val_acc: 0.6571\n",
      "Epoch 83/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7015 - acc: 0.7216 - val_loss: 0.9398 - val_acc: 0.6629\n",
      "Epoch 84/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6905 - acc: 0.7082 - val_loss: 0.9468 - val_acc: 0.6514\n",
      "Epoch 85/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6816 - acc: 0.7273 - val_loss: 0.9404 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6776 - acc: 0.7146 - val_loss: 0.9385 - val_acc: 0.6629\n",
      "Epoch 87/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6594 - acc: 0.7318 - val_loss: 0.9427 - val_acc: 0.6629\n",
      "Epoch 88/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6517 - acc: 0.7407 - val_loss: 0.9528 - val_acc: 0.6629\n",
      "Epoch 89/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6573 - acc: 0.7305 - val_loss: 0.9627 - val_acc: 0.6571\n",
      "Epoch 90/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6430 - acc: 0.7337 - val_loss: 0.9575 - val_acc: 0.6571\n",
      "Epoch 91/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6548 - acc: 0.7363 - val_loss: 0.9584 - val_acc: 0.6629\n",
      "Epoch 92/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6631 - acc: 0.7305 - val_loss: 0.9637 - val_acc: 0.6743\n",
      "Epoch 93/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6560 - acc: 0.7299 - val_loss: 0.9628 - val_acc: 0.6743\n",
      "Epoch 94/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6568 - acc: 0.7548 - val_loss: 0.9637 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6564 - acc: 0.7229 - val_loss: 0.9820 - val_acc: 0.6514\n",
      "Epoch 96/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6570 - acc: 0.7382 - val_loss: 0.9901 - val_acc: 0.6400\n",
      "Epoch 97/150\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6449 - acc: 0.7363 - val_loss: 0.9897 - val_acc: 0.6343\n",
      "Epoch 98/150\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6307 - acc: 0.7548 - val_loss: 0.9799 - val_acc: 0.6571\n",
      "Epoch 99/150\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6574 - acc: 0.7356 - val_loss: 0.9750 - val_acc: 0.6800\n",
      "Epoch 100/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6419 - acc: 0.7510 - val_loss: 0.9784 - val_acc: 0.6629\n",
      "Epoch 101/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6355 - acc: 0.7350 - val_loss: 0.9810 - val_acc: 0.6743\n",
      "Epoch 102/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6380 - acc: 0.7465 - val_loss: 0.9915 - val_acc: 0.6857\n",
      "Epoch 103/150\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6360 - acc: 0.7471 - val_loss: 0.9903 - val_acc: 0.6800\n",
      "Epoch 104/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6336 - acc: 0.7395 - val_loss: 0.9902 - val_acc: 0.6800\n",
      "Epoch 105/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6311 - acc: 0.7554 - val_loss: 0.9917 - val_acc: 0.6686\n",
      "Epoch 106/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6134 - acc: 0.7478 - val_loss: 0.9949 - val_acc: 0.6571\n",
      "Epoch 107/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6115 - acc: 0.7510 - val_loss: 0.9923 - val_acc: 0.6629\n",
      "Epoch 108/150\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6269 - acc: 0.7510 - val_loss: 0.9896 - val_acc: 0.6571\n",
      "Epoch 109/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6090 - acc: 0.7561 - val_loss: 0.9950 - val_acc: 0.6800\n",
      "Epoch 110/150\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6071 - acc: 0.7612 - val_loss: 1.0071 - val_acc: 0.6629\n",
      "Epoch 111/150\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6028 - acc: 0.7510 - val_loss: 1.0007 - val_acc: 0.6400\n",
      "Epoch 112/150\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6185 - acc: 0.7446 - val_loss: 1.0113 - val_acc: 0.6686\n",
      "Epoch 113/150\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6058 - acc: 0.7605 - val_loss: 1.0095 - val_acc: 0.6629\n",
      "Epoch 114/150\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6052 - acc: 0.7612 - val_loss: 1.0114 - val_acc: 0.6743\n",
      "Epoch 115/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5991 - acc: 0.7554 - val_loss: 1.0202 - val_acc: 0.6629\n",
      "Epoch 116/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6002 - acc: 0.7586 - val_loss: 1.0133 - val_acc: 0.6571\n",
      "Epoch 117/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6100 - acc: 0.7561 - val_loss: 1.0097 - val_acc: 0.6571\n",
      "Epoch 118/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5921 - acc: 0.7631 - val_loss: 1.0165 - val_acc: 0.6629\n",
      "Epoch 119/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5869 - acc: 0.7618 - val_loss: 1.0255 - val_acc: 0.6514\n",
      "Epoch 120/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6040 - acc: 0.7637 - val_loss: 1.0277 - val_acc: 0.6457\n",
      "Epoch 121/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5889 - acc: 0.7669 - val_loss: 1.0312 - val_acc: 0.6457\n",
      "Epoch 122/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5841 - acc: 0.7682 - val_loss: 1.0316 - val_acc: 0.6629\n",
      "Epoch 123/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5677 - acc: 0.7880 - val_loss: 1.0380 - val_acc: 0.6686\n",
      "Epoch 124/150\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.5972 - acc: 0.7567 - val_loss: 1.0411 - val_acc: 0.6343\n",
      "Epoch 125/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5818 - acc: 0.7656 - val_loss: 1.0286 - val_acc: 0.6514\n",
      "Epoch 126/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5590 - acc: 0.7765 - val_loss: 1.0246 - val_acc: 0.6686\n",
      "Epoch 127/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5810 - acc: 0.7599 - val_loss: 1.0306 - val_acc: 0.6571\n",
      "Epoch 128/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5552 - acc: 0.7759 - val_loss: 1.0422 - val_acc: 0.6400\n",
      "Epoch 129/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5847 - acc: 0.7522 - val_loss: 1.0491 - val_acc: 0.6514\n",
      "Epoch 130/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5700 - acc: 0.7714 - val_loss: 1.0437 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5727 - acc: 0.7682 - val_loss: 1.0388 - val_acc: 0.6514\n",
      "Epoch 132/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5528 - acc: 0.7842 - val_loss: 1.0405 - val_acc: 0.6629\n",
      "Epoch 133/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5783 - acc: 0.7669 - val_loss: 1.0525 - val_acc: 0.6571\n",
      "Epoch 134/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5698 - acc: 0.7669 - val_loss: 1.0541 - val_acc: 0.6743\n",
      "Epoch 135/150\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5692 - acc: 0.7701 - val_loss: 1.0457 - val_acc: 0.6629\n",
      "Epoch 136/150\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5309 - acc: 0.7893 - val_loss: 1.0504 - val_acc: 0.6629\n",
      "Epoch 137/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5491 - acc: 0.7752 - val_loss: 1.0551 - val_acc: 0.6514\n",
      "Epoch 138/150\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5470 - acc: 0.7893 - val_loss: 1.0572 - val_acc: 0.6629\n",
      "Epoch 139/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5230 - acc: 0.7995 - val_loss: 1.0588 - val_acc: 0.6514\n",
      "Epoch 140/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5541 - acc: 0.7867 - val_loss: 1.0659 - val_acc: 0.6571\n",
      "Epoch 141/150\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5416 - acc: 0.7848 - val_loss: 1.0630 - val_acc: 0.6514\n",
      "Epoch 142/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5356 - acc: 0.7886 - val_loss: 1.0624 - val_acc: 0.6514\n",
      "Epoch 143/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5368 - acc: 0.7880 - val_loss: 1.0601 - val_acc: 0.6514\n",
      "Epoch 144/150\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5427 - acc: 0.7848 - val_loss: 1.0806 - val_acc: 0.6400\n",
      "Epoch 145/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5416 - acc: 0.7829 - val_loss: 1.0776 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5453 - acc: 0.7829 - val_loss: 1.0707 - val_acc: 0.6400\n",
      "Epoch 147/150\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5328 - acc: 0.7944 - val_loss: 1.0797 - val_acc: 0.6514\n",
      "Epoch 148/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5156 - acc: 0.7937 - val_loss: 1.1005 - val_acc: 0.6457\n",
      "Epoch 149/150\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5344 - acc: 0.7905 - val_loss: 1.0831 - val_acc: 0.6629\n",
      "Epoch 150/150\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5275 - acc: 0.7925 - val_loss: 1.0925 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 55us/step\n",
      "1741/1741 [==============================] - 0s 34us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.4792 - acc: 0.3318 - val_loss: 1.1992 - val_acc: 0.5429\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 1.2711 - acc: 0.4186 - val_loss: 1.0907 - val_acc: 0.6000\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.1943 - acc: 0.4550 - val_loss: 1.0239 - val_acc: 0.6286\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.1217 - acc: 0.5137 - val_loss: 1.0121 - val_acc: 0.6114\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0910 - acc: 0.5169 - val_loss: 0.9860 - val_acc: 0.6057\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.0563 - acc: 0.5348 - val_loss: 0.9650 - val_acc: 0.6343\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.0469 - acc: 0.5616 - val_loss: 0.9521 - val_acc: 0.6286\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.0040 - acc: 0.5743 - val_loss: 0.9419 - val_acc: 0.6286\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0070 - acc: 0.5692 - val_loss: 0.9359 - val_acc: 0.6629\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9938 - acc: 0.5826 - val_loss: 0.9294 - val_acc: 0.6286\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9817 - acc: 0.5750 - val_loss: 0.9319 - val_acc: 0.6457\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9616 - acc: 0.5980 - val_loss: 0.9226 - val_acc: 0.6457\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.9786 - acc: 0.5865 - val_loss: 0.9205 - val_acc: 0.6457\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9444 - acc: 0.6075 - val_loss: 0.9118 - val_acc: 0.6743\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9387 - acc: 0.5929 - val_loss: 0.9065 - val_acc: 0.6686\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.9470 - acc: 0.5865 - val_loss: 0.9035 - val_acc: 0.6686\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.9398 - acc: 0.6037 - val_loss: 0.9038 - val_acc: 0.6629\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9424 - acc: 0.5973 - val_loss: 0.9055 - val_acc: 0.6743\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9136 - acc: 0.6228 - val_loss: 0.8997 - val_acc: 0.6800\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9183 - acc: 0.6094 - val_loss: 0.8932 - val_acc: 0.6629\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8986 - acc: 0.6203 - val_loss: 0.8947 - val_acc: 0.6800\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8825 - acc: 0.6445 - val_loss: 0.9028 - val_acc: 0.6571\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8922 - acc: 0.6420 - val_loss: 0.9032 - val_acc: 0.6629\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8683 - acc: 0.6362 - val_loss: 0.9010 - val_acc: 0.6743\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8688 - acc: 0.6496 - val_loss: 0.9068 - val_acc: 0.6800\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8614 - acc: 0.6445 - val_loss: 0.8939 - val_acc: 0.6743\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8554 - acc: 0.6426 - val_loss: 0.8923 - val_acc: 0.6857\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8522 - acc: 0.6484 - val_loss: 0.9053 - val_acc: 0.6629\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.8687 - acc: 0.6369 - val_loss: 0.9035 - val_acc: 0.6743\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8467 - acc: 0.6611 - val_loss: 0.9001 - val_acc: 0.6857\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8386 - acc: 0.6682 - val_loss: 0.9088 - val_acc: 0.6629\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8346 - acc: 0.6465 - val_loss: 0.9075 - val_acc: 0.6743\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8345 - acc: 0.6522 - val_loss: 0.9226 - val_acc: 0.6629\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8190 - acc: 0.6586 - val_loss: 0.9170 - val_acc: 0.6629\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8226 - acc: 0.6618 - val_loss: 0.9139 - val_acc: 0.6571\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8462 - acc: 0.6458 - val_loss: 0.9133 - val_acc: 0.6514\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8287 - acc: 0.6573 - val_loss: 0.9057 - val_acc: 0.6686\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8157 - acc: 0.6656 - val_loss: 0.9101 - val_acc: 0.6800\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8038 - acc: 0.6669 - val_loss: 0.9125 - val_acc: 0.6743\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8193 - acc: 0.6637 - val_loss: 0.9167 - val_acc: 0.6629\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8162 - acc: 0.6592 - val_loss: 0.9220 - val_acc: 0.6514\n",
      "Epoch 42/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8089 - acc: 0.6758 - val_loss: 0.9249 - val_acc: 0.6629\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7869 - acc: 0.6803 - val_loss: 0.9264 - val_acc: 0.6743\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7969 - acc: 0.6739 - val_loss: 0.9221 - val_acc: 0.6686\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8087 - acc: 0.6720 - val_loss: 0.9277 - val_acc: 0.6514\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7763 - acc: 0.6867 - val_loss: 0.9247 - val_acc: 0.6800\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7891 - acc: 0.6790 - val_loss: 0.9367 - val_acc: 0.6629\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7613 - acc: 0.6918 - val_loss: 0.9355 - val_acc: 0.6686\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7644 - acc: 0.6713 - val_loss: 0.9220 - val_acc: 0.6743\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7627 - acc: 0.6854 - val_loss: 0.9277 - val_acc: 0.6800\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7473 - acc: 0.7033 - val_loss: 0.9322 - val_acc: 0.6686\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7574 - acc: 0.6962 - val_loss: 0.9369 - val_acc: 0.6629\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7603 - acc: 0.6796 - val_loss: 0.9292 - val_acc: 0.6629\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7426 - acc: 0.6950 - val_loss: 0.9374 - val_acc: 0.6629\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7678 - acc: 0.6918 - val_loss: 0.9313 - val_acc: 0.6571\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7635 - acc: 0.6924 - val_loss: 0.9459 - val_acc: 0.6457\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7347 - acc: 0.6962 - val_loss: 0.9401 - val_acc: 0.6514\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7467 - acc: 0.6988 - val_loss: 0.9385 - val_acc: 0.6743\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7289 - acc: 0.7013 - val_loss: 0.9397 - val_acc: 0.6629\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7352 - acc: 0.7039 - val_loss: 0.9571 - val_acc: 0.6571\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7383 - acc: 0.6956 - val_loss: 0.9613 - val_acc: 0.6571\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7241 - acc: 0.7033 - val_loss: 0.9512 - val_acc: 0.6571\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7228 - acc: 0.7001 - val_loss: 0.9520 - val_acc: 0.6686\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7125 - acc: 0.7154 - val_loss: 0.9569 - val_acc: 0.6571\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7326 - acc: 0.6924 - val_loss: 0.9638 - val_acc: 0.6629\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7297 - acc: 0.7154 - val_loss: 0.9647 - val_acc: 0.6571\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7043 - acc: 0.7064 - val_loss: 0.9552 - val_acc: 0.6571\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7039 - acc: 0.7103 - val_loss: 0.9626 - val_acc: 0.6743\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7134 - acc: 0.7179 - val_loss: 0.9714 - val_acc: 0.6686\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6818 - acc: 0.7167 - val_loss: 0.9664 - val_acc: 0.6629\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6882 - acc: 0.7205 - val_loss: 0.9756 - val_acc: 0.6629\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7070 - acc: 0.7154 - val_loss: 0.9813 - val_acc: 0.6571\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7045 - acc: 0.7218 - val_loss: 0.9763 - val_acc: 0.6571\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6951 - acc: 0.7294 - val_loss: 0.9798 - val_acc: 0.6571\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6753 - acc: 0.7275 - val_loss: 0.9867 - val_acc: 0.6629\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6880 - acc: 0.7096 - val_loss: 0.9958 - val_acc: 0.6457\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6663 - acc: 0.7307 - val_loss: 0.9973 - val_acc: 0.6571\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6627 - acc: 0.7288 - val_loss: 0.9862 - val_acc: 0.6457\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6644 - acc: 0.7326 - val_loss: 0.9891 - val_acc: 0.6514\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6709 - acc: 0.7256 - val_loss: 0.9831 - val_acc: 0.6343\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6776 - acc: 0.7352 - val_loss: 0.9965 - val_acc: 0.6457\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6545 - acc: 0.7371 - val_loss: 0.9981 - val_acc: 0.6457\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6754 - acc: 0.7256 - val_loss: 0.9993 - val_acc: 0.6514\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6846 - acc: 0.7288 - val_loss: 1.0156 - val_acc: 0.6400\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6458 - acc: 0.7371 - val_loss: 1.0210 - val_acc: 0.6400\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6639 - acc: 0.7294 - val_loss: 1.0123 - val_acc: 0.6571\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6530 - acc: 0.7371 - val_loss: 1.0060 - val_acc: 0.6514\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6479 - acc: 0.7390 - val_loss: 1.0004 - val_acc: 0.6571\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6664 - acc: 0.7332 - val_loss: 1.0072 - val_acc: 0.6571\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6539 - acc: 0.7320 - val_loss: 1.0058 - val_acc: 0.6514\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6523 - acc: 0.7447 - val_loss: 0.9991 - val_acc: 0.6514\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6497 - acc: 0.7460 - val_loss: 1.0032 - val_acc: 0.6514\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6444 - acc: 0.7409 - val_loss: 1.0074 - val_acc: 0.6400\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6169 - acc: 0.7492 - val_loss: 1.0082 - val_acc: 0.6514\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6377 - acc: 0.7460 - val_loss: 1.0079 - val_acc: 0.6571\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6262 - acc: 0.7498 - val_loss: 0.9956 - val_acc: 0.6571\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6455 - acc: 0.7326 - val_loss: 1.0132 - val_acc: 0.6571\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6214 - acc: 0.7518 - val_loss: 1.0193 - val_acc: 0.6571\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6203 - acc: 0.7390 - val_loss: 1.0225 - val_acc: 0.6514\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6243 - acc: 0.7460 - val_loss: 1.0206 - val_acc: 0.6457\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6214 - acc: 0.7530 - val_loss: 1.0301 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6264 - acc: 0.7447 - val_loss: 1.0372 - val_acc: 0.6571\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6224 - acc: 0.7537 - val_loss: 1.0314 - val_acc: 0.6571\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6317 - acc: 0.7435 - val_loss: 1.0485 - val_acc: 0.6457\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6125 - acc: 0.7466 - val_loss: 1.0467 - val_acc: 0.6457\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6283 - acc: 0.7460 - val_loss: 1.0363 - val_acc: 0.6400\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6118 - acc: 0.7594 - val_loss: 1.0331 - val_acc: 0.6457\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6081 - acc: 0.7537 - val_loss: 1.0493 - val_acc: 0.6514\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6208 - acc: 0.7524 - val_loss: 1.0539 - val_acc: 0.6457\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5979 - acc: 0.7569 - val_loss: 1.0494 - val_acc: 0.6457\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.5994 - acc: 0.7588 - val_loss: 1.0508 - val_acc: 0.6514\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.6040 - acc: 0.7441 - val_loss: 1.0544 - val_acc: 0.6514\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5892 - acc: 0.7588 - val_loss: 1.0623 - val_acc: 0.6571\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.5905 - acc: 0.7613 - val_loss: 1.0658 - val_acc: 0.6400\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5814 - acc: 0.7498 - val_loss: 1.0734 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5928 - acc: 0.7741 - val_loss: 1.0744 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6078 - acc: 0.7575 - val_loss: 1.0767 - val_acc: 0.6457\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5858 - acc: 0.7511 - val_loss: 1.0772 - val_acc: 0.6457\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5740 - acc: 0.7652 - val_loss: 1.0707 - val_acc: 0.6457\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5769 - acc: 0.7703 - val_loss: 1.0769 - val_acc: 0.6400\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5720 - acc: 0.7741 - val_loss: 1.0799 - val_acc: 0.6514\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5698 - acc: 0.7703 - val_loss: 1.0948 - val_acc: 0.6571\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5558 - acc: 0.7805 - val_loss: 1.0921 - val_acc: 0.6457\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5745 - acc: 0.7594 - val_loss: 1.1041 - val_acc: 0.6400\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5655 - acc: 0.7613 - val_loss: 1.0835 - val_acc: 0.6514\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5555 - acc: 0.7817 - val_loss: 1.0947 - val_acc: 0.6571\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5607 - acc: 0.7715 - val_loss: 1.1000 - val_acc: 0.6514\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5476 - acc: 0.7875 - val_loss: 1.0998 - val_acc: 0.6571\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5556 - acc: 0.7862 - val_loss: 1.0856 - val_acc: 0.6514\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5506 - acc: 0.7856 - val_loss: 1.0975 - val_acc: 0.6514\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5300 - acc: 0.7888 - val_loss: 1.1006 - val_acc: 0.6571\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5487 - acc: 0.7728 - val_loss: 1.1112 - val_acc: 0.6514\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5334 - acc: 0.7849 - val_loss: 1.1141 - val_acc: 0.6514\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5477 - acc: 0.7920 - val_loss: 1.1165 - val_acc: 0.6514\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5582 - acc: 0.7798 - val_loss: 1.1225 - val_acc: 0.6457\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.5418 - acc: 0.7830 - val_loss: 1.1159 - val_acc: 0.6571\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5212 - acc: 0.7856 - val_loss: 1.1188 - val_acc: 0.6571\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5252 - acc: 0.7913 - val_loss: 1.1170 - val_acc: 0.6457\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.5318 - acc: 0.7837 - val_loss: 1.1362 - val_acc: 0.6457\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5253 - acc: 0.7951 - val_loss: 1.1283 - val_acc: 0.6400\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5210 - acc: 0.7805 - val_loss: 1.1262 - val_acc: 0.6571\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5253 - acc: 0.7977 - val_loss: 1.1243 - val_acc: 0.6457\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5368 - acc: 0.7824 - val_loss: 1.1578 - val_acc: 0.6343\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5300 - acc: 0.7926 - val_loss: 1.1582 - val_acc: 0.6286\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5110 - acc: 0.8022 - val_loss: 1.1380 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5224 - acc: 0.7894 - val_loss: 1.1557 - val_acc: 0.6457\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5140 - acc: 0.7996 - val_loss: 1.1480 - val_acc: 0.6457\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.5244 - acc: 0.7907 - val_loss: 1.1527 - val_acc: 0.6514\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4912 - acc: 0.8003 - val_loss: 1.1495 - val_acc: 0.6571\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5233 - acc: 0.7875 - val_loss: 1.1456 - val_acc: 0.6571\n",
      "193/193 [==============================] - 0s 48us/step\n",
      "1742/1742 [==============================] - 0s 30us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.4541 - acc: 0.2923 - val_loss: 1.1914 - val_acc: 0.5486\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.2724 - acc: 0.4193 - val_loss: 1.0841 - val_acc: 0.6229\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.1844 - acc: 0.4614 - val_loss: 1.0287 - val_acc: 0.6343\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.1320 - acc: 0.5035 - val_loss: 0.9944 - val_acc: 0.6629\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0961 - acc: 0.5214 - val_loss: 0.9676 - val_acc: 0.6571\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.0440 - acc: 0.5539 - val_loss: 0.9465 - val_acc: 0.6571\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0285 - acc: 0.5622 - val_loss: 0.9316 - val_acc: 0.6514\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0376 - acc: 0.5622 - val_loss: 0.9137 - val_acc: 0.6571\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9753 - acc: 0.5775 - val_loss: 0.9041 - val_acc: 0.6571\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9772 - acc: 0.5877 - val_loss: 0.8984 - val_acc: 0.6514\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9479 - acc: 0.5992 - val_loss: 0.8892 - val_acc: 0.6686\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9575 - acc: 0.5884 - val_loss: 0.8880 - val_acc: 0.6686\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9373 - acc: 0.6037 - val_loss: 0.8876 - val_acc: 0.6686\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9407 - acc: 0.6197 - val_loss: 0.8817 - val_acc: 0.6743\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9229 - acc: 0.6260 - val_loss: 0.8871 - val_acc: 0.6686\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9126 - acc: 0.6203 - val_loss: 0.8871 - val_acc: 0.6514\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9101 - acc: 0.6267 - val_loss: 0.8788 - val_acc: 0.6629\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9040 - acc: 0.6126 - val_loss: 0.8810 - val_acc: 0.6514\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9208 - acc: 0.6094 - val_loss: 0.8751 - val_acc: 0.6629\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8724 - acc: 0.6426 - val_loss: 0.8782 - val_acc: 0.6571\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8870 - acc: 0.6362 - val_loss: 0.8851 - val_acc: 0.6514\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8686 - acc: 0.6445 - val_loss: 0.8814 - val_acc: 0.6571\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8970 - acc: 0.6273 - val_loss: 0.8757 - val_acc: 0.6686\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8555 - acc: 0.6554 - val_loss: 0.8707 - val_acc: 0.6743\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8502 - acc: 0.6503 - val_loss: 0.8707 - val_acc: 0.6571\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8444 - acc: 0.6503 - val_loss: 0.8733 - val_acc: 0.6743\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8627 - acc: 0.6554 - val_loss: 0.8790 - val_acc: 0.6629\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8533 - acc: 0.6426 - val_loss: 0.8735 - val_acc: 0.6457\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8474 - acc: 0.6362 - val_loss: 0.8702 - val_acc: 0.6743\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8485 - acc: 0.6496 - val_loss: 0.8743 - val_acc: 0.6514\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8311 - acc: 0.6624 - val_loss: 0.8683 - val_acc: 0.6571\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8320 - acc: 0.6541 - val_loss: 0.8722 - val_acc: 0.6400\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8460 - acc: 0.6548 - val_loss: 0.8724 - val_acc: 0.6571\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8211 - acc: 0.6624 - val_loss: 0.8772 - val_acc: 0.6514\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8182 - acc: 0.6624 - val_loss: 0.8849 - val_acc: 0.6514\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8100 - acc: 0.6726 - val_loss: 0.8757 - val_acc: 0.6400\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8055 - acc: 0.6713 - val_loss: 0.8742 - val_acc: 0.6514\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7887 - acc: 0.6803 - val_loss: 0.8711 - val_acc: 0.6571\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8012 - acc: 0.6624 - val_loss: 0.8669 - val_acc: 0.6571\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7926 - acc: 0.6867 - val_loss: 0.8722 - val_acc: 0.6629\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7953 - acc: 0.6816 - val_loss: 0.8716 - val_acc: 0.6514\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7910 - acc: 0.6682 - val_loss: 0.8793 - val_acc: 0.6514\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7775 - acc: 0.6713 - val_loss: 0.8743 - val_acc: 0.6514\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7814 - acc: 0.6892 - val_loss: 0.8783 - val_acc: 0.6457\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7630 - acc: 0.6816 - val_loss: 0.8866 - val_acc: 0.6457\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7700 - acc: 0.6943 - val_loss: 0.8763 - val_acc: 0.6571\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7749 - acc: 0.6879 - val_loss: 0.8747 - val_acc: 0.6400\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7590 - acc: 0.6969 - val_loss: 0.8671 - val_acc: 0.6457\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7715 - acc: 0.6828 - val_loss: 0.8661 - val_acc: 0.6457\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7532 - acc: 0.6930 - val_loss: 0.8698 - val_acc: 0.6514\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7771 - acc: 0.6822 - val_loss: 0.8679 - val_acc: 0.6800\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7550 - acc: 0.6956 - val_loss: 0.8686 - val_acc: 0.6457\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7371 - acc: 0.6994 - val_loss: 0.8772 - val_acc: 0.6629\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7596 - acc: 0.6828 - val_loss: 0.8836 - val_acc: 0.6457\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7446 - acc: 0.6841 - val_loss: 0.8856 - val_acc: 0.6629\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7382 - acc: 0.6911 - val_loss: 0.8843 - val_acc: 0.6400\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7493 - acc: 0.6994 - val_loss: 0.8897 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7309 - acc: 0.6969 - val_loss: 0.8868 - val_acc: 0.6457\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7272 - acc: 0.7116 - val_loss: 0.8834 - val_acc: 0.6514\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7342 - acc: 0.7135 - val_loss: 0.8796 - val_acc: 0.6629\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7414 - acc: 0.6950 - val_loss: 0.8809 - val_acc: 0.6686\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7246 - acc: 0.7090 - val_loss: 0.8859 - val_acc: 0.6514\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7078 - acc: 0.7173 - val_loss: 0.8912 - val_acc: 0.6457\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7212 - acc: 0.7109 - val_loss: 0.8896 - val_acc: 0.6629\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7315 - acc: 0.7058 - val_loss: 0.8947 - val_acc: 0.6686\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7110 - acc: 0.7058 - val_loss: 0.8890 - val_acc: 0.6686\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7097 - acc: 0.7160 - val_loss: 0.8915 - val_acc: 0.6514\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7052 - acc: 0.7103 - val_loss: 0.8944 - val_acc: 0.6514\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6855 - acc: 0.7135 - val_loss: 0.8957 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7090 - acc: 0.7154 - val_loss: 0.8951 - val_acc: 0.6629\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6989 - acc: 0.7211 - val_loss: 0.8968 - val_acc: 0.6571\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6833 - acc: 0.7243 - val_loss: 0.8954 - val_acc: 0.6629\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6819 - acc: 0.7326 - val_loss: 0.8969 - val_acc: 0.6686\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6870 - acc: 0.7186 - val_loss: 0.8998 - val_acc: 0.6629\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6959 - acc: 0.7269 - val_loss: 0.9055 - val_acc: 0.6857\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6856 - acc: 0.7294 - val_loss: 0.9069 - val_acc: 0.6571\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6752 - acc: 0.7288 - val_loss: 0.9024 - val_acc: 0.6514\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6815 - acc: 0.7326 - val_loss: 0.9057 - val_acc: 0.6571\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6789 - acc: 0.7250 - val_loss: 0.9088 - val_acc: 0.6743\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6802 - acc: 0.7218 - val_loss: 0.9075 - val_acc: 0.6571\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6712 - acc: 0.7198 - val_loss: 0.9040 - val_acc: 0.6571\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6749 - acc: 0.7301 - val_loss: 0.9223 - val_acc: 0.6400\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6649 - acc: 0.7345 - val_loss: 0.9219 - val_acc: 0.6457\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6690 - acc: 0.7301 - val_loss: 0.9190 - val_acc: 0.6629\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6507 - acc: 0.7377 - val_loss: 0.9246 - val_acc: 0.6514\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6568 - acc: 0.7492 - val_loss: 0.9191 - val_acc: 0.6514\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6475 - acc: 0.7403 - val_loss: 0.9179 - val_acc: 0.6457\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6533 - acc: 0.7371 - val_loss: 0.9125 - val_acc: 0.6514\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6558 - acc: 0.7281 - val_loss: 0.9057 - val_acc: 0.6629\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6400 - acc: 0.7466 - val_loss: 0.9206 - val_acc: 0.6514\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6476 - acc: 0.7371 - val_loss: 0.9239 - val_acc: 0.6571\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6402 - acc: 0.7441 - val_loss: 0.9250 - val_acc: 0.6514\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6332 - acc: 0.7358 - val_loss: 0.9243 - val_acc: 0.6457\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6416 - acc: 0.7415 - val_loss: 0.9257 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6202 - acc: 0.7479 - val_loss: 0.9319 - val_acc: 0.6514\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6243 - acc: 0.7569 - val_loss: 0.9447 - val_acc: 0.6571\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6424 - acc: 0.7486 - val_loss: 0.9302 - val_acc: 0.6571\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6162 - acc: 0.7639 - val_loss: 0.9283 - val_acc: 0.6629\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6000 - acc: 0.7594 - val_loss: 0.9228 - val_acc: 0.6800\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6093 - acc: 0.7652 - val_loss: 0.9329 - val_acc: 0.6743\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6259 - acc: 0.7435 - val_loss: 0.9377 - val_acc: 0.6800\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6217 - acc: 0.7626 - val_loss: 0.9365 - val_acc: 0.6800\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6092 - acc: 0.7549 - val_loss: 0.9345 - val_acc: 0.6743\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5956 - acc: 0.7652 - val_loss: 0.9418 - val_acc: 0.6743\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6107 - acc: 0.7466 - val_loss: 0.9404 - val_acc: 0.6514\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5949 - acc: 0.7741 - val_loss: 0.9454 - val_acc: 0.6857\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6426 - acc: 0.7581 - val_loss: 0.9425 - val_acc: 0.6743\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.5830 - acc: 0.7779 - val_loss: 0.9402 - val_acc: 0.6743\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5987 - acc: 0.7626 - val_loss: 0.9419 - val_acc: 0.6571\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5980 - acc: 0.7613 - val_loss: 0.9501 - val_acc: 0.6686\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5919 - acc: 0.7677 - val_loss: 0.9467 - val_acc: 0.6571\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6066 - acc: 0.7492 - val_loss: 0.9452 - val_acc: 0.6629\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5988 - acc: 0.7658 - val_loss: 0.9451 - val_acc: 0.6686\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6005 - acc: 0.7594 - val_loss: 0.9472 - val_acc: 0.6514\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5846 - acc: 0.7747 - val_loss: 0.9437 - val_acc: 0.6686\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5870 - acc: 0.7683 - val_loss: 0.9442 - val_acc: 0.6743\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5912 - acc: 0.7696 - val_loss: 0.9699 - val_acc: 0.6686\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6062 - acc: 0.7575 - val_loss: 0.9742 - val_acc: 0.6571\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5830 - acc: 0.7728 - val_loss: 0.9545 - val_acc: 0.6686\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.5877 - acc: 0.7632 - val_loss: 0.9463 - val_acc: 0.6743\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5771 - acc: 0.7747 - val_loss: 0.9515 - val_acc: 0.6686\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.5581 - acc: 0.7766 - val_loss: 0.9544 - val_acc: 0.6629\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5828 - acc: 0.7754 - val_loss: 0.9661 - val_acc: 0.6514\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5485 - acc: 0.7907 - val_loss: 0.9646 - val_acc: 0.6571\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5428 - acc: 0.8015 - val_loss: 0.9709 - val_acc: 0.6686\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5701 - acc: 0.7741 - val_loss: 0.9754 - val_acc: 0.6514\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5570 - acc: 0.7690 - val_loss: 0.9675 - val_acc: 0.6629\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5520 - acc: 0.7830 - val_loss: 0.9687 - val_acc: 0.6457\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5599 - acc: 0.7773 - val_loss: 0.9754 - val_acc: 0.6514\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.5531 - acc: 0.7798 - val_loss: 0.9734 - val_acc: 0.6629\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5552 - acc: 0.7830 - val_loss: 0.9830 - val_acc: 0.6514\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5563 - acc: 0.7773 - val_loss: 0.9845 - val_acc: 0.6457\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5539 - acc: 0.7843 - val_loss: 0.9821 - val_acc: 0.6571\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5483 - acc: 0.7849 - val_loss: 0.9830 - val_acc: 0.6514\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5347 - acc: 0.7869 - val_loss: 0.9914 - val_acc: 0.6343\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.5229 - acc: 0.8060 - val_loss: 0.9857 - val_acc: 0.6571\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5490 - acc: 0.7875 - val_loss: 0.9939 - val_acc: 0.6343\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5540 - acc: 0.7811 - val_loss: 1.0020 - val_acc: 0.6400\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5291 - acc: 0.7945 - val_loss: 0.9932 - val_acc: 0.6286\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5118 - acc: 0.7862 - val_loss: 0.9984 - val_acc: 0.6286\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5161 - acc: 0.7817 - val_loss: 0.9995 - val_acc: 0.6286\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5416 - acc: 0.7945 - val_loss: 1.0109 - val_acc: 0.6229\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5340 - acc: 0.7983 - val_loss: 1.0160 - val_acc: 0.6400\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5173 - acc: 0.7983 - val_loss: 1.0198 - val_acc: 0.6457\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5165 - acc: 0.7862 - val_loss: 1.0042 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5129 - acc: 0.8047 - val_loss: 0.9990 - val_acc: 0.6629\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4964 - acc: 0.8060 - val_loss: 1.0047 - val_acc: 0.6400\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5259 - acc: 0.7862 - val_loss: 1.0196 - val_acc: 0.6400\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5265 - acc: 0.8022 - val_loss: 1.0172 - val_acc: 0.6286\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5213 - acc: 0.7932 - val_loss: 1.0150 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 51us/step\n",
      "1742/1742 [==============================] - 0s 33us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 9s 6ms/step - loss: 1.4277 - acc: 0.3197 - val_loss: 1.1837 - val_acc: 0.5200\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.2746 - acc: 0.4059 - val_loss: 1.0925 - val_acc: 0.6114\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.1866 - acc: 0.4697 - val_loss: 1.0361 - val_acc: 0.6114\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 1.1295 - acc: 0.4927 - val_loss: 0.9987 - val_acc: 0.6229\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 1.0867 - acc: 0.5207 - val_loss: 0.9710 - val_acc: 0.6343\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0568 - acc: 0.5373 - val_loss: 0.9483 - val_acc: 0.6800\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 1.0329 - acc: 0.5526 - val_loss: 0.9374 - val_acc: 0.6629\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0103 - acc: 0.5718 - val_loss: 0.9331 - val_acc: 0.6571\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9880 - acc: 0.5833 - val_loss: 0.9222 - val_acc: 0.6743\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9651 - acc: 0.5903 - val_loss: 0.9104 - val_acc: 0.6686\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.9584 - acc: 0.5846 - val_loss: 0.9125 - val_acc: 0.6457\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9622 - acc: 0.5826 - val_loss: 0.8928 - val_acc: 0.6571\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9449 - acc: 0.6158 - val_loss: 0.9077 - val_acc: 0.6800\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9232 - acc: 0.6031 - val_loss: 0.9035 - val_acc: 0.6686\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9223 - acc: 0.6075 - val_loss: 0.8940 - val_acc: 0.6686\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9191 - acc: 0.6203 - val_loss: 0.8893 - val_acc: 0.6743\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9094 - acc: 0.6126 - val_loss: 0.8872 - val_acc: 0.6857\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.9003 - acc: 0.6260 - val_loss: 0.9011 - val_acc: 0.6686\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8998 - acc: 0.6318 - val_loss: 0.8859 - val_acc: 0.6857\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8951 - acc: 0.6177 - val_loss: 0.8941 - val_acc: 0.6800\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8871 - acc: 0.6382 - val_loss: 0.8897 - val_acc: 0.6743\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8849 - acc: 0.6299 - val_loss: 0.8922 - val_acc: 0.6800\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8695 - acc: 0.6407 - val_loss: 0.8932 - val_acc: 0.6629\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8641 - acc: 0.6420 - val_loss: 0.8885 - val_acc: 0.6743\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8684 - acc: 0.6420 - val_loss: 0.8827 - val_acc: 0.6914\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8564 - acc: 0.6362 - val_loss: 0.8843 - val_acc: 0.6686\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8469 - acc: 0.6528 - val_loss: 0.8808 - val_acc: 0.6743\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8519 - acc: 0.6445 - val_loss: 0.8791 - val_acc: 0.6800\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8471 - acc: 0.6548 - val_loss: 0.8909 - val_acc: 0.6743\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8299 - acc: 0.6471 - val_loss: 0.8915 - val_acc: 0.6629\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8556 - acc: 0.6477 - val_loss: 0.8954 - val_acc: 0.6743\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.8307 - acc: 0.6643 - val_loss: 0.8945 - val_acc: 0.6800\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7997 - acc: 0.6854 - val_loss: 0.8956 - val_acc: 0.6857\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8236 - acc: 0.6656 - val_loss: 0.8982 - val_acc: 0.6686\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8332 - acc: 0.6535 - val_loss: 0.8926 - val_acc: 0.6400\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8340 - acc: 0.6682 - val_loss: 0.8970 - val_acc: 0.6457\n",
      "Epoch 37/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8053 - acc: 0.6701 - val_loss: 0.9042 - val_acc: 0.6629\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8184 - acc: 0.6611 - val_loss: 0.9022 - val_acc: 0.6629\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8059 - acc: 0.6854 - val_loss: 0.8933 - val_acc: 0.6686\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7858 - acc: 0.6911 - val_loss: 0.9081 - val_acc: 0.6400\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7956 - acc: 0.6726 - val_loss: 0.9072 - val_acc: 0.6571\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8055 - acc: 0.6726 - val_loss: 0.9085 - val_acc: 0.6571\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7903 - acc: 0.6611 - val_loss: 0.9089 - val_acc: 0.6343\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7797 - acc: 0.6765 - val_loss: 0.9134 - val_acc: 0.6514\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7905 - acc: 0.6739 - val_loss: 0.9210 - val_acc: 0.6400\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7935 - acc: 0.6739 - val_loss: 0.9107 - val_acc: 0.6743\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7634 - acc: 0.6803 - val_loss: 0.9193 - val_acc: 0.6743\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7782 - acc: 0.6886 - val_loss: 0.9266 - val_acc: 0.6400\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7751 - acc: 0.7007 - val_loss: 0.9222 - val_acc: 0.6400\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7572 - acc: 0.6918 - val_loss: 0.9303 - val_acc: 0.6457\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7440 - acc: 0.7096 - val_loss: 0.9278 - val_acc: 0.6457\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7600 - acc: 0.6930 - val_loss: 0.9171 - val_acc: 0.6629\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7569 - acc: 0.7020 - val_loss: 0.9078 - val_acc: 0.6800\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7665 - acc: 0.6701 - val_loss: 0.9057 - val_acc: 0.6857\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7355 - acc: 0.6988 - val_loss: 0.9204 - val_acc: 0.6571\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7574 - acc: 0.7001 - val_loss: 0.9259 - val_acc: 0.6457\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7233 - acc: 0.7077 - val_loss: 0.9243 - val_acc: 0.6457\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7508 - acc: 0.7064 - val_loss: 0.9297 - val_acc: 0.6686\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7364 - acc: 0.7103 - val_loss: 0.9233 - val_acc: 0.6400\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7255 - acc: 0.7135 - val_loss: 0.9261 - val_acc: 0.6400\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7468 - acc: 0.7020 - val_loss: 0.9236 - val_acc: 0.6514\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7230 - acc: 0.7109 - val_loss: 0.9226 - val_acc: 0.6343\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7430 - acc: 0.7039 - val_loss: 0.9377 - val_acc: 0.6400\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7239 - acc: 0.7154 - val_loss: 0.9341 - val_acc: 0.6229\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7174 - acc: 0.7064 - val_loss: 0.9399 - val_acc: 0.6457\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7156 - acc: 0.7090 - val_loss: 0.9372 - val_acc: 0.6514\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7236 - acc: 0.7071 - val_loss: 0.9327 - val_acc: 0.6514\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7036 - acc: 0.6988 - val_loss: 0.9365 - val_acc: 0.6571\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7134 - acc: 0.7141 - val_loss: 0.9419 - val_acc: 0.6400\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7098 - acc: 0.7026 - val_loss: 0.9330 - val_acc: 0.6514\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6957 - acc: 0.7211 - val_loss: 0.9387 - val_acc: 0.6343\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6841 - acc: 0.7243 - val_loss: 0.9440 - val_acc: 0.6343\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7144 - acc: 0.7090 - val_loss: 0.9481 - val_acc: 0.6286\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6971 - acc: 0.7154 - val_loss: 0.9602 - val_acc: 0.6514\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6815 - acc: 0.7205 - val_loss: 0.9576 - val_acc: 0.6629\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6809 - acc: 0.7230 - val_loss: 0.9566 - val_acc: 0.6457\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6857 - acc: 0.7256 - val_loss: 0.9601 - val_acc: 0.6571\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6800 - acc: 0.7288 - val_loss: 0.9575 - val_acc: 0.6514\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7003 - acc: 0.7205 - val_loss: 0.9533 - val_acc: 0.6457\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6690 - acc: 0.7301 - val_loss: 0.9656 - val_acc: 0.6400\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6683 - acc: 0.7256 - val_loss: 0.9650 - val_acc: 0.6571\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6752 - acc: 0.7313 - val_loss: 0.9679 - val_acc: 0.6514\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6547 - acc: 0.7364 - val_loss: 0.9606 - val_acc: 0.6686\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6712 - acc: 0.7224 - val_loss: 0.9616 - val_acc: 0.6686\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6674 - acc: 0.7332 - val_loss: 0.9736 - val_acc: 0.6571\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6822 - acc: 0.7275 - val_loss: 0.9826 - val_acc: 0.6514\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6505 - acc: 0.7307 - val_loss: 0.9658 - val_acc: 0.6686\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6601 - acc: 0.7352 - val_loss: 0.9693 - val_acc: 0.6457\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6487 - acc: 0.7428 - val_loss: 0.9676 - val_acc: 0.6800\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6377 - acc: 0.7409 - val_loss: 0.9782 - val_acc: 0.6514\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6431 - acc: 0.7390 - val_loss: 0.9724 - val_acc: 0.6286\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6575 - acc: 0.7326 - val_loss: 0.9791 - val_acc: 0.6400\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6513 - acc: 0.7428 - val_loss: 0.9762 - val_acc: 0.6571\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6421 - acc: 0.7403 - val_loss: 0.9834 - val_acc: 0.6457\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6236 - acc: 0.7524 - val_loss: 0.9941 - val_acc: 0.6229\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6308 - acc: 0.7479 - val_loss: 0.9852 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6359 - acc: 0.7613 - val_loss: 0.9910 - val_acc: 0.6343\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6118 - acc: 0.7537 - val_loss: 0.9758 - val_acc: 0.6743\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6516 - acc: 0.7390 - val_loss: 0.9954 - val_acc: 0.6343\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6222 - acc: 0.7511 - val_loss: 1.0081 - val_acc: 0.6400\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6333 - acc: 0.7498 - val_loss: 1.0100 - val_acc: 0.6457\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6059 - acc: 0.7639 - val_loss: 1.0066 - val_acc: 0.6571\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6317 - acc: 0.7549 - val_loss: 1.0005 - val_acc: 0.6571\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6117 - acc: 0.7549 - val_loss: 1.0031 - val_acc: 0.6400\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6206 - acc: 0.7556 - val_loss: 1.0075 - val_acc: 0.6400\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6115 - acc: 0.7607 - val_loss: 1.0170 - val_acc: 0.6400\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6068 - acc: 0.7613 - val_loss: 1.0084 - val_acc: 0.6400\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6072 - acc: 0.7543 - val_loss: 1.0208 - val_acc: 0.6286\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6155 - acc: 0.7537 - val_loss: 1.0249 - val_acc: 0.6400\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5950 - acc: 0.7722 - val_loss: 1.0306 - val_acc: 0.6229\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6056 - acc: 0.7703 - val_loss: 1.0110 - val_acc: 0.6400\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6015 - acc: 0.7703 - val_loss: 1.0139 - val_acc: 0.6514\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 45us/step - loss: 0.5847 - acc: 0.7626 - val_loss: 1.0204 - val_acc: 0.6400\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5766 - acc: 0.7722 - val_loss: 1.0271 - val_acc: 0.6457\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5841 - acc: 0.7652 - val_loss: 1.0253 - val_acc: 0.6400\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5798 - acc: 0.7639 - val_loss: 1.0294 - val_acc: 0.6514\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5712 - acc: 0.7677 - val_loss: 1.0288 - val_acc: 0.6514\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.5922 - acc: 0.7543 - val_loss: 1.0388 - val_acc: 0.6457\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - ETA: 0s - loss: 0.5821 - acc: 0.768 - 0s 62us/step - loss: 0.5717 - acc: 0.7728 - val_loss: 1.0379 - val_acc: 0.6400\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5981 - acc: 0.7671 - val_loss: 1.0208 - val_acc: 0.6457\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.5609 - acc: 0.7811 - val_loss: 1.0235 - val_acc: 0.6571\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.5737 - acc: 0.7709 - val_loss: 1.0381 - val_acc: 0.6114\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5749 - acc: 0.7735 - val_loss: 1.0513 - val_acc: 0.6171\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5536 - acc: 0.7773 - val_loss: 1.0533 - val_acc: 0.6343\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5586 - acc: 0.7735 - val_loss: 1.0381 - val_acc: 0.6400\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.5570 - acc: 0.7786 - val_loss: 1.0386 - val_acc: 0.6457\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.5490 - acc: 0.7747 - val_loss: 1.0492 - val_acc: 0.6343\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.5567 - acc: 0.7869 - val_loss: 1.0569 - val_acc: 0.6286\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5566 - acc: 0.7747 - val_loss: 1.0624 - val_acc: 0.6286\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.5521 - acc: 0.7894 - val_loss: 1.0515 - val_acc: 0.6571\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.5561 - acc: 0.7824 - val_loss: 1.0496 - val_acc: 0.6629\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.5474 - acc: 0.7817 - val_loss: 1.0539 - val_acc: 0.6571\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5525 - acc: 0.7811 - val_loss: 1.0601 - val_acc: 0.6400\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5489 - acc: 0.7900 - val_loss: 1.0621 - val_acc: 0.6343\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.5488 - acc: 0.7888 - val_loss: 1.0634 - val_acc: 0.6286\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.5352 - acc: 0.7824 - val_loss: 1.0670 - val_acc: 0.6400\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5507 - acc: 0.7849 - val_loss: 1.0849 - val_acc: 0.6343\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.5448 - acc: 0.7888 - val_loss: 1.0827 - val_acc: 0.6114\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.5509 - acc: 0.7741 - val_loss: 1.0831 - val_acc: 0.6343\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5419 - acc: 0.7779 - val_loss: 1.0938 - val_acc: 0.6286\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.5245 - acc: 0.7926 - val_loss: 1.1077 - val_acc: 0.6343\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5185 - acc: 0.7945 - val_loss: 1.1049 - val_acc: 0.6343\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5564 - acc: 0.7735 - val_loss: 1.0974 - val_acc: 0.6457\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5181 - acc: 0.7913 - val_loss: 1.1061 - val_acc: 0.6457\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5457 - acc: 0.7747 - val_loss: 1.1074 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.5271 - acc: 0.7837 - val_loss: 1.1026 - val_acc: 0.6400\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 104us/step - loss: 0.5226 - acc: 0.7913 - val_loss: 1.1000 - val_acc: 0.6400\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.5532 - acc: 0.7824 - val_loss: 1.1116 - val_acc: 0.6343\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.5346 - acc: 0.7920 - val_loss: 1.1220 - val_acc: 0.6457\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4952 - acc: 0.7951 - val_loss: 1.1173 - val_acc: 0.6229\n",
      "193/193 [==============================] - 0s 56us/step\n",
      "1742/1742 [==============================] - 0s 36us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.4806 - acc: 0.2980 - val_loss: 1.1988 - val_acc: 0.4743\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.2741 - acc: 0.4052 - val_loss: 1.0945 - val_acc: 0.5371\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.1812 - acc: 0.4907 - val_loss: 1.0377 - val_acc: 0.6114\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.1266 - acc: 0.5003 - val_loss: 1.0062 - val_acc: 0.6057\n",
      "Epoch 5/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 46us/step - loss: 1.1242 - acc: 0.4927 - val_loss: 0.9826 - val_acc: 0.6457\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0695 - acc: 0.5367 - val_loss: 0.9647 - val_acc: 0.6571\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.0468 - acc: 0.5475 - val_loss: 0.9452 - val_acc: 0.6514\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0319 - acc: 0.5546 - val_loss: 0.9312 - val_acc: 0.6800\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0009 - acc: 0.5795 - val_loss: 0.9313 - val_acc: 0.6800\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.0011 - acc: 0.5795 - val_loss: 0.9270 - val_acc: 0.6800\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9829 - acc: 0.5916 - val_loss: 0.9189 - val_acc: 0.6686\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9826 - acc: 0.5890 - val_loss: 0.9130 - val_acc: 0.6686\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9576 - acc: 0.5922 - val_loss: 0.9126 - val_acc: 0.6743\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9665 - acc: 0.5935 - val_loss: 0.9178 - val_acc: 0.6686\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9430 - acc: 0.6056 - val_loss: 0.9115 - val_acc: 0.6571\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9297 - acc: 0.6031 - val_loss: 0.9122 - val_acc: 0.6514\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9177 - acc: 0.6018 - val_loss: 0.9113 - val_acc: 0.6571\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9225 - acc: 0.6165 - val_loss: 0.8983 - val_acc: 0.6571\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.9120 - acc: 0.6228 - val_loss: 0.9072 - val_acc: 0.6400\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9062 - acc: 0.6299 - val_loss: 0.9088 - val_acc: 0.6571\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8924 - acc: 0.6394 - val_loss: 0.9010 - val_acc: 0.6629\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8871 - acc: 0.6286 - val_loss: 0.9079 - val_acc: 0.6857\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8884 - acc: 0.6362 - val_loss: 0.9063 - val_acc: 0.6800\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8608 - acc: 0.6369 - val_loss: 0.9014 - val_acc: 0.6857\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8885 - acc: 0.6452 - val_loss: 0.9040 - val_acc: 0.6686\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8721 - acc: 0.6433 - val_loss: 0.9094 - val_acc: 0.6743\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8540 - acc: 0.6465 - val_loss: 0.8987 - val_acc: 0.6743\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8831 - acc: 0.6318 - val_loss: 0.9025 - val_acc: 0.6686\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8336 - acc: 0.6407 - val_loss: 0.8999 - val_acc: 0.6571\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8454 - acc: 0.6516 - val_loss: 0.8939 - val_acc: 0.6571\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8658 - acc: 0.6414 - val_loss: 0.8913 - val_acc: 0.6743\n",
      "Epoch 32/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8474 - acc: 0.6592 - val_loss: 0.9009 - val_acc: 0.6686\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8456 - acc: 0.6656 - val_loss: 0.8978 - val_acc: 0.6800\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8404 - acc: 0.6599 - val_loss: 0.9013 - val_acc: 0.6514\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8339 - acc: 0.6688 - val_loss: 0.9053 - val_acc: 0.6400\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8155 - acc: 0.6784 - val_loss: 0.9165 - val_acc: 0.6514\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8163 - acc: 0.6669 - val_loss: 0.9269 - val_acc: 0.6400\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8329 - acc: 0.6611 - val_loss: 0.9265 - val_acc: 0.6286\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8155 - acc: 0.6548 - val_loss: 0.9195 - val_acc: 0.6343\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8078 - acc: 0.6803 - val_loss: 0.9187 - val_acc: 0.6286\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8232 - acc: 0.6592 - val_loss: 0.9188 - val_acc: 0.6571\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8054 - acc: 0.6822 - val_loss: 0.9159 - val_acc: 0.6457\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7967 - acc: 0.6777 - val_loss: 0.9132 - val_acc: 0.6229\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.8011 - acc: 0.6745 - val_loss: 0.9228 - val_acc: 0.6286\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8005 - acc: 0.6777 - val_loss: 0.9139 - val_acc: 0.6457\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8045 - acc: 0.6758 - val_loss: 0.9129 - val_acc: 0.6400\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8035 - acc: 0.6809 - val_loss: 0.9202 - val_acc: 0.6514\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7607 - acc: 0.7071 - val_loss: 0.9170 - val_acc: 0.6571\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7800 - acc: 0.6841 - val_loss: 0.9163 - val_acc: 0.6514\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8030 - acc: 0.6701 - val_loss: 0.9276 - val_acc: 0.6457\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7621 - acc: 0.7026 - val_loss: 0.9271 - val_acc: 0.6514\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7691 - acc: 0.6835 - val_loss: 0.9230 - val_acc: 0.6514\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7673 - acc: 0.6911 - val_loss: 0.9229 - val_acc: 0.6457\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7643 - acc: 0.6937 - val_loss: 0.9334 - val_acc: 0.6400\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7656 - acc: 0.6816 - val_loss: 0.9317 - val_acc: 0.6286\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7637 - acc: 0.6899 - val_loss: 0.9346 - val_acc: 0.6229\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7534 - acc: 0.7039 - val_loss: 0.9333 - val_acc: 0.6286\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7585 - acc: 0.6905 - val_loss: 0.9422 - val_acc: 0.6229\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7480 - acc: 0.7001 - val_loss: 0.9316 - val_acc: 0.6343\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7470 - acc: 0.7084 - val_loss: 0.9405 - val_acc: 0.6457\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7361 - acc: 0.7026 - val_loss: 0.9407 - val_acc: 0.6571\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7592 - acc: 0.6911 - val_loss: 0.9369 - val_acc: 0.6457\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7621 - acc: 0.6943 - val_loss: 0.9346 - val_acc: 0.6571\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7447 - acc: 0.7039 - val_loss: 0.9413 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7297 - acc: 0.7039 - val_loss: 0.9490 - val_acc: 0.6229\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7137 - acc: 0.7275 - val_loss: 0.9531 - val_acc: 0.6286\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7311 - acc: 0.7058 - val_loss: 0.9505 - val_acc: 0.6343\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7363 - acc: 0.7090 - val_loss: 0.9509 - val_acc: 0.6286\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7164 - acc: 0.7186 - val_loss: 0.9545 - val_acc: 0.6229\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7026 - acc: 0.7147 - val_loss: 0.9558 - val_acc: 0.6229\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6983 - acc: 0.7154 - val_loss: 0.9533 - val_acc: 0.6400\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7128 - acc: 0.7122 - val_loss: 0.9444 - val_acc: 0.6229\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7081 - acc: 0.7147 - val_loss: 0.9465 - val_acc: 0.6229\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7112 - acc: 0.7237 - val_loss: 0.9551 - val_acc: 0.6229\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6903 - acc: 0.7230 - val_loss: 0.9610 - val_acc: 0.6343\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7138 - acc: 0.7160 - val_loss: 0.9586 - val_acc: 0.6229\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6964 - acc: 0.7256 - val_loss: 0.9636 - val_acc: 0.6286\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.6837 - acc: 0.7313 - val_loss: 0.9638 - val_acc: 0.6286\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6965 - acc: 0.7173 - val_loss: 0.9585 - val_acc: 0.6286\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6799 - acc: 0.7473 - val_loss: 0.9633 - val_acc: 0.6343\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6956 - acc: 0.7122 - val_loss: 0.9602 - val_acc: 0.6457\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6889 - acc: 0.7211 - val_loss: 0.9680 - val_acc: 0.6400\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6695 - acc: 0.7447 - val_loss: 0.9693 - val_acc: 0.6457\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6829 - acc: 0.7256 - val_loss: 0.9609 - val_acc: 0.6571\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6822 - acc: 0.7307 - val_loss: 0.9621 - val_acc: 0.6286\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6913 - acc: 0.7218 - val_loss: 0.9669 - val_acc: 0.6343\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6701 - acc: 0.7281 - val_loss: 0.9647 - val_acc: 0.6514\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6578 - acc: 0.7390 - val_loss: 0.9778 - val_acc: 0.6400\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6594 - acc: 0.7447 - val_loss: 0.9746 - val_acc: 0.6343\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6914 - acc: 0.7237 - val_loss: 0.9787 - val_acc: 0.6286\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6648 - acc: 0.7326 - val_loss: 0.9703 - val_acc: 0.6400\n",
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6741 - acc: 0.7371 - val_loss: 0.9705 - val_acc: 0.6229\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6725 - acc: 0.7250 - val_loss: 0.9845 - val_acc: 0.6400\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6706 - acc: 0.7441 - val_loss: 0.9851 - val_acc: 0.6400\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6556 - acc: 0.7454 - val_loss: 0.9912 - val_acc: 0.6286\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6608 - acc: 0.7384 - val_loss: 0.9933 - val_acc: 0.6171\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6530 - acc: 0.7454 - val_loss: 0.9914 - val_acc: 0.6229\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6526 - acc: 0.7492 - val_loss: 0.9968 - val_acc: 0.6343\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6518 - acc: 0.7460 - val_loss: 0.9933 - val_acc: 0.6229\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6549 - acc: 0.7428 - val_loss: 0.9918 - val_acc: 0.6286\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6606 - acc: 0.7460 - val_loss: 0.9922 - val_acc: 0.6229\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6326 - acc: 0.7396 - val_loss: 1.0025 - val_acc: 0.6229\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6361 - acc: 0.7466 - val_loss: 1.0036 - val_acc: 0.6114\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6362 - acc: 0.7492 - val_loss: 1.0020 - val_acc: 0.6400\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6445 - acc: 0.7377 - val_loss: 0.9957 - val_acc: 0.6229\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6285 - acc: 0.7486 - val_loss: 0.9979 - val_acc: 0.6286\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6361 - acc: 0.7460 - val_loss: 1.0025 - val_acc: 0.6400\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6297 - acc: 0.7530 - val_loss: 1.0068 - val_acc: 0.6400\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6153 - acc: 0.7594 - val_loss: 1.0115 - val_acc: 0.6286\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6312 - acc: 0.7537 - val_loss: 1.0130 - val_acc: 0.6400\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6131 - acc: 0.7613 - val_loss: 1.0042 - val_acc: 0.6400\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6266 - acc: 0.7505 - val_loss: 1.0110 - val_acc: 0.6457\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6174 - acc: 0.7613 - val_loss: 1.0195 - val_acc: 0.6171\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6123 - acc: 0.7511 - val_loss: 1.0256 - val_acc: 0.6171\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6148 - acc: 0.7645 - val_loss: 1.0238 - val_acc: 0.6343\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6459 - acc: 0.7473 - val_loss: 1.0306 - val_acc: 0.6343\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5955 - acc: 0.7709 - val_loss: 1.0238 - val_acc: 0.6057\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6007 - acc: 0.7441 - val_loss: 1.0213 - val_acc: 0.6114\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6241 - acc: 0.7537 - val_loss: 1.0227 - val_acc: 0.6171\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5879 - acc: 0.7664 - val_loss: 1.0297 - val_acc: 0.6343\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5918 - acc: 0.7703 - val_loss: 1.0257 - val_acc: 0.6400\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5976 - acc: 0.7613 - val_loss: 1.0311 - val_acc: 0.6229\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5871 - acc: 0.7645 - val_loss: 1.0331 - val_acc: 0.6457\n",
      "Epoch 124/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5921 - acc: 0.7658 - val_loss: 1.0321 - val_acc: 0.6229\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5866 - acc: 0.7639 - val_loss: 1.0411 - val_acc: 0.6229\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5888 - acc: 0.7722 - val_loss: 1.0363 - val_acc: 0.6171\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5923 - acc: 0.7620 - val_loss: 1.0391 - val_acc: 0.6229\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6049 - acc: 0.7588 - val_loss: 1.0374 - val_acc: 0.6229\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5756 - acc: 0.7792 - val_loss: 1.0501 - val_acc: 0.6343\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5706 - acc: 0.7849 - val_loss: 1.0440 - val_acc: 0.6286\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5774 - acc: 0.7811 - val_loss: 1.0390 - val_acc: 0.6229\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5546 - acc: 0.7830 - val_loss: 1.0429 - val_acc: 0.6286\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5772 - acc: 0.7715 - val_loss: 1.0451 - val_acc: 0.6400\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5988 - acc: 0.7690 - val_loss: 1.0530 - val_acc: 0.6286\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5736 - acc: 0.7894 - val_loss: 1.0670 - val_acc: 0.6286\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5619 - acc: 0.7805 - val_loss: 1.0686 - val_acc: 0.6229\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5484 - acc: 0.7856 - val_loss: 1.0690 - val_acc: 0.6229\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5512 - acc: 0.7894 - val_loss: 1.0688 - val_acc: 0.6457\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5311 - acc: 0.7913 - val_loss: 1.0691 - val_acc: 0.6400\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5617 - acc: 0.7939 - val_loss: 1.0774 - val_acc: 0.6343\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5608 - acc: 0.7875 - val_loss: 1.0900 - val_acc: 0.6286\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5620 - acc: 0.7735 - val_loss: 1.0860 - val_acc: 0.6229\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.5373 - acc: 0.7913 - val_loss: 1.0799 - val_acc: 0.6400\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5638 - acc: 0.7900 - val_loss: 1.0724 - val_acc: 0.6343\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5352 - acc: 0.7951 - val_loss: 1.0695 - val_acc: 0.6457\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5564 - acc: 0.7875 - val_loss: 1.0780 - val_acc: 0.6286\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5566 - acc: 0.7811 - val_loss: 1.0695 - val_acc: 0.6400\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5500 - acc: 0.7862 - val_loss: 1.0729 - val_acc: 0.6171\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5245 - acc: 0.8034 - val_loss: 1.0763 - val_acc: 0.6343\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5450 - acc: 0.7856 - val_loss: 1.0819 - val_acc: 0.6457\n",
      "193/193 [==============================] - 0s 45us/step\n",
      "1742/1742 [==============================] - 0s 32us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/150\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5872 - acc: 0.2859 - val_loss: 1.2568 - val_acc: 0.4800\n",
      "Epoch 2/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.3228 - acc: 0.3867 - val_loss: 1.1304 - val_acc: 0.4743\n",
      "Epoch 3/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.2019 - acc: 0.4729 - val_loss: 1.0694 - val_acc: 0.5200\n",
      "Epoch 4/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.1251 - acc: 0.5112 - val_loss: 1.0406 - val_acc: 0.5314\n",
      "Epoch 5/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.1092 - acc: 0.5156 - val_loss: 1.0099 - val_acc: 0.5657\n",
      "Epoch 6/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0645 - acc: 0.5367 - val_loss: 0.9881 - val_acc: 0.5657\n",
      "Epoch 7/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0311 - acc: 0.5724 - val_loss: 0.9876 - val_acc: 0.5543\n",
      "Epoch 8/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0248 - acc: 0.5578 - val_loss: 0.9867 - val_acc: 0.5657\n",
      "Epoch 9/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9966 - acc: 0.5712 - val_loss: 0.9817 - val_acc: 0.5829\n",
      "Epoch 10/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0041 - acc: 0.5533 - val_loss: 0.9783 - val_acc: 0.5600\n",
      "Epoch 11/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9747 - acc: 0.5903 - val_loss: 0.9669 - val_acc: 0.5600\n",
      "Epoch 12/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9626 - acc: 0.5954 - val_loss: 0.9703 - val_acc: 0.5829\n",
      "Epoch 13/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.9505 - acc: 0.6011 - val_loss: 0.9645 - val_acc: 0.5657\n",
      "Epoch 14/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9267 - acc: 0.6139 - val_loss: 0.9650 - val_acc: 0.5657\n",
      "Epoch 15/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.9408 - acc: 0.6075 - val_loss: 0.9627 - val_acc: 0.5771\n",
      "Epoch 16/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9339 - acc: 0.5986 - val_loss: 0.9707 - val_acc: 0.5600\n",
      "Epoch 17/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9318 - acc: 0.6126 - val_loss: 0.9663 - val_acc: 0.5771\n",
      "Epoch 18/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8859 - acc: 0.6331 - val_loss: 0.9695 - val_acc: 0.5771\n",
      "Epoch 19/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8945 - acc: 0.6337 - val_loss: 0.9708 - val_acc: 0.5657\n",
      "Epoch 20/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9000 - acc: 0.6165 - val_loss: 0.9651 - val_acc: 0.5600\n",
      "Epoch 21/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8776 - acc: 0.6407 - val_loss: 0.9691 - val_acc: 0.5486\n",
      "Epoch 22/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9025 - acc: 0.6139 - val_loss: 0.9682 - val_acc: 0.5886\n",
      "Epoch 23/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8830 - acc: 0.6407 - val_loss: 0.9623 - val_acc: 0.5886\n",
      "Epoch 24/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8724 - acc: 0.6273 - val_loss: 0.9618 - val_acc: 0.5886\n",
      "Epoch 25/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8712 - acc: 0.6369 - val_loss: 0.9677 - val_acc: 0.5714\n",
      "Epoch 26/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8664 - acc: 0.6414 - val_loss: 0.9734 - val_acc: 0.5771\n",
      "Epoch 27/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8776 - acc: 0.6445 - val_loss: 0.9841 - val_acc: 0.5829\n",
      "Epoch 28/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8575 - acc: 0.6516 - val_loss: 0.9768 - val_acc: 0.5943\n",
      "Epoch 29/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8531 - acc: 0.6439 - val_loss: 0.9767 - val_acc: 0.5943\n",
      "Epoch 30/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8393 - acc: 0.6579 - val_loss: 0.9807 - val_acc: 0.5829\n",
      "Epoch 31/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8345 - acc: 0.6631 - val_loss: 0.9865 - val_acc: 0.5886\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8298 - acc: 0.6624 - val_loss: 0.9740 - val_acc: 0.5943\n",
      "Epoch 33/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8271 - acc: 0.6733 - val_loss: 0.9780 - val_acc: 0.6000\n",
      "Epoch 34/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8114 - acc: 0.6579 - val_loss: 0.9769 - val_acc: 0.6000\n",
      "Epoch 35/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8247 - acc: 0.6669 - val_loss: 0.9898 - val_acc: 0.5943\n",
      "Epoch 36/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8245 - acc: 0.6713 - val_loss: 0.9850 - val_acc: 0.5943\n",
      "Epoch 37/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8057 - acc: 0.6726 - val_loss: 0.9831 - val_acc: 0.6000\n",
      "Epoch 38/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8023 - acc: 0.6656 - val_loss: 0.9731 - val_acc: 0.6000\n",
      "Epoch 39/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8017 - acc: 0.6643 - val_loss: 0.9999 - val_acc: 0.5829\n",
      "Epoch 40/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8150 - acc: 0.6707 - val_loss: 1.0068 - val_acc: 0.5829\n",
      "Epoch 41/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7990 - acc: 0.6835 - val_loss: 1.0016 - val_acc: 0.5771\n",
      "Epoch 42/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7909 - acc: 0.6873 - val_loss: 0.9976 - val_acc: 0.5943\n",
      "Epoch 43/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7890 - acc: 0.6854 - val_loss: 0.9974 - val_acc: 0.5886\n",
      "Epoch 44/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7933 - acc: 0.6720 - val_loss: 1.0126 - val_acc: 0.5771\n",
      "Epoch 45/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7939 - acc: 0.6765 - val_loss: 1.0097 - val_acc: 0.5714\n",
      "Epoch 46/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7871 - acc: 0.6911 - val_loss: 1.0003 - val_acc: 0.5829\n",
      "Epoch 47/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7774 - acc: 0.6816 - val_loss: 1.0123 - val_acc: 0.5829\n",
      "Epoch 48/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7907 - acc: 0.6720 - val_loss: 1.0064 - val_acc: 0.5943\n",
      "Epoch 49/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7722 - acc: 0.6809 - val_loss: 1.0134 - val_acc: 0.6000\n",
      "Epoch 50/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7675 - acc: 0.6841 - val_loss: 1.0173 - val_acc: 0.5829\n",
      "Epoch 51/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7719 - acc: 0.6841 - val_loss: 1.0238 - val_acc: 0.5943\n",
      "Epoch 52/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7481 - acc: 0.7033 - val_loss: 1.0193 - val_acc: 0.6057\n",
      "Epoch 53/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7629 - acc: 0.6950 - val_loss: 1.0303 - val_acc: 0.6000\n",
      "Epoch 54/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7447 - acc: 0.6994 - val_loss: 1.0283 - val_acc: 0.5943\n",
      "Epoch 55/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7628 - acc: 0.6860 - val_loss: 1.0247 - val_acc: 0.5829\n",
      "Epoch 56/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7438 - acc: 0.6937 - val_loss: 1.0395 - val_acc: 0.5886\n",
      "Epoch 57/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7626 - acc: 0.6867 - val_loss: 1.0301 - val_acc: 0.5943\n",
      "Epoch 58/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7515 - acc: 0.6950 - val_loss: 1.0358 - val_acc: 0.5771\n",
      "Epoch 59/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7333 - acc: 0.7116 - val_loss: 1.0445 - val_acc: 0.5657\n",
      "Epoch 60/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7403 - acc: 0.6943 - val_loss: 1.0239 - val_acc: 0.5943\n",
      "Epoch 61/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7389 - acc: 0.7045 - val_loss: 1.0321 - val_acc: 0.5886\n",
      "Epoch 62/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7189 - acc: 0.7071 - val_loss: 1.0455 - val_acc: 0.5886\n",
      "Epoch 63/150\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7195 - acc: 0.7147 - val_loss: 1.0480 - val_acc: 0.5829\n",
      "Epoch 64/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7316 - acc: 0.7001 - val_loss: 1.0410 - val_acc: 0.5829\n",
      "Epoch 65/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7227 - acc: 0.7045 - val_loss: 1.0340 - val_acc: 0.5943\n",
      "Epoch 66/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7158 - acc: 0.7186 - val_loss: 1.0411 - val_acc: 0.5886\n",
      "Epoch 67/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7324 - acc: 0.7077 - val_loss: 1.0427 - val_acc: 0.5771\n",
      "Epoch 68/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7113 - acc: 0.7173 - val_loss: 1.0379 - val_acc: 0.5771\n",
      "Epoch 69/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7306 - acc: 0.7103 - val_loss: 1.0401 - val_acc: 0.5829\n",
      "Epoch 70/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7101 - acc: 0.7205 - val_loss: 1.0388 - val_acc: 0.5886\n",
      "Epoch 71/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7097 - acc: 0.7090 - val_loss: 1.0422 - val_acc: 0.5771\n",
      "Epoch 72/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7022 - acc: 0.7179 - val_loss: 1.0432 - val_acc: 0.5886\n",
      "Epoch 73/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7066 - acc: 0.7237 - val_loss: 1.0592 - val_acc: 0.5886\n",
      "Epoch 74/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7129 - acc: 0.7364 - val_loss: 1.0491 - val_acc: 0.5886\n",
      "Epoch 75/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6985 - acc: 0.7147 - val_loss: 1.0669 - val_acc: 0.5714\n",
      "Epoch 76/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6956 - acc: 0.7205 - val_loss: 1.0596 - val_acc: 0.5714\n",
      "Epoch 77/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6841 - acc: 0.7243 - val_loss: 1.0459 - val_acc: 0.5943\n",
      "Epoch 78/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7014 - acc: 0.7250 - val_loss: 1.0505 - val_acc: 0.5943\n",
      "Epoch 79/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6975 - acc: 0.7186 - val_loss: 1.0706 - val_acc: 0.5714\n",
      "Epoch 80/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6881 - acc: 0.7256 - val_loss: 1.0681 - val_acc: 0.5771\n",
      "Epoch 81/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6714 - acc: 0.7384 - val_loss: 1.0583 - val_acc: 0.5771\n",
      "Epoch 82/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6828 - acc: 0.7230 - val_loss: 1.0476 - val_acc: 0.6000\n",
      "Epoch 83/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6656 - acc: 0.7473 - val_loss: 1.0616 - val_acc: 0.5829\n",
      "Epoch 84/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6667 - acc: 0.7422 - val_loss: 1.0550 - val_acc: 0.5829\n",
      "Epoch 85/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6786 - acc: 0.7224 - val_loss: 1.0643 - val_acc: 0.5886\n",
      "Epoch 86/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6929 - acc: 0.7173 - val_loss: 1.0857 - val_acc: 0.5771\n",
      "Epoch 87/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6669 - acc: 0.7358 - val_loss: 1.0797 - val_acc: 0.5943\n",
      "Epoch 88/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6696 - acc: 0.7371 - val_loss: 1.0907 - val_acc: 0.5943\n",
      "Epoch 89/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6648 - acc: 0.7237 - val_loss: 1.0912 - val_acc: 0.5829\n",
      "Epoch 90/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6606 - acc: 0.7320 - val_loss: 1.0892 - val_acc: 0.5943\n",
      "Epoch 91/150\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6657 - acc: 0.7524 - val_loss: 1.0827 - val_acc: 0.5886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/150\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6588 - acc: 0.7422 - val_loss: 1.0807 - val_acc: 0.5886\n",
      "Epoch 93/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6565 - acc: 0.7498 - val_loss: 1.0939 - val_acc: 0.5886\n",
      "Epoch 94/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6321 - acc: 0.7511 - val_loss: 1.0896 - val_acc: 0.5886\n",
      "Epoch 95/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6675 - acc: 0.7301 - val_loss: 1.0825 - val_acc: 0.5886\n",
      "Epoch 96/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6292 - acc: 0.7377 - val_loss: 1.0908 - val_acc: 0.5943\n",
      "Epoch 97/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6480 - acc: 0.7364 - val_loss: 1.0967 - val_acc: 0.5714\n",
      "Epoch 98/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6389 - acc: 0.7518 - val_loss: 1.1043 - val_acc: 0.5886\n",
      "Epoch 99/150\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6463 - acc: 0.7543 - val_loss: 1.0845 - val_acc: 0.5829\n",
      "Epoch 100/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6424 - acc: 0.7479 - val_loss: 1.1117 - val_acc: 0.5771\n",
      "Epoch 101/150\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6553 - acc: 0.7454 - val_loss: 1.0906 - val_acc: 0.5886\n",
      "Epoch 102/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6232 - acc: 0.7632 - val_loss: 1.0877 - val_acc: 0.5829\n",
      "Epoch 103/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6167 - acc: 0.7677 - val_loss: 1.0923 - val_acc: 0.5886\n",
      "Epoch 104/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6422 - acc: 0.7428 - val_loss: 1.0948 - val_acc: 0.5886\n",
      "Epoch 105/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6160 - acc: 0.7626 - val_loss: 1.0996 - val_acc: 0.5829\n",
      "Epoch 106/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6317 - acc: 0.7454 - val_loss: 1.1097 - val_acc: 0.5829\n",
      "Epoch 107/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6167 - acc: 0.7613 - val_loss: 1.1136 - val_acc: 0.5829\n",
      "Epoch 108/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5995 - acc: 0.7715 - val_loss: 1.1109 - val_acc: 0.5829\n",
      "Epoch 109/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6137 - acc: 0.7664 - val_loss: 1.1088 - val_acc: 0.5771\n",
      "Epoch 110/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5917 - acc: 0.7709 - val_loss: 1.1178 - val_acc: 0.5771\n",
      "Epoch 111/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6029 - acc: 0.7594 - val_loss: 1.1316 - val_acc: 0.5657\n",
      "Epoch 112/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6126 - acc: 0.7569 - val_loss: 1.1259 - val_acc: 0.5771\n",
      "Epoch 113/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5958 - acc: 0.7741 - val_loss: 1.1280 - val_acc: 0.5771\n",
      "Epoch 114/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6177 - acc: 0.7639 - val_loss: 1.1305 - val_acc: 0.5886\n",
      "Epoch 115/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6086 - acc: 0.7518 - val_loss: 1.1265 - val_acc: 0.5771\n",
      "Epoch 116/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5886 - acc: 0.7683 - val_loss: 1.1246 - val_acc: 0.5714\n",
      "Epoch 117/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5816 - acc: 0.7703 - val_loss: 1.1179 - val_acc: 0.5829\n",
      "Epoch 118/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5938 - acc: 0.7626 - val_loss: 1.1320 - val_acc: 0.5600\n",
      "Epoch 119/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5716 - acc: 0.7779 - val_loss: 1.1298 - val_acc: 0.5771\n",
      "Epoch 120/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6014 - acc: 0.7658 - val_loss: 1.1269 - val_acc: 0.5714\n",
      "Epoch 121/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5891 - acc: 0.7722 - val_loss: 1.1276 - val_acc: 0.5657\n",
      "Epoch 122/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5735 - acc: 0.7747 - val_loss: 1.1255 - val_acc: 0.5714\n",
      "Epoch 123/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5967 - acc: 0.7773 - val_loss: 1.1372 - val_acc: 0.5657\n",
      "Epoch 124/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5948 - acc: 0.7594 - val_loss: 1.1359 - val_acc: 0.5600\n",
      "Epoch 125/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5878 - acc: 0.7792 - val_loss: 1.1362 - val_acc: 0.5657\n",
      "Epoch 126/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5857 - acc: 0.7747 - val_loss: 1.1410 - val_acc: 0.5714\n",
      "Epoch 127/150\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5685 - acc: 0.7920 - val_loss: 1.1541 - val_acc: 0.5714\n",
      "Epoch 128/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5931 - acc: 0.7645 - val_loss: 1.1432 - val_acc: 0.5771\n",
      "Epoch 129/150\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5708 - acc: 0.7862 - val_loss: 1.1548 - val_acc: 0.5714\n",
      "Epoch 130/150\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.5758 - acc: 0.7837 - val_loss: 1.1373 - val_acc: 0.5771\n",
      "Epoch 131/150\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5582 - acc: 0.7824 - val_loss: 1.1344 - val_acc: 0.5771\n",
      "Epoch 132/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5596 - acc: 0.7894 - val_loss: 1.1364 - val_acc: 0.5714\n",
      "Epoch 133/150\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5712 - acc: 0.7741 - val_loss: 1.1372 - val_acc: 0.5714\n",
      "Epoch 134/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5722 - acc: 0.7690 - val_loss: 1.1419 - val_acc: 0.5657\n",
      "Epoch 135/150\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.5700 - acc: 0.7728 - val_loss: 1.1542 - val_acc: 0.5714\n",
      "Epoch 136/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5459 - acc: 0.7817 - val_loss: 1.1721 - val_acc: 0.5600\n",
      "Epoch 137/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5490 - acc: 0.7837 - val_loss: 1.1744 - val_acc: 0.5714\n",
      "Epoch 138/150\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5627 - acc: 0.7881 - val_loss: 1.1636 - val_acc: 0.5543\n",
      "Epoch 139/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5665 - acc: 0.7849 - val_loss: 1.1716 - val_acc: 0.5543\n",
      "Epoch 140/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5523 - acc: 0.8003 - val_loss: 1.1724 - val_acc: 0.5771\n",
      "Epoch 141/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5541 - acc: 0.7888 - val_loss: 1.1734 - val_acc: 0.5771\n",
      "Epoch 142/150\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5420 - acc: 0.7843 - val_loss: 1.1675 - val_acc: 0.5829\n",
      "Epoch 143/150\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5391 - acc: 0.7996 - val_loss: 1.1806 - val_acc: 0.5829\n",
      "Epoch 144/150\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5503 - acc: 0.8015 - val_loss: 1.1755 - val_acc: 0.5829\n",
      "Epoch 145/150\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5509 - acc: 0.7728 - val_loss: 1.1939 - val_acc: 0.5657\n",
      "Epoch 146/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5369 - acc: 0.7971 - val_loss: 1.1990 - val_acc: 0.5543\n",
      "Epoch 147/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5299 - acc: 0.7964 - val_loss: 1.2004 - val_acc: 0.5657\n",
      "Epoch 148/150\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5414 - acc: 0.7817 - val_loss: 1.2088 - val_acc: 0.5657\n",
      "Epoch 149/150\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5242 - acc: 0.8060 - val_loss: 1.1965 - val_acc: 0.5600\n",
      "Epoch 150/150\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5108 - acc: 0.8047 - val_loss: 1.2048 - val_acc: 0.5600\n",
      "193/193 [==============================] - 0s 58us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742/1742 [==============================] - 0s 34us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.5039 - acc: 0.3059 - val_loss: 1.2263 - val_acc: 0.4857\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.3512 - acc: 0.3729 - val_loss: 1.1264 - val_acc: 0.5771\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.2318 - acc: 0.4496 - val_loss: 1.0723 - val_acc: 0.5771\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.2035 - acc: 0.4534 - val_loss: 1.0384 - val_acc: 0.6057\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.1787 - acc: 0.4738 - val_loss: 1.0166 - val_acc: 0.6171\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1517 - acc: 0.4936 - val_loss: 0.9953 - val_acc: 0.6057\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.1140 - acc: 0.5134 - val_loss: 0.9839 - val_acc: 0.6286\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0909 - acc: 0.5300 - val_loss: 0.9678 - val_acc: 0.6629\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0902 - acc: 0.5268 - val_loss: 0.9531 - val_acc: 0.6457\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0707 - acc: 0.5370 - val_loss: 0.9418 - val_acc: 0.6571\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.0538 - acc: 0.5530 - val_loss: 0.9386 - val_acc: 0.6743\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0332 - acc: 0.5504 - val_loss: 0.9354 - val_acc: 0.6629\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0356 - acc: 0.5536 - val_loss: 0.9260 - val_acc: 0.6857\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0057 - acc: 0.5785 - val_loss: 0.9197 - val_acc: 0.6743\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0018 - acc: 0.5587 - val_loss: 0.9155 - val_acc: 0.6800\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0004 - acc: 0.5683 - val_loss: 0.9114 - val_acc: 0.6686\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9843 - acc: 0.5805 - val_loss: 0.9132 - val_acc: 0.6743\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0023 - acc: 0.5626 - val_loss: 0.9103 - val_acc: 0.6629\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9837 - acc: 0.5837 - val_loss: 0.9081 - val_acc: 0.6686\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9693 - acc: 0.5747 - val_loss: 0.9074 - val_acc: 0.6629\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9854 - acc: 0.5817 - val_loss: 0.9066 - val_acc: 0.6914\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9503 - acc: 0.5900 - val_loss: 0.9100 - val_acc: 0.6857\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9560 - acc: 0.5958 - val_loss: 0.9060 - val_acc: 0.6743\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9660 - acc: 0.5837 - val_loss: 0.9033 - val_acc: 0.6514\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9483 - acc: 0.5951 - val_loss: 0.8987 - val_acc: 0.6457\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9302 - acc: 0.6034 - val_loss: 0.8953 - val_acc: 0.6571\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9264 - acc: 0.6079 - val_loss: 0.8941 - val_acc: 0.6571\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9468 - acc: 0.5990 - val_loss: 0.8943 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9420 - acc: 0.5990 - val_loss: 0.8926 - val_acc: 0.6514\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.9379 - acc: 0.5983 - val_loss: 0.8861 - val_acc: 0.6629\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9162 - acc: 0.6201 - val_loss: 0.8857 - val_acc: 0.6571\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9340 - acc: 0.6207 - val_loss: 0.8890 - val_acc: 0.6514\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9182 - acc: 0.6162 - val_loss: 0.8933 - val_acc: 0.6514\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9195 - acc: 0.6149 - val_loss: 0.8945 - val_acc: 0.6457\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8947 - acc: 0.6239 - val_loss: 0.8945 - val_acc: 0.6571\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8983 - acc: 0.6181 - val_loss: 0.8924 - val_acc: 0.6514\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9028 - acc: 0.6188 - val_loss: 0.8937 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8907 - acc: 0.6367 - val_loss: 0.8873 - val_acc: 0.6800\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8948 - acc: 0.6245 - val_loss: 0.8966 - val_acc: 0.6571\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8952 - acc: 0.6232 - val_loss: 0.8944 - val_acc: 0.6571\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8797 - acc: 0.6284 - val_loss: 0.8923 - val_acc: 0.6571\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 45us/step - loss: 0.8840 - acc: 0.6335 - val_loss: 0.8924 - val_acc: 0.6629\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8849 - acc: 0.6373 - val_loss: 0.8921 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8695 - acc: 0.6488 - val_loss: 0.8929 - val_acc: 0.6571\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8794 - acc: 0.6398 - val_loss: 0.8872 - val_acc: 0.6686\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8602 - acc: 0.6207 - val_loss: 0.8873 - val_acc: 0.6571\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8756 - acc: 0.6315 - val_loss: 0.8886 - val_acc: 0.6686\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8860 - acc: 0.6315 - val_loss: 0.8860 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8557 - acc: 0.6456 - val_loss: 0.8893 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8477 - acc: 0.6405 - val_loss: 0.8866 - val_acc: 0.6629\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8569 - acc: 0.6398 - val_loss: 0.8897 - val_acc: 0.6571\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8512 - acc: 0.6481 - val_loss: 0.8948 - val_acc: 0.6571\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8712 - acc: 0.6418 - val_loss: 0.8958 - val_acc: 0.6629\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8694 - acc: 0.6424 - val_loss: 0.8895 - val_acc: 0.6686\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8435 - acc: 0.6462 - val_loss: 0.8962 - val_acc: 0.6686\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8345 - acc: 0.6545 - val_loss: 0.8963 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8706 - acc: 0.6520 - val_loss: 0.8936 - val_acc: 0.6629\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8454 - acc: 0.6533 - val_loss: 0.8937 - val_acc: 0.6686\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8485 - acc: 0.6539 - val_loss: 0.8959 - val_acc: 0.6629\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8217 - acc: 0.6552 - val_loss: 0.8958 - val_acc: 0.6629\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8399 - acc: 0.6520 - val_loss: 0.8929 - val_acc: 0.6629\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8314 - acc: 0.6564 - val_loss: 0.8927 - val_acc: 0.6629\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8311 - acc: 0.6622 - val_loss: 0.9005 - val_acc: 0.6629\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8193 - acc: 0.6603 - val_loss: 0.8954 - val_acc: 0.6629\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8197 - acc: 0.6654 - val_loss: 0.8944 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8154 - acc: 0.6660 - val_loss: 0.8962 - val_acc: 0.6571\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8355 - acc: 0.6660 - val_loss: 0.9002 - val_acc: 0.6571\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8163 - acc: 0.6533 - val_loss: 0.8988 - val_acc: 0.6514\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.8146 - acc: 0.6756 - val_loss: 0.9055 - val_acc: 0.6514\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.8281 - acc: 0.6558 - val_loss: 0.9139 - val_acc: 0.6457\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7975 - acc: 0.6750 - val_loss: 0.9049 - val_acc: 0.6629\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8185 - acc: 0.6782 - val_loss: 0.9066 - val_acc: 0.6571\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8106 - acc: 0.6679 - val_loss: 0.9102 - val_acc: 0.6457\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7980 - acc: 0.6762 - val_loss: 0.9088 - val_acc: 0.6457\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8176 - acc: 0.6584 - val_loss: 0.9088 - val_acc: 0.6514\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8120 - acc: 0.6654 - val_loss: 0.9068 - val_acc: 0.6400\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8128 - acc: 0.6782 - val_loss: 0.9055 - val_acc: 0.6400\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7928 - acc: 0.6667 - val_loss: 0.9069 - val_acc: 0.6400\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8007 - acc: 0.6686 - val_loss: 0.9131 - val_acc: 0.6400\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7948 - acc: 0.6884 - val_loss: 0.9116 - val_acc: 0.6457\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8058 - acc: 0.6705 - val_loss: 0.9061 - val_acc: 0.6571\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8039 - acc: 0.6609 - val_loss: 0.9093 - val_acc: 0.6343\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7806 - acc: 0.6833 - val_loss: 0.9133 - val_acc: 0.6400\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.7870 - acc: 0.6718 - val_loss: 0.9096 - val_acc: 0.6400\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7968 - acc: 0.6743 - val_loss: 0.9142 - val_acc: 0.6400\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7773 - acc: 0.6833 - val_loss: 0.9140 - val_acc: 0.6457\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7807 - acc: 0.6852 - val_loss: 0.9143 - val_acc: 0.6400\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.7873 - acc: 0.6718 - val_loss: 0.9109 - val_acc: 0.6457\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.7863 - acc: 0.6788 - val_loss: 0.9080 - val_acc: 0.6457\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7707 - acc: 0.6807 - val_loss: 0.9174 - val_acc: 0.6343\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 119us/step - loss: 0.7733 - acc: 0.6852 - val_loss: 0.9201 - val_acc: 0.6400\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7728 - acc: 0.6858 - val_loss: 0.9186 - val_acc: 0.6343\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7778 - acc: 0.6922 - val_loss: 0.9146 - val_acc: 0.6343\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7750 - acc: 0.6890 - val_loss: 0.9190 - val_acc: 0.6343\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7629 - acc: 0.6935 - val_loss: 0.9219 - val_acc: 0.6286\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7638 - acc: 0.6871 - val_loss: 0.9231 - val_acc: 0.6343\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 114us/step - loss: 0.7514 - acc: 0.6980 - val_loss: 0.9301 - val_acc: 0.6343\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7669 - acc: 0.6967 - val_loss: 0.9328 - val_acc: 0.6343\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7574 - acc: 0.6890 - val_loss: 0.9327 - val_acc: 0.6343\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7850 - acc: 0.6948 - val_loss: 0.9355 - val_acc: 0.6343\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7583 - acc: 0.6948 - val_loss: 0.9330 - val_acc: 0.6400\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7857 - acc: 0.6877 - val_loss: 0.9314 - val_acc: 0.6286\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7590 - acc: 0.6986 - val_loss: 0.9271 - val_acc: 0.6286\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7471 - acc: 0.6980 - val_loss: 0.9177 - val_acc: 0.6343\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7555 - acc: 0.6922 - val_loss: 0.9246 - val_acc: 0.6343\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7457 - acc: 0.7024 - val_loss: 0.9231 - val_acc: 0.6286\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7499 - acc: 0.7082 - val_loss: 0.9257 - val_acc: 0.6286\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7644 - acc: 0.6826 - val_loss: 0.9245 - val_acc: 0.6343\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7588 - acc: 0.6922 - val_loss: 0.9273 - val_acc: 0.6343\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.7358 - acc: 0.7120 - val_loss: 0.9291 - val_acc: 0.6286\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7478 - acc: 0.6935 - val_loss: 0.9272 - val_acc: 0.6343\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7539 - acc: 0.7050 - val_loss: 0.9233 - val_acc: 0.6400\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7448 - acc: 0.6992 - val_loss: 0.9270 - val_acc: 0.6229\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7226 - acc: 0.7095 - val_loss: 0.9302 - val_acc: 0.6229\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7370 - acc: 0.6960 - val_loss: 0.9243 - val_acc: 0.6286\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7327 - acc: 0.7158 - val_loss: 0.9248 - val_acc: 0.6400\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7390 - acc: 0.7075 - val_loss: 0.9266 - val_acc: 0.6457\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7245 - acc: 0.7101 - val_loss: 0.9327 - val_acc: 0.6286\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7457 - acc: 0.7018 - val_loss: 0.9334 - val_acc: 0.6286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7398 - acc: 0.7056 - val_loss: 0.9383 - val_acc: 0.6286\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7256 - acc: 0.7120 - val_loss: 0.9383 - val_acc: 0.6343\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7330 - acc: 0.7133 - val_loss: 0.9361 - val_acc: 0.6286\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7346 - acc: 0.7011 - val_loss: 0.9447 - val_acc: 0.6343\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7216 - acc: 0.7075 - val_loss: 0.9464 - val_acc: 0.6286\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7423 - acc: 0.7031 - val_loss: 0.9443 - val_acc: 0.6343\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7104 - acc: 0.7197 - val_loss: 0.9456 - val_acc: 0.6400\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7045 - acc: 0.7126 - val_loss: 0.9479 - val_acc: 0.6343\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7020 - acc: 0.7178 - val_loss: 0.9462 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7209 - acc: 0.7133 - val_loss: 0.9484 - val_acc: 0.6400\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7192 - acc: 0.7063 - val_loss: 0.9422 - val_acc: 0.6514\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6965 - acc: 0.7331 - val_loss: 0.9420 - val_acc: 0.6400\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7087 - acc: 0.7190 - val_loss: 0.9502 - val_acc: 0.6400\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6992 - acc: 0.7101 - val_loss: 0.9519 - val_acc: 0.6286\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6966 - acc: 0.7165 - val_loss: 0.9456 - val_acc: 0.6343\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6998 - acc: 0.7356 - val_loss: 0.9514 - val_acc: 0.6343\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7246 - acc: 0.6986 - val_loss: 0.9526 - val_acc: 0.6343\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7082 - acc: 0.7209 - val_loss: 0.9506 - val_acc: 0.6286\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6862 - acc: 0.7305 - val_loss: 0.9549 - val_acc: 0.6286\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6919 - acc: 0.7216 - val_loss: 0.9578 - val_acc: 0.6286\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7154 - acc: 0.7146 - val_loss: 0.9594 - val_acc: 0.6400\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6996 - acc: 0.7254 - val_loss: 0.9548 - val_acc: 0.6400\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6905 - acc: 0.7280 - val_loss: 0.9517 - val_acc: 0.6343\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6859 - acc: 0.7331 - val_loss: 0.9606 - val_acc: 0.6286\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7063 - acc: 0.7133 - val_loss: 0.9542 - val_acc: 0.6343\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6879 - acc: 0.7286 - val_loss: 0.9568 - val_acc: 0.6286\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6901 - acc: 0.7216 - val_loss: 0.9547 - val_acc: 0.6286\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6873 - acc: 0.7261 - val_loss: 0.9582 - val_acc: 0.6343\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6813 - acc: 0.7241 - val_loss: 0.9594 - val_acc: 0.6400\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6919 - acc: 0.7299 - val_loss: 0.9567 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6761 - acc: 0.7407 - val_loss: 0.9597 - val_acc: 0.6343\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6765 - acc: 0.7286 - val_loss: 0.9640 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6825 - acc: 0.7350 - val_loss: 0.9602 - val_acc: 0.6400\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6984 - acc: 0.7273 - val_loss: 0.9609 - val_acc: 0.6457\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6679 - acc: 0.7356 - val_loss: 0.9669 - val_acc: 0.6343\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6822 - acc: 0.7433 - val_loss: 0.9699 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6716 - acc: 0.7337 - val_loss: 0.9678 - val_acc: 0.6229\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7043 - acc: 0.7203 - val_loss: 0.9659 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 102us/step - loss: 0.6800 - acc: 0.7363 - val_loss: 0.9609 - val_acc: 0.6514\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6795 - acc: 0.7248 - val_loss: 0.9679 - val_acc: 0.6286\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6557 - acc: 0.7388 - val_loss: 0.9773 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6687 - acc: 0.7382 - val_loss: 0.9780 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6766 - acc: 0.7184 - val_loss: 0.9752 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6834 - acc: 0.7273 - val_loss: 0.9781 - val_acc: 0.6286\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6684 - acc: 0.7478 - val_loss: 0.9819 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6587 - acc: 0.7382 - val_loss: 0.9886 - val_acc: 0.6343\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6724 - acc: 0.7337 - val_loss: 0.9857 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6571 - acc: 0.7478 - val_loss: 0.9843 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6589 - acc: 0.7369 - val_loss: 0.9834 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6614 - acc: 0.7490 - val_loss: 0.9819 - val_acc: 0.6400\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6599 - acc: 0.7261 - val_loss: 0.9823 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6603 - acc: 0.7439 - val_loss: 0.9858 - val_acc: 0.6571\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6428 - acc: 0.7497 - val_loss: 0.9993 - val_acc: 0.6400\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6553 - acc: 0.7363 - val_loss: 0.9969 - val_acc: 0.6457\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6530 - acc: 0.7497 - val_loss: 0.9955 - val_acc: 0.6343\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6592 - acc: 0.7465 - val_loss: 0.9915 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6463 - acc: 0.7401 - val_loss: 0.9909 - val_acc: 0.6400\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6457 - acc: 0.7439 - val_loss: 0.9883 - val_acc: 0.6457\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6452 - acc: 0.7497 - val_loss: 0.9914 - val_acc: 0.6457\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6584 - acc: 0.7465 - val_loss: 0.9995 - val_acc: 0.6229\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6447 - acc: 0.7427 - val_loss: 0.9991 - val_acc: 0.6286\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6447 - acc: 0.7490 - val_loss: 1.0034 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6404 - acc: 0.7446 - val_loss: 0.9953 - val_acc: 0.6571\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6474 - acc: 0.7395 - val_loss: 0.9961 - val_acc: 0.6514\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6375 - acc: 0.7516 - val_loss: 1.0039 - val_acc: 0.6286\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 97us/step - loss: 0.6393 - acc: 0.7478 - val_loss: 0.9995 - val_acc: 0.6286\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6323 - acc: 0.7625 - val_loss: 0.9955 - val_acc: 0.6343\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6322 - acc: 0.7573 - val_loss: 0.9957 - val_acc: 0.6229\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6507 - acc: 0.7458 - val_loss: 0.9921 - val_acc: 0.6229\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6455 - acc: 0.7388 - val_loss: 0.9910 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6393 - acc: 0.7548 - val_loss: 0.9980 - val_acc: 0.6286\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6268 - acc: 0.7471 - val_loss: 1.0069 - val_acc: 0.6114\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6195 - acc: 0.7542 - val_loss: 1.0023 - val_acc: 0.6229\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6188 - acc: 0.7688 - val_loss: 1.0047 - val_acc: 0.6343\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6248 - acc: 0.7605 - val_loss: 1.0074 - val_acc: 0.6343\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6453 - acc: 0.7484 - val_loss: 1.0062 - val_acc: 0.6286\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6340 - acc: 0.7529 - val_loss: 1.0079 - val_acc: 0.6286\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6358 - acc: 0.7529 - val_loss: 1.0019 - val_acc: 0.6400\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6228 - acc: 0.7497 - val_loss: 0.9973 - val_acc: 0.6343\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6344 - acc: 0.7516 - val_loss: 0.9997 - val_acc: 0.6400\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.6387 - acc: 0.7401 - val_loss: 1.0011 - val_acc: 0.6457\n",
      "194/194 [==============================] - 0s 50us/step\n",
      "1741/1741 [==============================] - 0s 32us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 9s 6ms/step - loss: 1.4978 - acc: 0.2976 - val_loss: 1.2388 - val_acc: 0.4571\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.3188 - acc: 0.3902 - val_loss: 1.1419 - val_acc: 0.5657\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.2702 - acc: 0.4272 - val_loss: 1.0857 - val_acc: 0.6057\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 1.2129 - acc: 0.4623 - val_loss: 1.0468 - val_acc: 0.6171\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.1582 - acc: 0.4898 - val_loss: 1.0200 - val_acc: 0.6229\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1291 - acc: 0.4955 - val_loss: 1.0055 - val_acc: 0.6457\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1190 - acc: 0.5077 - val_loss: 0.9933 - val_acc: 0.6286\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0844 - acc: 0.5377 - val_loss: 0.9741 - val_acc: 0.6343\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0805 - acc: 0.5300 - val_loss: 0.9606 - val_acc: 0.6571\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0410 - acc: 0.5607 - val_loss: 0.9536 - val_acc: 0.6514\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 1.0379 - acc: 0.5575 - val_loss: 0.9409 - val_acc: 0.6457\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0245 - acc: 0.5715 - val_loss: 0.9330 - val_acc: 0.6571\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0298 - acc: 0.5645 - val_loss: 0.9278 - val_acc: 0.6457\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0148 - acc: 0.5722 - val_loss: 0.9285 - val_acc: 0.6629\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9971 - acc: 0.5760 - val_loss: 0.9249 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9892 - acc: 0.5754 - val_loss: 0.9190 - val_acc: 0.6629\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9777 - acc: 0.5881 - val_loss: 0.9099 - val_acc: 0.6686\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9756 - acc: 0.5817 - val_loss: 0.9056 - val_acc: 0.6629\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9908 - acc: 0.5722 - val_loss: 0.9061 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9520 - acc: 0.6188 - val_loss: 0.9024 - val_acc: 0.6686\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9638 - acc: 0.5849 - val_loss: 0.9012 - val_acc: 0.6571\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9649 - acc: 0.5875 - val_loss: 0.9030 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9364 - acc: 0.6175 - val_loss: 0.9039 - val_acc: 0.6629\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.9450 - acc: 0.6041 - val_loss: 0.9037 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9364 - acc: 0.5977 - val_loss: 0.8926 - val_acc: 0.6686\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9229 - acc: 0.6041 - val_loss: 0.8880 - val_acc: 0.6571\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9216 - acc: 0.5913 - val_loss: 0.8900 - val_acc: 0.6800\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9219 - acc: 0.5958 - val_loss: 0.8911 - val_acc: 0.6743\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9277 - acc: 0.6105 - val_loss: 0.8922 - val_acc: 0.6629\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9008 - acc: 0.6277 - val_loss: 0.8948 - val_acc: 0.6800\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9215 - acc: 0.6111 - val_loss: 0.8965 - val_acc: 0.6857\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9099 - acc: 0.6296 - val_loss: 0.8940 - val_acc: 0.6686\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9009 - acc: 0.6226 - val_loss: 0.8914 - val_acc: 0.6514\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8772 - acc: 0.6379 - val_loss: 0.8915 - val_acc: 0.6629\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8992 - acc: 0.6245 - val_loss: 0.8915 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8766 - acc: 0.6386 - val_loss: 0.8977 - val_acc: 0.6629\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8964 - acc: 0.6201 - val_loss: 0.8936 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9036 - acc: 0.6354 - val_loss: 0.8942 - val_acc: 0.6743\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8677 - acc: 0.6360 - val_loss: 0.8932 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8891 - acc: 0.6367 - val_loss: 0.8949 - val_acc: 0.6686\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8666 - acc: 0.6386 - val_loss: 0.8965 - val_acc: 0.6629\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8619 - acc: 0.6373 - val_loss: 0.8926 - val_acc: 0.6514\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8568 - acc: 0.6507 - val_loss: 0.8959 - val_acc: 0.6686\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8574 - acc: 0.6379 - val_loss: 0.9007 - val_acc: 0.6629\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8562 - acc: 0.6488 - val_loss: 0.8956 - val_acc: 0.6629\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8622 - acc: 0.6392 - val_loss: 0.8921 - val_acc: 0.6400\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8641 - acc: 0.6430 - val_loss: 0.8945 - val_acc: 0.6457\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8528 - acc: 0.6494 - val_loss: 0.8956 - val_acc: 0.6629\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8602 - acc: 0.6437 - val_loss: 0.8966 - val_acc: 0.6571\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8495 - acc: 0.6488 - val_loss: 0.8973 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8428 - acc: 0.6711 - val_loss: 0.8933 - val_acc: 0.6400\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8406 - acc: 0.6469 - val_loss: 0.8952 - val_acc: 0.6457\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8387 - acc: 0.6590 - val_loss: 0.8916 - val_acc: 0.6457\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8349 - acc: 0.6558 - val_loss: 0.8934 - val_acc: 0.6514\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8383 - acc: 0.6494 - val_loss: 0.8960 - val_acc: 0.6571\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8248 - acc: 0.6731 - val_loss: 0.8930 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8298 - acc: 0.6654 - val_loss: 0.8975 - val_acc: 0.6514\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8221 - acc: 0.6635 - val_loss: 0.8991 - val_acc: 0.6571\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7975 - acc: 0.6814 - val_loss: 0.8973 - val_acc: 0.6514\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8229 - acc: 0.6609 - val_loss: 0.8990 - val_acc: 0.6514\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8079 - acc: 0.6622 - val_loss: 0.9011 - val_acc: 0.6457\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8188 - acc: 0.6584 - val_loss: 0.9015 - val_acc: 0.6343\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8121 - acc: 0.6711 - val_loss: 0.9044 - val_acc: 0.6400\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8119 - acc: 0.6801 - val_loss: 0.9113 - val_acc: 0.6457\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.8171 - acc: 0.6590 - val_loss: 0.9035 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7990 - acc: 0.6769 - val_loss: 0.9035 - val_acc: 0.6229\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8357 - acc: 0.6545 - val_loss: 0.9041 - val_acc: 0.6400\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7993 - acc: 0.6718 - val_loss: 0.9089 - val_acc: 0.6514\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8331 - acc: 0.6539 - val_loss: 0.9103 - val_acc: 0.6400\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7891 - acc: 0.6890 - val_loss: 0.9123 - val_acc: 0.6457\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7904 - acc: 0.6743 - val_loss: 0.9153 - val_acc: 0.6400\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8032 - acc: 0.6801 - val_loss: 0.9123 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7956 - acc: 0.6801 - val_loss: 0.9073 - val_acc: 0.6400\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7782 - acc: 0.6762 - val_loss: 0.9045 - val_acc: 0.6286\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7909 - acc: 0.6782 - val_loss: 0.9059 - val_acc: 0.6343\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7962 - acc: 0.6686 - val_loss: 0.9081 - val_acc: 0.6457\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7872 - acc: 0.6718 - val_loss: 0.9125 - val_acc: 0.6514\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7813 - acc: 0.6801 - val_loss: 0.9134 - val_acc: 0.6514\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7786 - acc: 0.6814 - val_loss: 0.9094 - val_acc: 0.6343\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7660 - acc: 0.7011 - val_loss: 0.9085 - val_acc: 0.6514\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7787 - acc: 0.6890 - val_loss: 0.9154 - val_acc: 0.6457\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7655 - acc: 0.6897 - val_loss: 0.9143 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7743 - acc: 0.6807 - val_loss: 0.9164 - val_acc: 0.6457\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7498 - acc: 0.6916 - val_loss: 0.9162 - val_acc: 0.6457\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7548 - acc: 0.6909 - val_loss: 0.9163 - val_acc: 0.6457\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7701 - acc: 0.6890 - val_loss: 0.9171 - val_acc: 0.6457\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7599 - acc: 0.6928 - val_loss: 0.9206 - val_acc: 0.6400\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7646 - acc: 0.6967 - val_loss: 0.9237 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7656 - acc: 0.6897 - val_loss: 0.9222 - val_acc: 0.6457\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7603 - acc: 0.6820 - val_loss: 0.9170 - val_acc: 0.6457\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7414 - acc: 0.6992 - val_loss: 0.9133 - val_acc: 0.6457\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7613 - acc: 0.6871 - val_loss: 0.9251 - val_acc: 0.6457\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7618 - acc: 0.6871 - val_loss: 0.9284 - val_acc: 0.6400\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7518 - acc: 0.6960 - val_loss: 0.9281 - val_acc: 0.6400\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7423 - acc: 0.7037 - val_loss: 0.9244 - val_acc: 0.6457\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7574 - acc: 0.7011 - val_loss: 0.9231 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7471 - acc: 0.6960 - val_loss: 0.9285 - val_acc: 0.6457\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7598 - acc: 0.6845 - val_loss: 0.9298 - val_acc: 0.6457\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7616 - acc: 0.6897 - val_loss: 0.9283 - val_acc: 0.6400\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7329 - acc: 0.6960 - val_loss: 0.9278 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7461 - acc: 0.6954 - val_loss: 0.9304 - val_acc: 0.6457\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7357 - acc: 0.7107 - val_loss: 0.9327 - val_acc: 0.6457\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7361 - acc: 0.7031 - val_loss: 0.9295 - val_acc: 0.6457\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7335 - acc: 0.7005 - val_loss: 0.9309 - val_acc: 0.6514\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7277 - acc: 0.7126 - val_loss: 0.9334 - val_acc: 0.6343\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7346 - acc: 0.7152 - val_loss: 0.9373 - val_acc: 0.6457\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7512 - acc: 0.6852 - val_loss: 0.9335 - val_acc: 0.6457\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7134 - acc: 0.7075 - val_loss: 0.9352 - val_acc: 0.6286\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7476 - acc: 0.7031 - val_loss: 0.9422 - val_acc: 0.6457\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7224 - acc: 0.7069 - val_loss: 0.9380 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7169 - acc: 0.7088 - val_loss: 0.9393 - val_acc: 0.6286\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7297 - acc: 0.7114 - val_loss: 0.9367 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7354 - acc: 0.7158 - val_loss: 0.9330 - val_acc: 0.6343\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7157 - acc: 0.7203 - val_loss: 0.9349 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7096 - acc: 0.7114 - val_loss: 0.9387 - val_acc: 0.6400\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 108us/step - loss: 0.7224 - acc: 0.7184 - val_loss: 0.9374 - val_acc: 0.6400\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7234 - acc: 0.7063 - val_loss: 0.9351 - val_acc: 0.6343\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7248 - acc: 0.7069 - val_loss: 0.9430 - val_acc: 0.6343\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7086 - acc: 0.7126 - val_loss: 0.9415 - val_acc: 0.6343\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6983 - acc: 0.7152 - val_loss: 0.9416 - val_acc: 0.6229\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7062 - acc: 0.7063 - val_loss: 0.9439 - val_acc: 0.6400\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7127 - acc: 0.7222 - val_loss: 0.9469 - val_acc: 0.6343\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7073 - acc: 0.7331 - val_loss: 0.9509 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7270 - acc: 0.7114 - val_loss: 0.9513 - val_acc: 0.6400\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7069 - acc: 0.7216 - val_loss: 0.9557 - val_acc: 0.6286\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7107 - acc: 0.7139 - val_loss: 0.9556 - val_acc: 0.6343\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7052 - acc: 0.7139 - val_loss: 0.9550 - val_acc: 0.6343\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7208 - acc: 0.7043 - val_loss: 0.9524 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6898 - acc: 0.7273 - val_loss: 0.9498 - val_acc: 0.6343\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6926 - acc: 0.7241 - val_loss: 0.9504 - val_acc: 0.6286\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6936 - acc: 0.7222 - val_loss: 0.9518 - val_acc: 0.6400\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6983 - acc: 0.7107 - val_loss: 0.9518 - val_acc: 0.6400\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6907 - acc: 0.7248 - val_loss: 0.9564 - val_acc: 0.6229\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6884 - acc: 0.7209 - val_loss: 0.9563 - val_acc: 0.6286\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7092 - acc: 0.7190 - val_loss: 0.9586 - val_acc: 0.6229\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6993 - acc: 0.7165 - val_loss: 0.9555 - val_acc: 0.6229\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6997 - acc: 0.7222 - val_loss: 0.9530 - val_acc: 0.6400\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6898 - acc: 0.7203 - val_loss: 0.9559 - val_acc: 0.6400\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6863 - acc: 0.7318 - val_loss: 0.9570 - val_acc: 0.6514\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6804 - acc: 0.7209 - val_loss: 0.9520 - val_acc: 0.6514\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6887 - acc: 0.7216 - val_loss: 0.9605 - val_acc: 0.6457\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6823 - acc: 0.7203 - val_loss: 0.9618 - val_acc: 0.6343\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6554 - acc: 0.7344 - val_loss: 0.9603 - val_acc: 0.6286\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6636 - acc: 0.7216 - val_loss: 0.9542 - val_acc: 0.6400\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6832 - acc: 0.7222 - val_loss: 0.9612 - val_acc: 0.6400\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6688 - acc: 0.7331 - val_loss: 0.9624 - val_acc: 0.6457\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6704 - acc: 0.7305 - val_loss: 0.9598 - val_acc: 0.6457\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6634 - acc: 0.7312 - val_loss: 0.9626 - val_acc: 0.6343\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6640 - acc: 0.7356 - val_loss: 0.9630 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6705 - acc: 0.7356 - val_loss: 0.9711 - val_acc: 0.6457\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6564 - acc: 0.7312 - val_loss: 0.9737 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6658 - acc: 0.7465 - val_loss: 0.9736 - val_acc: 0.6286\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6658 - acc: 0.7267 - val_loss: 0.9699 - val_acc: 0.6343\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6577 - acc: 0.7350 - val_loss: 0.9715 - val_acc: 0.6400\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6686 - acc: 0.7324 - val_loss: 0.9742 - val_acc: 0.6457\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6607 - acc: 0.7267 - val_loss: 0.9771 - val_acc: 0.6457\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6639 - acc: 0.7299 - val_loss: 0.9787 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6551 - acc: 0.7324 - val_loss: 0.9787 - val_acc: 0.6229\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6456 - acc: 0.7427 - val_loss: 0.9806 - val_acc: 0.6400\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6681 - acc: 0.7401 - val_loss: 0.9758 - val_acc: 0.6514\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6719 - acc: 0.7197 - val_loss: 0.9754 - val_acc: 0.6457\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6619 - acc: 0.7375 - val_loss: 0.9787 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6523 - acc: 0.7439 - val_loss: 0.9932 - val_acc: 0.6171\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6537 - acc: 0.7395 - val_loss: 0.9868 - val_acc: 0.6114\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6515 - acc: 0.7414 - val_loss: 0.9878 - val_acc: 0.6343\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6468 - acc: 0.7254 - val_loss: 0.9936 - val_acc: 0.6286\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6370 - acc: 0.7522 - val_loss: 0.9900 - val_acc: 0.6343\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6327 - acc: 0.7350 - val_loss: 0.9854 - val_acc: 0.6286\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6461 - acc: 0.7273 - val_loss: 0.9912 - val_acc: 0.6229\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6338 - acc: 0.7478 - val_loss: 0.9918 - val_acc: 0.6343\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6508 - acc: 0.7369 - val_loss: 0.9960 - val_acc: 0.6343\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6389 - acc: 0.7388 - val_loss: 0.9971 - val_acc: 0.6229\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6346 - acc: 0.7420 - val_loss: 0.9970 - val_acc: 0.6286\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6445 - acc: 0.7458 - val_loss: 0.9957 - val_acc: 0.6229\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6459 - acc: 0.7452 - val_loss: 0.9934 - val_acc: 0.6400\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6198 - acc: 0.7573 - val_loss: 0.9989 - val_acc: 0.6286\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6206 - acc: 0.7446 - val_loss: 1.0048 - val_acc: 0.6343\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6373 - acc: 0.7363 - val_loss: 1.0010 - val_acc: 0.6400\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6481 - acc: 0.7510 - val_loss: 1.0027 - val_acc: 0.6343\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6414 - acc: 0.7439 - val_loss: 1.0065 - val_acc: 0.6343\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6376 - acc: 0.7420 - val_loss: 1.0058 - val_acc: 0.6343\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6151 - acc: 0.7618 - val_loss: 1.0049 - val_acc: 0.6286\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6294 - acc: 0.7510 - val_loss: 1.0005 - val_acc: 0.6286\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6198 - acc: 0.7529 - val_loss: 1.0029 - val_acc: 0.6343\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6199 - acc: 0.7497 - val_loss: 1.0065 - val_acc: 0.6229\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6154 - acc: 0.7452 - val_loss: 1.0048 - val_acc: 0.6400\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6199 - acc: 0.7414 - val_loss: 1.0111 - val_acc: 0.6457\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5998 - acc: 0.7573 - val_loss: 1.0103 - val_acc: 0.6286\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6139 - acc: 0.7522 - val_loss: 1.0090 - val_acc: 0.6286\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5959 - acc: 0.7656 - val_loss: 1.0056 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6161 - acc: 0.7484 - val_loss: 1.0039 - val_acc: 0.6286\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6048 - acc: 0.7688 - val_loss: 1.0066 - val_acc: 0.6286\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6019 - acc: 0.7535 - val_loss: 1.0164 - val_acc: 0.6343\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6216 - acc: 0.7586 - val_loss: 1.0202 - val_acc: 0.6400\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6072 - acc: 0.7535 - val_loss: 1.0221 - val_acc: 0.6286\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5980 - acc: 0.7471 - val_loss: 1.0214 - val_acc: 0.6286\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6132 - acc: 0.7478 - val_loss: 1.0226 - val_acc: 0.6514\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6171 - acc: 0.7554 - val_loss: 1.0314 - val_acc: 0.6343\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6079 - acc: 0.7612 - val_loss: 1.0306 - val_acc: 0.6286\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6035 - acc: 0.7522 - val_loss: 1.0311 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 54us/step\n",
      "1741/1741 [==============================] - 0s 54us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 9s 6ms/step - loss: 1.4854 - acc: 0.2663 - val_loss: 1.2494 - val_acc: 0.4914\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.3351 - acc: 0.3704 - val_loss: 1.1533 - val_acc: 0.5543\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.2446 - acc: 0.4215 - val_loss: 1.0986 - val_acc: 0.6171\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.2282 - acc: 0.4419 - val_loss: 1.0698 - val_acc: 0.6057\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.1806 - acc: 0.4591 - val_loss: 1.0395 - val_acc: 0.6114\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 1.1192 - acc: 0.5026 - val_loss: 1.0209 - val_acc: 0.6171\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1067 - acc: 0.5198 - val_loss: 1.0032 - val_acc: 0.6057\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.0879 - acc: 0.5198 - val_loss: 0.9824 - val_acc: 0.6457\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 1.0741 - acc: 0.5326 - val_loss: 0.9756 - val_acc: 0.6400\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0656 - acc: 0.5287 - val_loss: 0.9683 - val_acc: 0.6400\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0411 - acc: 0.5607 - val_loss: 0.9581 - val_acc: 0.6343\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.0239 - acc: 0.5511 - val_loss: 0.9491 - val_acc: 0.6400\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.0113 - acc: 0.5766 - val_loss: 0.9442 - val_acc: 0.6286\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.0147 - acc: 0.5792 - val_loss: 0.9408 - val_acc: 0.6457\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9994 - acc: 0.5734 - val_loss: 0.9376 - val_acc: 0.6514\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9875 - acc: 0.5715 - val_loss: 0.9327 - val_acc: 0.6343\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9802 - acc: 0.5709 - val_loss: 0.9281 - val_acc: 0.6400\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.9784 - acc: 0.5677 - val_loss: 0.9241 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9914 - acc: 0.5658 - val_loss: 0.9194 - val_acc: 0.6857\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9439 - acc: 0.5856 - val_loss: 0.9199 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.9722 - acc: 0.5958 - val_loss: 0.9192 - val_acc: 0.6629\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9640 - acc: 0.5907 - val_loss: 0.9208 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9597 - acc: 0.5996 - val_loss: 0.9118 - val_acc: 0.6800\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9485 - acc: 0.6034 - val_loss: 0.9131 - val_acc: 0.6800\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9329 - acc: 0.6143 - val_loss: 0.9091 - val_acc: 0.6686\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9263 - acc: 0.6213 - val_loss: 0.9068 - val_acc: 0.6800\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9224 - acc: 0.6194 - val_loss: 0.9032 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9259 - acc: 0.6105 - val_loss: 0.9018 - val_acc: 0.6800\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9231 - acc: 0.6137 - val_loss: 0.9001 - val_acc: 0.6800\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9166 - acc: 0.6054 - val_loss: 0.8978 - val_acc: 0.6743\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.9137 - acc: 0.6111 - val_loss: 0.8976 - val_acc: 0.6743\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9125 - acc: 0.6149 - val_loss: 0.9001 - val_acc: 0.6686\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.9099 - acc: 0.6143 - val_loss: 0.8990 - val_acc: 0.6857\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8991 - acc: 0.6239 - val_loss: 0.8981 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8821 - acc: 0.6367 - val_loss: 0.8980 - val_acc: 0.6914\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8871 - acc: 0.6405 - val_loss: 0.8982 - val_acc: 0.6743\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8864 - acc: 0.6309 - val_loss: 0.9010 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8787 - acc: 0.6456 - val_loss: 0.8966 - val_acc: 0.6629\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8907 - acc: 0.6175 - val_loss: 0.8927 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8859 - acc: 0.6277 - val_loss: 0.8962 - val_acc: 0.6686\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8677 - acc: 0.6437 - val_loss: 0.8972 - val_acc: 0.6743\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8824 - acc: 0.6284 - val_loss: 0.8931 - val_acc: 0.6686\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8678 - acc: 0.6424 - val_loss: 0.8967 - val_acc: 0.6686\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.8684 - acc: 0.6437 - val_loss: 0.9010 - val_acc: 0.6857\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8751 - acc: 0.6360 - val_loss: 0.9014 - val_acc: 0.6743\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8672 - acc: 0.6424 - val_loss: 0.8991 - val_acc: 0.6629\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8712 - acc: 0.6418 - val_loss: 0.8969 - val_acc: 0.6571\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8546 - acc: 0.6469 - val_loss: 0.8952 - val_acc: 0.6629\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8887 - acc: 0.6258 - val_loss: 0.8927 - val_acc: 0.6800\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8320 - acc: 0.6456 - val_loss: 0.8942 - val_acc: 0.6743\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8483 - acc: 0.6392 - val_loss: 0.8914 - val_acc: 0.6743\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8465 - acc: 0.6692 - val_loss: 0.8889 - val_acc: 0.6686\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8388 - acc: 0.6533 - val_loss: 0.8878 - val_acc: 0.6743\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8373 - acc: 0.6679 - val_loss: 0.8899 - val_acc: 0.6629\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8451 - acc: 0.6533 - val_loss: 0.8920 - val_acc: 0.6629\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8352 - acc: 0.6469 - val_loss: 0.8936 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8302 - acc: 0.6648 - val_loss: 0.8974 - val_acc: 0.6686\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8358 - acc: 0.6507 - val_loss: 0.9000 - val_acc: 0.6686\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8340 - acc: 0.6577 - val_loss: 0.8967 - val_acc: 0.6686\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8466 - acc: 0.6481 - val_loss: 0.8969 - val_acc: 0.6743\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8219 - acc: 0.6616 - val_loss: 0.9043 - val_acc: 0.6629\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8185 - acc: 0.6558 - val_loss: 0.9006 - val_acc: 0.6743\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8132 - acc: 0.6737 - val_loss: 0.9023 - val_acc: 0.6686\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8128 - acc: 0.6628 - val_loss: 0.9068 - val_acc: 0.6514\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8175 - acc: 0.6545 - val_loss: 0.9095 - val_acc: 0.6629\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8285 - acc: 0.6475 - val_loss: 0.9008 - val_acc: 0.6514\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8263 - acc: 0.6609 - val_loss: 0.9041 - val_acc: 0.6629\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8243 - acc: 0.6679 - val_loss: 0.9028 - val_acc: 0.6686\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8045 - acc: 0.6711 - val_loss: 0.9018 - val_acc: 0.6629\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8162 - acc: 0.6769 - val_loss: 0.9059 - val_acc: 0.6571\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8094 - acc: 0.6731 - val_loss: 0.9085 - val_acc: 0.6571\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8103 - acc: 0.6750 - val_loss: 0.9107 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8133 - acc: 0.6814 - val_loss: 0.9122 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7902 - acc: 0.6705 - val_loss: 0.9138 - val_acc: 0.6343\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7836 - acc: 0.6884 - val_loss: 0.9072 - val_acc: 0.6343\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7962 - acc: 0.6724 - val_loss: 0.9056 - val_acc: 0.6514\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8113 - acc: 0.6590 - val_loss: 0.9062 - val_acc: 0.6400\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7789 - acc: 0.6801 - val_loss: 0.9054 - val_acc: 0.6457\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7815 - acc: 0.6871 - val_loss: 0.9056 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7926 - acc: 0.6884 - val_loss: 0.9154 - val_acc: 0.6400\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7808 - acc: 0.6826 - val_loss: 0.9138 - val_acc: 0.6457\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7934 - acc: 0.6852 - val_loss: 0.9066 - val_acc: 0.6571\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7852 - acc: 0.6858 - val_loss: 0.9157 - val_acc: 0.6400\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7784 - acc: 0.6897 - val_loss: 0.9112 - val_acc: 0.6343\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7862 - acc: 0.6775 - val_loss: 0.9105 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7864 - acc: 0.6705 - val_loss: 0.9063 - val_acc: 0.6457\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7808 - acc: 0.6737 - val_loss: 0.9130 - val_acc: 0.6400\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7703 - acc: 0.6865 - val_loss: 0.9113 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7898 - acc: 0.6737 - val_loss: 0.9120 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7630 - acc: 0.6807 - val_loss: 0.9151 - val_acc: 0.6457\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7580 - acc: 0.6897 - val_loss: 0.9185 - val_acc: 0.6400\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7724 - acc: 0.6852 - val_loss: 0.9206 - val_acc: 0.6343\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7548 - acc: 0.6935 - val_loss: 0.9222 - val_acc: 0.6343\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7547 - acc: 0.6890 - val_loss: 0.9200 - val_acc: 0.6457\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7754 - acc: 0.6897 - val_loss: 0.9184 - val_acc: 0.6514\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7457 - acc: 0.6986 - val_loss: 0.9209 - val_acc: 0.6571\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7401 - acc: 0.7043 - val_loss: 0.9234 - val_acc: 0.6400\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7619 - acc: 0.6954 - val_loss: 0.9201 - val_acc: 0.6571\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7614 - acc: 0.6871 - val_loss: 0.9265 - val_acc: 0.6400\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7618 - acc: 0.6922 - val_loss: 0.9187 - val_acc: 0.6400\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7324 - acc: 0.7011 - val_loss: 0.9187 - val_acc: 0.6686\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7478 - acc: 0.6992 - val_loss: 0.9255 - val_acc: 0.6629\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7379 - acc: 0.7146 - val_loss: 0.9262 - val_acc: 0.6571\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7546 - acc: 0.6948 - val_loss: 0.9256 - val_acc: 0.6514\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7509 - acc: 0.6960 - val_loss: 0.9227 - val_acc: 0.6686\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7668 - acc: 0.6922 - val_loss: 0.9259 - val_acc: 0.6457\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7384 - acc: 0.6986 - val_loss: 0.9240 - val_acc: 0.6571\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7416 - acc: 0.7043 - val_loss: 0.9227 - val_acc: 0.6629\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7381 - acc: 0.7075 - val_loss: 0.9281 - val_acc: 0.6629\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7383 - acc: 0.7088 - val_loss: 0.9202 - val_acc: 0.6743\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7346 - acc: 0.6928 - val_loss: 0.9249 - val_acc: 0.6457\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7510 - acc: 0.6858 - val_loss: 0.9248 - val_acc: 0.6343\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7360 - acc: 0.7107 - val_loss: 0.9235 - val_acc: 0.6514\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7278 - acc: 0.7075 - val_loss: 0.9318 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7374 - acc: 0.7184 - val_loss: 0.9393 - val_acc: 0.6400\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7318 - acc: 0.6948 - val_loss: 0.9332 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7189 - acc: 0.7018 - val_loss: 0.9311 - val_acc: 0.6571\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7272 - acc: 0.7056 - val_loss: 0.9331 - val_acc: 0.6457\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7208 - acc: 0.7056 - val_loss: 0.9346 - val_acc: 0.6400\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7178 - acc: 0.6954 - val_loss: 0.9368 - val_acc: 0.6571\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7049 - acc: 0.7229 - val_loss: 0.9412 - val_acc: 0.6743\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7071 - acc: 0.7095 - val_loss: 0.9470 - val_acc: 0.6571\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7131 - acc: 0.7088 - val_loss: 0.9400 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7246 - acc: 0.6999 - val_loss: 0.9366 - val_acc: 0.6686\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7193 - acc: 0.7197 - val_loss: 0.9396 - val_acc: 0.6400\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7055 - acc: 0.7101 - val_loss: 0.9359 - val_acc: 0.6514\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7000 - acc: 0.7133 - val_loss: 0.9431 - val_acc: 0.6629\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7182 - acc: 0.7184 - val_loss: 0.9472 - val_acc: 0.6457\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7072 - acc: 0.7184 - val_loss: 0.9557 - val_acc: 0.6571\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7192 - acc: 0.7152 - val_loss: 0.9490 - val_acc: 0.6629\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7063 - acc: 0.7069 - val_loss: 0.9497 - val_acc: 0.6457\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6953 - acc: 0.7178 - val_loss: 0.9496 - val_acc: 0.6514\n",
      "Epoch 133/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6952 - acc: 0.7190 - val_loss: 0.9517 - val_acc: 0.6457\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7067 - acc: 0.7107 - val_loss: 0.9487 - val_acc: 0.6457\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6834 - acc: 0.7209 - val_loss: 0.9536 - val_acc: 0.6343\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6946 - acc: 0.7235 - val_loss: 0.9522 - val_acc: 0.6457\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7008 - acc: 0.7139 - val_loss: 0.9573 - val_acc: 0.6229\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6828 - acc: 0.7248 - val_loss: 0.9529 - val_acc: 0.6514\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6920 - acc: 0.7267 - val_loss: 0.9501 - val_acc: 0.6514\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6854 - acc: 0.7216 - val_loss: 0.9510 - val_acc: 0.6457\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7033 - acc: 0.7292 - val_loss: 0.9541 - val_acc: 0.6229\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6677 - acc: 0.7286 - val_loss: 0.9565 - val_acc: 0.6229\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6900 - acc: 0.7248 - val_loss: 0.9552 - val_acc: 0.6343\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6759 - acc: 0.7344 - val_loss: 0.9568 - val_acc: 0.6514\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6795 - acc: 0.7363 - val_loss: 0.9580 - val_acc: 0.6286\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6695 - acc: 0.7478 - val_loss: 0.9631 - val_acc: 0.6343\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6618 - acc: 0.7324 - val_loss: 0.9635 - val_acc: 0.6343\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6970 - acc: 0.7050 - val_loss: 0.9623 - val_acc: 0.6343\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6735 - acc: 0.7248 - val_loss: 0.9653 - val_acc: 0.6286\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6737 - acc: 0.7414 - val_loss: 0.9631 - val_acc: 0.6343\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6957 - acc: 0.7139 - val_loss: 0.9630 - val_acc: 0.6286\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6798 - acc: 0.7133 - val_loss: 0.9672 - val_acc: 0.6457\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.6704 - acc: 0.7299 - val_loss: 0.9671 - val_acc: 0.6457\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6848 - acc: 0.7184 - val_loss: 0.9757 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6748 - acc: 0.7261 - val_loss: 0.9770 - val_acc: 0.6400\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6905 - acc: 0.7146 - val_loss: 0.9816 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6517 - acc: 0.7395 - val_loss: 0.9839 - val_acc: 0.6343\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6645 - acc: 0.7229 - val_loss: 0.9853 - val_acc: 0.6343\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6568 - acc: 0.7420 - val_loss: 0.9917 - val_acc: 0.6114\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6474 - acc: 0.7382 - val_loss: 0.9848 - val_acc: 0.6457\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6520 - acc: 0.7388 - val_loss: 0.9831 - val_acc: 0.6400\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6820 - acc: 0.7363 - val_loss: 0.9861 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6480 - acc: 0.7229 - val_loss: 0.9824 - val_acc: 0.6400\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6384 - acc: 0.7478 - val_loss: 0.9848 - val_acc: 0.6229\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6570 - acc: 0.7318 - val_loss: 0.9889 - val_acc: 0.6286\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6562 - acc: 0.7350 - val_loss: 0.9920 - val_acc: 0.6343\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6514 - acc: 0.7478 - val_loss: 0.9949 - val_acc: 0.6229\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6609 - acc: 0.7267 - val_loss: 0.9958 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6639 - acc: 0.7292 - val_loss: 1.0030 - val_acc: 0.6343\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6400 - acc: 0.7324 - val_loss: 1.0085 - val_acc: 0.6229\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6466 - acc: 0.7490 - val_loss: 0.9988 - val_acc: 0.6343\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6506 - acc: 0.7350 - val_loss: 1.0048 - val_acc: 0.6229\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - ETA: 0s - loss: 0.6680 - acc: 0.736 - 0s 64us/step - loss: 0.6556 - acc: 0.7452 - val_loss: 1.0040 - val_acc: 0.6229\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6302 - acc: 0.7554 - val_loss: 0.9995 - val_acc: 0.6343\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6272 - acc: 0.7478 - val_loss: 1.0026 - val_acc: 0.6286\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6362 - acc: 0.7503 - val_loss: 1.0074 - val_acc: 0.6286\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6703 - acc: 0.7318 - val_loss: 1.0091 - val_acc: 0.6343\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6341 - acc: 0.7395 - val_loss: 1.0052 - val_acc: 0.6343\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6382 - acc: 0.7395 - val_loss: 1.0084 - val_acc: 0.6343\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6455 - acc: 0.7452 - val_loss: 0.9982 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6304 - acc: 0.7292 - val_loss: 1.0005 - val_acc: 0.6457\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6448 - acc: 0.7324 - val_loss: 0.9990 - val_acc: 0.6457\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6415 - acc: 0.7318 - val_loss: 0.9995 - val_acc: 0.6343\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6334 - acc: 0.7548 - val_loss: 1.0086 - val_acc: 0.6457\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6279 - acc: 0.7516 - val_loss: 1.0082 - val_acc: 0.6286\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6215 - acc: 0.7414 - val_loss: 1.0082 - val_acc: 0.6229\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6316 - acc: 0.7395 - val_loss: 1.0068 - val_acc: 0.6400\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6163 - acc: 0.7593 - val_loss: 1.0082 - val_acc: 0.6343\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6145 - acc: 0.7701 - val_loss: 1.0119 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6150 - acc: 0.7561 - val_loss: 1.0123 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6109 - acc: 0.7542 - val_loss: 1.0137 - val_acc: 0.6286\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6220 - acc: 0.7465 - val_loss: 1.0114 - val_acc: 0.6400\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6163 - acc: 0.7554 - val_loss: 1.0101 - val_acc: 0.6400\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6223 - acc: 0.7561 - val_loss: 1.0091 - val_acc: 0.6400\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6131 - acc: 0.7561 - val_loss: 1.0096 - val_acc: 0.6400\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6106 - acc: 0.7465 - val_loss: 1.0092 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6194 - acc: 0.7586 - val_loss: 1.0160 - val_acc: 0.6457\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5932 - acc: 0.7701 - val_loss: 1.0165 - val_acc: 0.6400\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5952 - acc: 0.7599 - val_loss: 1.0211 - val_acc: 0.6400\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6218 - acc: 0.7452 - val_loss: 1.0254 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 54us/step\n",
      "1741/1741 [==============================] - 0s 35us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 9s 6ms/step - loss: 1.5779 - acc: 0.2861 - val_loss: 1.3105 - val_acc: 0.4400\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.3825 - acc: 0.3602 - val_loss: 1.1621 - val_acc: 0.5429\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.2868 - acc: 0.4215 - val_loss: 1.0882 - val_acc: 0.5714\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 1.2301 - acc: 0.4406 - val_loss: 1.0506 - val_acc: 0.6114\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1939 - acc: 0.4834 - val_loss: 1.0225 - val_acc: 0.6286\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1373 - acc: 0.5000 - val_loss: 1.0039 - val_acc: 0.6286\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1429 - acc: 0.4860 - val_loss: 0.9872 - val_acc: 0.6400\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1070 - acc: 0.5211 - val_loss: 0.9726 - val_acc: 0.6400\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.1077 - acc: 0.5077 - val_loss: 0.9619 - val_acc: 0.6514\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0770 - acc: 0.5223 - val_loss: 0.9560 - val_acc: 0.6571\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0630 - acc: 0.5402 - val_loss: 0.9468 - val_acc: 0.6629\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0748 - acc: 0.5294 - val_loss: 0.9411 - val_acc: 0.6571\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 1.0527 - acc: 0.5517 - val_loss: 0.9313 - val_acc: 0.6629\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 1.0334 - acc: 0.5562 - val_loss: 0.9267 - val_acc: 0.6686\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0156 - acc: 0.5677 - val_loss: 0.9245 - val_acc: 0.6514\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0214 - acc: 0.5613 - val_loss: 0.9175 - val_acc: 0.6743\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0050 - acc: 0.5683 - val_loss: 0.9119 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0028 - acc: 0.5779 - val_loss: 0.9100 - val_acc: 0.6800\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0002 - acc: 0.5766 - val_loss: 0.9072 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9732 - acc: 0.5837 - val_loss: 0.9060 - val_acc: 0.6686\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9822 - acc: 0.5722 - val_loss: 0.9078 - val_acc: 0.6629\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9678 - acc: 0.5945 - val_loss: 0.9091 - val_acc: 0.6743\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9656 - acc: 0.5843 - val_loss: 0.9055 - val_acc: 0.6857\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9843 - acc: 0.5728 - val_loss: 0.8972 - val_acc: 0.6743\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9679 - acc: 0.6034 - val_loss: 0.8941 - val_acc: 0.6686\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9379 - acc: 0.6149 - val_loss: 0.8958 - val_acc: 0.6857\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9530 - acc: 0.6015 - val_loss: 0.8959 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9437 - acc: 0.6060 - val_loss: 0.8947 - val_acc: 0.6686\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9477 - acc: 0.6111 - val_loss: 0.8862 - val_acc: 0.6857\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9313 - acc: 0.6041 - val_loss: 0.8853 - val_acc: 0.6857\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9250 - acc: 0.6137 - val_loss: 0.8900 - val_acc: 0.6800\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9509 - acc: 0.5888 - val_loss: 0.8920 - val_acc: 0.6686\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9277 - acc: 0.6079 - val_loss: 0.8861 - val_acc: 0.6857\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9200 - acc: 0.6130 - val_loss: 0.8863 - val_acc: 0.6743\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.9184 - acc: 0.6137 - val_loss: 0.8913 - val_acc: 0.6743\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8976 - acc: 0.6264 - val_loss: 0.8936 - val_acc: 0.6800\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8967 - acc: 0.6226 - val_loss: 0.8936 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9408 - acc: 0.6092 - val_loss: 0.8921 - val_acc: 0.6800\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8952 - acc: 0.6162 - val_loss: 0.8901 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8988 - acc: 0.6277 - val_loss: 0.8927 - val_acc: 0.6629\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8885 - acc: 0.6386 - val_loss: 0.8949 - val_acc: 0.6743\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9033 - acc: 0.6232 - val_loss: 0.8944 - val_acc: 0.6686\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.9048 - acc: 0.6284 - val_loss: 0.8955 - val_acc: 0.6571\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.9055 - acc: 0.6220 - val_loss: 0.8970 - val_acc: 0.6686\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8796 - acc: 0.6354 - val_loss: 0.8928 - val_acc: 0.6800\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8725 - acc: 0.6488 - val_loss: 0.8923 - val_acc: 0.6857\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8792 - acc: 0.6226 - val_loss: 0.8951 - val_acc: 0.6686\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8789 - acc: 0.6341 - val_loss: 0.8925 - val_acc: 0.6857\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8568 - acc: 0.6430 - val_loss: 0.8937 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8694 - acc: 0.6405 - val_loss: 0.8914 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8676 - acc: 0.6488 - val_loss: 0.8923 - val_acc: 0.6800\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8500 - acc: 0.6469 - val_loss: 0.8957 - val_acc: 0.6743\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8569 - acc: 0.6533 - val_loss: 0.8933 - val_acc: 0.6743\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8708 - acc: 0.6379 - val_loss: 0.8928 - val_acc: 0.6800\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8578 - acc: 0.6488 - val_loss: 0.8914 - val_acc: 0.6971\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8445 - acc: 0.6379 - val_loss: 0.8913 - val_acc: 0.6971\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8496 - acc: 0.6456 - val_loss: 0.8957 - val_acc: 0.6743\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8534 - acc: 0.6386 - val_loss: 0.8949 - val_acc: 0.6686\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8354 - acc: 0.6584 - val_loss: 0.8947 - val_acc: 0.6800\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8418 - acc: 0.6533 - val_loss: 0.9019 - val_acc: 0.6857\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8401 - acc: 0.6539 - val_loss: 0.9003 - val_acc: 0.6743\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8530 - acc: 0.6571 - val_loss: 0.8976 - val_acc: 0.6686\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8409 - acc: 0.6552 - val_loss: 0.8973 - val_acc: 0.6686\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.8366 - acc: 0.6488 - val_loss: 0.9010 - val_acc: 0.6686\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8302 - acc: 0.6596 - val_loss: 0.8994 - val_acc: 0.6743\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8166 - acc: 0.6705 - val_loss: 0.8987 - val_acc: 0.6686\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8275 - acc: 0.6731 - val_loss: 0.9017 - val_acc: 0.6743\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8193 - acc: 0.6686 - val_loss: 0.9031 - val_acc: 0.6571\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8172 - acc: 0.6679 - val_loss: 0.9059 - val_acc: 0.6686\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8320 - acc: 0.6654 - val_loss: 0.9110 - val_acc: 0.6686\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8054 - acc: 0.6718 - val_loss: 0.9058 - val_acc: 0.6686\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8039 - acc: 0.6679 - val_loss: 0.9104 - val_acc: 0.6571\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8027 - acc: 0.6692 - val_loss: 0.9060 - val_acc: 0.6800\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8166 - acc: 0.6616 - val_loss: 0.9072 - val_acc: 0.6800\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7984 - acc: 0.6711 - val_loss: 0.9122 - val_acc: 0.6743\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8237 - acc: 0.6564 - val_loss: 0.9140 - val_acc: 0.6800\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8103 - acc: 0.6699 - val_loss: 0.9104 - val_acc: 0.6743\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8038 - acc: 0.6858 - val_loss: 0.9151 - val_acc: 0.6571\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8206 - acc: 0.6711 - val_loss: 0.9181 - val_acc: 0.6571\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8064 - acc: 0.6641 - val_loss: 0.9118 - val_acc: 0.6686\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7968 - acc: 0.6737 - val_loss: 0.9116 - val_acc: 0.6686\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7903 - acc: 0.6916 - val_loss: 0.9130 - val_acc: 0.6800\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7924 - acc: 0.6750 - val_loss: 0.9177 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7947 - acc: 0.6833 - val_loss: 0.9179 - val_acc: 0.6514\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7926 - acc: 0.6718 - val_loss: 0.9200 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7830 - acc: 0.6782 - val_loss: 0.9180 - val_acc: 0.6800\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7691 - acc: 0.6935 - val_loss: 0.9184 - val_acc: 0.6686\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7900 - acc: 0.6807 - val_loss: 0.9202 - val_acc: 0.6743\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7834 - acc: 0.6935 - val_loss: 0.9187 - val_acc: 0.6629\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7749 - acc: 0.6788 - val_loss: 0.9253 - val_acc: 0.6571\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7900 - acc: 0.6807 - val_loss: 0.9228 - val_acc: 0.6743\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7866 - acc: 0.6903 - val_loss: 0.9260 - val_acc: 0.6686\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7684 - acc: 0.6877 - val_loss: 0.9249 - val_acc: 0.6686\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7767 - acc: 0.6877 - val_loss: 0.9285 - val_acc: 0.6571\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7746 - acc: 0.6839 - val_loss: 0.9344 - val_acc: 0.6571\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7789 - acc: 0.6986 - val_loss: 0.9319 - val_acc: 0.6629\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7805 - acc: 0.6731 - val_loss: 0.9317 - val_acc: 0.6800\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7647 - acc: 0.6903 - val_loss: 0.9273 - val_acc: 0.6857\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7725 - acc: 0.6788 - val_loss: 0.9309 - val_acc: 0.6629\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7813 - acc: 0.6794 - val_loss: 0.9316 - val_acc: 0.6571\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7590 - acc: 0.6909 - val_loss: 0.9264 - val_acc: 0.6629\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7695 - acc: 0.6903 - val_loss: 0.9241 - val_acc: 0.6686\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7547 - acc: 0.7069 - val_loss: 0.9235 - val_acc: 0.6743\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7444 - acc: 0.6954 - val_loss: 0.9279 - val_acc: 0.6743\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7410 - acc: 0.6967 - val_loss: 0.9262 - val_acc: 0.6857\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7586 - acc: 0.6865 - val_loss: 0.9317 - val_acc: 0.6743\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7694 - acc: 0.6928 - val_loss: 0.9348 - val_acc: 0.6800\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7425 - acc: 0.6941 - val_loss: 0.9422 - val_acc: 0.6686\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7411 - acc: 0.7152 - val_loss: 0.9426 - val_acc: 0.6686\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7472 - acc: 0.6928 - val_loss: 0.9346 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7486 - acc: 0.7018 - val_loss: 0.9375 - val_acc: 0.6743\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7435 - acc: 0.6877 - val_loss: 0.9360 - val_acc: 0.6743\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7358 - acc: 0.6986 - val_loss: 0.9363 - val_acc: 0.6857\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7516 - acc: 0.6852 - val_loss: 0.9354 - val_acc: 0.6629\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7482 - acc: 0.6980 - val_loss: 0.9375 - val_acc: 0.6743\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7608 - acc: 0.6973 - val_loss: 0.9392 - val_acc: 0.6743\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7336 - acc: 0.7011 - val_loss: 0.9415 - val_acc: 0.6743\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7383 - acc: 0.7095 - val_loss: 0.9420 - val_acc: 0.6743\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7339 - acc: 0.6935 - val_loss: 0.9496 - val_acc: 0.6743\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7333 - acc: 0.6954 - val_loss: 0.9609 - val_acc: 0.6629\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7291 - acc: 0.7005 - val_loss: 0.9542 - val_acc: 0.6457\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7439 - acc: 0.6986 - val_loss: 0.9497 - val_acc: 0.6629\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7225 - acc: 0.7171 - val_loss: 0.9479 - val_acc: 0.6571\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7264 - acc: 0.7088 - val_loss: 0.9464 - val_acc: 0.6629\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7309 - acc: 0.6973 - val_loss: 0.9489 - val_acc: 0.6743\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7308 - acc: 0.7107 - val_loss: 0.9534 - val_acc: 0.6571\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7302 - acc: 0.6992 - val_loss: 0.9488 - val_acc: 0.6743\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7002 - acc: 0.7229 - val_loss: 0.9521 - val_acc: 0.6571\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6983 - acc: 0.7043 - val_loss: 0.9488 - val_acc: 0.6629\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7214 - acc: 0.7082 - val_loss: 0.9535 - val_acc: 0.6743\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7175 - acc: 0.7063 - val_loss: 0.9484 - val_acc: 0.6629\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6973 - acc: 0.7286 - val_loss: 0.9515 - val_acc: 0.6514\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7230 - acc: 0.7114 - val_loss: 0.9544 - val_acc: 0.6686\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7034 - acc: 0.7069 - val_loss: 0.9575 - val_acc: 0.6514\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7115 - acc: 0.7158 - val_loss: 0.9539 - val_acc: 0.6571\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7122 - acc: 0.7011 - val_loss: 0.9579 - val_acc: 0.6629\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7043 - acc: 0.7126 - val_loss: 0.9633 - val_acc: 0.6629\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7109 - acc: 0.7165 - val_loss: 0.9662 - val_acc: 0.6629\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6947 - acc: 0.7133 - val_loss: 0.9628 - val_acc: 0.6571\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7063 - acc: 0.7241 - val_loss: 0.9644 - val_acc: 0.6629\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7026 - acc: 0.7388 - val_loss: 0.9692 - val_acc: 0.6686\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7157 - acc: 0.7005 - val_loss: 0.9697 - val_acc: 0.6686\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6878 - acc: 0.7222 - val_loss: 0.9641 - val_acc: 0.6743\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6887 - acc: 0.7197 - val_loss: 0.9628 - val_acc: 0.6743\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7005 - acc: 0.7241 - val_loss: 0.9658 - val_acc: 0.6686\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6965 - acc: 0.7165 - val_loss: 0.9637 - val_acc: 0.6571\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7028 - acc: 0.7082 - val_loss: 0.9631 - val_acc: 0.6743\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6923 - acc: 0.7146 - val_loss: 0.9625 - val_acc: 0.6743\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 88us/step - loss: 0.6859 - acc: 0.7337 - val_loss: 0.9626 - val_acc: 0.6629\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6922 - acc: 0.7235 - val_loss: 0.9731 - val_acc: 0.6686\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6828 - acc: 0.7305 - val_loss: 0.9780 - val_acc: 0.6514\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6807 - acc: 0.7235 - val_loss: 0.9737 - val_acc: 0.6457\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6832 - acc: 0.7075 - val_loss: 0.9776 - val_acc: 0.6686\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6988 - acc: 0.7101 - val_loss: 0.9801 - val_acc: 0.6686\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.6862 - acc: 0.7280 - val_loss: 0.9769 - val_acc: 0.6800\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6877 - acc: 0.7107 - val_loss: 0.9748 - val_acc: 0.6743\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6938 - acc: 0.7261 - val_loss: 0.9765 - val_acc: 0.6514\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6795 - acc: 0.7178 - val_loss: 0.9782 - val_acc: 0.6629\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6830 - acc: 0.7222 - val_loss: 0.9841 - val_acc: 0.6686\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6831 - acc: 0.7350 - val_loss: 0.9891 - val_acc: 0.6629\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6716 - acc: 0.7350 - val_loss: 0.9945 - val_acc: 0.6571\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6684 - acc: 0.7344 - val_loss: 0.9873 - val_acc: 0.6629\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6793 - acc: 0.7178 - val_loss: 0.9866 - val_acc: 0.6629\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6691 - acc: 0.7197 - val_loss: 0.9837 - val_acc: 0.6571\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6838 - acc: 0.7254 - val_loss: 0.9772 - val_acc: 0.6571\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6573 - acc: 0.7280 - val_loss: 0.9814 - val_acc: 0.6629\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6502 - acc: 0.7324 - val_loss: 0.9859 - val_acc: 0.6571\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6793 - acc: 0.7229 - val_loss: 0.9874 - val_acc: 0.6686\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6641 - acc: 0.7261 - val_loss: 0.9897 - val_acc: 0.6514\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.6527 - acc: 0.7312 - val_loss: 0.9890 - val_acc: 0.6686\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.6508 - acc: 0.7337 - val_loss: 0.9882 - val_acc: 0.6629\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6633 - acc: 0.7375 - val_loss: 0.9929 - val_acc: 0.6686\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 91us/step - loss: 0.6587 - acc: 0.7452 - val_loss: 0.9938 - val_acc: 0.6571\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.6746 - acc: 0.7190 - val_loss: 0.9952 - val_acc: 0.6514\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.6380 - acc: 0.7567 - val_loss: 0.9916 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6539 - acc: 0.7414 - val_loss: 0.9932 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.6309 - acc: 0.7369 - val_loss: 0.9977 - val_acc: 0.6457\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6565 - acc: 0.7324 - val_loss: 0.9997 - val_acc: 0.6457\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6676 - acc: 0.7350 - val_loss: 1.0077 - val_acc: 0.6457\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6548 - acc: 0.7388 - val_loss: 1.0141 - val_acc: 0.6343\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.6423 - acc: 0.7446 - val_loss: 1.0110 - val_acc: 0.6571\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6414 - acc: 0.7318 - val_loss: 1.0054 - val_acc: 0.6571\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6397 - acc: 0.7535 - val_loss: 1.0021 - val_acc: 0.6571\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6477 - acc: 0.7465 - val_loss: 1.0082 - val_acc: 0.6571\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6351 - acc: 0.7529 - val_loss: 1.0123 - val_acc: 0.6686\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6538 - acc: 0.7465 - val_loss: 1.0069 - val_acc: 0.6629\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6446 - acc: 0.7337 - val_loss: 1.0101 - val_acc: 0.6686\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6295 - acc: 0.7554 - val_loss: 1.0060 - val_acc: 0.6743\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6520 - acc: 0.7363 - val_loss: 1.0103 - val_acc: 0.6629\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6587 - acc: 0.7382 - val_loss: 1.0090 - val_acc: 0.6686\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6466 - acc: 0.7465 - val_loss: 1.0046 - val_acc: 0.6686\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6488 - acc: 0.7420 - val_loss: 1.0097 - val_acc: 0.6686\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6187 - acc: 0.7535 - val_loss: 1.0157 - val_acc: 0.6571\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6277 - acc: 0.7542 - val_loss: 1.0153 - val_acc: 0.6629\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6399 - acc: 0.7427 - val_loss: 1.0179 - val_acc: 0.6629\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6236 - acc: 0.7490 - val_loss: 1.0180 - val_acc: 0.6457\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6181 - acc: 0.7586 - val_loss: 1.0195 - val_acc: 0.6514\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6345 - acc: 0.7529 - val_loss: 1.0229 - val_acc: 0.6514\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6077 - acc: 0.7554 - val_loss: 1.0311 - val_acc: 0.6514\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6126 - acc: 0.7522 - val_loss: 1.0265 - val_acc: 0.6629\n",
      "194/194 [==============================] - 0s 65us/step\n",
      "1741/1741 [==============================] - 0s 38us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 9s 5ms/step - loss: 1.4823 - acc: 0.2842 - val_loss: 1.2642 - val_acc: 0.4286\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.3330 - acc: 0.3589 - val_loss: 1.1674 - val_acc: 0.5429\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.2466 - acc: 0.4368 - val_loss: 1.1079 - val_acc: 0.5657\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.2097 - acc: 0.4579 - val_loss: 1.0715 - val_acc: 0.5829\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.1775 - acc: 0.4617 - val_loss: 1.0456 - val_acc: 0.6114\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1303 - acc: 0.5013 - val_loss: 1.0251 - val_acc: 0.6286\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1137 - acc: 0.5140 - val_loss: 1.0051 - val_acc: 0.6286\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0930 - acc: 0.5172 - val_loss: 0.9904 - val_acc: 0.6343\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0879 - acc: 0.5185 - val_loss: 0.9846 - val_acc: 0.6457\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 1.0638 - acc: 0.5326 - val_loss: 0.9737 - val_acc: 0.6629\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 1.0625 - acc: 0.5249 - val_loss: 0.9592 - val_acc: 0.6514\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.0424 - acc: 0.5517 - val_loss: 0.9531 - val_acc: 0.6457\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0267 - acc: 0.5549 - val_loss: 0.9463 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.0103 - acc: 0.5715 - val_loss: 0.9391 - val_acc: 0.6857\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0144 - acc: 0.5639 - val_loss: 0.9352 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9882 - acc: 0.5900 - val_loss: 0.9287 - val_acc: 0.6571\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0000 - acc: 0.5773 - val_loss: 0.9218 - val_acc: 0.6686\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9859 - acc: 0.5709 - val_loss: 0.9187 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9767 - acc: 0.5888 - val_loss: 0.9179 - val_acc: 0.6800\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9833 - acc: 0.5856 - val_loss: 0.9094 - val_acc: 0.6514\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9809 - acc: 0.5741 - val_loss: 0.9084 - val_acc: 0.6686\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9721 - acc: 0.5939 - val_loss: 0.9039 - val_acc: 0.6629\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9648 - acc: 0.5830 - val_loss: 0.8996 - val_acc: 0.6743\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9549 - acc: 0.5888 - val_loss: 0.9004 - val_acc: 0.6743\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9443 - acc: 0.6079 - val_loss: 0.9009 - val_acc: 0.6629\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9174 - acc: 0.6290 - val_loss: 0.9008 - val_acc: 0.6743\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9325 - acc: 0.6181 - val_loss: 0.8961 - val_acc: 0.6457\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9157 - acc: 0.6143 - val_loss: 0.8983 - val_acc: 0.6514\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9297 - acc: 0.6092 - val_loss: 0.8940 - val_acc: 0.6629\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9085 - acc: 0.6137 - val_loss: 0.8972 - val_acc: 0.6686\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.9153 - acc: 0.6245 - val_loss: 0.8971 - val_acc: 0.6743\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9177 - acc: 0.6130 - val_loss: 0.8909 - val_acc: 0.6743\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9093 - acc: 0.6213 - val_loss: 0.8881 - val_acc: 0.6743\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9037 - acc: 0.6277 - val_loss: 0.8870 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8968 - acc: 0.6290 - val_loss: 0.8875 - val_acc: 0.6743\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8852 - acc: 0.6245 - val_loss: 0.8897 - val_acc: 0.6743\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8857 - acc: 0.6315 - val_loss: 0.8913 - val_acc: 0.6686\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8930 - acc: 0.6296 - val_loss: 0.8958 - val_acc: 0.6629\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8762 - acc: 0.6501 - val_loss: 0.8908 - val_acc: 0.6571\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.8924 - acc: 0.6245 - val_loss: 0.8886 - val_acc: 0.6686\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8874 - acc: 0.6367 - val_loss: 0.8878 - val_acc: 0.6686\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8909 - acc: 0.6232 - val_loss: 0.8876 - val_acc: 0.6571\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8858 - acc: 0.6494 - val_loss: 0.8868 - val_acc: 0.6629\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8855 - acc: 0.6328 - val_loss: 0.8885 - val_acc: 0.6686\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8620 - acc: 0.6481 - val_loss: 0.8879 - val_acc: 0.6686\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 90us/step - loss: 0.8894 - acc: 0.6398 - val_loss: 0.8863 - val_acc: 0.6571\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8571 - acc: 0.6577 - val_loss: 0.8849 - val_acc: 0.6571\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8542 - acc: 0.6386 - val_loss: 0.8903 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8727 - acc: 0.6367 - val_loss: 0.8877 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 85us/step - loss: 0.8340 - acc: 0.6552 - val_loss: 0.8870 - val_acc: 0.6629\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8579 - acc: 0.6475 - val_loss: 0.8899 - val_acc: 0.6629\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8395 - acc: 0.6520 - val_loss: 0.8935 - val_acc: 0.6743\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8429 - acc: 0.6513 - val_loss: 0.8934 - val_acc: 0.6686\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8473 - acc: 0.6475 - val_loss: 0.8926 - val_acc: 0.6629\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8424 - acc: 0.6526 - val_loss: 0.8911 - val_acc: 0.6743\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8384 - acc: 0.6622 - val_loss: 0.8965 - val_acc: 0.6743\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8471 - acc: 0.6571 - val_loss: 0.8906 - val_acc: 0.6743\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8416 - acc: 0.6526 - val_loss: 0.8955 - val_acc: 0.6629\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8234 - acc: 0.6590 - val_loss: 0.9011 - val_acc: 0.6800\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.8292 - acc: 0.6609 - val_loss: 0.8953 - val_acc: 0.6629\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8161 - acc: 0.6628 - val_loss: 0.8941 - val_acc: 0.6514\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.8077 - acc: 0.6648 - val_loss: 0.8913 - val_acc: 0.6514\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8370 - acc: 0.6488 - val_loss: 0.8968 - val_acc: 0.6457\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7956 - acc: 0.6711 - val_loss: 0.8952 - val_acc: 0.6629\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8128 - acc: 0.6692 - val_loss: 0.8962 - val_acc: 0.6743\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8015 - acc: 0.6609 - val_loss: 0.8921 - val_acc: 0.6743\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7932 - acc: 0.6890 - val_loss: 0.8932 - val_acc: 0.6686\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8158 - acc: 0.6756 - val_loss: 0.8943 - val_acc: 0.6857\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8038 - acc: 0.6622 - val_loss: 0.8919 - val_acc: 0.6743\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8110 - acc: 0.6628 - val_loss: 0.8939 - val_acc: 0.6629\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7849 - acc: 0.6782 - val_loss: 0.8953 - val_acc: 0.6514\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8186 - acc: 0.6603 - val_loss: 0.8996 - val_acc: 0.6629\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7862 - acc: 0.6711 - val_loss: 0.8949 - val_acc: 0.6571\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7931 - acc: 0.6718 - val_loss: 0.8965 - val_acc: 0.6686\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8234 - acc: 0.6667 - val_loss: 0.8985 - val_acc: 0.6743\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8023 - acc: 0.6731 - val_loss: 0.8983 - val_acc: 0.6857\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7746 - acc: 0.6750 - val_loss: 0.8985 - val_acc: 0.6629\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 99us/step - loss: 0.7890 - acc: 0.6794 - val_loss: 0.8978 - val_acc: 0.6686\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7838 - acc: 0.6711 - val_loss: 0.9006 - val_acc: 0.6857\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7885 - acc: 0.6826 - val_loss: 0.9005 - val_acc: 0.6743\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7762 - acc: 0.6794 - val_loss: 0.9095 - val_acc: 0.6629\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8027 - acc: 0.6711 - val_loss: 0.9120 - val_acc: 0.6800\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7914 - acc: 0.6705 - val_loss: 0.9073 - val_acc: 0.6743\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7689 - acc: 0.6801 - val_loss: 0.9045 - val_acc: 0.6743\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7829 - acc: 0.6737 - val_loss: 0.9088 - val_acc: 0.6629\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7461 - acc: 0.6922 - val_loss: 0.9067 - val_acc: 0.6800\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7655 - acc: 0.6820 - val_loss: 0.9086 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7546 - acc: 0.6992 - val_loss: 0.9159 - val_acc: 0.6686\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7822 - acc: 0.6801 - val_loss: 0.9145 - val_acc: 0.6571\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7583 - acc: 0.6967 - val_loss: 0.9124 - val_acc: 0.6571\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7604 - acc: 0.6865 - val_loss: 0.9109 - val_acc: 0.6571\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7718 - acc: 0.6858 - val_loss: 0.9150 - val_acc: 0.6457\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7587 - acc: 0.6839 - val_loss: 0.9139 - val_acc: 0.6571\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 89us/step - loss: 0.7623 - acc: 0.6992 - val_loss: 0.9080 - val_acc: 0.6743\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.7572 - acc: 0.6756 - val_loss: 0.9132 - val_acc: 0.6800\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7518 - acc: 0.7139 - val_loss: 0.9212 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7565 - acc: 0.6699 - val_loss: 0.9157 - val_acc: 0.6629\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7299 - acc: 0.7018 - val_loss: 0.9164 - val_acc: 0.6629\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7523 - acc: 0.6935 - val_loss: 0.9195 - val_acc: 0.6457\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7399 - acc: 0.7101 - val_loss: 0.9182 - val_acc: 0.6514\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7358 - acc: 0.7031 - val_loss: 0.9167 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7304 - acc: 0.7018 - val_loss: 0.9184 - val_acc: 0.6400\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7446 - acc: 0.6980 - val_loss: 0.9188 - val_acc: 0.6571\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7466 - acc: 0.7024 - val_loss: 0.9277 - val_acc: 0.6686\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 92us/step - loss: 0.7552 - acc: 0.6999 - val_loss: 0.9285 - val_acc: 0.6629\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7307 - acc: 0.7126 - val_loss: 0.9280 - val_acc: 0.6457\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.7510 - acc: 0.6788 - val_loss: 0.9302 - val_acc: 0.6400\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.7356 - acc: 0.6948 - val_loss: 0.9285 - val_acc: 0.6571\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 84us/step - loss: 0.7336 - acc: 0.6980 - val_loss: 0.9274 - val_acc: 0.6343\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7271 - acc: 0.7075 - val_loss: 0.9288 - val_acc: 0.6457\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7341 - acc: 0.6980 - val_loss: 0.9303 - val_acc: 0.6457\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7235 - acc: 0.7139 - val_loss: 0.9328 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7339 - acc: 0.6928 - val_loss: 0.9366 - val_acc: 0.6514\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7278 - acc: 0.7063 - val_loss: 0.9412 - val_acc: 0.6514\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7237 - acc: 0.6999 - val_loss: 0.9402 - val_acc: 0.6457\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7256 - acc: 0.7120 - val_loss: 0.9416 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.7336 - acc: 0.6948 - val_loss: 0.9391 - val_acc: 0.6343\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7192 - acc: 0.7152 - val_loss: 0.9374 - val_acc: 0.6457\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7151 - acc: 0.7184 - val_loss: 0.9357 - val_acc: 0.6629\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7167 - acc: 0.7120 - val_loss: 0.9370 - val_acc: 0.6571\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7190 - acc: 0.7037 - val_loss: 0.9384 - val_acc: 0.6514\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7106 - acc: 0.7107 - val_loss: 0.9391 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 0.7077 - acc: 0.7184 - val_loss: 0.9415 - val_acc: 0.6457\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7140 - acc: 0.7107 - val_loss: 0.9451 - val_acc: 0.6400\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7015 - acc: 0.7120 - val_loss: 0.9434 - val_acc: 0.6629\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7041 - acc: 0.7235 - val_loss: 0.9429 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7037 - acc: 0.7146 - val_loss: 0.9448 - val_acc: 0.6343\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7003 - acc: 0.7171 - val_loss: 0.9418 - val_acc: 0.6286\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6831 - acc: 0.7114 - val_loss: 0.9366 - val_acc: 0.6400\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7086 - acc: 0.7107 - val_loss: 0.9392 - val_acc: 0.6286\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7029 - acc: 0.7229 - val_loss: 0.9388 - val_acc: 0.6343\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6769 - acc: 0.7280 - val_loss: 0.9443 - val_acc: 0.6229\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6961 - acc: 0.7114 - val_loss: 0.9447 - val_acc: 0.6457\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6717 - acc: 0.7318 - val_loss: 0.9526 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6894 - acc: 0.7197 - val_loss: 0.9529 - val_acc: 0.6286\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6832 - acc: 0.7216 - val_loss: 0.9475 - val_acc: 0.6457\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6834 - acc: 0.7126 - val_loss: 0.9512 - val_acc: 0.6343\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6867 - acc: 0.7261 - val_loss: 0.9572 - val_acc: 0.6514\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6856 - acc: 0.7241 - val_loss: 0.9557 - val_acc: 0.6457\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6891 - acc: 0.7305 - val_loss: 0.9539 - val_acc: 0.6571\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6814 - acc: 0.7184 - val_loss: 0.9620 - val_acc: 0.6400\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6886 - acc: 0.7286 - val_loss: 0.9584 - val_acc: 0.6343\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6750 - acc: 0.7254 - val_loss: 0.9546 - val_acc: 0.6400\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6771 - acc: 0.7222 - val_loss: 0.9605 - val_acc: 0.6457\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6717 - acc: 0.7184 - val_loss: 0.9630 - val_acc: 0.6343\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6855 - acc: 0.7414 - val_loss: 0.9685 - val_acc: 0.6286\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6823 - acc: 0.7216 - val_loss: 0.9655 - val_acc: 0.6400\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6763 - acc: 0.7286 - val_loss: 0.9736 - val_acc: 0.6286\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6774 - acc: 0.7337 - val_loss: 0.9788 - val_acc: 0.6343\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6779 - acc: 0.7216 - val_loss: 0.9694 - val_acc: 0.6514\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6651 - acc: 0.7267 - val_loss: 0.9743 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6850 - acc: 0.7190 - val_loss: 0.9815 - val_acc: 0.6343\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6665 - acc: 0.7261 - val_loss: 0.9813 - val_acc: 0.6343\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6549 - acc: 0.7395 - val_loss: 0.9768 - val_acc: 0.6286\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6574 - acc: 0.7484 - val_loss: 0.9775 - val_acc: 0.6343\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6444 - acc: 0.7363 - val_loss: 0.9784 - val_acc: 0.6343\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6685 - acc: 0.7216 - val_loss: 0.9768 - val_acc: 0.6457\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6702 - acc: 0.7241 - val_loss: 0.9779 - val_acc: 0.6400\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6606 - acc: 0.7305 - val_loss: 0.9803 - val_acc: 0.6343\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6506 - acc: 0.7407 - val_loss: 0.9823 - val_acc: 0.6514\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6599 - acc: 0.7312 - val_loss: 0.9873 - val_acc: 0.6571\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6417 - acc: 0.7401 - val_loss: 0.9890 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6497 - acc: 0.7395 - val_loss: 0.9861 - val_acc: 0.6514\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6496 - acc: 0.7312 - val_loss: 0.9840 - val_acc: 0.6571\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6480 - acc: 0.7446 - val_loss: 0.9882 - val_acc: 0.6629\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6364 - acc: 0.7542 - val_loss: 0.9969 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6743 - acc: 0.7280 - val_loss: 0.9920 - val_acc: 0.6457\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6402 - acc: 0.7280 - val_loss: 0.9931 - val_acc: 0.6286\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6229 - acc: 0.7497 - val_loss: 0.9983 - val_acc: 0.6400\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6372 - acc: 0.7305 - val_loss: 1.0042 - val_acc: 0.6343\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6553 - acc: 0.7350 - val_loss: 1.0002 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6442 - acc: 0.7261 - val_loss: 0.9922 - val_acc: 0.6457\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6470 - acc: 0.7222 - val_loss: 0.9881 - val_acc: 0.6571\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6259 - acc: 0.7388 - val_loss: 0.9897 - val_acc: 0.6571\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6294 - acc: 0.7478 - val_loss: 0.9905 - val_acc: 0.6400\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6390 - acc: 0.7363 - val_loss: 0.9977 - val_acc: 0.6400\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6357 - acc: 0.7382 - val_loss: 0.9960 - val_acc: 0.6514\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6325 - acc: 0.7407 - val_loss: 1.0011 - val_acc: 0.6457\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6118 - acc: 0.7535 - val_loss: 1.0057 - val_acc: 0.6343\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6390 - acc: 0.7395 - val_loss: 1.0108 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6223 - acc: 0.7529 - val_loss: 1.0082 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6121 - acc: 0.7516 - val_loss: 1.0081 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5953 - acc: 0.7554 - val_loss: 1.0150 - val_acc: 0.6400\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6257 - acc: 0.7363 - val_loss: 1.0155 - val_acc: 0.6343\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6182 - acc: 0.7510 - val_loss: 1.0127 - val_acc: 0.6514\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6124 - acc: 0.7516 - val_loss: 1.0059 - val_acc: 0.6686\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6224 - acc: 0.7516 - val_loss: 1.0049 - val_acc: 0.6514\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6263 - acc: 0.7471 - val_loss: 1.0107 - val_acc: 0.6400\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6137 - acc: 0.7439 - val_loss: 1.0092 - val_acc: 0.6571\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5996 - acc: 0.7676 - val_loss: 1.0091 - val_acc: 0.6571\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.6348 - acc: 0.7605 - val_loss: 1.0063 - val_acc: 0.6629\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6198 - acc: 0.7631 - val_loss: 1.0059 - val_acc: 0.6457\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6138 - acc: 0.7478 - val_loss: 1.0132 - val_acc: 0.6457\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6033 - acc: 0.7580 - val_loss: 1.0189 - val_acc: 0.6457\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6153 - acc: 0.7529 - val_loss: 1.0211 - val_acc: 0.6457\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6087 - acc: 0.7458 - val_loss: 1.0230 - val_acc: 0.6629\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6009 - acc: 0.7458 - val_loss: 1.0259 - val_acc: 0.6514\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5925 - acc: 0.7580 - val_loss: 1.0263 - val_acc: 0.6514\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6273 - acc: 0.7414 - val_loss: 1.0272 - val_acc: 0.6400\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6079 - acc: 0.7554 - val_loss: 1.0206 - val_acc: 0.6629\n",
      "194/194 [==============================] - 0s 47us/step\n",
      "1741/1741 [==============================] - 0s 33us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 9s 6ms/step - loss: 1.5688 - acc: 0.2904 - val_loss: 1.2966 - val_acc: 0.3657\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.3911 - acc: 0.3631 - val_loss: 1.1491 - val_acc: 0.5543\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.2677 - acc: 0.3989 - val_loss: 1.0800 - val_acc: 0.6114\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.2168 - acc: 0.4588 - val_loss: 1.0457 - val_acc: 0.6343\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.1461 - acc: 0.4939 - val_loss: 1.0166 - val_acc: 0.6286\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.1380 - acc: 0.4927 - val_loss: 1.0012 - val_acc: 0.6229\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.1126 - acc: 0.5073 - val_loss: 0.9895 - val_acc: 0.6171\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0969 - acc: 0.5163 - val_loss: 0.9685 - val_acc: 0.6457\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 1.0742 - acc: 0.5233 - val_loss: 0.9562 - val_acc: 0.6571\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0793 - acc: 0.5278 - val_loss: 0.9504 - val_acc: 0.6343\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0540 - acc: 0.5399 - val_loss: 0.9416 - val_acc: 0.6629\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0350 - acc: 0.5673 - val_loss: 0.9338 - val_acc: 0.6857\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 1.0253 - acc: 0.5546 - val_loss: 0.9313 - val_acc: 0.6571\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 1.0229 - acc: 0.5737 - val_loss: 0.9274 - val_acc: 0.6743\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0334 - acc: 0.5539 - val_loss: 0.9250 - val_acc: 0.6571\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.0260 - acc: 0.5514 - val_loss: 0.9213 - val_acc: 0.6629\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9803 - acc: 0.5884 - val_loss: 0.9167 - val_acc: 0.6514\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9883 - acc: 0.5750 - val_loss: 0.9099 - val_acc: 0.6857\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9801 - acc: 0.5718 - val_loss: 0.9086 - val_acc: 0.6914\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.9792 - acc: 0.5763 - val_loss: 0.9065 - val_acc: 0.6857\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9790 - acc: 0.5750 - val_loss: 0.9000 - val_acc: 0.6857\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9545 - acc: 0.6075 - val_loss: 0.8982 - val_acc: 0.6857\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9457 - acc: 0.5922 - val_loss: 0.8969 - val_acc: 0.6800\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9379 - acc: 0.6043 - val_loss: 0.8942 - val_acc: 0.6800\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9605 - acc: 0.5935 - val_loss: 0.8983 - val_acc: 0.6857\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9370 - acc: 0.6126 - val_loss: 0.9014 - val_acc: 0.6800\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9265 - acc: 0.6101 - val_loss: 0.9003 - val_acc: 0.6800\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9294 - acc: 0.6088 - val_loss: 0.8958 - val_acc: 0.6857\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.9347 - acc: 0.6101 - val_loss: 0.8952 - val_acc: 0.6914\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9287 - acc: 0.5980 - val_loss: 0.8936 - val_acc: 0.6857\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9144 - acc: 0.6190 - val_loss: 0.8903 - val_acc: 0.6971\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9377 - acc: 0.6024 - val_loss: 0.8920 - val_acc: 0.6971\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9041 - acc: 0.6241 - val_loss: 0.8924 - val_acc: 0.6914\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.9210 - acc: 0.6107 - val_loss: 0.8939 - val_acc: 0.6971\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.9200 - acc: 0.6158 - val_loss: 0.8903 - val_acc: 0.6971\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9055 - acc: 0.6248 - val_loss: 0.8933 - val_acc: 0.6914\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 111us/step - loss: 0.8829 - acc: 0.6305 - val_loss: 0.8942 - val_acc: 0.6971\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8848 - acc: 0.6280 - val_loss: 0.8945 - val_acc: 0.6971\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 94us/step - loss: 0.8696 - acc: 0.6177 - val_loss: 0.9004 - val_acc: 0.6914\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 93us/step - loss: 0.8780 - acc: 0.6452 - val_loss: 0.9013 - val_acc: 0.6914\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8890 - acc: 0.6248 - val_loss: 0.8980 - val_acc: 0.6914\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8808 - acc: 0.6369 - val_loss: 0.8967 - val_acc: 0.6914\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8929 - acc: 0.6139 - val_loss: 0.8911 - val_acc: 0.6800\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.8803 - acc: 0.6394 - val_loss: 0.8925 - val_acc: 0.6914\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8750 - acc: 0.6401 - val_loss: 0.8966 - val_acc: 0.6914\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8661 - acc: 0.6426 - val_loss: 0.9002 - val_acc: 0.6800\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8551 - acc: 0.6286 - val_loss: 0.8951 - val_acc: 0.6914\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8721 - acc: 0.6522 - val_loss: 0.8987 - val_acc: 0.6857\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8702 - acc: 0.6343 - val_loss: 0.8994 - val_acc: 0.6857\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8642 - acc: 0.6362 - val_loss: 0.9051 - val_acc: 0.6914\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.8497 - acc: 0.6503 - val_loss: 0.9017 - val_acc: 0.6971\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8490 - acc: 0.6560 - val_loss: 0.8995 - val_acc: 0.6914\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8456 - acc: 0.6522 - val_loss: 0.8987 - val_acc: 0.7029\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 90us/step - loss: 0.8392 - acc: 0.6407 - val_loss: 0.9020 - val_acc: 0.6914\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 129us/step - loss: 0.8416 - acc: 0.6426 - val_loss: 0.9009 - val_acc: 0.6857\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 136us/step - loss: 0.8527 - acc: 0.6490 - val_loss: 0.8999 - val_acc: 0.6800\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.8371 - acc: 0.6567 - val_loss: 0.9013 - val_acc: 0.6800\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 85us/step - loss: 0.8564 - acc: 0.6420 - val_loss: 0.9063 - val_acc: 0.6914\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8331 - acc: 0.6535 - val_loss: 0.9047 - val_acc: 0.6914\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8347 - acc: 0.6599 - val_loss: 0.9035 - val_acc: 0.6857\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8285 - acc: 0.6771 - val_loss: 0.9060 - val_acc: 0.6857\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8289 - acc: 0.6548 - val_loss: 0.9051 - val_acc: 0.6857\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8205 - acc: 0.6726 - val_loss: 0.9016 - val_acc: 0.6914\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8262 - acc: 0.6688 - val_loss: 0.9083 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8238 - acc: 0.6541 - val_loss: 0.9066 - val_acc: 0.6857\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8220 - acc: 0.6777 - val_loss: 0.9051 - val_acc: 0.6800\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8135 - acc: 0.6650 - val_loss: 0.9067 - val_acc: 0.6857\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8073 - acc: 0.6694 - val_loss: 0.9113 - val_acc: 0.6857\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8125 - acc: 0.6573 - val_loss: 0.9101 - val_acc: 0.6914\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8115 - acc: 0.6656 - val_loss: 0.9085 - val_acc: 0.6914\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8105 - acc: 0.6637 - val_loss: 0.9069 - val_acc: 0.7029\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8167 - acc: 0.6618 - val_loss: 0.9094 - val_acc: 0.6857\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8193 - acc: 0.6682 - val_loss: 0.9085 - val_acc: 0.6971\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8045 - acc: 0.6650 - val_loss: 0.9094 - val_acc: 0.6914\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7898 - acc: 0.6726 - val_loss: 0.9142 - val_acc: 0.6857\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8103 - acc: 0.6637 - val_loss: 0.9124 - val_acc: 0.6743\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8005 - acc: 0.6784 - val_loss: 0.9166 - val_acc: 0.6914\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8049 - acc: 0.6796 - val_loss: 0.9165 - val_acc: 0.6971\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7740 - acc: 0.6905 - val_loss: 0.9161 - val_acc: 0.6800\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7896 - acc: 0.6745 - val_loss: 0.9132 - val_acc: 0.6857\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7745 - acc: 0.6803 - val_loss: 0.9170 - val_acc: 0.6914\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7870 - acc: 0.6707 - val_loss: 0.9172 - val_acc: 0.6800\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8141 - acc: 0.6592 - val_loss: 0.9184 - val_acc: 0.6857\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7894 - acc: 0.6860 - val_loss: 0.9209 - val_acc: 0.6857\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7665 - acc: 0.6924 - val_loss: 0.9256 - val_acc: 0.6743\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7868 - acc: 0.6822 - val_loss: 0.9254 - val_acc: 0.6686\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7749 - acc: 0.6803 - val_loss: 0.9230 - val_acc: 0.6800\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7669 - acc: 0.6873 - val_loss: 0.9238 - val_acc: 0.6743\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7848 - acc: 0.6835 - val_loss: 0.9225 - val_acc: 0.6914\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7724 - acc: 0.6873 - val_loss: 0.9239 - val_acc: 0.6914\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7698 - acc: 0.6988 - val_loss: 0.9219 - val_acc: 0.6914\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7769 - acc: 0.6918 - val_loss: 0.9276 - val_acc: 0.6800\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7759 - acc: 0.6860 - val_loss: 0.9252 - val_acc: 0.6857\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7523 - acc: 0.6930 - val_loss: 0.9259 - val_acc: 0.6857\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7661 - acc: 0.6867 - val_loss: 0.9285 - val_acc: 0.6857\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7705 - acc: 0.6867 - val_loss: 0.9276 - val_acc: 0.6914\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7718 - acc: 0.6956 - val_loss: 0.9298 - val_acc: 0.6800\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7523 - acc: 0.6899 - val_loss: 0.9350 - val_acc: 0.6743\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7526 - acc: 0.6975 - val_loss: 0.9327 - val_acc: 0.6857\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7537 - acc: 0.6937 - val_loss: 0.9371 - val_acc: 0.6800\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7599 - acc: 0.6956 - val_loss: 0.9291 - val_acc: 0.6914\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7330 - acc: 0.7090 - val_loss: 0.9292 - val_acc: 0.6857\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7555 - acc: 0.6828 - val_loss: 0.9346 - val_acc: 0.6914\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7346 - acc: 0.6994 - val_loss: 0.9342 - val_acc: 0.6914\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7451 - acc: 0.6937 - val_loss: 0.9344 - val_acc: 0.6914\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7433 - acc: 0.7084 - val_loss: 0.9354 - val_acc: 0.6800\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7425 - acc: 0.7020 - val_loss: 0.9412 - val_acc: 0.6857\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7437 - acc: 0.7001 - val_loss: 0.9424 - val_acc: 0.6914\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7414 - acc: 0.6956 - val_loss: 0.9395 - val_acc: 0.6743\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7349 - acc: 0.7033 - val_loss: 0.9395 - val_acc: 0.6857\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7320 - acc: 0.7096 - val_loss: 0.9318 - val_acc: 0.6800\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7331 - acc: 0.7045 - val_loss: 0.9409 - val_acc: 0.6857\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7491 - acc: 0.6860 - val_loss: 0.9425 - val_acc: 0.6743\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7358 - acc: 0.6994 - val_loss: 0.9431 - val_acc: 0.6743\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7358 - acc: 0.6892 - val_loss: 0.9401 - val_acc: 0.6857\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7266 - acc: 0.7186 - val_loss: 0.9428 - val_acc: 0.6857\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7417 - acc: 0.6969 - val_loss: 0.9477 - val_acc: 0.6686\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7268 - acc: 0.7154 - val_loss: 0.9427 - val_acc: 0.6629\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7354 - acc: 0.6930 - val_loss: 0.9497 - val_acc: 0.6743\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7093 - acc: 0.7090 - val_loss: 0.9551 - val_acc: 0.6743\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7169 - acc: 0.7071 - val_loss: 0.9561 - val_acc: 0.6743\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.7342 - acc: 0.7090 - val_loss: 0.9626 - val_acc: 0.6629\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7103 - acc: 0.7039 - val_loss: 0.9538 - val_acc: 0.6629\n",
      "Epoch 124/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7185 - acc: 0.7084 - val_loss: 0.9500 - val_acc: 0.6571\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7296 - acc: 0.6918 - val_loss: 0.9537 - val_acc: 0.6514\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7301 - acc: 0.7045 - val_loss: 0.9590 - val_acc: 0.6514\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6988 - acc: 0.7230 - val_loss: 0.9557 - val_acc: 0.6629\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6956 - acc: 0.7262 - val_loss: 0.9577 - val_acc: 0.6629\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7063 - acc: 0.7058 - val_loss: 0.9608 - val_acc: 0.6571\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6950 - acc: 0.7116 - val_loss: 0.9655 - val_acc: 0.6686\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6994 - acc: 0.7186 - val_loss: 0.9785 - val_acc: 0.6743\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7048 - acc: 0.7186 - val_loss: 0.9794 - val_acc: 0.6571\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7051 - acc: 0.7090 - val_loss: 0.9746 - val_acc: 0.6629\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6791 - acc: 0.7339 - val_loss: 0.9766 - val_acc: 0.6629\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7040 - acc: 0.7173 - val_loss: 0.9841 - val_acc: 0.6514\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6992 - acc: 0.7256 - val_loss: 0.9795 - val_acc: 0.6743\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6860 - acc: 0.7288 - val_loss: 0.9875 - val_acc: 0.6400\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6747 - acc: 0.7339 - val_loss: 0.9800 - val_acc: 0.6514\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7072 - acc: 0.7192 - val_loss: 0.9850 - val_acc: 0.6514\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6724 - acc: 0.7250 - val_loss: 0.9914 - val_acc: 0.6571\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6977 - acc: 0.7307 - val_loss: 0.9906 - val_acc: 0.6629\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6950 - acc: 0.7154 - val_loss: 0.9871 - val_acc: 0.6571\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6908 - acc: 0.7173 - val_loss: 0.9833 - val_acc: 0.6571\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6742 - acc: 0.7173 - val_loss: 0.9879 - val_acc: 0.6514\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6731 - acc: 0.7307 - val_loss: 0.9933 - val_acc: 0.6514\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6953 - acc: 0.7269 - val_loss: 0.9951 - val_acc: 0.6514\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6871 - acc: 0.7230 - val_loss: 0.9954 - val_acc: 0.6514\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6768 - acc: 0.7364 - val_loss: 0.9924 - val_acc: 0.6629\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6708 - acc: 0.7339 - val_loss: 0.9924 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6597 - acc: 0.7167 - val_loss: 0.9911 - val_acc: 0.6400\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6615 - acc: 0.7288 - val_loss: 0.9939 - val_acc: 0.6514\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6840 - acc: 0.7288 - val_loss: 0.9976 - val_acc: 0.6457\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6661 - acc: 0.7192 - val_loss: 0.9961 - val_acc: 0.6571\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6728 - acc: 0.7147 - val_loss: 0.9932 - val_acc: 0.6514\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6891 - acc: 0.7218 - val_loss: 0.9973 - val_acc: 0.6514\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6629 - acc: 0.7192 - val_loss: 1.0012 - val_acc: 0.6514\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6590 - acc: 0.7454 - val_loss: 1.0065 - val_acc: 0.6514\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6665 - acc: 0.7307 - val_loss: 1.0057 - val_acc: 0.6571\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6787 - acc: 0.7224 - val_loss: 1.0026 - val_acc: 0.6457\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6526 - acc: 0.7364 - val_loss: 1.0045 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6590 - acc: 0.7294 - val_loss: 1.0030 - val_acc: 0.6743\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6566 - acc: 0.7332 - val_loss: 1.0047 - val_acc: 0.6514\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6461 - acc: 0.7409 - val_loss: 1.0064 - val_acc: 0.6571\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6539 - acc: 0.7198 - val_loss: 1.0046 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6374 - acc: 0.7473 - val_loss: 1.0081 - val_acc: 0.6400\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6662 - acc: 0.7294 - val_loss: 1.0102 - val_acc: 0.6343\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6449 - acc: 0.7371 - val_loss: 1.0160 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6672 - acc: 0.7294 - val_loss: 1.0201 - val_acc: 0.6457\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6597 - acc: 0.7352 - val_loss: 1.0150 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6509 - acc: 0.7332 - val_loss: 1.0151 - val_acc: 0.6343\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6480 - acc: 0.7307 - val_loss: 1.0179 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6401 - acc: 0.7377 - val_loss: 1.0252 - val_acc: 0.6400\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6559 - acc: 0.7294 - val_loss: 1.0243 - val_acc: 0.6457\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6502 - acc: 0.7422 - val_loss: 1.0245 - val_acc: 0.6457\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.6488 - acc: 0.7352 - val_loss: 1.0313 - val_acc: 0.6514\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6527 - acc: 0.7492 - val_loss: 1.0263 - val_acc: 0.6343\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6413 - acc: 0.7358 - val_loss: 1.0284 - val_acc: 0.6343\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6497 - acc: 0.7326 - val_loss: 1.0269 - val_acc: 0.6286\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6357 - acc: 0.7384 - val_loss: 1.0275 - val_acc: 0.6400\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6409 - acc: 0.7358 - val_loss: 1.0301 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6245 - acc: 0.7403 - val_loss: 1.0359 - val_acc: 0.6286\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6377 - acc: 0.7498 - val_loss: 1.0369 - val_acc: 0.6343\n",
      "Epoch 183/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6454 - acc: 0.7422 - val_loss: 1.0311 - val_acc: 0.6400\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6138 - acc: 0.7518 - val_loss: 1.0281 - val_acc: 0.6571\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6315 - acc: 0.7511 - val_loss: 1.0311 - val_acc: 0.6571\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6543 - acc: 0.7320 - val_loss: 1.0338 - val_acc: 0.6343\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6259 - acc: 0.7492 - val_loss: 1.0362 - val_acc: 0.6400\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6299 - acc: 0.7422 - val_loss: 1.0400 - val_acc: 0.6400\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6244 - acc: 0.7479 - val_loss: 1.0440 - val_acc: 0.6400\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6105 - acc: 0.7479 - val_loss: 1.0377 - val_acc: 0.6514\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6242 - acc: 0.7479 - val_loss: 1.0362 - val_acc: 0.6457\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6197 - acc: 0.7556 - val_loss: 1.0438 - val_acc: 0.6457\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6269 - acc: 0.7498 - val_loss: 1.0498 - val_acc: 0.6400\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6249 - acc: 0.7524 - val_loss: 1.0412 - val_acc: 0.6457\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6058 - acc: 0.7607 - val_loss: 1.0456 - val_acc: 0.6571\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6053 - acc: 0.7632 - val_loss: 1.0527 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6142 - acc: 0.7543 - val_loss: 1.0518 - val_acc: 0.6457\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6024 - acc: 0.7658 - val_loss: 1.0585 - val_acc: 0.6457\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6061 - acc: 0.7613 - val_loss: 1.0515 - val_acc: 0.6457\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6151 - acc: 0.7396 - val_loss: 1.0544 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 52us/step\n",
      "1742/1742 [==============================] - 0s 57us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 9s 6ms/step - loss: 1.5320 - acc: 0.2712 - val_loss: 1.2669 - val_acc: 0.4971\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.3423 - acc: 0.3765 - val_loss: 1.1539 - val_acc: 0.5257\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.2371 - acc: 0.4474 - val_loss: 1.0919 - val_acc: 0.5600\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.1877 - acc: 0.4620 - val_loss: 1.0557 - val_acc: 0.5829\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.1552 - acc: 0.4837 - val_loss: 1.0281 - val_acc: 0.6114\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.1227 - acc: 0.5067 - val_loss: 1.0058 - val_acc: 0.6171\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.1080 - acc: 0.5278 - val_loss: 0.9869 - val_acc: 0.6114\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0815 - acc: 0.5246 - val_loss: 0.9717 - val_acc: 0.6286\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0790 - acc: 0.5258 - val_loss: 0.9634 - val_acc: 0.6400\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0606 - acc: 0.5329 - val_loss: 0.9527 - val_acc: 0.6286\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0250 - acc: 0.5475 - val_loss: 0.9397 - val_acc: 0.6571\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0383 - acc: 0.5392 - val_loss: 0.9298 - val_acc: 0.6571\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0390 - acc: 0.5546 - val_loss: 0.9238 - val_acc: 0.6686\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0146 - acc: 0.5756 - val_loss: 0.9185 - val_acc: 0.6571\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9813 - acc: 0.5839 - val_loss: 0.9146 - val_acc: 0.6400\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9806 - acc: 0.5814 - val_loss: 0.9104 - val_acc: 0.6514\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9932 - acc: 0.5712 - val_loss: 0.9068 - val_acc: 0.6629\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9763 - acc: 0.5903 - val_loss: 0.9006 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9605 - acc: 0.5986 - val_loss: 0.8925 - val_acc: 0.6686\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9586 - acc: 0.5929 - val_loss: 0.8940 - val_acc: 0.6629\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9668 - acc: 0.5871 - val_loss: 0.8879 - val_acc: 0.6514\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9490 - acc: 0.6114 - val_loss: 0.8878 - val_acc: 0.6400\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9438 - acc: 0.6139 - val_loss: 0.8868 - val_acc: 0.6514\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9460 - acc: 0.5833 - val_loss: 0.8863 - val_acc: 0.6286\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9315 - acc: 0.6133 - val_loss: 0.8816 - val_acc: 0.6457\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9343 - acc: 0.5980 - val_loss: 0.8792 - val_acc: 0.6571\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9197 - acc: 0.6126 - val_loss: 0.8834 - val_acc: 0.6571\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9340 - acc: 0.5935 - val_loss: 0.8833 - val_acc: 0.6800\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9183 - acc: 0.6120 - val_loss: 0.8813 - val_acc: 0.6743\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9088 - acc: 0.6190 - val_loss: 0.8814 - val_acc: 0.6686\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9137 - acc: 0.6107 - val_loss: 0.8816 - val_acc: 0.6571\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8949 - acc: 0.6197 - val_loss: 0.8758 - val_acc: 0.6514\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8860 - acc: 0.6305 - val_loss: 0.8736 - val_acc: 0.6514\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8870 - acc: 0.6248 - val_loss: 0.8701 - val_acc: 0.6686\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9038 - acc: 0.6158 - val_loss: 0.8700 - val_acc: 0.6743\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8799 - acc: 0.6477 - val_loss: 0.8700 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8807 - acc: 0.6222 - val_loss: 0.8664 - val_acc: 0.6686\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8732 - acc: 0.6235 - val_loss: 0.8678 - val_acc: 0.6800\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8652 - acc: 0.6305 - val_loss: 0.8642 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8730 - acc: 0.6337 - val_loss: 0.8635 - val_acc: 0.6743\n",
      "Epoch 41/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8716 - acc: 0.6554 - val_loss: 0.8610 - val_acc: 0.6686\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8552 - acc: 0.6394 - val_loss: 0.8644 - val_acc: 0.6571\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8519 - acc: 0.6484 - val_loss: 0.8645 - val_acc: 0.6571\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8546 - acc: 0.6433 - val_loss: 0.8620 - val_acc: 0.6743\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8543 - acc: 0.6452 - val_loss: 0.8635 - val_acc: 0.6686\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8551 - acc: 0.6452 - val_loss: 0.8664 - val_acc: 0.6743\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8596 - acc: 0.6452 - val_loss: 0.8659 - val_acc: 0.6629\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8554 - acc: 0.6433 - val_loss: 0.8652 - val_acc: 0.6457\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8701 - acc: 0.6375 - val_loss: 0.8678 - val_acc: 0.6514\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8360 - acc: 0.6490 - val_loss: 0.8696 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8442 - acc: 0.6567 - val_loss: 0.8686 - val_acc: 0.6514\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8319 - acc: 0.6624 - val_loss: 0.8673 - val_acc: 0.6686\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8366 - acc: 0.6554 - val_loss: 0.8665 - val_acc: 0.6686\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8464 - acc: 0.6484 - val_loss: 0.8678 - val_acc: 0.6514\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8240 - acc: 0.6643 - val_loss: 0.8696 - val_acc: 0.6400\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8223 - acc: 0.6662 - val_loss: 0.8698 - val_acc: 0.6400\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8311 - acc: 0.6643 - val_loss: 0.8702 - val_acc: 0.6457\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7991 - acc: 0.6765 - val_loss: 0.8676 - val_acc: 0.6629\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8101 - acc: 0.6548 - val_loss: 0.8686 - val_acc: 0.6571\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7992 - acc: 0.6707 - val_loss: 0.8662 - val_acc: 0.6514\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8164 - acc: 0.6701 - val_loss: 0.8596 - val_acc: 0.6686\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8180 - acc: 0.6611 - val_loss: 0.8603 - val_acc: 0.6629\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8227 - acc: 0.6650 - val_loss: 0.8589 - val_acc: 0.6629\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7925 - acc: 0.6707 - val_loss: 0.8584 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7912 - acc: 0.6771 - val_loss: 0.8628 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8145 - acc: 0.6720 - val_loss: 0.8612 - val_acc: 0.6457\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7898 - acc: 0.6822 - val_loss: 0.8634 - val_acc: 0.6514\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7873 - acc: 0.6841 - val_loss: 0.8608 - val_acc: 0.6514\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7876 - acc: 0.6745 - val_loss: 0.8612 - val_acc: 0.6571\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7854 - acc: 0.6835 - val_loss: 0.8653 - val_acc: 0.6571\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7912 - acc: 0.6720 - val_loss: 0.8650 - val_acc: 0.6457\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7923 - acc: 0.6739 - val_loss: 0.8637 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7857 - acc: 0.6784 - val_loss: 0.8622 - val_acc: 0.6514\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7756 - acc: 0.6765 - val_loss: 0.8626 - val_acc: 0.6457\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7735 - acc: 0.6899 - val_loss: 0.8640 - val_acc: 0.6400\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7847 - acc: 0.6765 - val_loss: 0.8660 - val_acc: 0.6571\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7748 - acc: 0.6854 - val_loss: 0.8703 - val_acc: 0.6400\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7572 - acc: 0.6950 - val_loss: 0.8659 - val_acc: 0.6514\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7644 - acc: 0.6860 - val_loss: 0.8653 - val_acc: 0.6571\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7792 - acc: 0.6758 - val_loss: 0.8680 - val_acc: 0.6457\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7616 - acc: 0.6937 - val_loss: 0.8699 - val_acc: 0.6571\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7455 - acc: 0.6918 - val_loss: 0.8706 - val_acc: 0.6629\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7627 - acc: 0.6828 - val_loss: 0.8697 - val_acc: 0.6457\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7796 - acc: 0.6822 - val_loss: 0.8714 - val_acc: 0.6514\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7727 - acc: 0.6816 - val_loss: 0.8776 - val_acc: 0.6400\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7664 - acc: 0.6943 - val_loss: 0.8815 - val_acc: 0.6457\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7381 - acc: 0.6981 - val_loss: 0.8777 - val_acc: 0.6457\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7637 - acc: 0.6886 - val_loss: 0.8793 - val_acc: 0.6457\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7588 - acc: 0.6784 - val_loss: 0.8721 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7699 - acc: 0.6918 - val_loss: 0.8744 - val_acc: 0.6457\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7329 - acc: 0.7128 - val_loss: 0.8796 - val_acc: 0.6343\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7529 - acc: 0.6841 - val_loss: 0.8797 - val_acc: 0.6571\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7546 - acc: 0.6899 - val_loss: 0.8778 - val_acc: 0.6457\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7588 - acc: 0.6867 - val_loss: 0.8771 - val_acc: 0.6457\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7562 - acc: 0.6969 - val_loss: 0.8800 - val_acc: 0.6457\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7423 - acc: 0.6988 - val_loss: 0.8770 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7237 - acc: 0.7167 - val_loss: 0.8859 - val_acc: 0.6457\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7374 - acc: 0.7045 - val_loss: 0.8902 - val_acc: 0.6457\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7421 - acc: 0.7052 - val_loss: 0.8842 - val_acc: 0.6514\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7365 - acc: 0.6988 - val_loss: 0.8828 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7451 - acc: 0.6962 - val_loss: 0.8885 - val_acc: 0.6457\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7350 - acc: 0.6981 - val_loss: 0.8844 - val_acc: 0.6457\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7361 - acc: 0.7064 - val_loss: 0.8846 - val_acc: 0.6457\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7236 - acc: 0.7077 - val_loss: 0.8850 - val_acc: 0.6343\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7301 - acc: 0.7103 - val_loss: 0.8866 - val_acc: 0.6457\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7360 - acc: 0.6930 - val_loss: 0.8869 - val_acc: 0.6457\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.7252 - acc: 0.6943 - val_loss: 0.8841 - val_acc: 0.6457\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7229 - acc: 0.7109 - val_loss: 0.8821 - val_acc: 0.6571\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7350 - acc: 0.7058 - val_loss: 0.8796 - val_acc: 0.6514\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7337 - acc: 0.7103 - val_loss: 0.8785 - val_acc: 0.6514\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7374 - acc: 0.6981 - val_loss: 0.8811 - val_acc: 0.6514\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7191 - acc: 0.7020 - val_loss: 0.8832 - val_acc: 0.6571\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7136 - acc: 0.7090 - val_loss: 0.8848 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7138 - acc: 0.7077 - val_loss: 0.8856 - val_acc: 0.6514\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7259 - acc: 0.7020 - val_loss: 0.8882 - val_acc: 0.6457\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6881 - acc: 0.7192 - val_loss: 0.8889 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7110 - acc: 0.7141 - val_loss: 0.8862 - val_acc: 0.6514\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7162 - acc: 0.7128 - val_loss: 0.8902 - val_acc: 0.6514\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6909 - acc: 0.7122 - val_loss: 0.8872 - val_acc: 0.6457\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6809 - acc: 0.7230 - val_loss: 0.8904 - val_acc: 0.6571\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7135 - acc: 0.7154 - val_loss: 0.8950 - val_acc: 0.6457\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6955 - acc: 0.7084 - val_loss: 0.8889 - val_acc: 0.6514\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6910 - acc: 0.7071 - val_loss: 0.8847 - val_acc: 0.6514\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6942 - acc: 0.7192 - val_loss: 0.8891 - val_acc: 0.6571\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6895 - acc: 0.7211 - val_loss: 0.8939 - val_acc: 0.6457\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7029 - acc: 0.7154 - val_loss: 0.8916 - val_acc: 0.6514\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7013 - acc: 0.7237 - val_loss: 0.8901 - val_acc: 0.6457\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7051 - acc: 0.7045 - val_loss: 0.8943 - val_acc: 0.6457\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6756 - acc: 0.7281 - val_loss: 0.8954 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7026 - acc: 0.7128 - val_loss: 0.8940 - val_acc: 0.6514\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6888 - acc: 0.7243 - val_loss: 0.8932 - val_acc: 0.6514\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6690 - acc: 0.7275 - val_loss: 0.8945 - val_acc: 0.6514\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6993 - acc: 0.7090 - val_loss: 0.8966 - val_acc: 0.6457\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6778 - acc: 0.7301 - val_loss: 0.9009 - val_acc: 0.6457\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6738 - acc: 0.7339 - val_loss: 0.9014 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6765 - acc: 0.7205 - val_loss: 0.8975 - val_acc: 0.6457\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6710 - acc: 0.7250 - val_loss: 0.8960 - val_acc: 0.6571\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6743 - acc: 0.7403 - val_loss: 0.8999 - val_acc: 0.6571\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6697 - acc: 0.7237 - val_loss: 0.9015 - val_acc: 0.6514\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6649 - acc: 0.7326 - val_loss: 0.9067 - val_acc: 0.6514\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6748 - acc: 0.7332 - val_loss: 0.9115 - val_acc: 0.6457\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6619 - acc: 0.7294 - val_loss: 0.9113 - val_acc: 0.6514\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6643 - acc: 0.7326 - val_loss: 0.9055 - val_acc: 0.6514\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6772 - acc: 0.7262 - val_loss: 0.9058 - val_acc: 0.6629\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6853 - acc: 0.7186 - val_loss: 0.9018 - val_acc: 0.6629\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6596 - acc: 0.7256 - val_loss: 0.9053 - val_acc: 0.6571\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6474 - acc: 0.7313 - val_loss: 0.9060 - val_acc: 0.6514\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6505 - acc: 0.7473 - val_loss: 0.9007 - val_acc: 0.6514\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6611 - acc: 0.7141 - val_loss: 0.8986 - val_acc: 0.6514\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6719 - acc: 0.7377 - val_loss: 0.9009 - val_acc: 0.6571\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6673 - acc: 0.7307 - val_loss: 0.9061 - val_acc: 0.6571\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6675 - acc: 0.7218 - val_loss: 0.9058 - val_acc: 0.6571\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6524 - acc: 0.7371 - val_loss: 0.9053 - val_acc: 0.6514\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6567 - acc: 0.7243 - val_loss: 0.9084 - val_acc: 0.6571\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6533 - acc: 0.7320 - val_loss: 0.9117 - val_acc: 0.6514\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6476 - acc: 0.7256 - val_loss: 0.9061 - val_acc: 0.6514\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6538 - acc: 0.7250 - val_loss: 0.9103 - val_acc: 0.6457\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6535 - acc: 0.7384 - val_loss: 0.9196 - val_acc: 0.6343\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6429 - acc: 0.7384 - val_loss: 0.9234 - val_acc: 0.6400\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6498 - acc: 0.7301 - val_loss: 0.9178 - val_acc: 0.6457\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6277 - acc: 0.7511 - val_loss: 0.9115 - val_acc: 0.6514\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6427 - acc: 0.7396 - val_loss: 0.9137 - val_acc: 0.6571\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6289 - acc: 0.7479 - val_loss: 0.9184 - val_acc: 0.6571\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6469 - acc: 0.7479 - val_loss: 0.9240 - val_acc: 0.6514\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6505 - acc: 0.7390 - val_loss: 0.9258 - val_acc: 0.6571\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6194 - acc: 0.7556 - val_loss: 0.9258 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6325 - acc: 0.7505 - val_loss: 0.9280 - val_acc: 0.6571\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6267 - acc: 0.7447 - val_loss: 0.9302 - val_acc: 0.6629\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6229 - acc: 0.7518 - val_loss: 0.9245 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6315 - acc: 0.7447 - val_loss: 0.9271 - val_acc: 0.6400\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6338 - acc: 0.7358 - val_loss: 0.9292 - val_acc: 0.6571\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6140 - acc: 0.7441 - val_loss: 0.9240 - val_acc: 0.6571\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6105 - acc: 0.7728 - val_loss: 0.9284 - val_acc: 0.6571\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6072 - acc: 0.7671 - val_loss: 0.9281 - val_acc: 0.6571\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6407 - acc: 0.7460 - val_loss: 0.9313 - val_acc: 0.6571\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6228 - acc: 0.7466 - val_loss: 0.9344 - val_acc: 0.6571\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6107 - acc: 0.7498 - val_loss: 0.9395 - val_acc: 0.6571\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6124 - acc: 0.7505 - val_loss: 0.9411 - val_acc: 0.6686\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6243 - acc: 0.7479 - val_loss: 0.9441 - val_acc: 0.6514\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6137 - acc: 0.7473 - val_loss: 0.9387 - val_acc: 0.6571\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6131 - acc: 0.7569 - val_loss: 0.9326 - val_acc: 0.6571\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6043 - acc: 0.7549 - val_loss: 0.9334 - val_acc: 0.6514\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5968 - acc: 0.7690 - val_loss: 0.9383 - val_acc: 0.6457\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6172 - acc: 0.7447 - val_loss: 0.9446 - val_acc: 0.6343\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5966 - acc: 0.7664 - val_loss: 0.9483 - val_acc: 0.6457\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5947 - acc: 0.7594 - val_loss: 0.9417 - val_acc: 0.6686\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6062 - acc: 0.7632 - val_loss: 0.9405 - val_acc: 0.6400\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6041 - acc: 0.7498 - val_loss: 0.9504 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5931 - acc: 0.7498 - val_loss: 0.9597 - val_acc: 0.6514\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6037 - acc: 0.7575 - val_loss: 0.9555 - val_acc: 0.6571\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5839 - acc: 0.7703 - val_loss: 0.9506 - val_acc: 0.6686\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6120 - acc: 0.7518 - val_loss: 0.9535 - val_acc: 0.6629\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5967 - acc: 0.7639 - val_loss: 0.9469 - val_acc: 0.6629\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5960 - acc: 0.7562 - val_loss: 0.9450 - val_acc: 0.6743\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5706 - acc: 0.7722 - val_loss: 0.9493 - val_acc: 0.6743\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6080 - acc: 0.7645 - val_loss: 0.9533 - val_acc: 0.6571\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5846 - acc: 0.7569 - val_loss: 0.9506 - val_acc: 0.6514\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5971 - acc: 0.7632 - val_loss: 0.9503 - val_acc: 0.6571\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.5995 - acc: 0.7562 - val_loss: 0.9611 - val_acc: 0.6457\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5923 - acc: 0.7652 - val_loss: 0.9610 - val_acc: 0.6514\n",
      "193/193 [==============================] - 0s 53us/step\n",
      "1742/1742 [==============================] - 0s 36us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 10s 7ms/step - loss: 1.5520 - acc: 0.2763 - val_loss: 1.2572 - val_acc: 0.4400\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 1.3625 - acc: 0.3631 - val_loss: 1.1500 - val_acc: 0.5257\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 115us/step - loss: 1.2777 - acc: 0.4199 - val_loss: 1.0947 - val_acc: 0.5371\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 112us/step - loss: 1.2510 - acc: 0.4474 - val_loss: 1.0637 - val_acc: 0.5714\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.1692 - acc: 0.4920 - val_loss: 1.0364 - val_acc: 0.5829\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 1.1675 - acc: 0.4863 - val_loss: 1.0158 - val_acc: 0.5943\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 1.1123 - acc: 0.4990 - val_loss: 0.9935 - val_acc: 0.5943\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 1.1118 - acc: 0.5227 - val_loss: 0.9802 - val_acc: 0.6000\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 1.0891 - acc: 0.5303 - val_loss: 0.9701 - val_acc: 0.6114\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.0666 - acc: 0.5233 - val_loss: 0.9576 - val_acc: 0.6229\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0724 - acc: 0.5418 - val_loss: 0.9490 - val_acc: 0.6114\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.0389 - acc: 0.5514 - val_loss: 0.9404 - val_acc: 0.6343\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0390 - acc: 0.5629 - val_loss: 0.9281 - val_acc: 0.6457\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0360 - acc: 0.5456 - val_loss: 0.9240 - val_acc: 0.6800\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0107 - acc: 0.5629 - val_loss: 0.9212 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0062 - acc: 0.5692 - val_loss: 0.9190 - val_acc: 0.6514\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0070 - acc: 0.5648 - val_loss: 0.9142 - val_acc: 0.6571\n",
      "Epoch 18/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9933 - acc: 0.5648 - val_loss: 0.9095 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0019 - acc: 0.5769 - val_loss: 0.9079 - val_acc: 0.6629\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.9717 - acc: 0.5973 - val_loss: 0.9036 - val_acc: 0.6571\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9588 - acc: 0.5967 - val_loss: 0.8994 - val_acc: 0.6857\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9538 - acc: 0.6069 - val_loss: 0.9013 - val_acc: 0.6800\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.9564 - acc: 0.5871 - val_loss: 0.9023 - val_acc: 0.6743\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.9461 - acc: 0.5999 - val_loss: 0.8981 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 81us/step - loss: 0.9303 - acc: 0.6216 - val_loss: 0.8933 - val_acc: 0.6743\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.9439 - acc: 0.5909 - val_loss: 0.8936 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9173 - acc: 0.6146 - val_loss: 0.8960 - val_acc: 0.6686\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9363 - acc: 0.6094 - val_loss: 0.8931 - val_acc: 0.6800\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9326 - acc: 0.6146 - val_loss: 0.8939 - val_acc: 0.6800\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 92us/step - loss: 0.9197 - acc: 0.6203 - val_loss: 0.8927 - val_acc: 0.6800\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.9086 - acc: 0.6171 - val_loss: 0.8946 - val_acc: 0.6743\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8957 - acc: 0.6165 - val_loss: 0.8919 - val_acc: 0.6857\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9256 - acc: 0.6094 - val_loss: 0.8942 - val_acc: 0.6914\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8950 - acc: 0.6235 - val_loss: 0.8938 - val_acc: 0.6914\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.9019 - acc: 0.6133 - val_loss: 0.8875 - val_acc: 0.6800\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8939 - acc: 0.6248 - val_loss: 0.8854 - val_acc: 0.6857\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8940 - acc: 0.6228 - val_loss: 0.8925 - val_acc: 0.6629\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8738 - acc: 0.6350 - val_loss: 0.8900 - val_acc: 0.6743\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.8685 - acc: 0.6439 - val_loss: 0.8967 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 89us/step - loss: 0.9007 - acc: 0.6190 - val_loss: 0.8960 - val_acc: 0.6629\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.8914 - acc: 0.6260 - val_loss: 0.8913 - val_acc: 0.6571\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8844 - acc: 0.6235 - val_loss: 0.8898 - val_acc: 0.6686\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8730 - acc: 0.6382 - val_loss: 0.8940 - val_acc: 0.6743\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8570 - acc: 0.6420 - val_loss: 0.8889 - val_acc: 0.6743\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8691 - acc: 0.6324 - val_loss: 0.8842 - val_acc: 0.6800\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8660 - acc: 0.6280 - val_loss: 0.8833 - val_acc: 0.6686\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8788 - acc: 0.6375 - val_loss: 0.8898 - val_acc: 0.6686\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8548 - acc: 0.6509 - val_loss: 0.8917 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8518 - acc: 0.6426 - val_loss: 0.8945 - val_acc: 0.6800\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8625 - acc: 0.6445 - val_loss: 0.8982 - val_acc: 0.6743\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8394 - acc: 0.6433 - val_loss: 0.8941 - val_acc: 0.6800\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8358 - acc: 0.6509 - val_loss: 0.8961 - val_acc: 0.6800\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8510 - acc: 0.6439 - val_loss: 0.8921 - val_acc: 0.6686\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8400 - acc: 0.6548 - val_loss: 0.8986 - val_acc: 0.6686\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8324 - acc: 0.6522 - val_loss: 0.8981 - val_acc: 0.6686\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8170 - acc: 0.6707 - val_loss: 0.8952 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8361 - acc: 0.6465 - val_loss: 0.8926 - val_acc: 0.6800\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8234 - acc: 0.6573 - val_loss: 0.8847 - val_acc: 0.6686\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8264 - acc: 0.6650 - val_loss: 0.8919 - val_acc: 0.6629\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8247 - acc: 0.6573 - val_loss: 0.8925 - val_acc: 0.6800\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8180 - acc: 0.6643 - val_loss: 0.8934 - val_acc: 0.6800\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8324 - acc: 0.6599 - val_loss: 0.8949 - val_acc: 0.6686\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8159 - acc: 0.6707 - val_loss: 0.8997 - val_acc: 0.6686\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8319 - acc: 0.6458 - val_loss: 0.9012 - val_acc: 0.6514\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8109 - acc: 0.6579 - val_loss: 0.9022 - val_acc: 0.6629\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8218 - acc: 0.6688 - val_loss: 0.9053 - val_acc: 0.6571\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8289 - acc: 0.6490 - val_loss: 0.9081 - val_acc: 0.6686\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7964 - acc: 0.6682 - val_loss: 0.9000 - val_acc: 0.6571\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8113 - acc: 0.6682 - val_loss: 0.9093 - val_acc: 0.6514\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8174 - acc: 0.6579 - val_loss: 0.9074 - val_acc: 0.6514\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8095 - acc: 0.6611 - val_loss: 0.9086 - val_acc: 0.6571\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8014 - acc: 0.6816 - val_loss: 0.9150 - val_acc: 0.6571\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7931 - acc: 0.6720 - val_loss: 0.9163 - val_acc: 0.6514\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8045 - acc: 0.6777 - val_loss: 0.9060 - val_acc: 0.6457\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7881 - acc: 0.6694 - val_loss: 0.9057 - val_acc: 0.6400\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7964 - acc: 0.6733 - val_loss: 0.9117 - val_acc: 0.6514\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7919 - acc: 0.6777 - val_loss: 0.9091 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7833 - acc: 0.6790 - val_loss: 0.9070 - val_acc: 0.6457\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7915 - acc: 0.6688 - val_loss: 0.9047 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7794 - acc: 0.6867 - val_loss: 0.9070 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7626 - acc: 0.6937 - val_loss: 0.9103 - val_acc: 0.6571\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7871 - acc: 0.6707 - val_loss: 0.9163 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7658 - acc: 0.6930 - val_loss: 0.9150 - val_acc: 0.6400\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7826 - acc: 0.6873 - val_loss: 0.9127 - val_acc: 0.6514\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7709 - acc: 0.6828 - val_loss: 0.9236 - val_acc: 0.6571\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7766 - acc: 0.6886 - val_loss: 0.9272 - val_acc: 0.6514\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7847 - acc: 0.6886 - val_loss: 0.9271 - val_acc: 0.6457\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7741 - acc: 0.6809 - val_loss: 0.9225 - val_acc: 0.6571\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7780 - acc: 0.6752 - val_loss: 0.9237 - val_acc: 0.6571\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7698 - acc: 0.6745 - val_loss: 0.9216 - val_acc: 0.6514\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7553 - acc: 0.7013 - val_loss: 0.9238 - val_acc: 0.6514\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7619 - acc: 0.6988 - val_loss: 0.9193 - val_acc: 0.6514\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7515 - acc: 0.6918 - val_loss: 0.9209 - val_acc: 0.6571\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7716 - acc: 0.6771 - val_loss: 0.9233 - val_acc: 0.6571\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7630 - acc: 0.6975 - val_loss: 0.9318 - val_acc: 0.6514\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7416 - acc: 0.6873 - val_loss: 0.9338 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7659 - acc: 0.6822 - val_loss: 0.9327 - val_acc: 0.6514\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7523 - acc: 0.6873 - val_loss: 0.9283 - val_acc: 0.6457\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7473 - acc: 0.6892 - val_loss: 0.9226 - val_acc: 0.6343\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7516 - acc: 0.6969 - val_loss: 0.9312 - val_acc: 0.6571\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7517 - acc: 0.7033 - val_loss: 0.9347 - val_acc: 0.6514\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7535 - acc: 0.6937 - val_loss: 0.9281 - val_acc: 0.6343\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7384 - acc: 0.7026 - val_loss: 0.9322 - val_acc: 0.6514\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7543 - acc: 0.6911 - val_loss: 0.9298 - val_acc: 0.6571\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7435 - acc: 0.7052 - val_loss: 0.9298 - val_acc: 0.6571\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7510 - acc: 0.6873 - val_loss: 0.9351 - val_acc: 0.6571\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7439 - acc: 0.7007 - val_loss: 0.9389 - val_acc: 0.6514\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7196 - acc: 0.7109 - val_loss: 0.9385 - val_acc: 0.6571\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7290 - acc: 0.6899 - val_loss: 0.9344 - val_acc: 0.6571\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7395 - acc: 0.7071 - val_loss: 0.9328 - val_acc: 0.6457\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7492 - acc: 0.6975 - val_loss: 0.9320 - val_acc: 0.6400\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7211 - acc: 0.7122 - val_loss: 0.9304 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7237 - acc: 0.6969 - val_loss: 0.9362 - val_acc: 0.6514\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7284 - acc: 0.7071 - val_loss: 0.9353 - val_acc: 0.6514\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7367 - acc: 0.6905 - val_loss: 0.9421 - val_acc: 0.6457\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7127 - acc: 0.6994 - val_loss: 0.9419 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7365 - acc: 0.6911 - val_loss: 0.9467 - val_acc: 0.6400\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7111 - acc: 0.7141 - val_loss: 0.9463 - val_acc: 0.6343\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7141 - acc: 0.7128 - val_loss: 0.9531 - val_acc: 0.6400\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7339 - acc: 0.7001 - val_loss: 0.9503 - val_acc: 0.6343\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7143 - acc: 0.7160 - val_loss: 0.9490 - val_acc: 0.6400\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7352 - acc: 0.7103 - val_loss: 0.9489 - val_acc: 0.6571\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7200 - acc: 0.7052 - val_loss: 0.9391 - val_acc: 0.6457\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7008 - acc: 0.7077 - val_loss: 0.9404 - val_acc: 0.6457\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7009 - acc: 0.7211 - val_loss: 0.9446 - val_acc: 0.6400\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7169 - acc: 0.7192 - val_loss: 0.9497 - val_acc: 0.6343\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7248 - acc: 0.7128 - val_loss: 0.9536 - val_acc: 0.6457\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7130 - acc: 0.7071 - val_loss: 0.9527 - val_acc: 0.6457\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7123 - acc: 0.7084 - val_loss: 0.9521 - val_acc: 0.6457\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6941 - acc: 0.7313 - val_loss: 0.9520 - val_acc: 0.6457\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6995 - acc: 0.7160 - val_loss: 0.9459 - val_acc: 0.6514\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7178 - acc: 0.7077 - val_loss: 0.9509 - val_acc: 0.6457\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6967 - acc: 0.7077 - val_loss: 0.9551 - val_acc: 0.6514\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7135 - acc: 0.7205 - val_loss: 0.9563 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6990 - acc: 0.7077 - val_loss: 0.9573 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6838 - acc: 0.7384 - val_loss: 0.9553 - val_acc: 0.6457\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6845 - acc: 0.7243 - val_loss: 0.9571 - val_acc: 0.6400\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6804 - acc: 0.7198 - val_loss: 0.9668 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6755 - acc: 0.7307 - val_loss: 0.9603 - val_acc: 0.6400\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6881 - acc: 0.7141 - val_loss: 0.9671 - val_acc: 0.6343\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6833 - acc: 0.7294 - val_loss: 0.9591 - val_acc: 0.6400\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6940 - acc: 0.7160 - val_loss: 0.9644 - val_acc: 0.6343\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6762 - acc: 0.7390 - val_loss: 0.9714 - val_acc: 0.6400\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6760 - acc: 0.7281 - val_loss: 0.9696 - val_acc: 0.6514\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6922 - acc: 0.7116 - val_loss: 0.9693 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6516 - acc: 0.7428 - val_loss: 0.9751 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6705 - acc: 0.7262 - val_loss: 0.9710 - val_acc: 0.6400\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6752 - acc: 0.7224 - val_loss: 0.9745 - val_acc: 0.6286\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6568 - acc: 0.7301 - val_loss: 0.9757 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6761 - acc: 0.7192 - val_loss: 0.9739 - val_acc: 0.6343\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6800 - acc: 0.7256 - val_loss: 0.9778 - val_acc: 0.6286\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6432 - acc: 0.7415 - val_loss: 0.9764 - val_acc: 0.6343\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6461 - acc: 0.7345 - val_loss: 0.9737 - val_acc: 0.6400\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6592 - acc: 0.7224 - val_loss: 0.9758 - val_acc: 0.6457\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6571 - acc: 0.7281 - val_loss: 0.9787 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6572 - acc: 0.7262 - val_loss: 0.9766 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6589 - acc: 0.7218 - val_loss: 0.9754 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6684 - acc: 0.7358 - val_loss: 0.9774 - val_acc: 0.6400\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6576 - acc: 0.7339 - val_loss: 0.9780 - val_acc: 0.6514\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6772 - acc: 0.7281 - val_loss: 0.9769 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6655 - acc: 0.7192 - val_loss: 0.9816 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6469 - acc: 0.7384 - val_loss: 0.9874 - val_acc: 0.6286\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6668 - acc: 0.7224 - val_loss: 0.9872 - val_acc: 0.6229\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6544 - acc: 0.7352 - val_loss: 0.9963 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6440 - acc: 0.7332 - val_loss: 0.9923 - val_acc: 0.6229\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6371 - acc: 0.7447 - val_loss: 0.9950 - val_acc: 0.6171\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 86us/step - loss: 0.6446 - acc: 0.7371 - val_loss: 0.9924 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6401 - acc: 0.7281 - val_loss: 0.9978 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6432 - acc: 0.7396 - val_loss: 1.0006 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6393 - acc: 0.7435 - val_loss: 0.9932 - val_acc: 0.6343\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6551 - acc: 0.7281 - val_loss: 0.9998 - val_acc: 0.6343\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6307 - acc: 0.7479 - val_loss: 1.0008 - val_acc: 0.6343\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6331 - acc: 0.7339 - val_loss: 1.0003 - val_acc: 0.6400\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6332 - acc: 0.7492 - val_loss: 0.9924 - val_acc: 0.6457\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6410 - acc: 0.7403 - val_loss: 0.9929 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6356 - acc: 0.7422 - val_loss: 0.9981 - val_acc: 0.6400\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6450 - acc: 0.7345 - val_loss: 1.0071 - val_acc: 0.6343\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6165 - acc: 0.7390 - val_loss: 1.0072 - val_acc: 0.6229\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6439 - acc: 0.7441 - val_loss: 1.0073 - val_acc: 0.6343\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6279 - acc: 0.7473 - val_loss: 0.9988 - val_acc: 0.6286\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6117 - acc: 0.7486 - val_loss: 0.9998 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6205 - acc: 0.7428 - val_loss: 1.0054 - val_acc: 0.6343\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6237 - acc: 0.7441 - val_loss: 1.0105 - val_acc: 0.6286\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6416 - acc: 0.7498 - val_loss: 1.0059 - val_acc: 0.6286\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6297 - acc: 0.7301 - val_loss: 1.0094 - val_acc: 0.6343\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5991 - acc: 0.7518 - val_loss: 1.0066 - val_acc: 0.6286\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6200 - acc: 0.7543 - val_loss: 1.0076 - val_acc: 0.6286\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6279 - acc: 0.7460 - val_loss: 1.0033 - val_acc: 0.6229\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6214 - acc: 0.7473 - val_loss: 1.0019 - val_acc: 0.6286\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6196 - acc: 0.7511 - val_loss: 1.0023 - val_acc: 0.6229\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5973 - acc: 0.7709 - val_loss: 1.0037 - val_acc: 0.6229\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6102 - acc: 0.7518 - val_loss: 1.0084 - val_acc: 0.6286\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6047 - acc: 0.7664 - val_loss: 1.0168 - val_acc: 0.6343\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6003 - acc: 0.7639 - val_loss: 1.0204 - val_acc: 0.6400\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6113 - acc: 0.7511 - val_loss: 1.0181 - val_acc: 0.6229\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.5973 - acc: 0.7492 - val_loss: 1.0181 - val_acc: 0.6286\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6204 - acc: 0.7492 - val_loss: 1.0157 - val_acc: 0.6400\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6158 - acc: 0.7409 - val_loss: 1.0136 - val_acc: 0.6343\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6120 - acc: 0.7524 - val_loss: 1.0165 - val_acc: 0.6286\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.5974 - acc: 0.7581 - val_loss: 1.0206 - val_acc: 0.6229\n",
      "193/193 [==============================] - 0s 59us/step\n",
      "1742/1742 [==============================] - 0s 28us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 8s 5ms/step - loss: 1.5842 - acc: 0.2757 - val_loss: 1.2781 - val_acc: 0.3829\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.3526 - acc: 0.3835 - val_loss: 1.1673 - val_acc: 0.5029\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.2708 - acc: 0.4046 - val_loss: 1.1055 - val_acc: 0.5886\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.2266 - acc: 0.4378 - val_loss: 1.0682 - val_acc: 0.6171\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.1713 - acc: 0.4690 - val_loss: 1.0424 - val_acc: 0.6114\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 1.1491 - acc: 0.4933 - val_loss: 1.0284 - val_acc: 0.6057\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.1152 - acc: 0.5137 - val_loss: 1.0073 - val_acc: 0.6286\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.1167 - acc: 0.5137 - val_loss: 0.9920 - val_acc: 0.6457\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0850 - acc: 0.5246 - val_loss: 0.9792 - val_acc: 0.6286\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.0782 - acc: 0.5392 - val_loss: 0.9642 - val_acc: 0.6286\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0556 - acc: 0.5373 - val_loss: 0.9609 - val_acc: 0.6229\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 1.0454 - acc: 0.5552 - val_loss: 0.9530 - val_acc: 0.6286\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0406 - acc: 0.5558 - val_loss: 0.9475 - val_acc: 0.6286\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0157 - acc: 0.5660 - val_loss: 0.9432 - val_acc: 0.6514\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 1.0193 - acc: 0.5571 - val_loss: 0.9370 - val_acc: 0.6400\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0092 - acc: 0.5680 - val_loss: 0.9340 - val_acc: 0.6629\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9806 - acc: 0.5877 - val_loss: 0.9298 - val_acc: 0.6571\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9932 - acc: 0.5731 - val_loss: 0.9203 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9889 - acc: 0.5795 - val_loss: 0.9198 - val_acc: 0.6514\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9896 - acc: 0.5775 - val_loss: 0.9180 - val_acc: 0.6629\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9641 - acc: 0.5948 - val_loss: 0.9191 - val_acc: 0.6514\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9639 - acc: 0.6050 - val_loss: 0.9130 - val_acc: 0.6857\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9634 - acc: 0.6069 - val_loss: 0.9107 - val_acc: 0.6686\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9508 - acc: 0.6152 - val_loss: 0.9065 - val_acc: 0.6914\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9289 - acc: 0.5999 - val_loss: 0.9079 - val_acc: 0.6857\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9384 - acc: 0.6069 - val_loss: 0.9082 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9444 - acc: 0.5916 - val_loss: 0.9015 - val_acc: 0.6686\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9115 - acc: 0.6165 - val_loss: 0.9048 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9140 - acc: 0.6299 - val_loss: 0.9039 - val_acc: 0.6686\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9372 - acc: 0.6094 - val_loss: 0.9045 - val_acc: 0.6800\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9287 - acc: 0.6101 - val_loss: 0.9007 - val_acc: 0.6857\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8935 - acc: 0.6203 - val_loss: 0.9008 - val_acc: 0.6800\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9025 - acc: 0.6286 - val_loss: 0.8973 - val_acc: 0.6743\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9073 - acc: 0.6254 - val_loss: 0.8973 - val_acc: 0.6914\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.8889 - acc: 0.6343 - val_loss: 0.8967 - val_acc: 0.6800\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8922 - acc: 0.6465 - val_loss: 0.9010 - val_acc: 0.6800\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8890 - acc: 0.6311 - val_loss: 0.9025 - val_acc: 0.6571\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8798 - acc: 0.6433 - val_loss: 0.8988 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8708 - acc: 0.6394 - val_loss: 0.8964 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8778 - acc: 0.6465 - val_loss: 0.8988 - val_acc: 0.6743\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8747 - acc: 0.6356 - val_loss: 0.8995 - val_acc: 0.6743\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8784 - acc: 0.6286 - val_loss: 0.9030 - val_acc: 0.6743\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8747 - acc: 0.6439 - val_loss: 0.8934 - val_acc: 0.6857\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8784 - acc: 0.6375 - val_loss: 0.8924 - val_acc: 0.6857\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8704 - acc: 0.6471 - val_loss: 0.8904 - val_acc: 0.6914\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8520 - acc: 0.6541 - val_loss: 0.8858 - val_acc: 0.6971\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8658 - acc: 0.6394 - val_loss: 0.8889 - val_acc: 0.6857\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8456 - acc: 0.6643 - val_loss: 0.8939 - val_acc: 0.6743\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8751 - acc: 0.6452 - val_loss: 0.8914 - val_acc: 0.6629\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8641 - acc: 0.6350 - val_loss: 0.8951 - val_acc: 0.6629\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8400 - acc: 0.6579 - val_loss: 0.8969 - val_acc: 0.6800\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8351 - acc: 0.6579 - val_loss: 0.8968 - val_acc: 0.6686\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8359 - acc: 0.6586 - val_loss: 0.9011 - val_acc: 0.6629\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8405 - acc: 0.6458 - val_loss: 0.8985 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8272 - acc: 0.6535 - val_loss: 0.8936 - val_acc: 0.6571\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8416 - acc: 0.6611 - val_loss: 0.8946 - val_acc: 0.6686\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8271 - acc: 0.6573 - val_loss: 0.8991 - val_acc: 0.6629\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8148 - acc: 0.6682 - val_loss: 0.9004 - val_acc: 0.6514\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8333 - acc: 0.6401 - val_loss: 0.8995 - val_acc: 0.6571\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8332 - acc: 0.6637 - val_loss: 0.8961 - val_acc: 0.6629\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8178 - acc: 0.6694 - val_loss: 0.9001 - val_acc: 0.6571\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8160 - acc: 0.6784 - val_loss: 0.8956 - val_acc: 0.6629\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8179 - acc: 0.6669 - val_loss: 0.9005 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8222 - acc: 0.6599 - val_loss: 0.8988 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8351 - acc: 0.6567 - val_loss: 0.9019 - val_acc: 0.6629\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8236 - acc: 0.6624 - val_loss: 0.9039 - val_acc: 0.6571\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8081 - acc: 0.6522 - val_loss: 0.8996 - val_acc: 0.6629\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8113 - acc: 0.6745 - val_loss: 0.9027 - val_acc: 0.6686\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8004 - acc: 0.6548 - val_loss: 0.9059 - val_acc: 0.6571\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8053 - acc: 0.6745 - val_loss: 0.9072 - val_acc: 0.6686\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7911 - acc: 0.6822 - val_loss: 0.9137 - val_acc: 0.6629\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7922 - acc: 0.6809 - val_loss: 0.9099 - val_acc: 0.6629\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7932 - acc: 0.6675 - val_loss: 0.9126 - val_acc: 0.6686\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7852 - acc: 0.6867 - val_loss: 0.9077 - val_acc: 0.6571\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7845 - acc: 0.6694 - val_loss: 0.9116 - val_acc: 0.6571\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7931 - acc: 0.6841 - val_loss: 0.9129 - val_acc: 0.6571\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7793 - acc: 0.6835 - val_loss: 0.9130 - val_acc: 0.6629\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7883 - acc: 0.6765 - val_loss: 0.9163 - val_acc: 0.6571\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7987 - acc: 0.6707 - val_loss: 0.9153 - val_acc: 0.6571\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7799 - acc: 0.6854 - val_loss: 0.9180 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7815 - acc: 0.6777 - val_loss: 0.9262 - val_acc: 0.6514\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8033 - acc: 0.6765 - val_loss: 0.9210 - val_acc: 0.6629\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7748 - acc: 0.6943 - val_loss: 0.9220 - val_acc: 0.6629\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7861 - acc: 0.6771 - val_loss: 0.9174 - val_acc: 0.6514\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7638 - acc: 0.6937 - val_loss: 0.9258 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7708 - acc: 0.6809 - val_loss: 0.9228 - val_acc: 0.6571\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7635 - acc: 0.6918 - val_loss: 0.9246 - val_acc: 0.6629\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7648 - acc: 0.6873 - val_loss: 0.9244 - val_acc: 0.6686\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7656 - acc: 0.6981 - val_loss: 0.9287 - val_acc: 0.6629\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7826 - acc: 0.6822 - val_loss: 0.9314 - val_acc: 0.6629\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7632 - acc: 0.6994 - val_loss: 0.9319 - val_acc: 0.6743\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7594 - acc: 0.6905 - val_loss: 0.9347 - val_acc: 0.6571\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7562 - acc: 0.6943 - val_loss: 0.9363 - val_acc: 0.6571\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7609 - acc: 0.6969 - val_loss: 0.9331 - val_acc: 0.6629\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7453 - acc: 0.6969 - val_loss: 0.9299 - val_acc: 0.6571\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7609 - acc: 0.6930 - val_loss: 0.9334 - val_acc: 0.6743\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7597 - acc: 0.6860 - val_loss: 0.9392 - val_acc: 0.6629\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7539 - acc: 0.7007 - val_loss: 0.9433 - val_acc: 0.6686\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7573 - acc: 0.6867 - val_loss: 0.9383 - val_acc: 0.6571\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7539 - acc: 0.6905 - val_loss: 0.9405 - val_acc: 0.6514\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7674 - acc: 0.6841 - val_loss: 0.9473 - val_acc: 0.6457\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7362 - acc: 0.7020 - val_loss: 0.9451 - val_acc: 0.6514\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7360 - acc: 0.6969 - val_loss: 0.9459 - val_acc: 0.6286\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7273 - acc: 0.6994 - val_loss: 0.9467 - val_acc: 0.6286\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7405 - acc: 0.7033 - val_loss: 0.9448 - val_acc: 0.6400\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7519 - acc: 0.6943 - val_loss: 0.9467 - val_acc: 0.6343\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7301 - acc: 0.7026 - val_loss: 0.9530 - val_acc: 0.6343\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7345 - acc: 0.7147 - val_loss: 0.9451 - val_acc: 0.6343\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7443 - acc: 0.6905 - val_loss: 0.9457 - val_acc: 0.6457\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7304 - acc: 0.7039 - val_loss: 0.9500 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7312 - acc: 0.7084 - val_loss: 0.9505 - val_acc: 0.6343\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7152 - acc: 0.7109 - val_loss: 0.9515 - val_acc: 0.6457\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7316 - acc: 0.7001 - val_loss: 0.9551 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7315 - acc: 0.7109 - val_loss: 0.9501 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7160 - acc: 0.7141 - val_loss: 0.9470 - val_acc: 0.6343\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7182 - acc: 0.7256 - val_loss: 0.9498 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7338 - acc: 0.7045 - val_loss: 0.9439 - val_acc: 0.6286\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7255 - acc: 0.7141 - val_loss: 0.9465 - val_acc: 0.6343\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7034 - acc: 0.7167 - val_loss: 0.9483 - val_acc: 0.6400\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7383 - acc: 0.7007 - val_loss: 0.9527 - val_acc: 0.6400\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7245 - acc: 0.6962 - val_loss: 0.9567 - val_acc: 0.6457\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7032 - acc: 0.7103 - val_loss: 0.9593 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6979 - acc: 0.7377 - val_loss: 0.9550 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7114 - acc: 0.7160 - val_loss: 0.9562 - val_acc: 0.6514\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7088 - acc: 0.7122 - val_loss: 0.9541 - val_acc: 0.6457\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7075 - acc: 0.6924 - val_loss: 0.9528 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7203 - acc: 0.6981 - val_loss: 0.9584 - val_acc: 0.6457\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7051 - acc: 0.7192 - val_loss: 0.9623 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7201 - acc: 0.7045 - val_loss: 0.9633 - val_acc: 0.6457\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7008 - acc: 0.7116 - val_loss: 0.9625 - val_acc: 0.6457\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7141 - acc: 0.7141 - val_loss: 0.9652 - val_acc: 0.6457\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7013 - acc: 0.7243 - val_loss: 0.9689 - val_acc: 0.6571\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7017 - acc: 0.7147 - val_loss: 0.9718 - val_acc: 0.6457\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7103 - acc: 0.7103 - val_loss: 0.9727 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6970 - acc: 0.7198 - val_loss: 0.9708 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6752 - acc: 0.7269 - val_loss: 0.9759 - val_acc: 0.6514\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6884 - acc: 0.7173 - val_loss: 0.9754 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.6900 - acc: 0.7026 - val_loss: 0.9734 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6770 - acc: 0.7256 - val_loss: 0.9705 - val_acc: 0.6514\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6772 - acc: 0.7269 - val_loss: 0.9739 - val_acc: 0.6514\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6871 - acc: 0.7377 - val_loss: 0.9713 - val_acc: 0.6457\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6795 - acc: 0.7415 - val_loss: 0.9770 - val_acc: 0.6343\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6833 - acc: 0.7243 - val_loss: 0.9822 - val_acc: 0.6286\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6725 - acc: 0.7288 - val_loss: 0.9836 - val_acc: 0.6343\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6780 - acc: 0.7301 - val_loss: 0.9840 - val_acc: 0.6343\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6876 - acc: 0.7281 - val_loss: 0.9854 - val_acc: 0.6286\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6714 - acc: 0.7384 - val_loss: 0.9859 - val_acc: 0.6343\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6758 - acc: 0.7256 - val_loss: 0.9831 - val_acc: 0.6400\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6845 - acc: 0.7288 - val_loss: 0.9828 - val_acc: 0.6457\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6806 - acc: 0.7224 - val_loss: 0.9855 - val_acc: 0.6400\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6790 - acc: 0.7307 - val_loss: 0.9898 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6817 - acc: 0.7173 - val_loss: 0.9899 - val_acc: 0.6343\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6717 - acc: 0.7358 - val_loss: 0.9886 - val_acc: 0.6571\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6677 - acc: 0.7320 - val_loss: 0.9875 - val_acc: 0.6514\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6525 - acc: 0.7524 - val_loss: 0.9923 - val_acc: 0.6400\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6440 - acc: 0.7505 - val_loss: 0.9925 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6611 - acc: 0.7390 - val_loss: 0.9909 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6622 - acc: 0.7352 - val_loss: 0.9978 - val_acc: 0.6286\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6734 - acc: 0.7250 - val_loss: 0.9960 - val_acc: 0.6343\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6307 - acc: 0.7505 - val_loss: 0.9943 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6494 - acc: 0.7556 - val_loss: 0.9935 - val_acc: 0.6457\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6722 - acc: 0.7288 - val_loss: 0.9981 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6632 - acc: 0.7422 - val_loss: 1.0003 - val_acc: 0.6343\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6555 - acc: 0.7415 - val_loss: 1.0006 - val_acc: 0.6514\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6383 - acc: 0.7460 - val_loss: 0.9998 - val_acc: 0.6514\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6522 - acc: 0.7422 - val_loss: 0.9988 - val_acc: 0.6571\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6498 - acc: 0.7415 - val_loss: 1.0054 - val_acc: 0.6514\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6454 - acc: 0.7486 - val_loss: 1.0031 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6382 - acc: 0.7549 - val_loss: 1.0053 - val_acc: 0.6343\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6489 - acc: 0.7428 - val_loss: 1.0086 - val_acc: 0.6343\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6569 - acc: 0.7396 - val_loss: 1.0069 - val_acc: 0.6457\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6334 - acc: 0.7524 - val_loss: 1.0055 - val_acc: 0.6400\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6311 - acc: 0.7428 - val_loss: 1.0100 - val_acc: 0.6571\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6467 - acc: 0.7339 - val_loss: 1.0105 - val_acc: 0.6457\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6438 - acc: 0.7435 - val_loss: 1.0135 - val_acc: 0.6514\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6312 - acc: 0.7537 - val_loss: 1.0112 - val_acc: 0.6286\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6346 - acc: 0.7620 - val_loss: 1.0132 - val_acc: 0.6286\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6166 - acc: 0.7562 - val_loss: 1.0198 - val_acc: 0.6286\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6333 - acc: 0.7460 - val_loss: 1.0200 - val_acc: 0.6286\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6488 - acc: 0.7377 - val_loss: 1.0166 - val_acc: 0.6286\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6251 - acc: 0.7466 - val_loss: 1.0174 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6455 - acc: 0.7518 - val_loss: 1.0178 - val_acc: 0.6457\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 46us/step - loss: 0.6114 - acc: 0.7524 - val_loss: 1.0156 - val_acc: 0.6343\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6400 - acc: 0.7518 - val_loss: 1.0197 - val_acc: 0.6400\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6183 - acc: 0.7581 - val_loss: 1.0166 - val_acc: 0.6400\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6242 - acc: 0.7562 - val_loss: 1.0156 - val_acc: 0.6400\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6210 - acc: 0.7569 - val_loss: 1.0179 - val_acc: 0.6457\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6239 - acc: 0.7549 - val_loss: 1.0262 - val_acc: 0.6343\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6162 - acc: 0.7556 - val_loss: 1.0284 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6332 - acc: 0.7518 - val_loss: 1.0250 - val_acc: 0.6229\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6177 - acc: 0.7594 - val_loss: 1.0240 - val_acc: 0.6286\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6102 - acc: 0.7511 - val_loss: 1.0197 - val_acc: 0.6343\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6020 - acc: 0.7760 - val_loss: 1.0254 - val_acc: 0.6286\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5965 - acc: 0.7543 - val_loss: 1.0345 - val_acc: 0.6343\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6137 - acc: 0.7575 - val_loss: 1.0333 - val_acc: 0.6457\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6236 - acc: 0.7569 - val_loss: 1.0391 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6082 - acc: 0.7466 - val_loss: 1.0395 - val_acc: 0.6343\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6267 - acc: 0.7460 - val_loss: 1.0341 - val_acc: 0.6343\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6058 - acc: 0.7709 - val_loss: 1.0297 - val_acc: 0.6286\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6296 - acc: 0.7543 - val_loss: 1.0281 - val_acc: 0.6343\n",
      "193/193 [==============================] - 0s 64us/step\n",
      "1742/1742 [==============================] - 0s 42us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 9s 6ms/step - loss: 1.4950 - acc: 0.2840 - val_loss: 1.2802 - val_acc: 0.4571\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.3129 - acc: 0.3784 - val_loss: 1.1914 - val_acc: 0.4514\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.2363 - acc: 0.4397 - val_loss: 1.1356 - val_acc: 0.4971\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.2127 - acc: 0.4563 - val_loss: 1.1026 - val_acc: 0.5086\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.1675 - acc: 0.4799 - val_loss: 1.0767 - val_acc: 0.5371\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.1347 - acc: 0.5041 - val_loss: 1.0550 - val_acc: 0.5486\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.1258 - acc: 0.5118 - val_loss: 1.0366 - val_acc: 0.5714\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.0825 - acc: 0.5188 - val_loss: 1.0278 - val_acc: 0.5714\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 1.0523 - acc: 0.5469 - val_loss: 1.0111 - val_acc: 0.5714\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0621 - acc: 0.5450 - val_loss: 1.0075 - val_acc: 0.5771\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0400 - acc: 0.5520 - val_loss: 1.0045 - val_acc: 0.5714\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 1.0538 - acc: 0.5565 - val_loss: 0.9897 - val_acc: 0.5771\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.0229 - acc: 0.5590 - val_loss: 0.9826 - val_acc: 0.5829\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.0266 - acc: 0.5348 - val_loss: 0.9891 - val_acc: 0.5886\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.0120 - acc: 0.5686 - val_loss: 0.9837 - val_acc: 0.5600\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9792 - acc: 0.5833 - val_loss: 0.9767 - val_acc: 0.5714\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9718 - acc: 0.5839 - val_loss: 0.9746 - val_acc: 0.5714\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9802 - acc: 0.5820 - val_loss: 0.9748 - val_acc: 0.5600\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9748 - acc: 0.5788 - val_loss: 0.9728 - val_acc: 0.5657\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9725 - acc: 0.5916 - val_loss: 0.9722 - val_acc: 0.5600\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.9506 - acc: 0.6069 - val_loss: 0.9789 - val_acc: 0.5714\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9532 - acc: 0.6056 - val_loss: 0.9718 - val_acc: 0.5657\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9498 - acc: 0.5948 - val_loss: 0.9708 - val_acc: 0.5657\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9520 - acc: 0.5999 - val_loss: 0.9688 - val_acc: 0.5543\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9443 - acc: 0.5960 - val_loss: 0.9727 - val_acc: 0.5543\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9359 - acc: 0.6114 - val_loss: 0.9746 - val_acc: 0.5543\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9322 - acc: 0.6120 - val_loss: 0.9641 - val_acc: 0.5600\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9341 - acc: 0.6126 - val_loss: 0.9688 - val_acc: 0.5600\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9093 - acc: 0.6260 - val_loss: 0.9726 - val_acc: 0.5771\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9133 - acc: 0.6158 - val_loss: 0.9748 - val_acc: 0.5657\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9052 - acc: 0.6139 - val_loss: 0.9727 - val_acc: 0.5600\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9336 - acc: 0.6158 - val_loss: 0.9707 - val_acc: 0.5600\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9148 - acc: 0.6114 - val_loss: 0.9739 - val_acc: 0.5714\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9096 - acc: 0.6280 - val_loss: 0.9757 - val_acc: 0.5657\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9002 - acc: 0.6235 - val_loss: 0.9785 - val_acc: 0.5829\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9053 - acc: 0.6101 - val_loss: 0.9724 - val_acc: 0.5657\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8747 - acc: 0.6318 - val_loss: 0.9697 - val_acc: 0.5714\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8916 - acc: 0.6248 - val_loss: 0.9738 - val_acc: 0.5771\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8817 - acc: 0.6458 - val_loss: 0.9727 - val_acc: 0.5714\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8951 - acc: 0.6311 - val_loss: 0.9641 - val_acc: 0.5714\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8850 - acc: 0.6394 - val_loss: 0.9715 - val_acc: 0.5771\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8865 - acc: 0.6337 - val_loss: 0.9722 - val_acc: 0.5771\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8806 - acc: 0.6382 - val_loss: 0.9725 - val_acc: 0.5829\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8547 - acc: 0.6452 - val_loss: 0.9703 - val_acc: 0.5771\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8691 - acc: 0.6477 - val_loss: 0.9741 - val_acc: 0.5829\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8588 - acc: 0.6560 - val_loss: 0.9737 - val_acc: 0.5886\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8474 - acc: 0.6637 - val_loss: 0.9755 - val_acc: 0.5829\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8677 - acc: 0.6445 - val_loss: 0.9689 - val_acc: 0.5829\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8735 - acc: 0.6541 - val_loss: 0.9651 - val_acc: 0.5943\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8510 - acc: 0.6522 - val_loss: 0.9679 - val_acc: 0.5886\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8406 - acc: 0.6560 - val_loss: 0.9762 - val_acc: 0.5771\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8454 - acc: 0.6324 - val_loss: 0.9714 - val_acc: 0.5829\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8644 - acc: 0.6490 - val_loss: 0.9717 - val_acc: 0.5829\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8302 - acc: 0.6694 - val_loss: 0.9739 - val_acc: 0.5771\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8324 - acc: 0.6528 - val_loss: 0.9789 - val_acc: 0.6000\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8435 - acc: 0.6484 - val_loss: 0.9806 - val_acc: 0.6000\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8419 - acc: 0.6586 - val_loss: 0.9791 - val_acc: 0.5943\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.8262 - acc: 0.6573 - val_loss: 0.9841 - val_acc: 0.5943\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.8536 - acc: 0.6471 - val_loss: 0.9827 - val_acc: 0.6000\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8391 - acc: 0.6535 - val_loss: 0.9893 - val_acc: 0.5943\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8203 - acc: 0.6752 - val_loss: 0.9898 - val_acc: 0.6000\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8304 - acc: 0.6516 - val_loss: 0.9856 - val_acc: 0.5886\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8354 - acc: 0.6484 - val_loss: 0.9818 - val_acc: 0.5771\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.8328 - acc: 0.6599 - val_loss: 0.9771 - val_acc: 0.5829\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.8076 - acc: 0.6567 - val_loss: 0.9792 - val_acc: 0.5829\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8160 - acc: 0.6765 - val_loss: 0.9822 - val_acc: 0.5886\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8077 - acc: 0.6745 - val_loss: 0.9848 - val_acc: 0.5943\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8099 - acc: 0.6758 - val_loss: 0.9911 - val_acc: 0.5943\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7935 - acc: 0.6828 - val_loss: 0.9878 - val_acc: 0.5886\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8203 - acc: 0.6548 - val_loss: 0.9813 - val_acc: 0.6057\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.8091 - acc: 0.6682 - val_loss: 0.9861 - val_acc: 0.5886\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7983 - acc: 0.6765 - val_loss: 0.9909 - val_acc: 0.5943\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7948 - acc: 0.6688 - val_loss: 0.9862 - val_acc: 0.5829\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7951 - acc: 0.6643 - val_loss: 0.9953 - val_acc: 0.5829\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 82us/step - loss: 0.8138 - acc: 0.6752 - val_loss: 1.0009 - val_acc: 0.5886\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7923 - acc: 0.6918 - val_loss: 0.9981 - val_acc: 0.5886\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7929 - acc: 0.6809 - val_loss: 0.9997 - val_acc: 0.5829\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7868 - acc: 0.6707 - val_loss: 1.0028 - val_acc: 0.5886\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.7988 - acc: 0.6860 - val_loss: 1.0030 - val_acc: 0.5829\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7815 - acc: 0.6765 - val_loss: 1.0027 - val_acc: 0.5886\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7628 - acc: 0.6867 - val_loss: 0.9994 - val_acc: 0.5829\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8051 - acc: 0.6752 - val_loss: 1.0012 - val_acc: 0.5829\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7931 - acc: 0.6701 - val_loss: 0.9967 - val_acc: 0.5943\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7860 - acc: 0.6796 - val_loss: 0.9947 - val_acc: 0.5886\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7676 - acc: 0.6816 - val_loss: 0.9930 - val_acc: 0.5771\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7904 - acc: 0.6720 - val_loss: 0.9976 - val_acc: 0.5829\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7693 - acc: 0.7020 - val_loss: 0.9887 - val_acc: 0.5829\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7842 - acc: 0.6847 - val_loss: 0.9864 - val_acc: 0.5886\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7838 - acc: 0.6790 - val_loss: 0.9996 - val_acc: 0.5886\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7395 - acc: 0.6981 - val_loss: 0.9976 - val_acc: 0.5886\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7590 - acc: 0.7013 - val_loss: 1.0009 - val_acc: 0.5829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7654 - acc: 0.6879 - val_loss: 0.9956 - val_acc: 0.5886\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7566 - acc: 0.7039 - val_loss: 1.0013 - val_acc: 0.5829\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.7482 - acc: 0.7033 - val_loss: 1.0082 - val_acc: 0.5886\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 79us/step - loss: 0.7708 - acc: 0.6969 - val_loss: 1.0000 - val_acc: 0.5886\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 98us/step - loss: 0.7639 - acc: 0.6828 - val_loss: 1.0029 - val_acc: 0.5886\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7550 - acc: 0.7033 - val_loss: 1.0041 - val_acc: 0.5886\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7676 - acc: 0.6943 - val_loss: 1.0006 - val_acc: 0.5886\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7547 - acc: 0.6969 - val_loss: 1.0045 - val_acc: 0.5886\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7456 - acc: 0.6911 - val_loss: 1.0113 - val_acc: 0.5943\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7465 - acc: 0.6981 - val_loss: 1.0084 - val_acc: 0.5771\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7493 - acc: 0.6886 - val_loss: 1.0193 - val_acc: 0.5771\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7557 - acc: 0.6867 - val_loss: 1.0181 - val_acc: 0.5829\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7507 - acc: 0.6962 - val_loss: 1.0206 - val_acc: 0.5886\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7420 - acc: 0.7160 - val_loss: 1.0156 - val_acc: 0.5943\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 102us/step - loss: 0.7370 - acc: 0.7045 - val_loss: 1.0185 - val_acc: 0.5829\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7421 - acc: 0.7020 - val_loss: 1.0174 - val_acc: 0.5829\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.7339 - acc: 0.7116 - val_loss: 1.0111 - val_acc: 0.5886\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7350 - acc: 0.7084 - val_loss: 1.0151 - val_acc: 0.5943\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7264 - acc: 0.7096 - val_loss: 1.0195 - val_acc: 0.5943\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7180 - acc: 0.7058 - val_loss: 1.0176 - val_acc: 0.5943\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7310 - acc: 0.7071 - val_loss: 1.0160 - val_acc: 0.5943\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7233 - acc: 0.7058 - val_loss: 1.0234 - val_acc: 0.5886\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7317 - acc: 0.6981 - val_loss: 1.0149 - val_acc: 0.6000\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7156 - acc: 0.7103 - val_loss: 1.0139 - val_acc: 0.5886\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7194 - acc: 0.7122 - val_loss: 1.0144 - val_acc: 0.5829\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7332 - acc: 0.7033 - val_loss: 1.0174 - val_acc: 0.5886\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 78us/step - loss: 0.7220 - acc: 0.7039 - val_loss: 1.0208 - val_acc: 0.5829\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7277 - acc: 0.7020 - val_loss: 1.0218 - val_acc: 0.5829\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7265 - acc: 0.7116 - val_loss: 1.0261 - val_acc: 0.5829\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7214 - acc: 0.7116 - val_loss: 1.0262 - val_acc: 0.5886\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7092 - acc: 0.7096 - val_loss: 1.0256 - val_acc: 0.5829\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7094 - acc: 0.7007 - val_loss: 1.0350 - val_acc: 0.5829\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7173 - acc: 0.7135 - val_loss: 1.0359 - val_acc: 0.5829\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 80us/step - loss: 0.7001 - acc: 0.7186 - val_loss: 1.0355 - val_acc: 0.5771\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 84us/step - loss: 0.7199 - acc: 0.7013 - val_loss: 1.0342 - val_acc: 0.5771\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6962 - acc: 0.7154 - val_loss: 1.0385 - val_acc: 0.5771\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7126 - acc: 0.7173 - val_loss: 1.0345 - val_acc: 0.5829\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7138 - acc: 0.7147 - val_loss: 1.0394 - val_acc: 0.5829\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 83us/step - loss: 0.6910 - acc: 0.7218 - val_loss: 1.0311 - val_acc: 0.5829\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.7033 - acc: 0.7147 - val_loss: 1.0434 - val_acc: 0.5829\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6910 - acc: 0.7345 - val_loss: 1.0452 - val_acc: 0.5771\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6948 - acc: 0.7237 - val_loss: 1.0420 - val_acc: 0.5829\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7050 - acc: 0.7179 - val_loss: 1.0437 - val_acc: 0.5714\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 91us/step - loss: 0.6995 - acc: 0.7332 - val_loss: 1.0453 - val_acc: 0.5829\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6974 - acc: 0.7269 - val_loss: 1.0349 - val_acc: 0.5943\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6874 - acc: 0.7211 - val_loss: 1.0325 - val_acc: 0.5943\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6926 - acc: 0.7250 - val_loss: 1.0421 - val_acc: 0.5943\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6975 - acc: 0.7186 - val_loss: 1.0457 - val_acc: 0.5886\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6776 - acc: 0.7262 - val_loss: 1.0421 - val_acc: 0.5829\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6833 - acc: 0.7250 - val_loss: 1.0434 - val_acc: 0.5886\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6703 - acc: 0.7320 - val_loss: 1.0443 - val_acc: 0.5886\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6755 - acc: 0.7377 - val_loss: 1.0427 - val_acc: 0.5886\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6940 - acc: 0.7294 - val_loss: 1.0524 - val_acc: 0.5829\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.6597 - acc: 0.7479 - val_loss: 1.0557 - val_acc: 0.5771\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6822 - acc: 0.7230 - val_loss: 1.0569 - val_acc: 0.5829\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6892 - acc: 0.7269 - val_loss: 1.0461 - val_acc: 0.5829\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6762 - acc: 0.7230 - val_loss: 1.0533 - val_acc: 0.5886\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6732 - acc: 0.7339 - val_loss: 1.0451 - val_acc: 0.5943\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6890 - acc: 0.7224 - val_loss: 1.0516 - val_acc: 0.5829\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6637 - acc: 0.7250 - val_loss: 1.0517 - val_acc: 0.5886\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6543 - acc: 0.7384 - val_loss: 1.0534 - val_acc: 0.5943\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6893 - acc: 0.7294 - val_loss: 1.0552 - val_acc: 0.5771\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6708 - acc: 0.7230 - val_loss: 1.0582 - val_acc: 0.5886\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.6675 - acc: 0.7288 - val_loss: 1.0549 - val_acc: 0.5829\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6701 - acc: 0.7371 - val_loss: 1.0599 - val_acc: 0.5829\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6817 - acc: 0.7352 - val_loss: 1.0579 - val_acc: 0.5943\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.6640 - acc: 0.7409 - val_loss: 1.0594 - val_acc: 0.5829\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6612 - acc: 0.7377 - val_loss: 1.0574 - val_acc: 0.5771\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6547 - acc: 0.7435 - val_loss: 1.0662 - val_acc: 0.5829\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6605 - acc: 0.7466 - val_loss: 1.0634 - val_acc: 0.5886\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6458 - acc: 0.7403 - val_loss: 1.0664 - val_acc: 0.5886\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6442 - acc: 0.7435 - val_loss: 1.0649 - val_acc: 0.5943\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6458 - acc: 0.7320 - val_loss: 1.0721 - val_acc: 0.5886\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6476 - acc: 0.7505 - val_loss: 1.0750 - val_acc: 0.5829\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6442 - acc: 0.7447 - val_loss: 1.0739 - val_acc: 0.5829\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6532 - acc: 0.7250 - val_loss: 1.0616 - val_acc: 0.5886\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6325 - acc: 0.7492 - val_loss: 1.0723 - val_acc: 0.5886\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6393 - acc: 0.7345 - val_loss: 1.0763 - val_acc: 0.5886\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - ETA: 0s - loss: 0.6379 - acc: 0.736 - 0s 72us/step - loss: 0.6560 - acc: 0.7345 - val_loss: 1.0884 - val_acc: 0.5714\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6590 - acc: 0.7352 - val_loss: 1.0693 - val_acc: 0.5829\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6528 - acc: 0.7313 - val_loss: 1.0665 - val_acc: 0.5886\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6492 - acc: 0.7364 - val_loss: 1.0701 - val_acc: 0.5771\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6441 - acc: 0.7377 - val_loss: 1.0811 - val_acc: 0.5943\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6285 - acc: 0.7422 - val_loss: 1.0807 - val_acc: 0.5829\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6473 - acc: 0.7409 - val_loss: 1.0806 - val_acc: 0.5886\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6439 - acc: 0.7384 - val_loss: 1.0884 - val_acc: 0.5886\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6242 - acc: 0.7473 - val_loss: 1.0904 - val_acc: 0.5943\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6340 - acc: 0.7390 - val_loss: 1.0896 - val_acc: 0.5886\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.6366 - acc: 0.7498 - val_loss: 1.0875 - val_acc: 0.5829\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.6458 - acc: 0.7479 - val_loss: 1.0951 - val_acc: 0.5829\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6429 - acc: 0.7428 - val_loss: 1.0988 - val_acc: 0.5829\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.6278 - acc: 0.7396 - val_loss: 1.0935 - val_acc: 0.5886\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6251 - acc: 0.7549 - val_loss: 1.0926 - val_acc: 0.5943\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6365 - acc: 0.7498 - val_loss: 1.1034 - val_acc: 0.5829\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6207 - acc: 0.7620 - val_loss: 1.1016 - val_acc: 0.5886\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6339 - acc: 0.7447 - val_loss: 1.0944 - val_acc: 0.6000\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6297 - acc: 0.7415 - val_loss: 1.1043 - val_acc: 0.5886\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6232 - acc: 0.7581 - val_loss: 1.0989 - val_acc: 0.5943\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6192 - acc: 0.7537 - val_loss: 1.1035 - val_acc: 0.5943\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6230 - acc: 0.7479 - val_loss: 1.1023 - val_acc: 0.5829\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.6048 - acc: 0.7594 - val_loss: 1.1041 - val_acc: 0.5714\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6256 - acc: 0.7556 - val_loss: 1.1057 - val_acc: 0.5714\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6020 - acc: 0.7652 - val_loss: 1.1035 - val_acc: 0.5886\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6318 - acc: 0.7569 - val_loss: 1.1101 - val_acc: 0.5829\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6199 - acc: 0.7556 - val_loss: 1.1129 - val_acc: 0.5829\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6052 - acc: 0.7645 - val_loss: 1.1121 - val_acc: 0.5771\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6001 - acc: 0.7632 - val_loss: 1.1063 - val_acc: 0.5829\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6187 - acc: 0.7594 - val_loss: 1.1136 - val_acc: 0.5771\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6078 - acc: 0.7505 - val_loss: 1.1102 - val_acc: 0.5886\n",
      "193/193 [==============================] - 0s 55us/step\n",
      "1742/1742 [==============================] - 0s 35us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 9s 6ms/step - loss: 1.4823 - acc: 0.3257 - val_loss: 1.1833 - val_acc: 0.5600\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.2923 - acc: 0.4112 - val_loss: 1.0728 - val_acc: 0.6343\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.1888 - acc: 0.4642 - val_loss: 1.0155 - val_acc: 0.6571\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.1259 - acc: 0.5134 - val_loss: 0.9939 - val_acc: 0.6629\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.1059 - acc: 0.5038 - val_loss: 0.9662 - val_acc: 0.6800\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.0719 - acc: 0.5313 - val_loss: 0.9540 - val_acc: 0.6629\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0437 - acc: 0.5479 - val_loss: 0.9356 - val_acc: 0.6686\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0131 - acc: 0.5594 - val_loss: 0.9252 - val_acc: 0.6857\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.0011 - acc: 0.5664 - val_loss: 0.9211 - val_acc: 0.6857\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9999 - acc: 0.5722 - val_loss: 0.9168 - val_acc: 0.6914\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9705 - acc: 0.5856 - val_loss: 0.9009 - val_acc: 0.6857\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9579 - acc: 0.5913 - val_loss: 0.9095 - val_acc: 0.6686\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.9389 - acc: 0.6098 - val_loss: 0.8972 - val_acc: 0.7086\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9624 - acc: 0.6022 - val_loss: 0.8991 - val_acc: 0.6857\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9328 - acc: 0.6022 - val_loss: 0.9054 - val_acc: 0.6743\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9344 - acc: 0.6015 - val_loss: 0.8975 - val_acc: 0.6971\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9212 - acc: 0.6137 - val_loss: 0.8978 - val_acc: 0.6743\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 83us/step - loss: 0.9132 - acc: 0.5824 - val_loss: 0.8919 - val_acc: 0.6743\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9183 - acc: 0.5971 - val_loss: 0.8971 - val_acc: 0.6686\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8997 - acc: 0.6232 - val_loss: 0.8915 - val_acc: 0.6800\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.9109 - acc: 0.6162 - val_loss: 0.8950 - val_acc: 0.6629\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8906 - acc: 0.6379 - val_loss: 0.8916 - val_acc: 0.6743\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8957 - acc: 0.6360 - val_loss: 0.8951 - val_acc: 0.6457\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8917 - acc: 0.6181 - val_loss: 0.9011 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8755 - acc: 0.6367 - val_loss: 0.8940 - val_acc: 0.6629\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8646 - acc: 0.6481 - val_loss: 0.8900 - val_acc: 0.6629\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8702 - acc: 0.6360 - val_loss: 0.8900 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8414 - acc: 0.6660 - val_loss: 0.9014 - val_acc: 0.6457\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.8653 - acc: 0.6443 - val_loss: 0.9054 - val_acc: 0.6629\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.8409 - acc: 0.6411 - val_loss: 0.9112 - val_acc: 0.6514\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8451 - acc: 0.6628 - val_loss: 0.9031 - val_acc: 0.6514\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8432 - acc: 0.6603 - val_loss: 0.9003 - val_acc: 0.6629\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8510 - acc: 0.6577 - val_loss: 0.9001 - val_acc: 0.6857\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8434 - acc: 0.6558 - val_loss: 0.9091 - val_acc: 0.6686\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8235 - acc: 0.6481 - val_loss: 0.9127 - val_acc: 0.6514\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8220 - acc: 0.6641 - val_loss: 0.9137 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8131 - acc: 0.6648 - val_loss: 0.9062 - val_acc: 0.6571\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8151 - acc: 0.6609 - val_loss: 0.9110 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8078 - acc: 0.6552 - val_loss: 0.9159 - val_acc: 0.6457\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8147 - acc: 0.6539 - val_loss: 0.9098 - val_acc: 0.6514\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7970 - acc: 0.6603 - val_loss: 0.9114 - val_acc: 0.6571\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7851 - acc: 0.6839 - val_loss: 0.9117 - val_acc: 0.6457\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7959 - acc: 0.6724 - val_loss: 0.9111 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7752 - acc: 0.6833 - val_loss: 0.9095 - val_acc: 0.6514\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7741 - acc: 0.6724 - val_loss: 0.9077 - val_acc: 0.6571\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7840 - acc: 0.6743 - val_loss: 0.9115 - val_acc: 0.6514\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7887 - acc: 0.6718 - val_loss: 0.9245 - val_acc: 0.6457\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7791 - acc: 0.6750 - val_loss: 0.9120 - val_acc: 0.6571\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7758 - acc: 0.6801 - val_loss: 0.9072 - val_acc: 0.6514\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7664 - acc: 0.6935 - val_loss: 0.9088 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7862 - acc: 0.6839 - val_loss: 0.9145 - val_acc: 0.6629\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7699 - acc: 0.6986 - val_loss: 0.9134 - val_acc: 0.6514\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7568 - acc: 0.6967 - val_loss: 0.9210 - val_acc: 0.6514\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7506 - acc: 0.7043 - val_loss: 0.9239 - val_acc: 0.6514\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7550 - acc: 0.6935 - val_loss: 0.9243 - val_acc: 0.6514\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7589 - acc: 0.6871 - val_loss: 0.9352 - val_acc: 0.6514\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7351 - acc: 0.6973 - val_loss: 0.9300 - val_acc: 0.6514\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7457 - acc: 0.7063 - val_loss: 0.9321 - val_acc: 0.6514\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7335 - acc: 0.6992 - val_loss: 0.9348 - val_acc: 0.6571\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7436 - acc: 0.7095 - val_loss: 0.9384 - val_acc: 0.6343\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7501 - acc: 0.6999 - val_loss: 0.9342 - val_acc: 0.6286\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7293 - acc: 0.7133 - val_loss: 0.9324 - val_acc: 0.6400\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 87us/step - loss: 0.7402 - acc: 0.6839 - val_loss: 0.9382 - val_acc: 0.6286\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7192 - acc: 0.7056 - val_loss: 0.9383 - val_acc: 0.6343\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7178 - acc: 0.7178 - val_loss: 0.9420 - val_acc: 0.6400\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7340 - acc: 0.7011 - val_loss: 0.9423 - val_acc: 0.6514\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7202 - acc: 0.7037 - val_loss: 0.9383 - val_acc: 0.6457\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7160 - acc: 0.7139 - val_loss: 0.9396 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7154 - acc: 0.7190 - val_loss: 0.9421 - val_acc: 0.6400\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7077 - acc: 0.7248 - val_loss: 0.9487 - val_acc: 0.6400\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7178 - acc: 0.7126 - val_loss: 0.9496 - val_acc: 0.6514\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7265 - acc: 0.7107 - val_loss: 0.9470 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7084 - acc: 0.7101 - val_loss: 0.9535 - val_acc: 0.6343\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7070 - acc: 0.7190 - val_loss: 0.9533 - val_acc: 0.6400\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6887 - acc: 0.7241 - val_loss: 0.9517 - val_acc: 0.6514\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6876 - acc: 0.7235 - val_loss: 0.9486 - val_acc: 0.6400\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6981 - acc: 0.7273 - val_loss: 0.9462 - val_acc: 0.6514\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6983 - acc: 0.7241 - val_loss: 0.9550 - val_acc: 0.6343\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6956 - acc: 0.7248 - val_loss: 0.9535 - val_acc: 0.6457\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6958 - acc: 0.7197 - val_loss: 0.9540 - val_acc: 0.6400\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6786 - acc: 0.7324 - val_loss: 0.9655 - val_acc: 0.6400\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6900 - acc: 0.7248 - val_loss: 0.9612 - val_acc: 0.6514\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 98us/step - loss: 0.6905 - acc: 0.7286 - val_loss: 0.9583 - val_acc: 0.6571\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 106us/step - loss: 0.6498 - acc: 0.7241 - val_loss: 0.9671 - val_acc: 0.6457\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6763 - acc: 0.7229 - val_loss: 0.9736 - val_acc: 0.6400\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6660 - acc: 0.7241 - val_loss: 0.9507 - val_acc: 0.6629\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6926 - acc: 0.7261 - val_loss: 0.9535 - val_acc: 0.6686\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6627 - acc: 0.7503 - val_loss: 0.9559 - val_acc: 0.6514\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6743 - acc: 0.7401 - val_loss: 0.9683 - val_acc: 0.6400\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6581 - acc: 0.7337 - val_loss: 0.9685 - val_acc: 0.6286\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6666 - acc: 0.7356 - val_loss: 0.9690 - val_acc: 0.6400\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6760 - acc: 0.7427 - val_loss: 0.9750 - val_acc: 0.6514\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6460 - acc: 0.7497 - val_loss: 0.9753 - val_acc: 0.6286\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6667 - acc: 0.7382 - val_loss: 0.9728 - val_acc: 0.6343\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6553 - acc: 0.7369 - val_loss: 0.9731 - val_acc: 0.6457\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.6655 - acc: 0.7305 - val_loss: 0.9773 - val_acc: 0.6400\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6314 - acc: 0.7433 - val_loss: 0.9687 - val_acc: 0.6514\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6271 - acc: 0.7484 - val_loss: 0.9775 - val_acc: 0.6457\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6407 - acc: 0.7433 - val_loss: 0.9816 - val_acc: 0.6400\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6377 - acc: 0.7427 - val_loss: 0.9828 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6094 - acc: 0.7791 - val_loss: 0.9830 - val_acc: 0.6400\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6560 - acc: 0.7484 - val_loss: 0.9891 - val_acc: 0.6629\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6200 - acc: 0.7688 - val_loss: 0.9972 - val_acc: 0.6571\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6272 - acc: 0.7548 - val_loss: 0.9998 - val_acc: 0.6514\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6175 - acc: 0.7561 - val_loss: 0.9962 - val_acc: 0.6686\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6279 - acc: 0.7484 - val_loss: 0.9999 - val_acc: 0.6629\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6142 - acc: 0.7593 - val_loss: 1.0036 - val_acc: 0.6514\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6178 - acc: 0.7561 - val_loss: 1.0056 - val_acc: 0.6514\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5980 - acc: 0.7688 - val_loss: 1.0016 - val_acc: 0.6571\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6182 - acc: 0.7548 - val_loss: 1.0076 - val_acc: 0.6457\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5912 - acc: 0.7759 - val_loss: 1.0082 - val_acc: 0.6457\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6055 - acc: 0.7759 - val_loss: 1.0033 - val_acc: 0.6514\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6100 - acc: 0.7637 - val_loss: 1.0168 - val_acc: 0.6400\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5974 - acc: 0.7644 - val_loss: 1.0134 - val_acc: 0.6457\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6020 - acc: 0.7535 - val_loss: 1.0154 - val_acc: 0.6457\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6264 - acc: 0.7439 - val_loss: 1.0209 - val_acc: 0.6457\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5815 - acc: 0.7816 - val_loss: 1.0316 - val_acc: 0.6400\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5715 - acc: 0.7771 - val_loss: 1.0285 - val_acc: 0.6514\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5686 - acc: 0.7765 - val_loss: 1.0324 - val_acc: 0.6457\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5783 - acc: 0.7739 - val_loss: 1.0311 - val_acc: 0.6457\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5820 - acc: 0.7797 - val_loss: 1.0218 - val_acc: 0.6457\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5914 - acc: 0.7695 - val_loss: 1.0229 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5802 - acc: 0.7784 - val_loss: 1.0262 - val_acc: 0.6457\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5864 - acc: 0.7791 - val_loss: 1.0302 - val_acc: 0.6514\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5874 - acc: 0.7784 - val_loss: 1.0344 - val_acc: 0.6400\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5861 - acc: 0.7784 - val_loss: 1.0371 - val_acc: 0.6629\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5882 - acc: 0.7765 - val_loss: 1.0379 - val_acc: 0.6400\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5780 - acc: 0.7759 - val_loss: 1.0264 - val_acc: 0.6571\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5778 - acc: 0.7937 - val_loss: 1.0281 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5869 - acc: 0.7880 - val_loss: 1.0431 - val_acc: 0.6514\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5797 - acc: 0.7848 - val_loss: 1.0467 - val_acc: 0.6286\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5932 - acc: 0.7739 - val_loss: 1.0434 - val_acc: 0.6571\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5703 - acc: 0.7778 - val_loss: 1.0384 - val_acc: 0.6629\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5627 - acc: 0.7854 - val_loss: 1.0371 - val_acc: 0.6629\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5577 - acc: 0.7810 - val_loss: 1.0460 - val_acc: 0.6514\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 82us/step - loss: 0.5639 - acc: 0.7822 - val_loss: 1.0478 - val_acc: 0.6514\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5503 - acc: 0.7867 - val_loss: 1.0458 - val_acc: 0.6571\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5600 - acc: 0.7822 - val_loss: 1.0484 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5600 - acc: 0.7886 - val_loss: 1.0609 - val_acc: 0.6457\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5453 - acc: 0.7867 - val_loss: 1.0652 - val_acc: 0.6457\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5511 - acc: 0.7880 - val_loss: 1.0739 - val_acc: 0.6457\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5561 - acc: 0.7861 - val_loss: 1.0759 - val_acc: 0.6286\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5483 - acc: 0.7880 - val_loss: 1.0739 - val_acc: 0.6400\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5369 - acc: 0.7937 - val_loss: 1.0719 - val_acc: 0.6400\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5364 - acc: 0.8027 - val_loss: 1.0676 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5219 - acc: 0.7989 - val_loss: 1.0619 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5473 - acc: 0.7822 - val_loss: 1.0654 - val_acc: 0.6400\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5398 - acc: 0.7937 - val_loss: 1.0587 - val_acc: 0.6457\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5116 - acc: 0.8046 - val_loss: 1.0605 - val_acc: 0.6514\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5500 - acc: 0.7791 - val_loss: 1.0723 - val_acc: 0.6514\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5453 - acc: 0.8014 - val_loss: 1.0807 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5318 - acc: 0.8033 - val_loss: 1.0722 - val_acc: 0.6457\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5170 - acc: 0.7950 - val_loss: 1.0803 - val_acc: 0.6400\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5246 - acc: 0.8052 - val_loss: 1.0811 - val_acc: 0.6457\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5182 - acc: 0.8014 - val_loss: 1.0795 - val_acc: 0.6457\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5040 - acc: 0.8091 - val_loss: 1.0790 - val_acc: 0.6457\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5338 - acc: 0.7982 - val_loss: 1.0786 - val_acc: 0.6343\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5094 - acc: 0.8040 - val_loss: 1.0865 - val_acc: 0.6343\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5289 - acc: 0.8072 - val_loss: 1.0863 - val_acc: 0.6514\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5282 - acc: 0.8001 - val_loss: 1.0877 - val_acc: 0.6457\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5105 - acc: 0.8059 - val_loss: 1.0936 - val_acc: 0.6400\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5267 - acc: 0.8116 - val_loss: 1.0960 - val_acc: 0.6457\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5259 - acc: 0.8155 - val_loss: 1.1008 - val_acc: 0.6457\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5000 - acc: 0.8072 - val_loss: 1.1067 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5067 - acc: 0.8123 - val_loss: 1.0996 - val_acc: 0.6400\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4916 - acc: 0.8225 - val_loss: 1.1092 - val_acc: 0.6400\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5364 - acc: 0.7963 - val_loss: 1.1037 - val_acc: 0.6514\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5062 - acc: 0.8091 - val_loss: 1.1033 - val_acc: 0.6229\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4906 - acc: 0.8180 - val_loss: 1.1093 - val_acc: 0.6514\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.4886 - acc: 0.8123 - val_loss: 1.1086 - val_acc: 0.6514\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5010 - acc: 0.8078 - val_loss: 1.1080 - val_acc: 0.6514\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4897 - acc: 0.8167 - val_loss: 1.1119 - val_acc: 0.6514\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4835 - acc: 0.8186 - val_loss: 1.1162 - val_acc: 0.6571\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4938 - acc: 0.8110 - val_loss: 1.1212 - val_acc: 0.6400\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4597 - acc: 0.8225 - val_loss: 1.1205 - val_acc: 0.6400\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.4805 - acc: 0.8103 - val_loss: 1.1180 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.4985 - acc: 0.8091 - val_loss: 1.1144 - val_acc: 0.6571\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.4865 - acc: 0.8244 - val_loss: 1.1161 - val_acc: 0.6571\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4996 - acc: 0.8046 - val_loss: 1.1136 - val_acc: 0.6571\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5013 - acc: 0.8186 - val_loss: 1.1115 - val_acc: 0.6457\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4798 - acc: 0.8404 - val_loss: 1.1160 - val_acc: 0.6571\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4860 - acc: 0.8116 - val_loss: 1.1304 - val_acc: 0.6571\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4707 - acc: 0.8206 - val_loss: 1.1399 - val_acc: 0.6457\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4853 - acc: 0.8040 - val_loss: 1.1388 - val_acc: 0.6229\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4856 - acc: 0.8091 - val_loss: 1.1287 - val_acc: 0.6400\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4986 - acc: 0.8059 - val_loss: 1.1247 - val_acc: 0.6457\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4701 - acc: 0.8206 - val_loss: 1.1268 - val_acc: 0.6400\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4668 - acc: 0.8359 - val_loss: 1.1355 - val_acc: 0.6457\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4901 - acc: 0.8193 - val_loss: 1.1391 - val_acc: 0.6514\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.4485 - acc: 0.8301 - val_loss: 1.1395 - val_acc: 0.6571\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.4737 - acc: 0.8225 - val_loss: 1.1508 - val_acc: 0.6571\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4720 - acc: 0.8161 - val_loss: 1.1415 - val_acc: 0.6514\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.4821 - acc: 0.8282 - val_loss: 1.1569 - val_acc: 0.6571\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.4710 - acc: 0.8257 - val_loss: 1.1496 - val_acc: 0.6514\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4751 - acc: 0.8225 - val_loss: 1.1538 - val_acc: 0.6571\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4467 - acc: 0.8436 - val_loss: 1.1575 - val_acc: 0.6457\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4624 - acc: 0.8378 - val_loss: 1.1520 - val_acc: 0.6457\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4780 - acc: 0.8269 - val_loss: 1.1560 - val_acc: 0.6571\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.4687 - acc: 0.8321 - val_loss: 1.1618 - val_acc: 0.6571\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4463 - acc: 0.8263 - val_loss: 1.1674 - val_acc: 0.6629\n",
      "194/194 [==============================] - 0s 56us/step\n",
      "1741/1741 [==============================] - 0s 39us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 8s 5ms/step - loss: 1.5557 - acc: 0.3084 - val_loss: 1.2181 - val_acc: 0.4171\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.2856 - acc: 0.4068 - val_loss: 1.0732 - val_acc: 0.6114\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.2125 - acc: 0.4610 - val_loss: 1.0105 - val_acc: 0.6171\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.1414 - acc: 0.5038 - val_loss: 0.9780 - val_acc: 0.6286\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 1.0880 - acc: 0.5198 - val_loss: 0.9630 - val_acc: 0.6457\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0707 - acc: 0.5504 - val_loss: 0.9479 - val_acc: 0.6457\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 1.0514 - acc: 0.5473 - val_loss: 0.9366 - val_acc: 0.6571\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 1.0423 - acc: 0.5421 - val_loss: 0.9190 - val_acc: 0.6571\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 1.0021 - acc: 0.5632 - val_loss: 0.9093 - val_acc: 0.6400\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9911 - acc: 0.5785 - val_loss: 0.9048 - val_acc: 0.6514\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9619 - acc: 0.6022 - val_loss: 0.8974 - val_acc: 0.6686\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9598 - acc: 0.6003 - val_loss: 0.9042 - val_acc: 0.6800\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9666 - acc: 0.5811 - val_loss: 0.8952 - val_acc: 0.6686\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9374 - acc: 0.6124 - val_loss: 0.8924 - val_acc: 0.6629\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9534 - acc: 0.6111 - val_loss: 0.8879 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9341 - acc: 0.6188 - val_loss: 0.8876 - val_acc: 0.6686\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9236 - acc: 0.6137 - val_loss: 0.8847 - val_acc: 0.6743\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.9148 - acc: 0.6175 - val_loss: 0.8796 - val_acc: 0.6514\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9023 - acc: 0.6220 - val_loss: 0.8849 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8955 - acc: 0.6130 - val_loss: 0.8830 - val_acc: 0.6571\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8900 - acc: 0.6379 - val_loss: 0.8910 - val_acc: 0.6514\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8827 - acc: 0.6411 - val_loss: 0.8806 - val_acc: 0.6457\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8838 - acc: 0.6264 - val_loss: 0.8866 - val_acc: 0.6457\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8650 - acc: 0.6469 - val_loss: 0.8819 - val_acc: 0.6629\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8704 - acc: 0.6584 - val_loss: 0.8770 - val_acc: 0.6571\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8533 - acc: 0.6462 - val_loss: 0.8872 - val_acc: 0.6629\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8660 - acc: 0.6469 - val_loss: 0.8906 - val_acc: 0.6629\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8562 - acc: 0.6481 - val_loss: 0.8904 - val_acc: 0.6514\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8339 - acc: 0.6635 - val_loss: 0.8877 - val_acc: 0.6571\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8403 - acc: 0.6462 - val_loss: 0.8902 - val_acc: 0.6629\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8228 - acc: 0.6705 - val_loss: 0.8842 - val_acc: 0.6514\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8350 - acc: 0.6609 - val_loss: 0.8903 - val_acc: 0.6400\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8339 - acc: 0.6520 - val_loss: 0.8862 - val_acc: 0.6457\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8208 - acc: 0.6577 - val_loss: 0.8866 - val_acc: 0.6686\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8352 - acc: 0.6699 - val_loss: 0.8904 - val_acc: 0.6457\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8365 - acc: 0.6705 - val_loss: 0.8924 - val_acc: 0.6514\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7955 - acc: 0.6737 - val_loss: 0.8946 - val_acc: 0.6400\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7954 - acc: 0.6794 - val_loss: 0.8977 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.8089 - acc: 0.6820 - val_loss: 0.8958 - val_acc: 0.6514\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7795 - acc: 0.6877 - val_loss: 0.9000 - val_acc: 0.6343\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7892 - acc: 0.6756 - val_loss: 0.9146 - val_acc: 0.6457\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7657 - acc: 0.6890 - val_loss: 0.9074 - val_acc: 0.6514\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7795 - acc: 0.6762 - val_loss: 0.9018 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7697 - acc: 0.6852 - val_loss: 0.9009 - val_acc: 0.6343\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7683 - acc: 0.6941 - val_loss: 0.9028 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 77us/step - loss: 0.7820 - acc: 0.6877 - val_loss: 0.9117 - val_acc: 0.6400\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7670 - acc: 0.7050 - val_loss: 0.9161 - val_acc: 0.6457\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7650 - acc: 0.6877 - val_loss: 0.9143 - val_acc: 0.6457\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7688 - acc: 0.6928 - val_loss: 0.9160 - val_acc: 0.6571\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7863 - acc: 0.6769 - val_loss: 0.9256 - val_acc: 0.6457\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7495 - acc: 0.6967 - val_loss: 0.9186 - val_acc: 0.6400\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7599 - acc: 0.6814 - val_loss: 0.9100 - val_acc: 0.6571\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7617 - acc: 0.6922 - val_loss: 0.9110 - val_acc: 0.6514\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7369 - acc: 0.7133 - val_loss: 0.9166 - val_acc: 0.6514\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7528 - acc: 0.7056 - val_loss: 0.9074 - val_acc: 0.6514\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7457 - acc: 0.6986 - val_loss: 0.9120 - val_acc: 0.6400\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7448 - acc: 0.6999 - val_loss: 0.9051 - val_acc: 0.6457\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7255 - acc: 0.7120 - val_loss: 0.9069 - val_acc: 0.6457\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7451 - acc: 0.7050 - val_loss: 0.9055 - val_acc: 0.6629\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7294 - acc: 0.7114 - val_loss: 0.9076 - val_acc: 0.6686\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7302 - acc: 0.7229 - val_loss: 0.9099 - val_acc: 0.6514\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7203 - acc: 0.7152 - val_loss: 0.9149 - val_acc: 0.6629\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7200 - acc: 0.7165 - val_loss: 0.9123 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7193 - acc: 0.7165 - val_loss: 0.9170 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7139 - acc: 0.7222 - val_loss: 0.9174 - val_acc: 0.6571\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7024 - acc: 0.7267 - val_loss: 0.9186 - val_acc: 0.6686\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7007 - acc: 0.7126 - val_loss: 0.9191 - val_acc: 0.6571\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7068 - acc: 0.7344 - val_loss: 0.9311 - val_acc: 0.6457\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7018 - acc: 0.7158 - val_loss: 0.9229 - val_acc: 0.6571\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7183 - acc: 0.7152 - val_loss: 0.9354 - val_acc: 0.6400\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7091 - acc: 0.7241 - val_loss: 0.9378 - val_acc: 0.6286\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6899 - acc: 0.7267 - val_loss: 0.9381 - val_acc: 0.6343\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6944 - acc: 0.7152 - val_loss: 0.9432 - val_acc: 0.6629\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6978 - acc: 0.7305 - val_loss: 0.9355 - val_acc: 0.6571\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6753 - acc: 0.7292 - val_loss: 0.9413 - val_acc: 0.6571\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6802 - acc: 0.7324 - val_loss: 0.9439 - val_acc: 0.6514\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6842 - acc: 0.7324 - val_loss: 0.9562 - val_acc: 0.6457\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6818 - acc: 0.7190 - val_loss: 0.9474 - val_acc: 0.6571\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6468 - acc: 0.7407 - val_loss: 0.9590 - val_acc: 0.6457\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6739 - acc: 0.7273 - val_loss: 0.9552 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6459 - acc: 0.7516 - val_loss: 0.9568 - val_acc: 0.6629\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6653 - acc: 0.7331 - val_loss: 0.9605 - val_acc: 0.6571\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6394 - acc: 0.7382 - val_loss: 0.9601 - val_acc: 0.6400\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6426 - acc: 0.7471 - val_loss: 0.9625 - val_acc: 0.6457\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6592 - acc: 0.7446 - val_loss: 0.9657 - val_acc: 0.6571\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6515 - acc: 0.7375 - val_loss: 0.9664 - val_acc: 0.6457\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6422 - acc: 0.7427 - val_loss: 0.9781 - val_acc: 0.6571\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6513 - acc: 0.7427 - val_loss: 0.9662 - val_acc: 0.6514\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6477 - acc: 0.7356 - val_loss: 0.9676 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6265 - acc: 0.7465 - val_loss: 0.9755 - val_acc: 0.6571\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6386 - acc: 0.7542 - val_loss: 0.9782 - val_acc: 0.6457\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6502 - acc: 0.7388 - val_loss: 0.9865 - val_acc: 0.6514\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6508 - acc: 0.7458 - val_loss: 0.9929 - val_acc: 0.6457\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6434 - acc: 0.7516 - val_loss: 0.9943 - val_acc: 0.6343\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.6303 - acc: 0.7522 - val_loss: 0.9952 - val_acc: 0.6400\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6280 - acc: 0.7548 - val_loss: 0.9936 - val_acc: 0.6400\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.6225 - acc: 0.7497 - val_loss: 1.0026 - val_acc: 0.6514\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - ETA: 0s - loss: 0.6322 - acc: 0.751 - 0s 52us/step - loss: 0.6411 - acc: 0.7465 - val_loss: 1.0064 - val_acc: 0.6457\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6211 - acc: 0.7625 - val_loss: 0.9990 - val_acc: 0.6457\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6250 - acc: 0.7554 - val_loss: 1.0107 - val_acc: 0.6400\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6125 - acc: 0.7637 - val_loss: 1.0083 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5993 - acc: 0.7618 - val_loss: 1.0017 - val_acc: 0.6514\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5960 - acc: 0.7573 - val_loss: 1.0091 - val_acc: 0.6400\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6101 - acc: 0.7522 - val_loss: 1.0063 - val_acc: 0.6629\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6163 - acc: 0.7612 - val_loss: 1.0076 - val_acc: 0.6686\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5762 - acc: 0.7663 - val_loss: 1.0135 - val_acc: 0.6686\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6018 - acc: 0.7516 - val_loss: 1.0138 - val_acc: 0.6571\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5858 - acc: 0.7765 - val_loss: 1.0106 - val_acc: 0.6514\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5998 - acc: 0.7656 - val_loss: 1.0130 - val_acc: 0.6571\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5838 - acc: 0.7739 - val_loss: 1.0057 - val_acc: 0.6686\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5978 - acc: 0.7778 - val_loss: 1.0136 - val_acc: 0.6629\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6147 - acc: 0.7516 - val_loss: 1.0104 - val_acc: 0.6400\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5981 - acc: 0.7573 - val_loss: 1.0175 - val_acc: 0.6571\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5952 - acc: 0.7618 - val_loss: 1.0266 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5759 - acc: 0.7797 - val_loss: 1.0223 - val_acc: 0.6457\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5954 - acc: 0.7771 - val_loss: 1.0177 - val_acc: 0.6286\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5698 - acc: 0.7893 - val_loss: 1.0231 - val_acc: 0.6457\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5679 - acc: 0.7816 - val_loss: 1.0304 - val_acc: 0.6343\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5653 - acc: 0.7733 - val_loss: 1.0353 - val_acc: 0.6400\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5801 - acc: 0.7797 - val_loss: 1.0389 - val_acc: 0.6229\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5870 - acc: 0.7765 - val_loss: 1.0415 - val_acc: 0.6400\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5492 - acc: 0.7880 - val_loss: 1.0490 - val_acc: 0.6457\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5802 - acc: 0.7644 - val_loss: 1.0494 - val_acc: 0.6343\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5682 - acc: 0.7778 - val_loss: 1.0467 - val_acc: 0.6400\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5706 - acc: 0.7816 - val_loss: 1.0452 - val_acc: 0.6457\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5641 - acc: 0.7829 - val_loss: 1.0399 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5592 - acc: 0.7854 - val_loss: 1.0607 - val_acc: 0.6400\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5718 - acc: 0.7803 - val_loss: 1.0763 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5435 - acc: 0.7771 - val_loss: 1.0569 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5577 - acc: 0.7912 - val_loss: 1.0542 - val_acc: 0.6457\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.5607 - acc: 0.7912 - val_loss: 1.0575 - val_acc: 0.6571\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5537 - acc: 0.7803 - val_loss: 1.0602 - val_acc: 0.6457\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5320 - acc: 0.7957 - val_loss: 1.0703 - val_acc: 0.6571\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5596 - acc: 0.7765 - val_loss: 1.0690 - val_acc: 0.6343\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5623 - acc: 0.7727 - val_loss: 1.0674 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5327 - acc: 0.7982 - val_loss: 1.0695 - val_acc: 0.6514\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5407 - acc: 0.7842 - val_loss: 1.0814 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5366 - acc: 0.7918 - val_loss: 1.0767 - val_acc: 0.6514\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5367 - acc: 0.7963 - val_loss: 1.0775 - val_acc: 0.6571\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5197 - acc: 0.8155 - val_loss: 1.0828 - val_acc: 0.6343\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5377 - acc: 0.7995 - val_loss: 1.0702 - val_acc: 0.6457\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5380 - acc: 0.8040 - val_loss: 1.0822 - val_acc: 0.6457\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5120 - acc: 0.8084 - val_loss: 1.0941 - val_acc: 0.6400\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.5334 - acc: 0.7957 - val_loss: 1.0905 - val_acc: 0.6400\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5037 - acc: 0.8065 - val_loss: 1.0858 - val_acc: 0.6400\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5123 - acc: 0.8103 - val_loss: 1.0926 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5056 - acc: 0.8142 - val_loss: 1.0948 - val_acc: 0.6343\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5125 - acc: 0.8046 - val_loss: 1.1083 - val_acc: 0.6400\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5288 - acc: 0.8052 - val_loss: 1.1111 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5361 - acc: 0.7816 - val_loss: 1.1241 - val_acc: 0.6514\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5147 - acc: 0.8078 - val_loss: 1.1282 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5300 - acc: 0.8014 - val_loss: 1.1237 - val_acc: 0.6457\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5154 - acc: 0.8020 - val_loss: 1.1250 - val_acc: 0.6457\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5259 - acc: 0.7944 - val_loss: 1.1167 - val_acc: 0.6400\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4851 - acc: 0.8269 - val_loss: 1.1207 - val_acc: 0.6514\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5125 - acc: 0.8027 - val_loss: 1.1162 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4978 - acc: 0.8142 - val_loss: 1.1279 - val_acc: 0.6286\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5106 - acc: 0.8142 - val_loss: 1.1334 - val_acc: 0.6286\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4914 - acc: 0.8174 - val_loss: 1.1338 - val_acc: 0.6286\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5090 - acc: 0.8020 - val_loss: 1.1311 - val_acc: 0.6343\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5023 - acc: 0.8123 - val_loss: 1.1392 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.4880 - acc: 0.8161 - val_loss: 1.1463 - val_acc: 0.6343\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4865 - acc: 0.8199 - val_loss: 1.1500 - val_acc: 0.6171\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.4900 - acc: 0.8231 - val_loss: 1.1451 - val_acc: 0.6457\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5040 - acc: 0.7937 - val_loss: 1.1508 - val_acc: 0.6343\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4973 - acc: 0.8123 - val_loss: 1.1366 - val_acc: 0.6571\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5063 - acc: 0.8110 - val_loss: 1.1419 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4949 - acc: 0.8110 - val_loss: 1.1514 - val_acc: 0.6343\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5098 - acc: 0.8065 - val_loss: 1.1402 - val_acc: 0.6286\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.4908 - acc: 0.8027 - val_loss: 1.1510 - val_acc: 0.6171\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5003 - acc: 0.8091 - val_loss: 1.1458 - val_acc: 0.6229\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4873 - acc: 0.8174 - val_loss: 1.1495 - val_acc: 0.6171\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.4748 - acc: 0.8167 - val_loss: 1.1407 - val_acc: 0.6229\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4694 - acc: 0.8212 - val_loss: 1.1638 - val_acc: 0.6114\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4822 - acc: 0.8186 - val_loss: 1.1489 - val_acc: 0.6286\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4829 - acc: 0.8065 - val_loss: 1.1539 - val_acc: 0.6400\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.4742 - acc: 0.8180 - val_loss: 1.1510 - val_acc: 0.6229\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.4501 - acc: 0.8269 - val_loss: 1.1577 - val_acc: 0.6343\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.4696 - acc: 0.8212 - val_loss: 1.1594 - val_acc: 0.6514\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4747 - acc: 0.8250 - val_loss: 1.1746 - val_acc: 0.6286\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.4670 - acc: 0.8129 - val_loss: 1.1690 - val_acc: 0.6229\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.4839 - acc: 0.8346 - val_loss: 1.1757 - val_acc: 0.6286\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.4539 - acc: 0.8404 - val_loss: 1.1761 - val_acc: 0.6286\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.4535 - acc: 0.8423 - val_loss: 1.1761 - val_acc: 0.6343\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4594 - acc: 0.8212 - val_loss: 1.1821 - val_acc: 0.6229\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4545 - acc: 0.8352 - val_loss: 1.1873 - val_acc: 0.6171\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4519 - acc: 0.8372 - val_loss: 1.1798 - val_acc: 0.6343\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.4678 - acc: 0.8257 - val_loss: 1.1834 - val_acc: 0.6286\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4494 - acc: 0.8340 - val_loss: 1.1928 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.4380 - acc: 0.8225 - val_loss: 1.1919 - val_acc: 0.6343\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.4396 - acc: 0.8410 - val_loss: 1.1993 - val_acc: 0.6114\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4325 - acc: 0.8327 - val_loss: 1.1994 - val_acc: 0.6229\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4441 - acc: 0.8327 - val_loss: 1.2012 - val_acc: 0.6343\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.4458 - acc: 0.8301 - val_loss: 1.2013 - val_acc: 0.6286\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4294 - acc: 0.8397 - val_loss: 1.2035 - val_acc: 0.6286\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4573 - acc: 0.8359 - val_loss: 1.2145 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4381 - acc: 0.8359 - val_loss: 1.2178 - val_acc: 0.6171\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4410 - acc: 0.8301 - val_loss: 1.2148 - val_acc: 0.6114\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.4479 - acc: 0.8301 - val_loss: 1.2187 - val_acc: 0.6114\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.4295 - acc: 0.8365 - val_loss: 1.2300 - val_acc: 0.6114\n",
      "194/194 [==============================] - 0s 61us/step\n",
      "1741/1741 [==============================] - 0s 40us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 9s 6ms/step - loss: 1.4498 - acc: 0.3078 - val_loss: 1.1784 - val_acc: 0.5200\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.2801 - acc: 0.4119 - val_loss: 1.0809 - val_acc: 0.5886\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1955 - acc: 0.4719 - val_loss: 1.0358 - val_acc: 0.6514\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.1412 - acc: 0.5038 - val_loss: 0.9962 - val_acc: 0.6457\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.1037 - acc: 0.5089 - val_loss: 0.9710 - val_acc: 0.6514\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.0854 - acc: 0.5166 - val_loss: 0.9621 - val_acc: 0.6629\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0481 - acc: 0.5485 - val_loss: 0.9421 - val_acc: 0.6514\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 1.0400 - acc: 0.5453 - val_loss: 0.9345 - val_acc: 0.6514\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0059 - acc: 0.5664 - val_loss: 0.9222 - val_acc: 0.6686\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9698 - acc: 0.5817 - val_loss: 0.9182 - val_acc: 0.6229\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9703 - acc: 0.5913 - val_loss: 0.9098 - val_acc: 0.6457\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9679 - acc: 0.5900 - val_loss: 0.9094 - val_acc: 0.6514\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.9447 - acc: 0.6041 - val_loss: 0.9078 - val_acc: 0.6457\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.9496 - acc: 0.6022 - val_loss: 0.9098 - val_acc: 0.6514\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.9235 - acc: 0.6003 - val_loss: 0.9116 - val_acc: 0.6571\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9355 - acc: 0.5971 - val_loss: 0.9090 - val_acc: 0.6629\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9200 - acc: 0.6188 - val_loss: 0.9002 - val_acc: 0.6743\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8951 - acc: 0.6284 - val_loss: 0.9017 - val_acc: 0.6629\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9092 - acc: 0.6309 - val_loss: 0.8976 - val_acc: 0.6857\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9015 - acc: 0.6290 - val_loss: 0.9030 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8828 - acc: 0.6284 - val_loss: 0.9015 - val_acc: 0.6914\n",
      "Epoch 22/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8885 - acc: 0.6296 - val_loss: 0.9000 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8758 - acc: 0.6315 - val_loss: 0.8998 - val_acc: 0.6800\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8633 - acc: 0.6424 - val_loss: 0.8950 - val_acc: 0.6800\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8719 - acc: 0.6456 - val_loss: 0.9009 - val_acc: 0.6743\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8667 - acc: 0.6437 - val_loss: 0.8959 - val_acc: 0.6743\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8583 - acc: 0.6494 - val_loss: 0.8954 - val_acc: 0.6743\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.8621 - acc: 0.6513 - val_loss: 0.8917 - val_acc: 0.6629\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8582 - acc: 0.6430 - val_loss: 0.8950 - val_acc: 0.6800\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8499 - acc: 0.6501 - val_loss: 0.8965 - val_acc: 0.6743\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8565 - acc: 0.6494 - val_loss: 0.8998 - val_acc: 0.6686\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8420 - acc: 0.6507 - val_loss: 0.8965 - val_acc: 0.6800\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8239 - acc: 0.6686 - val_loss: 0.8953 - val_acc: 0.6629\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8076 - acc: 0.6794 - val_loss: 0.8983 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8202 - acc: 0.6622 - val_loss: 0.8964 - val_acc: 0.6571\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8315 - acc: 0.6558 - val_loss: 0.9013 - val_acc: 0.6629\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8171 - acc: 0.6609 - val_loss: 0.8964 - val_acc: 0.6686\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8258 - acc: 0.6667 - val_loss: 0.9000 - val_acc: 0.6571\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8241 - acc: 0.6628 - val_loss: 0.9070 - val_acc: 0.6571\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8159 - acc: 0.6686 - val_loss: 0.8936 - val_acc: 0.6743\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7979 - acc: 0.6865 - val_loss: 0.8981 - val_acc: 0.6629\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8102 - acc: 0.6641 - val_loss: 0.9030 - val_acc: 0.6686\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8005 - acc: 0.6667 - val_loss: 0.8969 - val_acc: 0.6743\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7852 - acc: 0.6788 - val_loss: 0.8990 - val_acc: 0.6457\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.7953 - acc: 0.6699 - val_loss: 0.8977 - val_acc: 0.6686\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7921 - acc: 0.6871 - val_loss: 0.9034 - val_acc: 0.6743\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7745 - acc: 0.6737 - val_loss: 0.9082 - val_acc: 0.6629\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7747 - acc: 0.6762 - val_loss: 0.9034 - val_acc: 0.6514\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7613 - acc: 0.6865 - val_loss: 0.9152 - val_acc: 0.6400\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7697 - acc: 0.6839 - val_loss: 0.9177 - val_acc: 0.6343\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7677 - acc: 0.6935 - val_loss: 0.9103 - val_acc: 0.6571\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7772 - acc: 0.6897 - val_loss: 0.9078 - val_acc: 0.6514\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7505 - acc: 0.6928 - val_loss: 0.9029 - val_acc: 0.6457\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7562 - acc: 0.7031 - val_loss: 0.9172 - val_acc: 0.6457\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7534 - acc: 0.7031 - val_loss: 0.9156 - val_acc: 0.6571\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7469 - acc: 0.6884 - val_loss: 0.9190 - val_acc: 0.6514\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7577 - acc: 0.6973 - val_loss: 0.9220 - val_acc: 0.6400\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7488 - acc: 0.6928 - val_loss: 0.9252 - val_acc: 0.6457\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7306 - acc: 0.7037 - val_loss: 0.9216 - val_acc: 0.6343\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7480 - acc: 0.6884 - val_loss: 0.9230 - val_acc: 0.6400\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7382 - acc: 0.6967 - val_loss: 0.9255 - val_acc: 0.6514\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7235 - acc: 0.7069 - val_loss: 0.9172 - val_acc: 0.6457\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7214 - acc: 0.7139 - val_loss: 0.9230 - val_acc: 0.6400\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7289 - acc: 0.7024 - val_loss: 0.9217 - val_acc: 0.6343\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.7207 - acc: 0.7229 - val_loss: 0.9243 - val_acc: 0.6629\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 46us/step - loss: 0.7370 - acc: 0.7088 - val_loss: 0.9281 - val_acc: 0.6514\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7229 - acc: 0.7139 - val_loss: 0.9328 - val_acc: 0.6114\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7048 - acc: 0.7126 - val_loss: 0.9300 - val_acc: 0.6229\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7103 - acc: 0.7190 - val_loss: 0.9314 - val_acc: 0.6457\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7089 - acc: 0.6967 - val_loss: 0.9290 - val_acc: 0.6457\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6911 - acc: 0.7248 - val_loss: 0.9315 - val_acc: 0.6400\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7038 - acc: 0.7088 - val_loss: 0.9288 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7064 - acc: 0.7133 - val_loss: 0.9424 - val_acc: 0.6229\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6915 - acc: 0.7069 - val_loss: 0.9548 - val_acc: 0.6286\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7016 - acc: 0.6992 - val_loss: 0.9430 - val_acc: 0.6457\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6890 - acc: 0.7184 - val_loss: 0.9505 - val_acc: 0.6286\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6955 - acc: 0.7063 - val_loss: 0.9479 - val_acc: 0.6457\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6888 - acc: 0.7286 - val_loss: 0.9425 - val_acc: 0.6514\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6896 - acc: 0.7190 - val_loss: 0.9475 - val_acc: 0.6629\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6988 - acc: 0.7133 - val_loss: 0.9508 - val_acc: 0.6629\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6730 - acc: 0.7254 - val_loss: 0.9527 - val_acc: 0.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6838 - acc: 0.7088 - val_loss: 0.9537 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6567 - acc: 0.7350 - val_loss: 0.9522 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6873 - acc: 0.7146 - val_loss: 0.9520 - val_acc: 0.6686\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6941 - acc: 0.7241 - val_loss: 0.9502 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6568 - acc: 0.7292 - val_loss: 0.9612 - val_acc: 0.6286\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.6496 - acc: 0.7420 - val_loss: 0.9700 - val_acc: 0.6514\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6808 - acc: 0.7273 - val_loss: 0.9739 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6721 - acc: 0.7350 - val_loss: 0.9721 - val_acc: 0.6286\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6586 - acc: 0.7439 - val_loss: 0.9683 - val_acc: 0.6629\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6623 - acc: 0.7382 - val_loss: 0.9643 - val_acc: 0.6686\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6552 - acc: 0.7478 - val_loss: 0.9709 - val_acc: 0.6571\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6608 - acc: 0.7395 - val_loss: 0.9669 - val_acc: 0.6343\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6671 - acc: 0.7369 - val_loss: 0.9686 - val_acc: 0.6571\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6605 - acc: 0.7554 - val_loss: 0.9834 - val_acc: 0.6514\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 74us/step - loss: 0.6297 - acc: 0.7529 - val_loss: 0.9813 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6685 - acc: 0.7388 - val_loss: 0.9836 - val_acc: 0.6343\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6575 - acc: 0.7369 - val_loss: 0.9714 - val_acc: 0.6514\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6435 - acc: 0.7395 - val_loss: 0.9704 - val_acc: 0.6571\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6355 - acc: 0.7375 - val_loss: 0.9666 - val_acc: 0.6457\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6314 - acc: 0.7586 - val_loss: 0.9694 - val_acc: 0.6343\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.6317 - acc: 0.7522 - val_loss: 0.9760 - val_acc: 0.6400\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6395 - acc: 0.7446 - val_loss: 0.9784 - val_acc: 0.6457\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6312 - acc: 0.7471 - val_loss: 0.9858 - val_acc: 0.6343\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6329 - acc: 0.7414 - val_loss: 0.9955 - val_acc: 0.6343\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6176 - acc: 0.7669 - val_loss: 1.0029 - val_acc: 0.6286\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6101 - acc: 0.7625 - val_loss: 1.0031 - val_acc: 0.6457\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6327 - acc: 0.7414 - val_loss: 1.0058 - val_acc: 0.6457\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6221 - acc: 0.7516 - val_loss: 1.0065 - val_acc: 0.6514\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6064 - acc: 0.7548 - val_loss: 1.0064 - val_acc: 0.6514\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5899 - acc: 0.7701 - val_loss: 1.0093 - val_acc: 0.6686\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6093 - acc: 0.7548 - val_loss: 1.0088 - val_acc: 0.6743\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6023 - acc: 0.7605 - val_loss: 1.0080 - val_acc: 0.6571\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5984 - acc: 0.7637 - val_loss: 1.0116 - val_acc: 0.6571\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5918 - acc: 0.7663 - val_loss: 1.0204 - val_acc: 0.6629\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6013 - acc: 0.7561 - val_loss: 1.0162 - val_acc: 0.6457\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5970 - acc: 0.7478 - val_loss: 1.0313 - val_acc: 0.6514\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6045 - acc: 0.7708 - val_loss: 1.0390 - val_acc: 0.6400\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6011 - acc: 0.7605 - val_loss: 1.0248 - val_acc: 0.6457\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5815 - acc: 0.7810 - val_loss: 1.0254 - val_acc: 0.6343\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5943 - acc: 0.7637 - val_loss: 1.0252 - val_acc: 0.6571\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5946 - acc: 0.7567 - val_loss: 1.0211 - val_acc: 0.6343\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5959 - acc: 0.7669 - val_loss: 1.0257 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6003 - acc: 0.7599 - val_loss: 1.0266 - val_acc: 0.6514\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5770 - acc: 0.7708 - val_loss: 1.0375 - val_acc: 0.6571\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5968 - acc: 0.7708 - val_loss: 1.0387 - val_acc: 0.6571\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5784 - acc: 0.7765 - val_loss: 1.0390 - val_acc: 0.6629\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5659 - acc: 0.7791 - val_loss: 1.0395 - val_acc: 0.6514\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5590 - acc: 0.7752 - val_loss: 1.0438 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5611 - acc: 0.7791 - val_loss: 1.0469 - val_acc: 0.6457\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5635 - acc: 0.7759 - val_loss: 1.0505 - val_acc: 0.6514\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5589 - acc: 0.7759 - val_loss: 1.0483 - val_acc: 0.6629\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5670 - acc: 0.7816 - val_loss: 1.0641 - val_acc: 0.6686\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.5764 - acc: 0.7701 - val_loss: 1.0772 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5673 - acc: 0.7765 - val_loss: 1.0631 - val_acc: 0.6514\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5614 - acc: 0.7771 - val_loss: 1.0673 - val_acc: 0.6571\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5517 - acc: 0.7861 - val_loss: 1.0661 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5638 - acc: 0.7739 - val_loss: 1.0673 - val_acc: 0.6400\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5461 - acc: 0.7778 - val_loss: 1.0704 - val_acc: 0.6400\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5463 - acc: 0.7810 - val_loss: 1.0682 - val_acc: 0.6514\n",
      "Epoch 141/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5568 - acc: 0.7816 - val_loss: 1.0746 - val_acc: 0.6514\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5462 - acc: 0.7854 - val_loss: 1.0824 - val_acc: 0.6457\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5550 - acc: 0.7816 - val_loss: 1.0853 - val_acc: 0.6514\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5445 - acc: 0.7963 - val_loss: 1.0838 - val_acc: 0.6514\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5701 - acc: 0.7759 - val_loss: 1.0855 - val_acc: 0.6343\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5167 - acc: 0.7905 - val_loss: 1.0869 - val_acc: 0.6457\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 96us/step - loss: 0.5316 - acc: 0.7995 - val_loss: 1.0844 - val_acc: 0.6457\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5368 - acc: 0.7861 - val_loss: 1.0911 - val_acc: 0.6457\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5252 - acc: 0.7950 - val_loss: 1.1032 - val_acc: 0.6343\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5326 - acc: 0.7937 - val_loss: 1.1125 - val_acc: 0.6400\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5506 - acc: 0.7937 - val_loss: 1.1103 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.5151 - acc: 0.8046 - val_loss: 1.1128 - val_acc: 0.6286\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.5053 - acc: 0.8135 - val_loss: 1.1077 - val_acc: 0.6400\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5010 - acc: 0.8072 - val_loss: 1.1067 - val_acc: 0.6400\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5338 - acc: 0.7937 - val_loss: 1.1139 - val_acc: 0.6514\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.5274 - acc: 0.8001 - val_loss: 1.1240 - val_acc: 0.6571\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 94us/step - loss: 0.5377 - acc: 0.7937 - val_loss: 1.1264 - val_acc: 0.6514\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 144us/step - loss: 0.5375 - acc: 0.7899 - val_loss: 1.1248 - val_acc: 0.6571\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 95us/step - loss: 0.5038 - acc: 0.8027 - val_loss: 1.1384 - val_acc: 0.6229\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5090 - acc: 0.8059 - val_loss: 1.1455 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5070 - acc: 0.7989 - val_loss: 1.1405 - val_acc: 0.6457\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5079 - acc: 0.8059 - val_loss: 1.1354 - val_acc: 0.6514\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4956 - acc: 0.8180 - val_loss: 1.1434 - val_acc: 0.6343\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5110 - acc: 0.8116 - val_loss: 1.1411 - val_acc: 0.6286\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5049 - acc: 0.8072 - val_loss: 1.1443 - val_acc: 0.6457\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 72us/step - loss: 0.5172 - acc: 0.8033 - val_loss: 1.1411 - val_acc: 0.6457\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4980 - acc: 0.8116 - val_loss: 1.1402 - val_acc: 0.6286\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.4860 - acc: 0.8103 - val_loss: 1.1379 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5159 - acc: 0.8103 - val_loss: 1.1413 - val_acc: 0.6629\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5042 - acc: 0.8020 - val_loss: 1.1421 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.4942 - acc: 0.8167 - val_loss: 1.1389 - val_acc: 0.6457\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4947 - acc: 0.8059 - val_loss: 1.1509 - val_acc: 0.6343\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 81us/step - loss: 0.4883 - acc: 0.8238 - val_loss: 1.1562 - val_acc: 0.6343\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.4932 - acc: 0.8167 - val_loss: 1.1552 - val_acc: 0.6400\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.4872 - acc: 0.8046 - val_loss: 1.1551 - val_acc: 0.6571\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5110 - acc: 0.8116 - val_loss: 1.1452 - val_acc: 0.6514\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.4974 - acc: 0.8033 - val_loss: 1.1522 - val_acc: 0.6457\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4982 - acc: 0.8161 - val_loss: 1.1753 - val_acc: 0.6400\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4742 - acc: 0.8212 - val_loss: 1.1756 - val_acc: 0.6514\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.5001 - acc: 0.8052 - val_loss: 1.1740 - val_acc: 0.6457\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.4893 - acc: 0.8014 - val_loss: 1.1766 - val_acc: 0.6514\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4792 - acc: 0.8186 - val_loss: 1.1758 - val_acc: 0.6457\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4793 - acc: 0.8180 - val_loss: 1.1691 - val_acc: 0.6571\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4601 - acc: 0.8257 - val_loss: 1.1592 - val_acc: 0.6571\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.4762 - acc: 0.8250 - val_loss: 1.1630 - val_acc: 0.6571\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.4813 - acc: 0.8033 - val_loss: 1.1749 - val_acc: 0.6457\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.4862 - acc: 0.8052 - val_loss: 1.1712 - val_acc: 0.6571\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4833 - acc: 0.8110 - val_loss: 1.1829 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4663 - acc: 0.8199 - val_loss: 1.1764 - val_acc: 0.6571\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4455 - acc: 0.8269 - val_loss: 1.1804 - val_acc: 0.6514\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.4593 - acc: 0.8155 - val_loss: 1.1864 - val_acc: 0.6629\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4776 - acc: 0.8314 - val_loss: 1.2006 - val_acc: 0.6629\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.4507 - acc: 0.8257 - val_loss: 1.1844 - val_acc: 0.6629\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4378 - acc: 0.8282 - val_loss: 1.1878 - val_acc: 0.6629\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4518 - acc: 0.8314 - val_loss: 1.1856 - val_acc: 0.6457\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4614 - acc: 0.8314 - val_loss: 1.1853 - val_acc: 0.6571\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4477 - acc: 0.8257 - val_loss: 1.1942 - val_acc: 0.6457\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4281 - acc: 0.8346 - val_loss: 1.2014 - val_acc: 0.6400\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4290 - acc: 0.8372 - val_loss: 1.2053 - val_acc: 0.6514\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.4394 - acc: 0.8378 - val_loss: 1.2034 - val_acc: 0.6400\n",
      "194/194 [==============================] - 0s 49us/step\n",
      "1741/1741 [==============================] - 0s 38us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 9s 6ms/step - loss: 1.4979 - acc: 0.3078 - val_loss: 1.1706 - val_acc: 0.5086\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 1.2861 - acc: 0.4202 - val_loss: 1.0640 - val_acc: 0.5886\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 1.1981 - acc: 0.4630 - val_loss: 1.0075 - val_acc: 0.6114\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 1.1383 - acc: 0.5102 - val_loss: 0.9785 - val_acc: 0.6514\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 1.1128 - acc: 0.5128 - val_loss: 0.9570 - val_acc: 0.6343\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.0889 - acc: 0.5166 - val_loss: 0.9407 - val_acc: 0.6743\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.0665 - acc: 0.5294 - val_loss: 0.9287 - val_acc: 0.6743\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0402 - acc: 0.5594 - val_loss: 0.9210 - val_acc: 0.6457\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 1.0110 - acc: 0.5613 - val_loss: 0.9205 - val_acc: 0.6457\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9975 - acc: 0.5696 - val_loss: 0.9063 - val_acc: 0.6229\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.9831 - acc: 0.5766 - val_loss: 0.8955 - val_acc: 0.6686\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9722 - acc: 0.5843 - val_loss: 0.8920 - val_acc: 0.6571\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9675 - acc: 0.5830 - val_loss: 0.8943 - val_acc: 0.6571\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9504 - acc: 0.6015 - val_loss: 0.9003 - val_acc: 0.6514\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9522 - acc: 0.5977 - val_loss: 0.8926 - val_acc: 0.6743\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.9257 - acc: 0.5990 - val_loss: 0.8921 - val_acc: 0.6629\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9416 - acc: 0.5964 - val_loss: 0.8843 - val_acc: 0.6914\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.9281 - acc: 0.6130 - val_loss: 0.8842 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9341 - acc: 0.6073 - val_loss: 0.8825 - val_acc: 0.6629\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9309 - acc: 0.6149 - val_loss: 0.8868 - val_acc: 0.6629\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.9110 - acc: 0.6201 - val_loss: 0.8846 - val_acc: 0.6914\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.8933 - acc: 0.6245 - val_loss: 0.8911 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9080 - acc: 0.6105 - val_loss: 0.8978 - val_acc: 0.6571\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8792 - acc: 0.6322 - val_loss: 0.8966 - val_acc: 0.6571\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8821 - acc: 0.6392 - val_loss: 0.8952 - val_acc: 0.6686\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8787 - acc: 0.6271 - val_loss: 0.8916 - val_acc: 0.6629\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8652 - acc: 0.6290 - val_loss: 0.9031 - val_acc: 0.6343\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8912 - acc: 0.6232 - val_loss: 0.9016 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8596 - acc: 0.6456 - val_loss: 0.8994 - val_acc: 0.6571\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8506 - acc: 0.6488 - val_loss: 0.8976 - val_acc: 0.6686\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.8272 - acc: 0.6616 - val_loss: 0.9027 - val_acc: 0.6457\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.8667 - acc: 0.6303 - val_loss: 0.9001 - val_acc: 0.6629\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8345 - acc: 0.6577 - val_loss: 0.8899 - val_acc: 0.6629\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8472 - acc: 0.6564 - val_loss: 0.8914 - val_acc: 0.6571\n",
      "Epoch 35/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8525 - acc: 0.6456 - val_loss: 0.8856 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.8122 - acc: 0.6699 - val_loss: 0.8912 - val_acc: 0.6686\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8103 - acc: 0.6705 - val_loss: 0.8946 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.8225 - acc: 0.6686 - val_loss: 0.8978 - val_acc: 0.6800\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8004 - acc: 0.6705 - val_loss: 0.9008 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8142 - acc: 0.6603 - val_loss: 0.9051 - val_acc: 0.6571\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8232 - acc: 0.6564 - val_loss: 0.9001 - val_acc: 0.6800\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7908 - acc: 0.6705 - val_loss: 0.8969 - val_acc: 0.6629\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 78us/step - loss: 0.7921 - acc: 0.6577 - val_loss: 0.8997 - val_acc: 0.6629\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8021 - acc: 0.6858 - val_loss: 0.9045 - val_acc: 0.6629\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7993 - acc: 0.6731 - val_loss: 0.9097 - val_acc: 0.6743\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7941 - acc: 0.6679 - val_loss: 0.9067 - val_acc: 0.6686\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7868 - acc: 0.6762 - val_loss: 0.9109 - val_acc: 0.6743\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7902 - acc: 0.6775 - val_loss: 0.9144 - val_acc: 0.6800\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7809 - acc: 0.6737 - val_loss: 0.9198 - val_acc: 0.6800\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7827 - acc: 0.6794 - val_loss: 0.9241 - val_acc: 0.6571\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7635 - acc: 0.6992 - val_loss: 0.9235 - val_acc: 0.6686\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7637 - acc: 0.6782 - val_loss: 0.9188 - val_acc: 0.6629\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7747 - acc: 0.6865 - val_loss: 0.9200 - val_acc: 0.6857\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7605 - acc: 0.6903 - val_loss: 0.9171 - val_acc: 0.6800\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.7649 - acc: 0.6871 - val_loss: 0.9261 - val_acc: 0.6743\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7735 - acc: 0.6941 - val_loss: 0.9249 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7463 - acc: 0.6948 - val_loss: 0.9284 - val_acc: 0.6686\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.7587 - acc: 0.6986 - val_loss: 0.9401 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 79us/step - loss: 0.7512 - acc: 0.6903 - val_loss: 0.9350 - val_acc: 0.6686\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7702 - acc: 0.6967 - val_loss: 0.9390 - val_acc: 0.6743\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7642 - acc: 0.6877 - val_loss: 0.9340 - val_acc: 0.6571\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.7266 - acc: 0.7043 - val_loss: 0.9281 - val_acc: 0.6571\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7417 - acc: 0.6980 - val_loss: 0.9266 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7468 - acc: 0.6992 - val_loss: 0.9295 - val_acc: 0.6514\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7316 - acc: 0.7063 - val_loss: 0.9333 - val_acc: 0.6343\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.7330 - acc: 0.7069 - val_loss: 0.9394 - val_acc: 0.6343\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7408 - acc: 0.6992 - val_loss: 0.9396 - val_acc: 0.6571\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7338 - acc: 0.7069 - val_loss: 0.9406 - val_acc: 0.6457\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.7203 - acc: 0.6973 - val_loss: 0.9359 - val_acc: 0.6800\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.7276 - acc: 0.7114 - val_loss: 0.9490 - val_acc: 0.6629\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7322 - acc: 0.7018 - val_loss: 0.9419 - val_acc: 0.6457\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.7143 - acc: 0.7139 - val_loss: 0.9488 - val_acc: 0.6457\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7111 - acc: 0.7107 - val_loss: 0.9425 - val_acc: 0.6400\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7182 - acc: 0.7114 - val_loss: 0.9514 - val_acc: 0.6286\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6932 - acc: 0.7152 - val_loss: 0.9578 - val_acc: 0.6514\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7011 - acc: 0.7171 - val_loss: 0.9628 - val_acc: 0.6571\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6951 - acc: 0.7324 - val_loss: 0.9635 - val_acc: 0.6457\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.7252 - acc: 0.7114 - val_loss: 0.9624 - val_acc: 0.6343\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6973 - acc: 0.7209 - val_loss: 0.9625 - val_acc: 0.6457\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.6877 - acc: 0.7197 - val_loss: 0.9666 - val_acc: 0.6514\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7040 - acc: 0.7069 - val_loss: 0.9688 - val_acc: 0.6514\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6982 - acc: 0.7254 - val_loss: 0.9764 - val_acc: 0.6514\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.7077 - acc: 0.7018 - val_loss: 0.9703 - val_acc: 0.6400\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7000 - acc: 0.7063 - val_loss: 0.9742 - val_acc: 0.6686\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7058 - acc: 0.7178 - val_loss: 0.9770 - val_acc: 0.6400\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6756 - acc: 0.7299 - val_loss: 0.9713 - val_acc: 0.6229\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6821 - acc: 0.7248 - val_loss: 0.9758 - val_acc: 0.6229\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 47us/step - loss: 0.6852 - acc: 0.7152 - val_loss: 0.9777 - val_acc: 0.6343\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6789 - acc: 0.7280 - val_loss: 0.9786 - val_acc: 0.6229\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7118 - acc: 0.7178 - val_loss: 0.9757 - val_acc: 0.6400\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7002 - acc: 0.7011 - val_loss: 0.9804 - val_acc: 0.6457\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6534 - acc: 0.7427 - val_loss: 0.9838 - val_acc: 0.6686\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6638 - acc: 0.7337 - val_loss: 0.9857 - val_acc: 0.6514\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6574 - acc: 0.7439 - val_loss: 0.9940 - val_acc: 0.6514\n",
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6650 - acc: 0.7197 - val_loss: 0.9947 - val_acc: 0.6343\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6507 - acc: 0.7356 - val_loss: 0.9946 - val_acc: 0.6229\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6605 - acc: 0.7420 - val_loss: 1.0032 - val_acc: 0.6286\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6730 - acc: 0.7292 - val_loss: 1.0096 - val_acc: 0.6343\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6554 - acc: 0.7331 - val_loss: 1.0090 - val_acc: 0.6343\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6305 - acc: 0.7548 - val_loss: 1.0092 - val_acc: 0.6286\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6344 - acc: 0.7516 - val_loss: 1.0219 - val_acc: 0.6286\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6488 - acc: 0.7407 - val_loss: 1.0311 - val_acc: 0.6229\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6270 - acc: 0.7439 - val_loss: 1.0304 - val_acc: 0.6286\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6143 - acc: 0.7433 - val_loss: 1.0306 - val_acc: 0.6171\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6306 - acc: 0.7631 - val_loss: 1.0248 - val_acc: 0.6286\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6481 - acc: 0.7407 - val_loss: 1.0299 - val_acc: 0.6229\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6309 - acc: 0.7350 - val_loss: 1.0339 - val_acc: 0.6400\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6136 - acc: 0.7503 - val_loss: 1.0306 - val_acc: 0.6229\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6222 - acc: 0.7471 - val_loss: 1.0292 - val_acc: 0.6343\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.6340 - acc: 0.7529 - val_loss: 1.0248 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6073 - acc: 0.7701 - val_loss: 1.0258 - val_acc: 0.6457\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6158 - acc: 0.7561 - val_loss: 1.0355 - val_acc: 0.6343\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6283 - acc: 0.7452 - val_loss: 1.0411 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5984 - acc: 0.7605 - val_loss: 1.0311 - val_acc: 0.6457\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5956 - acc: 0.7682 - val_loss: 1.0425 - val_acc: 0.6514\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6054 - acc: 0.7688 - val_loss: 1.0467 - val_acc: 0.6514\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6123 - acc: 0.7529 - val_loss: 1.0453 - val_acc: 0.6400\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6008 - acc: 0.7535 - val_loss: 1.0553 - val_acc: 0.6229\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6228 - acc: 0.7503 - val_loss: 1.0493 - val_acc: 0.6400\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6025 - acc: 0.7554 - val_loss: 1.0514 - val_acc: 0.6286\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5876 - acc: 0.7542 - val_loss: 1.0494 - val_acc: 0.6286\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 71us/step - loss: 0.6069 - acc: 0.7618 - val_loss: 1.0588 - val_acc: 0.6229\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.6014 - acc: 0.7720 - val_loss: 1.0621 - val_acc: 0.6171\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5784 - acc: 0.7656 - val_loss: 1.0601 - val_acc: 0.6400\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.6008 - acc: 0.7669 - val_loss: 1.0634 - val_acc: 0.6343\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 66us/step - loss: 0.5977 - acc: 0.7567 - val_loss: 1.0723 - val_acc: 0.6514\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6004 - acc: 0.7586 - val_loss: 1.0786 - val_acc: 0.6343\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5783 - acc: 0.7644 - val_loss: 1.0803 - val_acc: 0.6343\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5920 - acc: 0.7637 - val_loss: 1.0738 - val_acc: 0.6457\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.5661 - acc: 0.7835 - val_loss: 1.0709 - val_acc: 0.6400\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5870 - acc: 0.7708 - val_loss: 1.0640 - val_acc: 0.6400\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6127 - acc: 0.7548 - val_loss: 1.0650 - val_acc: 0.6400\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5837 - acc: 0.7535 - val_loss: 1.0739 - val_acc: 0.6343\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5854 - acc: 0.7637 - val_loss: 1.0651 - val_acc: 0.6229\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5833 - acc: 0.7637 - val_loss: 1.0710 - val_acc: 0.6286\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5622 - acc: 0.7727 - val_loss: 1.0691 - val_acc: 0.6343\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5795 - acc: 0.7631 - val_loss: 1.0766 - val_acc: 0.6457\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5536 - acc: 0.7854 - val_loss: 1.0853 - val_acc: 0.6343\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5588 - acc: 0.7803 - val_loss: 1.0841 - val_acc: 0.6229\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5703 - acc: 0.7746 - val_loss: 1.0915 - val_acc: 0.6400\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5591 - acc: 0.7784 - val_loss: 1.1010 - val_acc: 0.6343\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5558 - acc: 0.7810 - val_loss: 1.0896 - val_acc: 0.6514\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5745 - acc: 0.7701 - val_loss: 1.0906 - val_acc: 0.6229\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.5572 - acc: 0.7771 - val_loss: 1.0977 - val_acc: 0.6457\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5429 - acc: 0.7822 - val_loss: 1.1020 - val_acc: 0.6457\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5577 - acc: 0.7688 - val_loss: 1.1002 - val_acc: 0.6514\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5456 - acc: 0.7765 - val_loss: 1.0913 - val_acc: 0.6286\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5516 - acc: 0.7957 - val_loss: 1.1019 - val_acc: 0.6229\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5628 - acc: 0.7765 - val_loss: 1.1045 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5643 - acc: 0.7797 - val_loss: 1.1041 - val_acc: 0.6400\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5077 - acc: 0.7963 - val_loss: 1.1117 - val_acc: 0.6457\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5354 - acc: 0.7822 - val_loss: 1.1150 - val_acc: 0.6229\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5476 - acc: 0.7835 - val_loss: 1.1146 - val_acc: 0.6171\n",
      "Epoch 154/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5399 - acc: 0.7912 - val_loss: 1.1220 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5442 - acc: 0.7842 - val_loss: 1.1293 - val_acc: 0.6171\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5465 - acc: 0.7880 - val_loss: 1.1218 - val_acc: 0.6286\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5373 - acc: 0.7848 - val_loss: 1.1267 - val_acc: 0.6457\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5486 - acc: 0.7771 - val_loss: 1.1191 - val_acc: 0.6457\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5479 - acc: 0.7963 - val_loss: 1.1179 - val_acc: 0.6457\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5194 - acc: 0.7982 - val_loss: 1.1247 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.5410 - acc: 0.7937 - val_loss: 1.1174 - val_acc: 0.6457\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5677 - acc: 0.7822 - val_loss: 1.1148 - val_acc: 0.6343\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5043 - acc: 0.7957 - val_loss: 1.1218 - val_acc: 0.6457\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5154 - acc: 0.7950 - val_loss: 1.1338 - val_acc: 0.6514\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5146 - acc: 0.7963 - val_loss: 1.1313 - val_acc: 0.6457\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.5298 - acc: 0.7912 - val_loss: 1.1431 - val_acc: 0.6571\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5227 - acc: 0.7905 - val_loss: 1.1387 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5161 - acc: 0.7842 - val_loss: 1.1356 - val_acc: 0.6514\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5055 - acc: 0.8084 - val_loss: 1.1363 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5047 - acc: 0.8027 - val_loss: 1.1407 - val_acc: 0.6343\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5166 - acc: 0.8065 - val_loss: 1.1412 - val_acc: 0.6457\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5010 - acc: 0.8084 - val_loss: 1.1555 - val_acc: 0.6286\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5097 - acc: 0.8001 - val_loss: 1.1542 - val_acc: 0.6286\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5067 - acc: 0.8014 - val_loss: 1.1528 - val_acc: 0.6400\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5194 - acc: 0.8103 - val_loss: 1.1674 - val_acc: 0.6229\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5069 - acc: 0.8014 - val_loss: 1.1720 - val_acc: 0.6400\n",
      "Epoch 177/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5046 - acc: 0.7937 - val_loss: 1.1748 - val_acc: 0.6171\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 68us/step - loss: 0.5193 - acc: 0.7957 - val_loss: 1.1774 - val_acc: 0.6286\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.5178 - acc: 0.8033 - val_loss: 1.1893 - val_acc: 0.6343\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4972 - acc: 0.8110 - val_loss: 1.1903 - val_acc: 0.6229\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4910 - acc: 0.8059 - val_loss: 1.1883 - val_acc: 0.6286\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.4976 - acc: 0.8052 - val_loss: 1.1982 - val_acc: 0.6343\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5040 - acc: 0.8167 - val_loss: 1.2058 - val_acc: 0.6171\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5069 - acc: 0.8084 - val_loss: 1.1960 - val_acc: 0.6229\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4820 - acc: 0.8129 - val_loss: 1.1889 - val_acc: 0.6286\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4819 - acc: 0.8193 - val_loss: 1.2020 - val_acc: 0.6171\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4896 - acc: 0.8263 - val_loss: 1.2019 - val_acc: 0.6171\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5000 - acc: 0.8040 - val_loss: 1.2015 - val_acc: 0.6114\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4770 - acc: 0.8186 - val_loss: 1.2103 - val_acc: 0.6343\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.4775 - acc: 0.8199 - val_loss: 1.2147 - val_acc: 0.6343\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.4834 - acc: 0.8199 - val_loss: 1.2089 - val_acc: 0.6514\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.4823 - acc: 0.8078 - val_loss: 1.2106 - val_acc: 0.6343\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5026 - acc: 0.8052 - val_loss: 1.2064 - val_acc: 0.6229\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.4633 - acc: 0.8321 - val_loss: 1.2189 - val_acc: 0.6286\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.4796 - acc: 0.8186 - val_loss: 1.2180 - val_acc: 0.6343\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4766 - acc: 0.8167 - val_loss: 1.2133 - val_acc: 0.6286\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4880 - acc: 0.8008 - val_loss: 1.2141 - val_acc: 0.6171\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4655 - acc: 0.8129 - val_loss: 1.2208 - val_acc: 0.6171\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.4463 - acc: 0.8346 - val_loss: 1.2211 - val_acc: 0.6343\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.4563 - acc: 0.8365 - val_loss: 1.2228 - val_acc: 0.6229\n",
      "194/194 [==============================] - 0s 79us/step\n",
      "1741/1741 [==============================] - 0s 41us/step\n",
      "Train on 1566 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1566/1566 [==============================] - 11s 7ms/step - loss: 1.6241 - acc: 0.2746 - val_loss: 1.2350 - val_acc: 0.4629\n",
      "Epoch 2/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 1.3117 - acc: 0.3787 - val_loss: 1.0873 - val_acc: 0.6057\n",
      "Epoch 3/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.2013 - acc: 0.4668 - val_loss: 1.0192 - val_acc: 0.6229\n",
      "Epoch 4/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 1.1529 - acc: 0.4815 - val_loss: 0.9893 - val_acc: 0.6229\n",
      "Epoch 5/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.1180 - acc: 0.5038 - val_loss: 0.9649 - val_acc: 0.6514\n",
      "Epoch 6/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 1.0611 - acc: 0.5498 - val_loss: 0.9553 - val_acc: 0.6629\n",
      "Epoch 7/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 1.0449 - acc: 0.5415 - val_loss: 0.9439 - val_acc: 0.6686\n",
      "Epoch 8/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 1.0295 - acc: 0.5383 - val_loss: 0.9301 - val_acc: 0.6800\n",
      "Epoch 9/200\n",
      "1566/1566 [==============================] - 0s 76us/step - loss: 1.0140 - acc: 0.5626 - val_loss: 0.9242 - val_acc: 0.6457\n",
      "Epoch 10/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 1.0030 - acc: 0.5715 - val_loss: 0.9162 - val_acc: 0.6571\n",
      "Epoch 11/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9971 - acc: 0.5683 - val_loss: 0.9110 - val_acc: 0.6571\n",
      "Epoch 12/200\n",
      "1566/1566 [==============================] - 0s 73us/step - loss: 0.9819 - acc: 0.5900 - val_loss: 0.9025 - val_acc: 0.6686\n",
      "Epoch 13/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.9643 - acc: 0.5926 - val_loss: 0.9022 - val_acc: 0.6800\n",
      "Epoch 14/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.9492 - acc: 0.6047 - val_loss: 0.8984 - val_acc: 0.6629\n",
      "Epoch 15/200\n",
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.9731 - acc: 0.5862 - val_loss: 0.9042 - val_acc: 0.6686\n",
      "Epoch 16/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.9139 - acc: 0.6054 - val_loss: 0.9011 - val_acc: 0.6800\n",
      "Epoch 17/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9183 - acc: 0.6086 - val_loss: 0.9017 - val_acc: 0.6571\n",
      "Epoch 18/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.9344 - acc: 0.6073 - val_loss: 0.8979 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9084 - acc: 0.6386 - val_loss: 0.9049 - val_acc: 0.6686\n",
      "Epoch 20/200\n",
      "1566/1566 [==============================] - 0s 70us/step - loss: 0.9099 - acc: 0.6194 - val_loss: 0.9058 - val_acc: 0.6743\n",
      "Epoch 21/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.9034 - acc: 0.6277 - val_loss: 0.8973 - val_acc: 0.6629\n",
      "Epoch 22/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8987 - acc: 0.6252 - val_loss: 0.8982 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.8866 - acc: 0.6322 - val_loss: 0.8960 - val_acc: 0.6514\n",
      "Epoch 24/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.8906 - acc: 0.6322 - val_loss: 0.9011 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "1566/1566 [==============================] - 0s 86us/step - loss: 0.8958 - acc: 0.6347 - val_loss: 0.8919 - val_acc: 0.6629\n",
      "Epoch 26/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.8629 - acc: 0.6405 - val_loss: 0.8967 - val_acc: 0.6457\n",
      "Epoch 27/200\n",
      "1566/1566 [==============================] - 0s 80us/step - loss: 0.8646 - acc: 0.6507 - val_loss: 0.8986 - val_acc: 0.6457\n",
      "Epoch 28/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.8653 - acc: 0.6360 - val_loss: 0.8908 - val_acc: 0.6629\n",
      "Epoch 29/200\n",
      "1566/1566 [==============================] - 0s 75us/step - loss: 0.8399 - acc: 0.6437 - val_loss: 0.8909 - val_acc: 0.6629\n",
      "Epoch 30/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8714 - acc: 0.6424 - val_loss: 0.8964 - val_acc: 0.6343\n",
      "Epoch 31/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8467 - acc: 0.6450 - val_loss: 0.8934 - val_acc: 0.6686\n",
      "Epoch 32/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.8527 - acc: 0.6481 - val_loss: 0.8939 - val_acc: 0.6743\n",
      "Epoch 33/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.8452 - acc: 0.6539 - val_loss: 0.8920 - val_acc: 0.6514\n",
      "Epoch 34/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8452 - acc: 0.6558 - val_loss: 0.8862 - val_acc: 0.6629\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.8430 - acc: 0.6628 - val_loss: 0.8978 - val_acc: 0.6629\n",
      "Epoch 36/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8332 - acc: 0.6494 - val_loss: 0.9011 - val_acc: 0.6571\n",
      "Epoch 37/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8414 - acc: 0.6571 - val_loss: 0.9003 - val_acc: 0.6857\n",
      "Epoch 38/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.8379 - acc: 0.6526 - val_loss: 0.9072 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.8424 - acc: 0.6494 - val_loss: 0.8949 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.8280 - acc: 0.6456 - val_loss: 0.8939 - val_acc: 0.6743\n",
      "Epoch 41/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.8099 - acc: 0.6622 - val_loss: 0.9007 - val_acc: 0.6800\n",
      "Epoch 42/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7961 - acc: 0.6788 - val_loss: 0.8941 - val_acc: 0.6743\n",
      "Epoch 43/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7972 - acc: 0.6750 - val_loss: 0.8988 - val_acc: 0.6514\n",
      "Epoch 44/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.7953 - acc: 0.6660 - val_loss: 0.8999 - val_acc: 0.6629\n",
      "Epoch 45/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.8171 - acc: 0.6775 - val_loss: 0.8991 - val_acc: 0.6457\n",
      "Epoch 46/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7799 - acc: 0.6877 - val_loss: 0.8994 - val_acc: 0.6571\n",
      "Epoch 47/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.8017 - acc: 0.6750 - val_loss: 0.9058 - val_acc: 0.6571\n",
      "Epoch 48/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7848 - acc: 0.6820 - val_loss: 0.9032 - val_acc: 0.6686\n",
      "Epoch 49/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7947 - acc: 0.6801 - val_loss: 0.9068 - val_acc: 0.6686\n",
      "Epoch 50/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7696 - acc: 0.6890 - val_loss: 0.9162 - val_acc: 0.6514\n",
      "Epoch 51/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7814 - acc: 0.6865 - val_loss: 0.9238 - val_acc: 0.6571\n",
      "Epoch 52/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7640 - acc: 0.6756 - val_loss: 0.9209 - val_acc: 0.6571\n",
      "Epoch 53/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7654 - acc: 0.6960 - val_loss: 0.9221 - val_acc: 0.6629\n",
      "Epoch 54/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.7470 - acc: 0.7043 - val_loss: 0.9163 - val_acc: 0.6571\n",
      "Epoch 55/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.7695 - acc: 0.6897 - val_loss: 0.9241 - val_acc: 0.6629\n",
      "Epoch 56/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.7537 - acc: 0.6960 - val_loss: 0.9288 - val_acc: 0.6571\n",
      "Epoch 57/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7508 - acc: 0.6839 - val_loss: 0.9298 - val_acc: 0.6686\n",
      "Epoch 58/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7592 - acc: 0.6992 - val_loss: 0.9380 - val_acc: 0.6514\n",
      "Epoch 59/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7406 - acc: 0.6967 - val_loss: 0.9232 - val_acc: 0.6457\n",
      "Epoch 60/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7382 - acc: 0.7120 - val_loss: 0.9306 - val_acc: 0.6629\n",
      "Epoch 61/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7529 - acc: 0.6884 - val_loss: 0.9245 - val_acc: 0.6571\n",
      "Epoch 62/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7357 - acc: 0.6954 - val_loss: 0.9269 - val_acc: 0.6514\n",
      "Epoch 63/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7485 - acc: 0.7037 - val_loss: 0.9444 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.7186 - acc: 0.6999 - val_loss: 0.9278 - val_acc: 0.6457\n",
      "Epoch 65/200\n",
      "1566/1566 [==============================] - 0s 48us/step - loss: 0.7416 - acc: 0.7069 - val_loss: 0.9280 - val_acc: 0.6514\n",
      "Epoch 66/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.7188 - acc: 0.7063 - val_loss: 0.9422 - val_acc: 0.6457\n",
      "Epoch 67/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7198 - acc: 0.7011 - val_loss: 0.9513 - val_acc: 0.6571\n",
      "Epoch 68/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7191 - acc: 0.6973 - val_loss: 0.9441 - val_acc: 0.6571\n",
      "Epoch 69/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.7114 - acc: 0.6992 - val_loss: 0.9540 - val_acc: 0.6629\n",
      "Epoch 70/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7117 - acc: 0.7152 - val_loss: 0.9467 - val_acc: 0.6629\n",
      "Epoch 71/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.7234 - acc: 0.7101 - val_loss: 0.9493 - val_acc: 0.6686\n",
      "Epoch 72/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.7162 - acc: 0.7248 - val_loss: 0.9437 - val_acc: 0.6571\n",
      "Epoch 73/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7020 - acc: 0.7146 - val_loss: 0.9511 - val_acc: 0.6571\n",
      "Epoch 74/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6956 - acc: 0.7107 - val_loss: 0.9511 - val_acc: 0.6514\n",
      "Epoch 75/200\n",
      "1566/1566 [==============================] - 0s 67us/step - loss: 0.7116 - acc: 0.7101 - val_loss: 0.9541 - val_acc: 0.6343\n",
      "Epoch 76/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.7041 - acc: 0.7120 - val_loss: 0.9691 - val_acc: 0.6457\n",
      "Epoch 77/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6849 - acc: 0.7165 - val_loss: 0.9711 - val_acc: 0.6400\n",
      "Epoch 78/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.7023 - acc: 0.7165 - val_loss: 0.9614 - val_acc: 0.6400\n",
      "Epoch 79/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6867 - acc: 0.7190 - val_loss: 0.9627 - val_acc: 0.6457\n",
      "Epoch 80/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6694 - acc: 0.7292 - val_loss: 0.9699 - val_acc: 0.6514\n",
      "Epoch 81/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.6842 - acc: 0.7190 - val_loss: 0.9724 - val_acc: 0.6571\n",
      "Epoch 82/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6644 - acc: 0.7350 - val_loss: 0.9695 - val_acc: 0.6457\n",
      "Epoch 83/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6770 - acc: 0.7267 - val_loss: 0.9571 - val_acc: 0.6514\n",
      "Epoch 84/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6840 - acc: 0.7254 - val_loss: 0.9558 - val_acc: 0.6457\n",
      "Epoch 85/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6718 - acc: 0.7427 - val_loss: 0.9697 - val_acc: 0.6400\n",
      "Epoch 86/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.6693 - acc: 0.7420 - val_loss: 0.9680 - val_acc: 0.6514\n",
      "Epoch 87/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6745 - acc: 0.7280 - val_loss: 0.9739 - val_acc: 0.6343\n",
      "Epoch 88/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6437 - acc: 0.7401 - val_loss: 0.9832 - val_acc: 0.6343\n",
      "Epoch 89/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6572 - acc: 0.7420 - val_loss: 0.9829 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6625 - acc: 0.7248 - val_loss: 0.9791 - val_acc: 0.6514\n",
      "Epoch 91/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6637 - acc: 0.7305 - val_loss: 0.9922 - val_acc: 0.6514\n",
      "Epoch 92/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6652 - acc: 0.7241 - val_loss: 0.9952 - val_acc: 0.6400\n",
      "Epoch 93/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6434 - acc: 0.7446 - val_loss: 0.9910 - val_acc: 0.6514\n",
      "Epoch 94/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6593 - acc: 0.7197 - val_loss: 0.9829 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6455 - acc: 0.7382 - val_loss: 0.9855 - val_acc: 0.6457\n",
      "Epoch 96/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6484 - acc: 0.7363 - val_loss: 0.9799 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1566/1566 [==============================] - 0s 61us/step - loss: 0.6602 - acc: 0.7273 - val_loss: 0.9814 - val_acc: 0.6457\n",
      "Epoch 98/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6482 - acc: 0.7382 - val_loss: 0.9890 - val_acc: 0.6400\n",
      "Epoch 99/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6384 - acc: 0.7465 - val_loss: 1.0025 - val_acc: 0.6457\n",
      "Epoch 100/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6331 - acc: 0.7427 - val_loss: 1.0165 - val_acc: 0.6400\n",
      "Epoch 101/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.6181 - acc: 0.7554 - val_loss: 1.0098 - val_acc: 0.6400\n",
      "Epoch 102/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.6195 - acc: 0.7471 - val_loss: 1.0115 - val_acc: 0.6457\n",
      "Epoch 103/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6274 - acc: 0.7478 - val_loss: 0.9990 - val_acc: 0.6457\n",
      "Epoch 104/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6071 - acc: 0.7516 - val_loss: 1.0033 - val_acc: 0.6514\n",
      "Epoch 105/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.6349 - acc: 0.7369 - val_loss: 1.0085 - val_acc: 0.6286\n",
      "Epoch 106/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6308 - acc: 0.7484 - val_loss: 1.0103 - val_acc: 0.6343\n",
      "Epoch 107/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.6097 - acc: 0.7637 - val_loss: 1.0243 - val_acc: 0.6400\n",
      "Epoch 108/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6207 - acc: 0.7618 - val_loss: 1.0140 - val_acc: 0.6514\n",
      "Epoch 109/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6022 - acc: 0.7586 - val_loss: 1.0246 - val_acc: 0.6514\n",
      "Epoch 110/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.6114 - acc: 0.7478 - val_loss: 1.0293 - val_acc: 0.6514\n",
      "Epoch 111/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.6003 - acc: 0.7746 - val_loss: 1.0358 - val_acc: 0.6571\n",
      "Epoch 112/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5976 - acc: 0.7663 - val_loss: 1.0313 - val_acc: 0.6514\n",
      "Epoch 113/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.6025 - acc: 0.7593 - val_loss: 1.0352 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.6051 - acc: 0.7490 - val_loss: 1.0331 - val_acc: 0.6514\n",
      "Epoch 115/200\n",
      "1566/1566 [==============================] - 0s 60us/step - loss: 0.6227 - acc: 0.7561 - val_loss: 1.0522 - val_acc: 0.6400\n",
      "Epoch 116/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.6127 - acc: 0.7580 - val_loss: 1.0512 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5950 - acc: 0.7637 - val_loss: 1.0548 - val_acc: 0.6343\n",
      "Epoch 118/200\n",
      "1566/1566 [==============================] - 0s 64us/step - loss: 0.5692 - acc: 0.7663 - val_loss: 1.0564 - val_acc: 0.6343\n",
      "Epoch 119/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5903 - acc: 0.7631 - val_loss: 1.0573 - val_acc: 0.6343\n",
      "Epoch 120/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5943 - acc: 0.7599 - val_loss: 1.0643 - val_acc: 0.6400\n",
      "Epoch 121/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5844 - acc: 0.7714 - val_loss: 1.0585 - val_acc: 0.6400\n",
      "Epoch 122/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5822 - acc: 0.7554 - val_loss: 1.0741 - val_acc: 0.6400\n",
      "Epoch 123/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5792 - acc: 0.7727 - val_loss: 1.0707 - val_acc: 0.6514\n",
      "Epoch 124/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5717 - acc: 0.7682 - val_loss: 1.0724 - val_acc: 0.6286\n",
      "Epoch 125/200\n",
      "1566/1566 [==============================] - 0s 58us/step - loss: 0.5898 - acc: 0.7612 - val_loss: 1.0857 - val_acc: 0.6343\n",
      "Epoch 126/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5763 - acc: 0.7714 - val_loss: 1.0865 - val_acc: 0.6286\n",
      "Epoch 127/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5755 - acc: 0.7695 - val_loss: 1.0860 - val_acc: 0.6286\n",
      "Epoch 128/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5793 - acc: 0.7618 - val_loss: 1.0875 - val_acc: 0.6229\n",
      "Epoch 129/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5786 - acc: 0.7656 - val_loss: 1.0765 - val_acc: 0.6286\n",
      "Epoch 130/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5522 - acc: 0.7739 - val_loss: 1.0928 - val_acc: 0.6343\n",
      "Epoch 131/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5826 - acc: 0.7701 - val_loss: 1.0896 - val_acc: 0.6400\n",
      "Epoch 132/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5580 - acc: 0.7746 - val_loss: 1.0950 - val_acc: 0.6229\n",
      "Epoch 133/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5579 - acc: 0.7771 - val_loss: 1.0894 - val_acc: 0.6286\n",
      "Epoch 134/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5431 - acc: 0.7784 - val_loss: 1.0877 - val_acc: 0.6286\n",
      "Epoch 135/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.5559 - acc: 0.7797 - val_loss: 1.1056 - val_acc: 0.6229\n",
      "Epoch 136/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5887 - acc: 0.7644 - val_loss: 1.1031 - val_acc: 0.6229\n",
      "Epoch 137/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5359 - acc: 0.7829 - val_loss: 1.1054 - val_acc: 0.6229\n",
      "Epoch 138/200\n",
      "1566/1566 [==============================] - 0s 63us/step - loss: 0.5514 - acc: 0.7733 - val_loss: 1.1117 - val_acc: 0.6286\n",
      "Epoch 139/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5537 - acc: 0.7867 - val_loss: 1.1180 - val_acc: 0.6286\n",
      "Epoch 140/200\n",
      "1566/1566 [==============================] - 0s 62us/step - loss: 0.5516 - acc: 0.7816 - val_loss: 1.1138 - val_acc: 0.6343\n",
      "Epoch 141/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5475 - acc: 0.7842 - val_loss: 1.1211 - val_acc: 0.6400\n",
      "Epoch 142/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5408 - acc: 0.7822 - val_loss: 1.1115 - val_acc: 0.6457\n",
      "Epoch 143/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5439 - acc: 0.7803 - val_loss: 1.1301 - val_acc: 0.6343\n",
      "Epoch 144/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5192 - acc: 0.7982 - val_loss: 1.1276 - val_acc: 0.6286\n",
      "Epoch 145/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5333 - acc: 0.7925 - val_loss: 1.1468 - val_acc: 0.6229\n",
      "Epoch 146/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.5258 - acc: 0.7905 - val_loss: 1.1461 - val_acc: 0.6286\n",
      "Epoch 147/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5412 - acc: 0.7963 - val_loss: 1.1456 - val_acc: 0.6286\n",
      "Epoch 148/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5265 - acc: 0.7893 - val_loss: 1.1583 - val_acc: 0.6286\n",
      "Epoch 149/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.5130 - acc: 0.7867 - val_loss: 1.1619 - val_acc: 0.6229\n",
      "Epoch 150/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5234 - acc: 0.7918 - val_loss: 1.1612 - val_acc: 0.6286\n",
      "Epoch 151/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.5318 - acc: 0.7848 - val_loss: 1.1597 - val_acc: 0.6286\n",
      "Epoch 152/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5217 - acc: 0.7829 - val_loss: 1.1656 - val_acc: 0.6343\n",
      "Epoch 153/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.5021 - acc: 0.8040 - val_loss: 1.1481 - val_acc: 0.6343\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 0s 69us/step - loss: 0.5252 - acc: 0.7816 - val_loss: 1.1578 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4941 - acc: 0.8033 - val_loss: 1.1515 - val_acc: 0.6229\n",
      "Epoch 156/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4942 - acc: 0.8091 - val_loss: 1.1549 - val_acc: 0.6229\n",
      "Epoch 157/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.5143 - acc: 0.7957 - val_loss: 1.1782 - val_acc: 0.6286\n",
      "Epoch 158/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4998 - acc: 0.8084 - val_loss: 1.1728 - val_acc: 0.6171\n",
      "Epoch 159/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.5039 - acc: 0.7925 - val_loss: 1.1684 - val_acc: 0.6229\n",
      "Epoch 160/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.5169 - acc: 0.8065 - val_loss: 1.1792 - val_acc: 0.6229\n",
      "Epoch 161/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4940 - acc: 0.8059 - val_loss: 1.1698 - val_acc: 0.6171\n",
      "Epoch 162/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4769 - acc: 0.8212 - val_loss: 1.1855 - val_acc: 0.6171\n",
      "Epoch 163/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4970 - acc: 0.7957 - val_loss: 1.1881 - val_acc: 0.6229\n",
      "Epoch 164/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4693 - acc: 0.8097 - val_loss: 1.1798 - val_acc: 0.6229\n",
      "Epoch 165/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.5092 - acc: 0.7925 - val_loss: 1.1797 - val_acc: 0.6286\n",
      "Epoch 166/200\n",
      "1566/1566 [==============================] - 0s 65us/step - loss: 0.5021 - acc: 0.8001 - val_loss: 1.1810 - val_acc: 0.6229\n",
      "Epoch 167/200\n",
      "1566/1566 [==============================] - 0s 59us/step - loss: 0.4873 - acc: 0.8116 - val_loss: 1.1732 - val_acc: 0.6286\n",
      "Epoch 168/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4877 - acc: 0.8155 - val_loss: 1.1765 - val_acc: 0.6343\n",
      "Epoch 169/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4858 - acc: 0.8046 - val_loss: 1.1795 - val_acc: 0.6286\n",
      "Epoch 170/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4834 - acc: 0.8097 - val_loss: 1.2088 - val_acc: 0.6286\n",
      "Epoch 171/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4866 - acc: 0.8148 - val_loss: 1.2044 - val_acc: 0.6286\n",
      "Epoch 172/200\n",
      "1566/1566 [==============================] - 0s 56us/step - loss: 0.4910 - acc: 0.8116 - val_loss: 1.2064 - val_acc: 0.6229\n",
      "Epoch 173/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4729 - acc: 0.8135 - val_loss: 1.2015 - val_acc: 0.6171\n",
      "Epoch 174/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4864 - acc: 0.7989 - val_loss: 1.2081 - val_acc: 0.6171\n",
      "Epoch 175/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4740 - acc: 0.8180 - val_loss: 1.2025 - val_acc: 0.6171\n",
      "Epoch 176/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4680 - acc: 0.8263 - val_loss: 1.1985 - val_acc: 0.6229\n",
      "Epoch 177/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4775 - acc: 0.8059 - val_loss: 1.2148 - val_acc: 0.6171\n",
      "Epoch 178/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4933 - acc: 0.8116 - val_loss: 1.2285 - val_acc: 0.6057\n",
      "Epoch 179/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4847 - acc: 0.8103 - val_loss: 1.2166 - val_acc: 0.6171\n",
      "Epoch 180/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4519 - acc: 0.8321 - val_loss: 1.2136 - val_acc: 0.6171\n",
      "Epoch 181/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4656 - acc: 0.8186 - val_loss: 1.2157 - val_acc: 0.6114\n",
      "Epoch 182/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4673 - acc: 0.8129 - val_loss: 1.2250 - val_acc: 0.6171\n",
      "Epoch 183/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4780 - acc: 0.8148 - val_loss: 1.2172 - val_acc: 0.6171\n",
      "Epoch 184/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4531 - acc: 0.8238 - val_loss: 1.2214 - val_acc: 0.6171\n",
      "Epoch 185/200\n",
      "1566/1566 [==============================] - 0s 49us/step - loss: 0.4721 - acc: 0.8167 - val_loss: 1.2068 - val_acc: 0.6286\n",
      "Epoch 186/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4514 - acc: 0.8167 - val_loss: 1.2047 - val_acc: 0.6114\n",
      "Epoch 187/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4721 - acc: 0.8269 - val_loss: 1.2245 - val_acc: 0.6171\n",
      "Epoch 188/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4442 - acc: 0.8282 - val_loss: 1.2195 - val_acc: 0.6229\n",
      "Epoch 189/200\n",
      "1566/1566 [==============================] - 0s 55us/step - loss: 0.4649 - acc: 0.8193 - val_loss: 1.2252 - val_acc: 0.6171\n",
      "Epoch 190/200\n",
      "1566/1566 [==============================] - 0s 57us/step - loss: 0.4474 - acc: 0.8295 - val_loss: 1.2446 - val_acc: 0.6057\n",
      "Epoch 191/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4757 - acc: 0.8129 - val_loss: 1.2558 - val_acc: 0.6114\n",
      "Epoch 192/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4623 - acc: 0.8218 - val_loss: 1.2500 - val_acc: 0.6114\n",
      "Epoch 193/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4759 - acc: 0.8129 - val_loss: 1.2644 - val_acc: 0.6057\n",
      "Epoch 194/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4186 - acc: 0.8372 - val_loss: 1.2649 - val_acc: 0.6114\n",
      "Epoch 195/200\n",
      "1566/1566 [==============================] - 0s 51us/step - loss: 0.4532 - acc: 0.8238 - val_loss: 1.2612 - val_acc: 0.6171\n",
      "Epoch 196/200\n",
      "1566/1566 [==============================] - 0s 50us/step - loss: 0.4612 - acc: 0.8257 - val_loss: 1.2664 - val_acc: 0.6171\n",
      "Epoch 197/200\n",
      "1566/1566 [==============================] - 0s 52us/step - loss: 0.4605 - acc: 0.8244 - val_loss: 1.2644 - val_acc: 0.6171\n",
      "Epoch 198/200\n",
      "1566/1566 [==============================] - 0s 54us/step - loss: 0.4447 - acc: 0.8333 - val_loss: 1.2777 - val_acc: 0.6114\n",
      "Epoch 199/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4457 - acc: 0.8193 - val_loss: 1.2864 - val_acc: 0.6114\n",
      "Epoch 200/200\n",
      "1566/1566 [==============================] - 0s 53us/step - loss: 0.4546 - acc: 0.8174 - val_loss: 1.2885 - val_acc: 0.6114\n",
      "194/194 [==============================] - 0s 53us/step\n",
      "1741/1741 [==============================] - 0s 36us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 9s 6ms/step - loss: 1.4910 - acc: 0.2961 - val_loss: 1.1988 - val_acc: 0.4971\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.2874 - acc: 0.3995 - val_loss: 1.0866 - val_acc: 0.5886\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.2047 - acc: 0.4601 - val_loss: 1.0292 - val_acc: 0.6171\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.1267 - acc: 0.4869 - val_loss: 0.9943 - val_acc: 0.6514\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.0999 - acc: 0.5080 - val_loss: 0.9781 - val_acc: 0.6629\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0450 - acc: 0.5469 - val_loss: 0.9648 - val_acc: 0.6686\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0325 - acc: 0.5609 - val_loss: 0.9510 - val_acc: 0.6800\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0116 - acc: 0.5546 - val_loss: 0.9318 - val_acc: 0.6857\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 1.0041 - acc: 0.5743 - val_loss: 0.9212 - val_acc: 0.6743\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9838 - acc: 0.5782 - val_loss: 0.9190 - val_acc: 0.6743\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.9653 - acc: 0.5954 - val_loss: 0.9115 - val_acc: 0.6800\n",
      "Epoch 12/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9646 - acc: 0.5788 - val_loss: 0.9113 - val_acc: 0.6857\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9487 - acc: 0.6075 - val_loss: 0.9058 - val_acc: 0.6800\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9350 - acc: 0.5935 - val_loss: 0.9027 - val_acc: 0.6914\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9362 - acc: 0.5986 - val_loss: 0.8989 - val_acc: 0.6743\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9383 - acc: 0.6043 - val_loss: 0.9064 - val_acc: 0.6914\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9257 - acc: 0.6158 - val_loss: 0.9037 - val_acc: 0.6914\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8869 - acc: 0.6280 - val_loss: 0.9068 - val_acc: 0.6800\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9094 - acc: 0.6177 - val_loss: 0.9128 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9106 - acc: 0.6318 - val_loss: 0.9082 - val_acc: 0.6800\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8922 - acc: 0.6273 - val_loss: 0.9058 - val_acc: 0.6914\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8975 - acc: 0.6305 - val_loss: 0.9113 - val_acc: 0.6857\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8800 - acc: 0.6337 - val_loss: 0.9024 - val_acc: 0.6914\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8554 - acc: 0.6509 - val_loss: 0.9075 - val_acc: 0.6743\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8677 - acc: 0.6452 - val_loss: 0.9025 - val_acc: 0.6971\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8523 - acc: 0.6509 - val_loss: 0.8985 - val_acc: 0.6857\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8726 - acc: 0.6414 - val_loss: 0.8975 - val_acc: 0.6914\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8674 - acc: 0.6426 - val_loss: 0.8965 - val_acc: 0.6800\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8476 - acc: 0.6490 - val_loss: 0.8947 - val_acc: 0.6800\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8379 - acc: 0.6471 - val_loss: 0.8963 - val_acc: 0.6914\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8474 - acc: 0.6452 - val_loss: 0.8974 - val_acc: 0.6857\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8284 - acc: 0.6535 - val_loss: 0.9012 - val_acc: 0.6914\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8194 - acc: 0.6567 - val_loss: 0.9054 - val_acc: 0.6857\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8281 - acc: 0.6586 - val_loss: 0.9076 - val_acc: 0.6857\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8190 - acc: 0.6752 - val_loss: 0.9117 - val_acc: 0.6857\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8408 - acc: 0.6567 - val_loss: 0.9132 - val_acc: 0.6857\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8293 - acc: 0.6611 - val_loss: 0.9157 - val_acc: 0.6971\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.8103 - acc: 0.6777 - val_loss: 0.9141 - val_acc: 0.6857\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8224 - acc: 0.6471 - val_loss: 0.9086 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7975 - acc: 0.6631 - val_loss: 0.9106 - val_acc: 0.6800\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8102 - acc: 0.6688 - val_loss: 0.9152 - val_acc: 0.7029\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8070 - acc: 0.6720 - val_loss: 0.9232 - val_acc: 0.6857\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7756 - acc: 0.6847 - val_loss: 0.9249 - val_acc: 0.7029\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7918 - acc: 0.6777 - val_loss: 0.9231 - val_acc: 0.6971\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7703 - acc: 0.6854 - val_loss: 0.9224 - val_acc: 0.7029\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7923 - acc: 0.6784 - val_loss: 0.9292 - val_acc: 0.7029\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7618 - acc: 0.6867 - val_loss: 0.9180 - val_acc: 0.6857\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7670 - acc: 0.6981 - val_loss: 0.9260 - val_acc: 0.6971\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7670 - acc: 0.6873 - val_loss: 0.9133 - val_acc: 0.6971\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.7790 - acc: 0.6739 - val_loss: 0.9199 - val_acc: 0.7029\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7651 - acc: 0.6841 - val_loss: 0.9320 - val_acc: 0.6914\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7608 - acc: 0.6847 - val_loss: 0.9296 - val_acc: 0.6971\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7687 - acc: 0.6956 - val_loss: 0.9257 - val_acc: 0.6971\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7491 - acc: 0.6892 - val_loss: 0.9333 - val_acc: 0.6914\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7571 - acc: 0.6803 - val_loss: 0.9380 - val_acc: 0.6857\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7370 - acc: 0.7033 - val_loss: 0.9524 - val_acc: 0.6743\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7425 - acc: 0.6911 - val_loss: 0.9436 - val_acc: 0.6857\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7487 - acc: 0.6899 - val_loss: 0.9570 - val_acc: 0.6857\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7464 - acc: 0.6943 - val_loss: 0.9437 - val_acc: 0.6686\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7469 - acc: 0.6969 - val_loss: 0.9554 - val_acc: 0.6800\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7507 - acc: 0.6943 - val_loss: 0.9529 - val_acc: 0.6914\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7337 - acc: 0.7020 - val_loss: 0.9570 - val_acc: 0.6686\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7247 - acc: 0.7033 - val_loss: 0.9590 - val_acc: 0.6686\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7374 - acc: 0.6860 - val_loss: 0.9624 - val_acc: 0.6571\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6871 - acc: 0.7205 - val_loss: 0.9526 - val_acc: 0.6971\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7185 - acc: 0.7173 - val_loss: 0.9531 - val_acc: 0.6743\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7205 - acc: 0.7128 - val_loss: 0.9594 - val_acc: 0.6743\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7086 - acc: 0.7096 - val_loss: 0.9683 - val_acc: 0.6800\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7009 - acc: 0.7173 - val_loss: 0.9658 - val_acc: 0.6743\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7145 - acc: 0.7154 - val_loss: 0.9776 - val_acc: 0.6686\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6984 - acc: 0.7352 - val_loss: 0.9635 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6784 - acc: 0.7179 - val_loss: 0.9646 - val_acc: 0.6629\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7194 - acc: 0.7179 - val_loss: 0.9652 - val_acc: 0.6686\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6905 - acc: 0.7243 - val_loss: 0.9773 - val_acc: 0.6686\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7052 - acc: 0.7135 - val_loss: 0.9686 - val_acc: 0.6743\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6959 - acc: 0.7218 - val_loss: 0.9667 - val_acc: 0.6800\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6714 - acc: 0.7352 - val_loss: 0.9692 - val_acc: 0.6743\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6745 - acc: 0.7269 - val_loss: 0.9807 - val_acc: 0.6629\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6908 - acc: 0.7250 - val_loss: 0.9766 - val_acc: 0.6629\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6718 - acc: 0.7384 - val_loss: 0.9820 - val_acc: 0.6514\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6577 - acc: 0.7301 - val_loss: 0.9864 - val_acc: 0.6457\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6824 - acc: 0.7326 - val_loss: 0.9821 - val_acc: 0.6686\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6481 - acc: 0.7403 - val_loss: 0.9939 - val_acc: 0.6629\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6668 - acc: 0.7243 - val_loss: 0.9940 - val_acc: 0.6686\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6690 - acc: 0.7294 - val_loss: 0.9843 - val_acc: 0.6514\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6577 - acc: 0.7332 - val_loss: 0.9971 - val_acc: 0.6571\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6560 - acc: 0.7294 - val_loss: 0.9951 - val_acc: 0.6457\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6425 - acc: 0.7358 - val_loss: 0.9924 - val_acc: 0.6514\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6474 - acc: 0.7441 - val_loss: 0.9992 - val_acc: 0.6571\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6494 - acc: 0.7486 - val_loss: 1.0111 - val_acc: 0.6571\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6673 - acc: 0.7243 - val_loss: 1.0070 - val_acc: 0.6400\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6543 - acc: 0.7307 - val_loss: 1.0065 - val_acc: 0.6457\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6560 - acc: 0.7301 - val_loss: 1.0127 - val_acc: 0.6457\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6150 - acc: 0.7454 - val_loss: 1.0147 - val_acc: 0.6400\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6115 - acc: 0.7620 - val_loss: 1.0193 - val_acc: 0.6571\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6365 - acc: 0.7422 - val_loss: 1.0294 - val_acc: 0.6514\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6290 - acc: 0.7524 - val_loss: 1.0251 - val_acc: 0.6457\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6170 - acc: 0.7575 - val_loss: 1.0375 - val_acc: 0.6571\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6249 - acc: 0.7549 - val_loss: 1.0459 - val_acc: 0.6571\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6228 - acc: 0.7530 - val_loss: 1.0331 - val_acc: 0.6629\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6150 - acc: 0.7524 - val_loss: 1.0326 - val_acc: 0.6571\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6072 - acc: 0.7524 - val_loss: 1.0240 - val_acc: 0.6514\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6144 - acc: 0.7569 - val_loss: 1.0250 - val_acc: 0.6629\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6022 - acc: 0.7588 - val_loss: 1.0262 - val_acc: 0.6629\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6242 - acc: 0.7473 - val_loss: 1.0317 - val_acc: 0.6686\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6272 - acc: 0.7518 - val_loss: 1.0437 - val_acc: 0.6629\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6107 - acc: 0.7562 - val_loss: 1.0454 - val_acc: 0.6686\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6069 - acc: 0.7543 - val_loss: 1.0586 - val_acc: 0.6743\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6189 - acc: 0.7454 - val_loss: 1.0499 - val_acc: 0.6686\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5969 - acc: 0.7556 - val_loss: 1.0502 - val_acc: 0.6629\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6050 - acc: 0.7543 - val_loss: 1.0449 - val_acc: 0.6571\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5911 - acc: 0.7754 - val_loss: 1.0541 - val_acc: 0.6571\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5986 - acc: 0.7709 - val_loss: 1.0588 - val_acc: 0.6629\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6107 - acc: 0.7524 - val_loss: 1.0561 - val_acc: 0.6457\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5947 - acc: 0.7588 - val_loss: 1.0560 - val_acc: 0.6571\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5778 - acc: 0.7709 - val_loss: 1.0582 - val_acc: 0.6457\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5947 - acc: 0.7562 - val_loss: 1.0582 - val_acc: 0.6514\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6019 - acc: 0.7754 - val_loss: 1.0668 - val_acc: 0.6457\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5641 - acc: 0.7792 - val_loss: 1.0704 - val_acc: 0.6514\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5676 - acc: 0.7741 - val_loss: 1.0629 - val_acc: 0.6686\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6002 - acc: 0.7575 - val_loss: 1.0558 - val_acc: 0.6686\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5904 - acc: 0.7728 - val_loss: 1.0629 - val_acc: 0.6400\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5774 - acc: 0.7862 - val_loss: 1.0725 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5582 - acc: 0.7837 - val_loss: 1.0782 - val_acc: 0.6457\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5627 - acc: 0.7792 - val_loss: 1.0755 - val_acc: 0.6629\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5556 - acc: 0.7754 - val_loss: 1.0854 - val_acc: 0.6457\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5620 - acc: 0.7811 - val_loss: 1.0885 - val_acc: 0.6457\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5546 - acc: 0.7798 - val_loss: 1.1009 - val_acc: 0.6571\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5892 - acc: 0.7741 - val_loss: 1.0940 - val_acc: 0.6514\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5551 - acc: 0.7747 - val_loss: 1.0882 - val_acc: 0.6457\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5603 - acc: 0.7741 - val_loss: 1.0903 - val_acc: 0.6343\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5302 - acc: 0.7977 - val_loss: 1.1167 - val_acc: 0.6629\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5709 - acc: 0.7786 - val_loss: 1.0989 - val_acc: 0.6514\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5504 - acc: 0.7939 - val_loss: 1.1060 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5581 - acc: 0.7843 - val_loss: 1.1175 - val_acc: 0.6457\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.5297 - acc: 0.8022 - val_loss: 1.1226 - val_acc: 0.6457\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5303 - acc: 0.7849 - val_loss: 1.1270 - val_acc: 0.6400\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5298 - acc: 0.7881 - val_loss: 1.1370 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5409 - acc: 0.7843 - val_loss: 1.1405 - val_acc: 0.6457\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5235 - acc: 0.7996 - val_loss: 1.1479 - val_acc: 0.6343\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5493 - acc: 0.7971 - val_loss: 1.1402 - val_acc: 0.6457\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5134 - acc: 0.7939 - val_loss: 1.1381 - val_acc: 0.6400\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5377 - acc: 0.7983 - val_loss: 1.1305 - val_acc: 0.6343\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5177 - acc: 0.7983 - val_loss: 1.1431 - val_acc: 0.6343\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5324 - acc: 0.7996 - val_loss: 1.1465 - val_acc: 0.6343\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - ETA: 0s - loss: 0.5359 - acc: 0.796 - 0s 50us/step - loss: 0.5344 - acc: 0.7945 - val_loss: 1.1454 - val_acc: 0.6400\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5129 - acc: 0.7900 - val_loss: 1.1390 - val_acc: 0.6343\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5084 - acc: 0.7932 - val_loss: 1.1441 - val_acc: 0.6343\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5061 - acc: 0.7939 - val_loss: 1.1477 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5183 - acc: 0.8003 - val_loss: 1.1450 - val_acc: 0.6286\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5218 - acc: 0.7996 - val_loss: 1.1656 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5154 - acc: 0.8015 - val_loss: 1.1549 - val_acc: 0.6229\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5249 - acc: 0.8047 - val_loss: 1.1590 - val_acc: 0.6400\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5227 - acc: 0.8028 - val_loss: 1.1650 - val_acc: 0.6229\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5352 - acc: 0.7971 - val_loss: 1.1715 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4987 - acc: 0.8111 - val_loss: 1.1774 - val_acc: 0.6343\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5111 - acc: 0.7971 - val_loss: 1.1550 - val_acc: 0.6343\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5244 - acc: 0.7964 - val_loss: 1.1752 - val_acc: 0.6343\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5130 - acc: 0.8111 - val_loss: 1.1867 - val_acc: 0.6343\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5014 - acc: 0.8079 - val_loss: 1.1714 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4956 - acc: 0.8137 - val_loss: 1.1755 - val_acc: 0.6400\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4969 - acc: 0.8041 - val_loss: 1.1891 - val_acc: 0.6286\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5014 - acc: 0.8079 - val_loss: 1.1809 - val_acc: 0.6229\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5069 - acc: 0.8015 - val_loss: 1.1781 - val_acc: 0.6286\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4962 - acc: 0.8098 - val_loss: 1.1728 - val_acc: 0.6457\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.4864 - acc: 0.8137 - val_loss: 1.1847 - val_acc: 0.6400\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4857 - acc: 0.8117 - val_loss: 1.1833 - val_acc: 0.6343\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5068 - acc: 0.8015 - val_loss: 1.1767 - val_acc: 0.6343\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4713 - acc: 0.8162 - val_loss: 1.1845 - val_acc: 0.6343\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5256 - acc: 0.8003 - val_loss: 1.1853 - val_acc: 0.6286\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4705 - acc: 0.8149 - val_loss: 1.1843 - val_acc: 0.6114\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5023 - acc: 0.8117 - val_loss: 1.1859 - val_acc: 0.6343\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4953 - acc: 0.8092 - val_loss: 1.1945 - val_acc: 0.6286\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4751 - acc: 0.8124 - val_loss: 1.2062 - val_acc: 0.6171\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4652 - acc: 0.8156 - val_loss: 1.2131 - val_acc: 0.6343\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4556 - acc: 0.8277 - val_loss: 1.2114 - val_acc: 0.6286\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4861 - acc: 0.8028 - val_loss: 1.2154 - val_acc: 0.6229\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5001 - acc: 0.8060 - val_loss: 1.2165 - val_acc: 0.6229\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4847 - acc: 0.8041 - val_loss: 1.2172 - val_acc: 0.6343\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.4656 - acc: 0.8226 - val_loss: 1.2158 - val_acc: 0.6343\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4695 - acc: 0.8277 - val_loss: 1.2349 - val_acc: 0.6286\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4755 - acc: 0.8239 - val_loss: 1.2425 - val_acc: 0.6457\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4386 - acc: 0.8366 - val_loss: 1.2454 - val_acc: 0.6229\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4600 - acc: 0.8239 - val_loss: 1.2367 - val_acc: 0.6343\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4472 - acc: 0.8258 - val_loss: 1.2258 - val_acc: 0.6343\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4722 - acc: 0.8092 - val_loss: 1.2240 - val_acc: 0.6286\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4541 - acc: 0.8424 - val_loss: 1.2388 - val_acc: 0.6229\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4530 - acc: 0.8315 - val_loss: 1.2521 - val_acc: 0.6343\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4540 - acc: 0.8283 - val_loss: 1.2578 - val_acc: 0.6400\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4552 - acc: 0.8283 - val_loss: 1.2539 - val_acc: 0.6343\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4790 - acc: 0.8220 - val_loss: 1.2613 - val_acc: 0.6229\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.4553 - acc: 0.8271 - val_loss: 1.2292 - val_acc: 0.6229\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4582 - acc: 0.8245 - val_loss: 1.2272 - val_acc: 0.6343\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.4650 - acc: 0.8130 - val_loss: 1.2312 - val_acc: 0.6286\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4527 - acc: 0.8322 - val_loss: 1.2470 - val_acc: 0.6286\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.4346 - acc: 0.8366 - val_loss: 1.2709 - val_acc: 0.6171\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4312 - acc: 0.8360 - val_loss: 1.2601 - val_acc: 0.6343\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4311 - acc: 0.8437 - val_loss: 1.2654 - val_acc: 0.6286\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4560 - acc: 0.8226 - val_loss: 1.2652 - val_acc: 0.6286\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4212 - acc: 0.8373 - val_loss: 1.2630 - val_acc: 0.6114\n",
      "193/193 [==============================] - 0s 58us/step\n",
      "1742/1742 [==============================] - 0s 38us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 9s 6ms/step - loss: 1.4301 - acc: 0.3223 - val_loss: 1.1591 - val_acc: 0.5429\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 1.2670 - acc: 0.4148 - val_loss: 1.0526 - val_acc: 0.6057\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.1751 - acc: 0.4735 - val_loss: 1.0113 - val_acc: 0.6000\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 1.1263 - acc: 0.5016 - val_loss: 0.9707 - val_acc: 0.6229\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0760 - acc: 0.5329 - val_loss: 0.9500 - val_acc: 0.6286\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 1.0377 - acc: 0.5424 - val_loss: 0.9388 - val_acc: 0.6457\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 1.0428 - acc: 0.5399 - val_loss: 0.9196 - val_acc: 0.6400\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.0230 - acc: 0.5641 - val_loss: 0.9007 - val_acc: 0.6571\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9778 - acc: 0.5750 - val_loss: 0.8973 - val_acc: 0.6457\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9774 - acc: 0.5897 - val_loss: 0.8976 - val_acc: 0.6286\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.9573 - acc: 0.5980 - val_loss: 0.8883 - val_acc: 0.6343\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9509 - acc: 0.5922 - val_loss: 0.8878 - val_acc: 0.6400\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9339 - acc: 0.6158 - val_loss: 0.8846 - val_acc: 0.6457\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.9328 - acc: 0.6165 - val_loss: 0.8811 - val_acc: 0.6400\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9092 - acc: 0.6101 - val_loss: 0.8754 - val_acc: 0.6571\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.9102 - acc: 0.6350 - val_loss: 0.8740 - val_acc: 0.6514\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9139 - acc: 0.6152 - val_loss: 0.8791 - val_acc: 0.6514\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9021 - acc: 0.6133 - val_loss: 0.8784 - val_acc: 0.6629\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.8912 - acc: 0.6388 - val_loss: 0.8738 - val_acc: 0.6857\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8855 - acc: 0.6241 - val_loss: 0.8735 - val_acc: 0.6629\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8693 - acc: 0.6228 - val_loss: 0.8674 - val_acc: 0.6571\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8698 - acc: 0.6401 - val_loss: 0.8602 - val_acc: 0.6800\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.8690 - acc: 0.6375 - val_loss: 0.8609 - val_acc: 0.6686\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8626 - acc: 0.6465 - val_loss: 0.8569 - val_acc: 0.6857\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8525 - acc: 0.6567 - val_loss: 0.8637 - val_acc: 0.6571\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8554 - acc: 0.6490 - val_loss: 0.8671 - val_acc: 0.6629\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8442 - acc: 0.6509 - val_loss: 0.8612 - val_acc: 0.6629\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8427 - acc: 0.6426 - val_loss: 0.8638 - val_acc: 0.6743\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8521 - acc: 0.6452 - val_loss: 0.8635 - val_acc: 0.6857\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8386 - acc: 0.6528 - val_loss: 0.8723 - val_acc: 0.6629\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8263 - acc: 0.6567 - val_loss: 0.8733 - val_acc: 0.6857\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8187 - acc: 0.6662 - val_loss: 0.8697 - val_acc: 0.6800\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8376 - acc: 0.6465 - val_loss: 0.8611 - val_acc: 0.6857\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8267 - acc: 0.6586 - val_loss: 0.8615 - val_acc: 0.6857\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8312 - acc: 0.6713 - val_loss: 0.8601 - val_acc: 0.6743\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8179 - acc: 0.6662 - val_loss: 0.8572 - val_acc: 0.6743\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8082 - acc: 0.6662 - val_loss: 0.8604 - val_acc: 0.6743\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8050 - acc: 0.6713 - val_loss: 0.8666 - val_acc: 0.6686\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7982 - acc: 0.6650 - val_loss: 0.8696 - val_acc: 0.6743\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8071 - acc: 0.6637 - val_loss: 0.8712 - val_acc: 0.6629\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7698 - acc: 0.6847 - val_loss: 0.8658 - val_acc: 0.6743\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7865 - acc: 0.6739 - val_loss: 0.8676 - val_acc: 0.6686\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7824 - acc: 0.6835 - val_loss: 0.8689 - val_acc: 0.6629\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7823 - acc: 0.6752 - val_loss: 0.8683 - val_acc: 0.6800\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7847 - acc: 0.6835 - val_loss: 0.8674 - val_acc: 0.6629\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7675 - acc: 0.6924 - val_loss: 0.8647 - val_acc: 0.6629\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7563 - acc: 0.7064 - val_loss: 0.8691 - val_acc: 0.6800\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7711 - acc: 0.6835 - val_loss: 0.8750 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7689 - acc: 0.6847 - val_loss: 0.8718 - val_acc: 0.6743\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7621 - acc: 0.6867 - val_loss: 0.8706 - val_acc: 0.6629\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.7623 - acc: 0.6847 - val_loss: 0.8815 - val_acc: 0.6571\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7652 - acc: 0.6867 - val_loss: 0.8808 - val_acc: 0.6800\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7487 - acc: 0.7013 - val_loss: 0.8854 - val_acc: 0.6571\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7407 - acc: 0.7160 - val_loss: 0.8813 - val_acc: 0.6743\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7599 - acc: 0.6860 - val_loss: 0.8809 - val_acc: 0.6629\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7551 - acc: 0.6930 - val_loss: 0.8815 - val_acc: 0.6629\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7471 - acc: 0.6911 - val_loss: 0.8702 - val_acc: 0.6686\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.7354 - acc: 0.6873 - val_loss: 0.8749 - val_acc: 0.6571\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7245 - acc: 0.6994 - val_loss: 0.8841 - val_acc: 0.6629\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7343 - acc: 0.7039 - val_loss: 0.8800 - val_acc: 0.6743\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7334 - acc: 0.7026 - val_loss: 0.8826 - val_acc: 0.6743\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7064 - acc: 0.7179 - val_loss: 0.8843 - val_acc: 0.6629\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7099 - acc: 0.7084 - val_loss: 0.8829 - val_acc: 0.6629\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7300 - acc: 0.6975 - val_loss: 0.8844 - val_acc: 0.6686\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7110 - acc: 0.7071 - val_loss: 0.8887 - val_acc: 0.6686\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7151 - acc: 0.7096 - val_loss: 0.8846 - val_acc: 0.6686\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6944 - acc: 0.7103 - val_loss: 0.8877 - val_acc: 0.6686\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7015 - acc: 0.7275 - val_loss: 0.8978 - val_acc: 0.6743\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7037 - acc: 0.7237 - val_loss: 0.8939 - val_acc: 0.6800\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7283 - acc: 0.7103 - val_loss: 0.8970 - val_acc: 0.6743\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6868 - acc: 0.7173 - val_loss: 0.8987 - val_acc: 0.6686\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6871 - acc: 0.7230 - val_loss: 0.8947 - val_acc: 0.6800\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6914 - acc: 0.7141 - val_loss: 0.8984 - val_acc: 0.6800\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6974 - acc: 0.7167 - val_loss: 0.8966 - val_acc: 0.6800\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6940 - acc: 0.7224 - val_loss: 0.8961 - val_acc: 0.6686\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6834 - acc: 0.7339 - val_loss: 0.9044 - val_acc: 0.6686\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.6589 - acc: 0.7313 - val_loss: 0.9004 - val_acc: 0.6743\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 100us/step - loss: 0.6562 - acc: 0.7384 - val_loss: 0.9000 - val_acc: 0.6686\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6932 - acc: 0.7173 - val_loss: 0.9001 - val_acc: 0.6686\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6695 - acc: 0.7294 - val_loss: 0.9082 - val_acc: 0.6686\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6799 - acc: 0.7237 - val_loss: 0.9146 - val_acc: 0.6800\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6742 - acc: 0.7294 - val_loss: 0.9161 - val_acc: 0.6743\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6384 - acc: 0.7358 - val_loss: 0.9222 - val_acc: 0.6743\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6603 - acc: 0.7364 - val_loss: 0.9204 - val_acc: 0.6743\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6550 - acc: 0.7352 - val_loss: 0.9208 - val_acc: 0.6686\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6457 - acc: 0.7454 - val_loss: 0.9239 - val_acc: 0.6800\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6514 - acc: 0.7479 - val_loss: 0.9222 - val_acc: 0.6857\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6545 - acc: 0.7415 - val_loss: 0.9106 - val_acc: 0.6686\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6456 - acc: 0.7460 - val_loss: 0.9126 - val_acc: 0.6743\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6358 - acc: 0.7409 - val_loss: 0.9150 - val_acc: 0.6743\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6271 - acc: 0.7575 - val_loss: 0.9221 - val_acc: 0.6743\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6365 - acc: 0.7505 - val_loss: 0.9282 - val_acc: 0.6686\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6280 - acc: 0.7390 - val_loss: 0.9182 - val_acc: 0.6800\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6303 - acc: 0.7518 - val_loss: 0.9248 - val_acc: 0.6686\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6103 - acc: 0.7524 - val_loss: 0.9274 - val_acc: 0.6743\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6425 - acc: 0.7409 - val_loss: 0.9392 - val_acc: 0.6686\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6326 - acc: 0.7498 - val_loss: 0.9267 - val_acc: 0.6743\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6296 - acc: 0.7384 - val_loss: 0.9177 - val_acc: 0.6686\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.6215 - acc: 0.7466 - val_loss: 0.9194 - val_acc: 0.6686\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6294 - acc: 0.7384 - val_loss: 0.9251 - val_acc: 0.6743\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6123 - acc: 0.7486 - val_loss: 0.9253 - val_acc: 0.6743\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6057 - acc: 0.7530 - val_loss: 0.9330 - val_acc: 0.6800\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6206 - acc: 0.7588 - val_loss: 0.9261 - val_acc: 0.6857\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5886 - acc: 0.7588 - val_loss: 0.9357 - val_acc: 0.6743\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6284 - acc: 0.7530 - val_loss: 0.9389 - val_acc: 0.6686\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5988 - acc: 0.7588 - val_loss: 0.9404 - val_acc: 0.6857\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.5923 - acc: 0.7556 - val_loss: 0.9385 - val_acc: 0.6800\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5923 - acc: 0.7613 - val_loss: 0.9445 - val_acc: 0.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5984 - acc: 0.7556 - val_loss: 0.9482 - val_acc: 0.6743\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6098 - acc: 0.7530 - val_loss: 0.9387 - val_acc: 0.6743\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5818 - acc: 0.7754 - val_loss: 0.9440 - val_acc: 0.6686\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5817 - acc: 0.7824 - val_loss: 0.9490 - val_acc: 0.6629\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5830 - acc: 0.7715 - val_loss: 0.9612 - val_acc: 0.6686\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5852 - acc: 0.7690 - val_loss: 0.9591 - val_acc: 0.6686\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5977 - acc: 0.7601 - val_loss: 0.9621 - val_acc: 0.6743\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5725 - acc: 0.7709 - val_loss: 0.9518 - val_acc: 0.6800\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5587 - acc: 0.7703 - val_loss: 0.9577 - val_acc: 0.6686\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5744 - acc: 0.7747 - val_loss: 0.9664 - val_acc: 0.6857\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5941 - acc: 0.7639 - val_loss: 0.9673 - val_acc: 0.6800\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5798 - acc: 0.7620 - val_loss: 0.9662 - val_acc: 0.6914\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5772 - acc: 0.7709 - val_loss: 0.9740 - val_acc: 0.6686\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5598 - acc: 0.7754 - val_loss: 0.9597 - val_acc: 0.6857\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5770 - acc: 0.7766 - val_loss: 0.9620 - val_acc: 0.6629\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5699 - acc: 0.7856 - val_loss: 0.9532 - val_acc: 0.6686\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5738 - acc: 0.7683 - val_loss: 0.9530 - val_acc: 0.6629\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5542 - acc: 0.7900 - val_loss: 0.9594 - val_acc: 0.6571\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5479 - acc: 0.7875 - val_loss: 0.9718 - val_acc: 0.6571\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5795 - acc: 0.7696 - val_loss: 0.9550 - val_acc: 0.6686\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5544 - acc: 0.7779 - val_loss: 0.9578 - val_acc: 0.6743\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5318 - acc: 0.7932 - val_loss: 0.9640 - val_acc: 0.6743\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5637 - acc: 0.7760 - val_loss: 0.9679 - val_acc: 0.6629\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5587 - acc: 0.7811 - val_loss: 0.9739 - val_acc: 0.6743\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5549 - acc: 0.7766 - val_loss: 0.9846 - val_acc: 0.6857\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5341 - acc: 0.7900 - val_loss: 0.9899 - val_acc: 0.6629\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5307 - acc: 0.8015 - val_loss: 0.9907 - val_acc: 0.6571\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5407 - acc: 0.7817 - val_loss: 0.9816 - val_acc: 0.6686\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5427 - acc: 0.7862 - val_loss: 0.9936 - val_acc: 0.6686\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5159 - acc: 0.7971 - val_loss: 0.9977 - val_acc: 0.6629\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5124 - acc: 0.8022 - val_loss: 0.9986 - val_acc: 0.6686\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.5477 - acc: 0.7722 - val_loss: 0.9919 - val_acc: 0.6686\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5210 - acc: 0.8028 - val_loss: 0.9840 - val_acc: 0.6686\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5556 - acc: 0.7824 - val_loss: 0.9895 - val_acc: 0.6629\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5229 - acc: 0.7990 - val_loss: 0.9892 - val_acc: 0.6686\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5184 - acc: 0.7990 - val_loss: 0.9924 - val_acc: 0.6686\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5173 - acc: 0.7792 - val_loss: 0.9998 - val_acc: 0.6743\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5305 - acc: 0.7964 - val_loss: 1.0055 - val_acc: 0.6686\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.5245 - acc: 0.7932 - val_loss: 1.0053 - val_acc: 0.6686\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5388 - acc: 0.7837 - val_loss: 1.0094 - val_acc: 0.6571\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5238 - acc: 0.7817 - val_loss: 1.0005 - val_acc: 0.6629\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4861 - acc: 0.8130 - val_loss: 0.9967 - val_acc: 0.6743\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5128 - acc: 0.7920 - val_loss: 0.9927 - val_acc: 0.6629\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5283 - acc: 0.7920 - val_loss: 1.0136 - val_acc: 0.6400\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5116 - acc: 0.8015 - val_loss: 1.0151 - val_acc: 0.6686\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5115 - acc: 0.7926 - val_loss: 1.0005 - val_acc: 0.6743\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5009 - acc: 0.8079 - val_loss: 1.0147 - val_acc: 0.6800\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5045 - acc: 0.8034 - val_loss: 1.0150 - val_acc: 0.6743\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4941 - acc: 0.8156 - val_loss: 1.0267 - val_acc: 0.6743\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5016 - acc: 0.8060 - val_loss: 1.0220 - val_acc: 0.6686\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.4962 - acc: 0.8143 - val_loss: 1.0235 - val_acc: 0.6686\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4974 - acc: 0.8003 - val_loss: 1.0287 - val_acc: 0.6629\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5025 - acc: 0.8105 - val_loss: 1.0271 - val_acc: 0.6743\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.4758 - acc: 0.8111 - val_loss: 1.0250 - val_acc: 0.6743\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4807 - acc: 0.8168 - val_loss: 1.0266 - val_acc: 0.6857\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4868 - acc: 0.8041 - val_loss: 1.0252 - val_acc: 0.6571\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4912 - acc: 0.8162 - val_loss: 1.0324 - val_acc: 0.6686\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4798 - acc: 0.8124 - val_loss: 1.0265 - val_acc: 0.6629\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4794 - acc: 0.8207 - val_loss: 1.0390 - val_acc: 0.6686\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4921 - acc: 0.8130 - val_loss: 1.0492 - val_acc: 0.6571\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4800 - acc: 0.8124 - val_loss: 1.0443 - val_acc: 0.6571\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5012 - acc: 0.8003 - val_loss: 1.0390 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5019 - acc: 0.8086 - val_loss: 1.0283 - val_acc: 0.6629\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4759 - acc: 0.8277 - val_loss: 1.0384 - val_acc: 0.6457\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.4985 - acc: 0.7996 - val_loss: 1.0410 - val_acc: 0.6457\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4779 - acc: 0.8143 - val_loss: 1.0480 - val_acc: 0.6629\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.4664 - acc: 0.8296 - val_loss: 1.0430 - val_acc: 0.6457\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4537 - acc: 0.8271 - val_loss: 1.0483 - val_acc: 0.6629\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4727 - acc: 0.8207 - val_loss: 1.0553 - val_acc: 0.6457\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.4614 - acc: 0.8200 - val_loss: 1.0529 - val_acc: 0.6571\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4708 - acc: 0.8168 - val_loss: 1.0518 - val_acc: 0.6629\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4606 - acc: 0.8226 - val_loss: 1.0597 - val_acc: 0.6686\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4846 - acc: 0.8309 - val_loss: 1.0549 - val_acc: 0.6514\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4675 - acc: 0.8162 - val_loss: 1.0642 - val_acc: 0.6457\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4473 - acc: 0.8277 - val_loss: 1.0745 - val_acc: 0.6400\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.4843 - acc: 0.8073 - val_loss: 1.0656 - val_acc: 0.6571\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4727 - acc: 0.8258 - val_loss: 1.0717 - val_acc: 0.6514\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4626 - acc: 0.8060 - val_loss: 1.0497 - val_acc: 0.6686\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4496 - acc: 0.8188 - val_loss: 1.0442 - val_acc: 0.6743\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4557 - acc: 0.8207 - val_loss: 1.0412 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4590 - acc: 0.8271 - val_loss: 1.0432 - val_acc: 0.6400\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.4570 - acc: 0.8213 - val_loss: 1.0705 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4398 - acc: 0.8283 - val_loss: 1.0913 - val_acc: 0.6400\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4595 - acc: 0.8322 - val_loss: 1.0913 - val_acc: 0.6457\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4630 - acc: 0.8328 - val_loss: 1.0864 - val_acc: 0.6400\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4257 - acc: 0.8309 - val_loss: 1.0804 - val_acc: 0.6514\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4271 - acc: 0.8309 - val_loss: 1.0853 - val_acc: 0.6457\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4362 - acc: 0.8239 - val_loss: 1.0871 - val_acc: 0.6400\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4448 - acc: 0.8239 - val_loss: 1.0850 - val_acc: 0.6514\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4364 - acc: 0.8277 - val_loss: 1.0917 - val_acc: 0.6343\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4189 - acc: 0.8488 - val_loss: 1.0971 - val_acc: 0.6343\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4581 - acc: 0.8207 - val_loss: 1.0992 - val_acc: 0.6400\n",
      "193/193 [==============================] - 0s 64us/step\n",
      "1742/1742 [==============================] - 0s 39us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 9s 6ms/step - loss: 1.5389 - acc: 0.3025 - val_loss: 1.2001 - val_acc: 0.4514\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.2538 - acc: 0.4205 - val_loss: 1.0657 - val_acc: 0.5771\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 1.1590 - acc: 0.4773 - val_loss: 1.0163 - val_acc: 0.6229\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.1390 - acc: 0.4876 - val_loss: 0.9819 - val_acc: 0.6343\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 1.0847 - acc: 0.5118 - val_loss: 0.9640 - val_acc: 0.6457\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.0572 - acc: 0.5265 - val_loss: 0.9362 - val_acc: 0.6457\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 1.0387 - acc: 0.5412 - val_loss: 0.9260 - val_acc: 0.6400\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.0198 - acc: 0.5571 - val_loss: 0.9174 - val_acc: 0.6514\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.0012 - acc: 0.5750 - val_loss: 0.9128 - val_acc: 0.6686\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.9888 - acc: 0.5833 - val_loss: 0.9080 - val_acc: 0.6743\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.9711 - acc: 0.5954 - val_loss: 0.9015 - val_acc: 0.6743\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.9505 - acc: 0.5960 - val_loss: 0.8935 - val_acc: 0.6571\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9564 - acc: 0.5973 - val_loss: 0.8866 - val_acc: 0.6857\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.9405 - acc: 0.6088 - val_loss: 0.8965 - val_acc: 0.6971\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.9409 - acc: 0.6050 - val_loss: 0.8958 - val_acc: 0.6800\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9286 - acc: 0.6018 - val_loss: 0.8936 - val_acc: 0.7029\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.9384 - acc: 0.6158 - val_loss: 0.8932 - val_acc: 0.6914\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9114 - acc: 0.6184 - val_loss: 0.8956 - val_acc: 0.6743\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9259 - acc: 0.6107 - val_loss: 0.9009 - val_acc: 0.6743\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9073 - acc: 0.6190 - val_loss: 0.9026 - val_acc: 0.6800\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8860 - acc: 0.6216 - val_loss: 0.9041 - val_acc: 0.6686\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8936 - acc: 0.6241 - val_loss: 0.8898 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.8838 - acc: 0.6337 - val_loss: 0.8852 - val_acc: 0.6629\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.8951 - acc: 0.6343 - val_loss: 0.8807 - val_acc: 0.6857\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8782 - acc: 0.6407 - val_loss: 0.8886 - val_acc: 0.6686\n",
      "Epoch 26/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8674 - acc: 0.6522 - val_loss: 0.8910 - val_acc: 0.6743\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8522 - acc: 0.6503 - val_loss: 0.8891 - val_acc: 0.6800\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8484 - acc: 0.6407 - val_loss: 0.8952 - val_acc: 0.6686\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8541 - acc: 0.6631 - val_loss: 0.9032 - val_acc: 0.6914\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8498 - acc: 0.6484 - val_loss: 0.8942 - val_acc: 0.6743\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8558 - acc: 0.6554 - val_loss: 0.8929 - val_acc: 0.6743\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8365 - acc: 0.6496 - val_loss: 0.8970 - val_acc: 0.6686\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8225 - acc: 0.6560 - val_loss: 0.9026 - val_acc: 0.6571\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8187 - acc: 0.6726 - val_loss: 0.8901 - val_acc: 0.6800\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8212 - acc: 0.6599 - val_loss: 0.8839 - val_acc: 0.6800\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8153 - acc: 0.6541 - val_loss: 0.8934 - val_acc: 0.6857\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8307 - acc: 0.6471 - val_loss: 0.8933 - val_acc: 0.6686\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8075 - acc: 0.6650 - val_loss: 0.9050 - val_acc: 0.6857\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.8315 - acc: 0.6573 - val_loss: 0.9048 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8189 - acc: 0.6662 - val_loss: 0.8966 - val_acc: 0.6629\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.8037 - acc: 0.6675 - val_loss: 0.9011 - val_acc: 0.6571\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.8003 - acc: 0.6822 - val_loss: 0.9094 - val_acc: 0.6686\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8024 - acc: 0.6777 - val_loss: 0.9080 - val_acc: 0.6571\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.8001 - acc: 0.6726 - val_loss: 0.9008 - val_acc: 0.6457\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7927 - acc: 0.6828 - val_loss: 0.9047 - val_acc: 0.6400\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7780 - acc: 0.6867 - val_loss: 0.9203 - val_acc: 0.6457\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7641 - acc: 0.6828 - val_loss: 0.9275 - val_acc: 0.6571\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 47us/step - loss: 0.7524 - acc: 0.7007 - val_loss: 0.9336 - val_acc: 0.6286\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7788 - acc: 0.6899 - val_loss: 0.9279 - val_acc: 0.6400\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7746 - acc: 0.6790 - val_loss: 0.9190 - val_acc: 0.6457\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7663 - acc: 0.6899 - val_loss: 0.9276 - val_acc: 0.6457\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7824 - acc: 0.6816 - val_loss: 0.9305 - val_acc: 0.6457\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7668 - acc: 0.6835 - val_loss: 0.9259 - val_acc: 0.6629\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7418 - acc: 0.6835 - val_loss: 0.9197 - val_acc: 0.6514\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7652 - acc: 0.6796 - val_loss: 0.9190 - val_acc: 0.6457\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7576 - acc: 0.6975 - val_loss: 0.9169 - val_acc: 0.6571\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.7552 - acc: 0.6886 - val_loss: 0.9216 - val_acc: 0.6343\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7558 - acc: 0.6981 - val_loss: 0.9313 - val_acc: 0.6400\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7271 - acc: 0.7179 - val_loss: 0.9483 - val_acc: 0.6457\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7369 - acc: 0.6962 - val_loss: 0.9367 - val_acc: 0.6457\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7427 - acc: 0.7001 - val_loss: 0.9387 - val_acc: 0.6400\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7259 - acc: 0.7013 - val_loss: 0.9393 - val_acc: 0.6400\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7283 - acc: 0.6988 - val_loss: 0.9283 - val_acc: 0.6571\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7312 - acc: 0.7039 - val_loss: 0.9206 - val_acc: 0.6743\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7399 - acc: 0.7096 - val_loss: 0.9316 - val_acc: 0.6629\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.7460 - acc: 0.6994 - val_loss: 0.9362 - val_acc: 0.6457\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7126 - acc: 0.7122 - val_loss: 0.9413 - val_acc: 0.6343\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 74us/step - loss: 0.7354 - acc: 0.7020 - val_loss: 0.9288 - val_acc: 0.6686\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7180 - acc: 0.7211 - val_loss: 0.9316 - val_acc: 0.6571\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7182 - acc: 0.7084 - val_loss: 0.9327 - val_acc: 0.6514\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7240 - acc: 0.7128 - val_loss: 0.9369 - val_acc: 0.6629\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7081 - acc: 0.7116 - val_loss: 0.9319 - val_acc: 0.6629\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6819 - acc: 0.7288 - val_loss: 0.9317 - val_acc: 0.6686\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7129 - acc: 0.7058 - val_loss: 0.9335 - val_acc: 0.6571\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7002 - acc: 0.7192 - val_loss: 0.9391 - val_acc: 0.6686\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6962 - acc: 0.7320 - val_loss: 0.9347 - val_acc: 0.6743\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6949 - acc: 0.7243 - val_loss: 0.9359 - val_acc: 0.6571\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6745 - acc: 0.7275 - val_loss: 0.9539 - val_acc: 0.6457\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6917 - acc: 0.7173 - val_loss: 0.9549 - val_acc: 0.6514\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7078 - acc: 0.7052 - val_loss: 0.9501 - val_acc: 0.6571\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6732 - acc: 0.7275 - val_loss: 0.9588 - val_acc: 0.6457\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6784 - acc: 0.7288 - val_loss: 0.9586 - val_acc: 0.6629\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6709 - acc: 0.7250 - val_loss: 0.9616 - val_acc: 0.6571\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6759 - acc: 0.7422 - val_loss: 0.9605 - val_acc: 0.6457\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6780 - acc: 0.7332 - val_loss: 0.9645 - val_acc: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6766 - acc: 0.7396 - val_loss: 0.9608 - val_acc: 0.6400\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6800 - acc: 0.7301 - val_loss: 0.9737 - val_acc: 0.6400\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6743 - acc: 0.7281 - val_loss: 0.9733 - val_acc: 0.6400\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6657 - acc: 0.7250 - val_loss: 0.9650 - val_acc: 0.6514\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6408 - acc: 0.7473 - val_loss: 0.9769 - val_acc: 0.6457\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6554 - acc: 0.7339 - val_loss: 0.9819 - val_acc: 0.6343\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6527 - acc: 0.7441 - val_loss: 0.9609 - val_acc: 0.6400\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6591 - acc: 0.7390 - val_loss: 0.9716 - val_acc: 0.6229\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6471 - acc: 0.7313 - val_loss: 0.9732 - val_acc: 0.6629\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6433 - acc: 0.7390 - val_loss: 0.9702 - val_acc: 0.6514\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6457 - acc: 0.7486 - val_loss: 0.9939 - val_acc: 0.6457\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6441 - acc: 0.7396 - val_loss: 0.9856 - val_acc: 0.6686\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6307 - acc: 0.7396 - val_loss: 0.9832 - val_acc: 0.6571\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6369 - acc: 0.7511 - val_loss: 0.9930 - val_acc: 0.6286\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6362 - acc: 0.7441 - val_loss: 0.9931 - val_acc: 0.6400\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6401 - acc: 0.7403 - val_loss: 0.9964 - val_acc: 0.6343\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6216 - acc: 0.7518 - val_loss: 0.9982 - val_acc: 0.6286\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6396 - acc: 0.7435 - val_loss: 0.9939 - val_acc: 0.6457\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6173 - acc: 0.7505 - val_loss: 0.9918 - val_acc: 0.6571\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6246 - acc: 0.7486 - val_loss: 0.9877 - val_acc: 0.6571\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6163 - acc: 0.7626 - val_loss: 0.9968 - val_acc: 0.6514\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6212 - acc: 0.7524 - val_loss: 0.9977 - val_acc: 0.6457\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6112 - acc: 0.7428 - val_loss: 1.0102 - val_acc: 0.6400\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6272 - acc: 0.7562 - val_loss: 1.0077 - val_acc: 0.6571\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6195 - acc: 0.7384 - val_loss: 1.0112 - val_acc: 0.6400\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6088 - acc: 0.7447 - val_loss: 1.0081 - val_acc: 0.6514\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5927 - acc: 0.7575 - val_loss: 1.0067 - val_acc: 0.6514\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5943 - acc: 0.7683 - val_loss: 1.0165 - val_acc: 0.6457\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5978 - acc: 0.7588 - val_loss: 1.0169 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6108 - acc: 0.7524 - val_loss: 1.0323 - val_acc: 0.6343\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5981 - acc: 0.7524 - val_loss: 1.0325 - val_acc: 0.6457\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.5944 - acc: 0.7524 - val_loss: 1.0328 - val_acc: 0.6571\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5897 - acc: 0.7645 - val_loss: 1.0352 - val_acc: 0.6400\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.5829 - acc: 0.7817 - val_loss: 1.0426 - val_acc: 0.6343\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5889 - acc: 0.7645 - val_loss: 1.0441 - val_acc: 0.6514\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5773 - acc: 0.7722 - val_loss: 1.0419 - val_acc: 0.6400\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5907 - acc: 0.7549 - val_loss: 1.0427 - val_acc: 0.6400\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.5762 - acc: 0.7690 - val_loss: 1.0420 - val_acc: 0.6400\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5780 - acc: 0.7639 - val_loss: 1.0459 - val_acc: 0.6400\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5678 - acc: 0.7658 - val_loss: 1.0381 - val_acc: 0.6514\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5870 - acc: 0.7671 - val_loss: 1.0554 - val_acc: 0.6400\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5590 - acc: 0.7652 - val_loss: 1.0604 - val_acc: 0.6400\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.5599 - acc: 0.7683 - val_loss: 1.0615 - val_acc: 0.6400\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.5513 - acc: 0.7824 - val_loss: 1.0621 - val_acc: 0.6400\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5573 - acc: 0.7728 - val_loss: 1.0599 - val_acc: 0.6457\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5672 - acc: 0.7581 - val_loss: 1.0635 - val_acc: 0.6571\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5648 - acc: 0.7735 - val_loss: 1.0681 - val_acc: 0.6400\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5658 - acc: 0.7798 - val_loss: 1.0634 - val_acc: 0.6457\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5442 - acc: 0.7843 - val_loss: 1.0708 - val_acc: 0.6343\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5621 - acc: 0.7709 - val_loss: 1.0807 - val_acc: 0.6343\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5407 - acc: 0.7932 - val_loss: 1.0780 - val_acc: 0.6457\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5433 - acc: 0.7805 - val_loss: 1.0717 - val_acc: 0.6514\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5704 - acc: 0.7620 - val_loss: 1.0721 - val_acc: 0.6457\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5374 - acc: 0.7869 - val_loss: 1.0757 - val_acc: 0.6457\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5310 - acc: 0.7849 - val_loss: 1.0925 - val_acc: 0.6457\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5298 - acc: 0.7875 - val_loss: 1.0939 - val_acc: 0.6343\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5398 - acc: 0.7824 - val_loss: 1.1134 - val_acc: 0.6400\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5525 - acc: 0.7856 - val_loss: 1.1095 - val_acc: 0.6400\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.5271 - acc: 0.7913 - val_loss: 1.1017 - val_acc: 0.6400\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.5085 - acc: 0.7932 - val_loss: 1.1178 - val_acc: 0.6400\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.5200 - acc: 0.7939 - val_loss: 1.1099 - val_acc: 0.6343\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5501 - acc: 0.7824 - val_loss: 1.1038 - val_acc: 0.6286\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5191 - acc: 0.7913 - val_loss: 1.1020 - val_acc: 0.6286\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5343 - acc: 0.7862 - val_loss: 1.1069 - val_acc: 0.6457\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5304 - acc: 0.7894 - val_loss: 1.1162 - val_acc: 0.6400\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5226 - acc: 0.7830 - val_loss: 1.1069 - val_acc: 0.6343\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.5098 - acc: 0.7881 - val_loss: 1.1064 - val_acc: 0.6286\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5047 - acc: 0.8047 - val_loss: 1.1222 - val_acc: 0.6229\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5100 - acc: 0.7888 - val_loss: 1.1282 - val_acc: 0.6343\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5105 - acc: 0.8009 - val_loss: 1.1326 - val_acc: 0.6343\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5471 - acc: 0.7741 - val_loss: 1.1182 - val_acc: 0.6343\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5060 - acc: 0.7996 - val_loss: 1.1329 - val_acc: 0.6229\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.5113 - acc: 0.7913 - val_loss: 1.1371 - val_acc: 0.6229\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5041 - acc: 0.8009 - val_loss: 1.1334 - val_acc: 0.6229\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4849 - acc: 0.8054 - val_loss: 1.1450 - val_acc: 0.6229\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4847 - acc: 0.8086 - val_loss: 1.1343 - val_acc: 0.6343\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5221 - acc: 0.7926 - val_loss: 1.1379 - val_acc: 0.6400\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5133 - acc: 0.7875 - val_loss: 1.1481 - val_acc: 0.6457\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5091 - acc: 0.8034 - val_loss: 1.1430 - val_acc: 0.6400\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.4925 - acc: 0.7996 - val_loss: 1.1447 - val_acc: 0.6343\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4947 - acc: 0.8041 - val_loss: 1.1440 - val_acc: 0.6400\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5065 - acc: 0.7945 - val_loss: 1.1577 - val_acc: 0.6286\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5121 - acc: 0.7996 - val_loss: 1.1459 - val_acc: 0.6343\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.4973 - acc: 0.8066 - val_loss: 1.1460 - val_acc: 0.6514\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.4938 - acc: 0.7932 - val_loss: 1.1443 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4817 - acc: 0.7996 - val_loss: 1.1584 - val_acc: 0.6457\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4723 - acc: 0.8086 - val_loss: 1.1565 - val_acc: 0.6286\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4820 - acc: 0.8079 - val_loss: 1.1516 - val_acc: 0.6343\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4783 - acc: 0.8066 - val_loss: 1.1650 - val_acc: 0.6343\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4910 - acc: 0.8130 - val_loss: 1.1773 - val_acc: 0.6286\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4726 - acc: 0.8162 - val_loss: 1.1722 - val_acc: 0.6343\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4865 - acc: 0.8149 - val_loss: 1.1672 - val_acc: 0.6400\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4788 - acc: 0.8168 - val_loss: 1.1636 - val_acc: 0.6343\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4675 - acc: 0.8137 - val_loss: 1.1672 - val_acc: 0.6286\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4727 - acc: 0.8207 - val_loss: 1.1774 - val_acc: 0.6171\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4756 - acc: 0.8117 - val_loss: 1.1855 - val_acc: 0.6229\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4769 - acc: 0.8060 - val_loss: 1.1873 - val_acc: 0.6229\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.4734 - acc: 0.8137 - val_loss: 1.1885 - val_acc: 0.6229\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4603 - acc: 0.8271 - val_loss: 1.1824 - val_acc: 0.6229\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4625 - acc: 0.8258 - val_loss: 1.1866 - val_acc: 0.6229\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4368 - acc: 0.8341 - val_loss: 1.1872 - val_acc: 0.6171\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4567 - acc: 0.8258 - val_loss: 1.1893 - val_acc: 0.6286\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.4596 - acc: 0.8245 - val_loss: 1.1955 - val_acc: 0.6343\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4787 - acc: 0.8137 - val_loss: 1.2054 - val_acc: 0.6457\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4675 - acc: 0.8156 - val_loss: 1.2091 - val_acc: 0.6343\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4568 - acc: 0.8175 - val_loss: 1.2064 - val_acc: 0.6343\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.4420 - acc: 0.8322 - val_loss: 1.2107 - val_acc: 0.6400\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4562 - acc: 0.8354 - val_loss: 1.2141 - val_acc: 0.6343\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4545 - acc: 0.8239 - val_loss: 1.2103 - val_acc: 0.6343\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4406 - acc: 0.8271 - val_loss: 1.1997 - val_acc: 0.6400\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4366 - acc: 0.8373 - val_loss: 1.2154 - val_acc: 0.6457\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4433 - acc: 0.8264 - val_loss: 1.2228 - val_acc: 0.6400\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.4279 - acc: 0.8424 - val_loss: 1.2230 - val_acc: 0.6343\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.4527 - acc: 0.8226 - val_loss: 1.2219 - val_acc: 0.6229\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4366 - acc: 0.8328 - val_loss: 1.2201 - val_acc: 0.6286\n",
      "193/193 [==============================] - 0s 57us/step\n",
      "1742/1742 [==============================] - 0s 38us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 9s 6ms/step - loss: 1.5238 - acc: 0.2987 - val_loss: 1.2281 - val_acc: 0.4629\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.2820 - acc: 0.3976 - val_loss: 1.0768 - val_acc: 0.6000\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 57us/step - loss: 1.1591 - acc: 0.4952 - val_loss: 1.0210 - val_acc: 0.6343\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.1234 - acc: 0.5144 - val_loss: 0.9817 - val_acc: 0.6629\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.0765 - acc: 0.5303 - val_loss: 0.9644 - val_acc: 0.6514\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0490 - acc: 0.5590 - val_loss: 0.9551 - val_acc: 0.6343\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0435 - acc: 0.5507 - val_loss: 0.9409 - val_acc: 0.6286\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0284 - acc: 0.5641 - val_loss: 0.9306 - val_acc: 0.6629\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9972 - acc: 0.5839 - val_loss: 0.9266 - val_acc: 0.6571\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.9928 - acc: 0.5852 - val_loss: 0.9291 - val_acc: 0.6457\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.9701 - acc: 0.5865 - val_loss: 0.9198 - val_acc: 0.6343\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9703 - acc: 0.5820 - val_loss: 0.9150 - val_acc: 0.6629\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9500 - acc: 0.5980 - val_loss: 0.9136 - val_acc: 0.6743\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9539 - acc: 0.5846 - val_loss: 0.9130 - val_acc: 0.6571\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9459 - acc: 0.6011 - val_loss: 0.9132 - val_acc: 0.6400\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9295 - acc: 0.6203 - val_loss: 0.9116 - val_acc: 0.6686\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9234 - acc: 0.5986 - val_loss: 0.9079 - val_acc: 0.6800\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9119 - acc: 0.6056 - val_loss: 0.9056 - val_acc: 0.6743\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9175 - acc: 0.6050 - val_loss: 0.9058 - val_acc: 0.6629\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8921 - acc: 0.6260 - val_loss: 0.9191 - val_acc: 0.6343\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9003 - acc: 0.6375 - val_loss: 0.9162 - val_acc: 0.6571\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8973 - acc: 0.6228 - val_loss: 0.9087 - val_acc: 0.6743\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8932 - acc: 0.6388 - val_loss: 0.9083 - val_acc: 0.6343\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8886 - acc: 0.6369 - val_loss: 0.9052 - val_acc: 0.6514\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8801 - acc: 0.6388 - val_loss: 0.9110 - val_acc: 0.6571\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8812 - acc: 0.6465 - val_loss: 0.9034 - val_acc: 0.6457\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8514 - acc: 0.6484 - val_loss: 0.9080 - val_acc: 0.6800\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8557 - acc: 0.6490 - val_loss: 0.9062 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8539 - acc: 0.6458 - val_loss: 0.9118 - val_acc: 0.6629\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8524 - acc: 0.6611 - val_loss: 0.9131 - val_acc: 0.6571\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.8719 - acc: 0.6471 - val_loss: 0.9143 - val_acc: 0.6400\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8563 - acc: 0.6605 - val_loss: 0.9162 - val_acc: 0.6457\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8585 - acc: 0.6445 - val_loss: 0.9172 - val_acc: 0.6400\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8511 - acc: 0.6548 - val_loss: 0.9207 - val_acc: 0.6514\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8246 - acc: 0.6611 - val_loss: 0.9105 - val_acc: 0.6514\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8206 - acc: 0.6688 - val_loss: 0.9114 - val_acc: 0.6571\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7996 - acc: 0.6669 - val_loss: 0.9176 - val_acc: 0.6457\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7990 - acc: 0.6822 - val_loss: 0.9117 - val_acc: 0.6400\n",
      "Epoch 39/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7940 - acc: 0.6777 - val_loss: 0.9158 - val_acc: 0.6457\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8013 - acc: 0.6796 - val_loss: 0.9204 - val_acc: 0.6571\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.8210 - acc: 0.6484 - val_loss: 0.9121 - val_acc: 0.6514\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8045 - acc: 0.6726 - val_loss: 0.9179 - val_acc: 0.6571\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7911 - acc: 0.6790 - val_loss: 0.9157 - val_acc: 0.6457\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7911 - acc: 0.6707 - val_loss: 0.9220 - val_acc: 0.6343\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.7909 - acc: 0.6733 - val_loss: 0.9287 - val_acc: 0.6514\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7873 - acc: 0.6765 - val_loss: 0.9231 - val_acc: 0.6400\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7798 - acc: 0.6777 - val_loss: 0.9253 - val_acc: 0.6400\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7890 - acc: 0.6854 - val_loss: 0.9238 - val_acc: 0.6457\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7989 - acc: 0.6739 - val_loss: 0.9319 - val_acc: 0.6343\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.7686 - acc: 0.6905 - val_loss: 0.9420 - val_acc: 0.6400\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7813 - acc: 0.6816 - val_loss: 0.9359 - val_acc: 0.6571\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7685 - acc: 0.6873 - val_loss: 0.9360 - val_acc: 0.6514\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7683 - acc: 0.6950 - val_loss: 0.9315 - val_acc: 0.6400\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7679 - acc: 0.6886 - val_loss: 0.9309 - val_acc: 0.6457\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7367 - acc: 0.7020 - val_loss: 0.9353 - val_acc: 0.6343\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7622 - acc: 0.6899 - val_loss: 0.9473 - val_acc: 0.6229\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.7564 - acc: 0.6867 - val_loss: 0.9540 - val_acc: 0.6400\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7383 - acc: 0.7103 - val_loss: 0.9559 - val_acc: 0.6400\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7368 - acc: 0.7039 - val_loss: 0.9570 - val_acc: 0.6400\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7500 - acc: 0.7001 - val_loss: 0.9548 - val_acc: 0.6286\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7453 - acc: 0.7128 - val_loss: 0.9589 - val_acc: 0.6343\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7262 - acc: 0.7090 - val_loss: 0.9515 - val_acc: 0.6229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7305 - acc: 0.7045 - val_loss: 0.9564 - val_acc: 0.6343\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7298 - acc: 0.7045 - val_loss: 0.9644 - val_acc: 0.6229\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7335 - acc: 0.7058 - val_loss: 0.9698 - val_acc: 0.6229\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7137 - acc: 0.7026 - val_loss: 0.9647 - val_acc: 0.6229\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6962 - acc: 0.7281 - val_loss: 0.9587 - val_acc: 0.6229\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.7065 - acc: 0.7250 - val_loss: 0.9531 - val_acc: 0.6286\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7068 - acc: 0.7179 - val_loss: 0.9637 - val_acc: 0.6171\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7149 - acc: 0.7147 - val_loss: 0.9618 - val_acc: 0.6286\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6968 - acc: 0.7173 - val_loss: 0.9654 - val_acc: 0.6286\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6934 - acc: 0.7218 - val_loss: 0.9708 - val_acc: 0.6286\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6948 - acc: 0.7230 - val_loss: 0.9682 - val_acc: 0.6229\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6980 - acc: 0.7281 - val_loss: 0.9653 - val_acc: 0.6400\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6877 - acc: 0.7262 - val_loss: 0.9756 - val_acc: 0.6343\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6854 - acc: 0.7198 - val_loss: 0.9766 - val_acc: 0.6400\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6796 - acc: 0.7390 - val_loss: 0.9918 - val_acc: 0.6343\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6868 - acc: 0.7237 - val_loss: 0.9981 - val_acc: 0.6286\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6762 - acc: 0.7230 - val_loss: 0.9957 - val_acc: 0.6229\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6667 - acc: 0.7332 - val_loss: 0.9939 - val_acc: 0.6286\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6771 - acc: 0.7332 - val_loss: 0.9991 - val_acc: 0.6229\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.6958 - acc: 0.7294 - val_loss: 0.9958 - val_acc: 0.6343\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.6695 - acc: 0.7435 - val_loss: 0.9945 - val_acc: 0.6229\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6723 - acc: 0.7256 - val_loss: 0.9946 - val_acc: 0.6171\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6857 - acc: 0.7294 - val_loss: 0.9877 - val_acc: 0.6000\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.6476 - acc: 0.7377 - val_loss: 0.9958 - val_acc: 0.6057\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6557 - acc: 0.7473 - val_loss: 0.9968 - val_acc: 0.6229\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6715 - acc: 0.7332 - val_loss: 1.0057 - val_acc: 0.6057\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6746 - acc: 0.7320 - val_loss: 1.0139 - val_acc: 0.6171\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6681 - acc: 0.7415 - val_loss: 1.0085 - val_acc: 0.6114\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6541 - acc: 0.7537 - val_loss: 1.0045 - val_acc: 0.6229\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6562 - acc: 0.7441 - val_loss: 0.9997 - val_acc: 0.6400\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6578 - acc: 0.7409 - val_loss: 1.0084 - val_acc: 0.6286\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6562 - acc: 0.7332 - val_loss: 1.0013 - val_acc: 0.6229\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6441 - acc: 0.7479 - val_loss: 1.0035 - val_acc: 0.6286\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6411 - acc: 0.7364 - val_loss: 1.0013 - val_acc: 0.6286\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6362 - acc: 0.7562 - val_loss: 1.0118 - val_acc: 0.6229\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6242 - acc: 0.7518 - val_loss: 1.0171 - val_acc: 0.6229\n",
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6384 - acc: 0.7632 - val_loss: 1.0131 - val_acc: 0.6457\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6566 - acc: 0.7466 - val_loss: 1.0174 - val_acc: 0.6286\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6378 - acc: 0.7530 - val_loss: 1.0256 - val_acc: 0.6343\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6269 - acc: 0.7460 - val_loss: 1.0253 - val_acc: 0.6286\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6391 - acc: 0.7601 - val_loss: 1.0199 - val_acc: 0.6114\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6124 - acc: 0.7658 - val_loss: 1.0289 - val_acc: 0.6114\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 77us/step - loss: 0.6061 - acc: 0.7690 - val_loss: 1.0263 - val_acc: 0.6171\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6109 - acc: 0.7594 - val_loss: 1.0351 - val_acc: 0.6229\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6253 - acc: 0.7454 - val_loss: 1.0280 - val_acc: 0.6171\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6171 - acc: 0.7562 - val_loss: 1.0304 - val_acc: 0.6171\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5935 - acc: 0.7683 - val_loss: 1.0323 - val_acc: 0.6171\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6137 - acc: 0.7594 - val_loss: 1.0309 - val_acc: 0.6229\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5993 - acc: 0.7613 - val_loss: 1.0339 - val_acc: 0.6171\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.5943 - acc: 0.7652 - val_loss: 1.0475 - val_acc: 0.6171\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6201 - acc: 0.7696 - val_loss: 1.0451 - val_acc: 0.6171\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5923 - acc: 0.7760 - val_loss: 1.0510 - val_acc: 0.6286\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6162 - acc: 0.7569 - val_loss: 1.0496 - val_acc: 0.6286\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6152 - acc: 0.7632 - val_loss: 1.0456 - val_acc: 0.6343\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6014 - acc: 0.7639 - val_loss: 1.0487 - val_acc: 0.6343\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.5651 - acc: 0.7805 - val_loss: 1.0490 - val_acc: 0.6286\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5745 - acc: 0.7786 - val_loss: 1.0477 - val_acc: 0.6286\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.5734 - acc: 0.7754 - val_loss: 1.0503 - val_acc: 0.6229\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5878 - acc: 0.7817 - val_loss: 1.0612 - val_acc: 0.6229\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5690 - acc: 0.7932 - val_loss: 1.0538 - val_acc: 0.6286\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5950 - acc: 0.7696 - val_loss: 1.0565 - val_acc: 0.6229\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5818 - acc: 0.7715 - val_loss: 1.0714 - val_acc: 0.6057\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5900 - acc: 0.7773 - val_loss: 1.0637 - val_acc: 0.6171\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5821 - acc: 0.7849 - val_loss: 1.0593 - val_acc: 0.6171\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5888 - acc: 0.7690 - val_loss: 1.0725 - val_acc: 0.6114\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5707 - acc: 0.7728 - val_loss: 1.0742 - val_acc: 0.6171\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.5514 - acc: 0.7824 - val_loss: 1.0754 - val_acc: 0.6114\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5716 - acc: 0.7837 - val_loss: 1.0773 - val_acc: 0.6057\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5566 - acc: 0.7875 - val_loss: 1.0868 - val_acc: 0.6114\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 66us/step - loss: 0.5779 - acc: 0.7805 - val_loss: 1.0710 - val_acc: 0.6229\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5755 - acc: 0.7779 - val_loss: 1.0626 - val_acc: 0.6229\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5520 - acc: 0.7856 - val_loss: 1.0715 - val_acc: 0.6171\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.5599 - acc: 0.7888 - val_loss: 1.0781 - val_acc: 0.6114\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5527 - acc: 0.7913 - val_loss: 1.0775 - val_acc: 0.6171\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5562 - acc: 0.7741 - val_loss: 1.0739 - val_acc: 0.6229\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5393 - acc: 0.7964 - val_loss: 1.0784 - val_acc: 0.6171\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5516 - acc: 0.7817 - val_loss: 1.0899 - val_acc: 0.6229\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5500 - acc: 0.7849 - val_loss: 1.0926 - val_acc: 0.6171\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 68us/step - loss: 0.5520 - acc: 0.7971 - val_loss: 1.0908 - val_acc: 0.6171\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5425 - acc: 0.7939 - val_loss: 1.0856 - val_acc: 0.6171\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5281 - acc: 0.7996 - val_loss: 1.0885 - val_acc: 0.6229\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5486 - acc: 0.7805 - val_loss: 1.0978 - val_acc: 0.6286\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5496 - acc: 0.7881 - val_loss: 1.0976 - val_acc: 0.6229\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5666 - acc: 0.7830 - val_loss: 1.1088 - val_acc: 0.6229\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5498 - acc: 0.7977 - val_loss: 1.1110 - val_acc: 0.6286\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5295 - acc: 0.7900 - val_loss: 1.1136 - val_acc: 0.6457\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5323 - acc: 0.8022 - val_loss: 1.1200 - val_acc: 0.6400\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.5271 - acc: 0.8028 - val_loss: 1.1202 - val_acc: 0.6343\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5321 - acc: 0.7881 - val_loss: 1.1203 - val_acc: 0.6400\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4989 - acc: 0.7990 - val_loss: 1.1227 - val_acc: 0.6343\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5101 - acc: 0.8117 - val_loss: 1.1230 - val_acc: 0.6343\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5337 - acc: 0.7888 - val_loss: 1.1181 - val_acc: 0.6286\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5119 - acc: 0.8060 - val_loss: 1.1128 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5316 - acc: 0.7964 - val_loss: 1.1101 - val_acc: 0.6229\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5063 - acc: 0.7964 - val_loss: 1.1148 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.5064 - acc: 0.7932 - val_loss: 1.1230 - val_acc: 0.6171\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5192 - acc: 0.7971 - val_loss: 1.1284 - val_acc: 0.6457\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.4992 - acc: 0.8149 - val_loss: 1.1323 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5055 - acc: 0.7990 - val_loss: 1.1316 - val_acc: 0.6457\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5263 - acc: 0.8066 - val_loss: 1.1370 - val_acc: 0.6457\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5092 - acc: 0.8130 - val_loss: 1.1364 - val_acc: 0.6514\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5259 - acc: 0.8009 - val_loss: 1.1444 - val_acc: 0.6343\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4948 - acc: 0.8098 - val_loss: 1.1465 - val_acc: 0.6229\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4932 - acc: 0.8175 - val_loss: 1.1500 - val_acc: 0.6229\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5021 - acc: 0.8034 - val_loss: 1.1550 - val_acc: 0.6343\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5222 - acc: 0.8003 - val_loss: 1.1557 - val_acc: 0.6343\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5127 - acc: 0.8086 - val_loss: 1.1522 - val_acc: 0.6457\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4835 - acc: 0.8156 - val_loss: 1.1622 - val_acc: 0.6457\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4989 - acc: 0.8117 - val_loss: 1.1675 - val_acc: 0.6286\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4999 - acc: 0.8054 - val_loss: 1.1694 - val_acc: 0.6286\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5074 - acc: 0.8054 - val_loss: 1.1729 - val_acc: 0.6286\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5021 - acc: 0.8092 - val_loss: 1.1789 - val_acc: 0.6286\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5144 - acc: 0.7977 - val_loss: 1.1735 - val_acc: 0.6571\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4840 - acc: 0.8124 - val_loss: 1.1748 - val_acc: 0.6286\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4816 - acc: 0.8168 - val_loss: 1.1723 - val_acc: 0.6400\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.4877 - acc: 0.8098 - val_loss: 1.1719 - val_acc: 0.6400\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4735 - acc: 0.8168 - val_loss: 1.1752 - val_acc: 0.6286\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.4932 - acc: 0.8041 - val_loss: 1.1792 - val_acc: 0.6286\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4573 - acc: 0.8392 - val_loss: 1.1776 - val_acc: 0.6229\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4843 - acc: 0.8117 - val_loss: 1.1942 - val_acc: 0.6229\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4804 - acc: 0.8188 - val_loss: 1.1952 - val_acc: 0.6343\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4728 - acc: 0.8309 - val_loss: 1.1881 - val_acc: 0.6457\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.4713 - acc: 0.8207 - val_loss: 1.1952 - val_acc: 0.6514\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4757 - acc: 0.8137 - val_loss: 1.1948 - val_acc: 0.6343\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4800 - acc: 0.8149 - val_loss: 1.1857 - val_acc: 0.6286\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4585 - acc: 0.8322 - val_loss: 1.1887 - val_acc: 0.6514\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.4704 - acc: 0.8328 - val_loss: 1.2017 - val_acc: 0.6286\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4636 - acc: 0.8283 - val_loss: 1.2059 - val_acc: 0.6171\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4538 - acc: 0.8347 - val_loss: 1.2067 - val_acc: 0.6343\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4281 - acc: 0.8322 - val_loss: 1.2166 - val_acc: 0.6286\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.4500 - acc: 0.8360 - val_loss: 1.2118 - val_acc: 0.6286\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4555 - acc: 0.8283 - val_loss: 1.2016 - val_acc: 0.6343\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4631 - acc: 0.8277 - val_loss: 1.2055 - val_acc: 0.6400\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 75us/step - loss: 0.4580 - acc: 0.8245 - val_loss: 1.2137 - val_acc: 0.6343\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.4684 - acc: 0.8239 - val_loss: 1.2174 - val_acc: 0.6571\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 70us/step - loss: 0.4500 - acc: 0.8296 - val_loss: 1.2214 - val_acc: 0.6457\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 87us/step - loss: 0.4570 - acc: 0.8302 - val_loss: 1.2261 - val_acc: 0.6457\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 67us/step - loss: 0.4777 - acc: 0.8239 - val_loss: 1.2323 - val_acc: 0.6457\n",
      "193/193 [==============================] - 0s 79us/step\n",
      "1742/1742 [==============================] - 0s 41us/step\n",
      "Train on 1567 samples, validate on 175 samples\n",
      "Epoch 1/200\n",
      "1567/1567 [==============================] - 10s 6ms/step - loss: 1.5187 - acc: 0.2706 - val_loss: 1.2204 - val_acc: 0.4343\n",
      "Epoch 2/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 1.2694 - acc: 0.4365 - val_loss: 1.1178 - val_acc: 0.5086\n",
      "Epoch 3/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.1936 - acc: 0.4671 - val_loss: 1.0759 - val_acc: 0.5029\n",
      "Epoch 4/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 1.1461 - acc: 0.4856 - val_loss: 1.0424 - val_acc: 0.5429\n",
      "Epoch 5/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 1.1134 - acc: 0.5112 - val_loss: 1.0213 - val_acc: 0.5600\n",
      "Epoch 6/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 1.0771 - acc: 0.5335 - val_loss: 1.0108 - val_acc: 0.5486\n",
      "Epoch 7/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 1.0402 - acc: 0.5412 - val_loss: 1.0156 - val_acc: 0.5429\n",
      "Epoch 8/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 1.0506 - acc: 0.5463 - val_loss: 1.0015 - val_acc: 0.5657\n",
      "Epoch 9/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 1.0149 - acc: 0.5654 - val_loss: 0.9825 - val_acc: 0.5657\n",
      "Epoch 10/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.9822 - acc: 0.5750 - val_loss: 0.9810 - val_acc: 0.5486\n",
      "Epoch 11/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9930 - acc: 0.5641 - val_loss: 0.9881 - val_acc: 0.5771\n",
      "Epoch 12/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.9607 - acc: 0.5858 - val_loss: 0.9913 - val_acc: 0.5829\n",
      "Epoch 13/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.9440 - acc: 0.6075 - val_loss: 0.9820 - val_acc: 0.5771\n",
      "Epoch 14/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.9237 - acc: 0.6152 - val_loss: 0.9939 - val_acc: 0.5714\n",
      "Epoch 15/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9417 - acc: 0.6024 - val_loss: 0.9758 - val_acc: 0.5714\n",
      "Epoch 16/200\n",
      "1567/1567 [==============================] - 0s 76us/step - loss: 0.9350 - acc: 0.6158 - val_loss: 0.9767 - val_acc: 0.5829\n",
      "Epoch 17/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.9272 - acc: 0.6401 - val_loss: 0.9789 - val_acc: 0.5657\n",
      "Epoch 18/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.9231 - acc: 0.6094 - val_loss: 0.9743 - val_acc: 0.5600\n",
      "Epoch 19/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8998 - acc: 0.6350 - val_loss: 0.9757 - val_acc: 0.5714\n",
      "Epoch 20/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8886 - acc: 0.6311 - val_loss: 0.9702 - val_acc: 0.5771\n",
      "Epoch 21/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8813 - acc: 0.6343 - val_loss: 0.9719 - val_acc: 0.5714\n",
      "Epoch 22/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8999 - acc: 0.6165 - val_loss: 0.9689 - val_acc: 0.5943\n",
      "Epoch 23/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8904 - acc: 0.6267 - val_loss: 0.9689 - val_acc: 0.5714\n",
      "Epoch 24/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.8641 - acc: 0.6350 - val_loss: 0.9709 - val_acc: 0.5657\n",
      "Epoch 25/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.8736 - acc: 0.6375 - val_loss: 0.9743 - val_acc: 0.5829\n",
      "Epoch 26/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8722 - acc: 0.6362 - val_loss: 0.9872 - val_acc: 0.5829\n",
      "Epoch 27/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8669 - acc: 0.6407 - val_loss: 0.9819 - val_acc: 0.5829\n",
      "Epoch 28/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.8538 - acc: 0.6433 - val_loss: 0.9693 - val_acc: 0.5886\n",
      "Epoch 29/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.8494 - acc: 0.6394 - val_loss: 0.9773 - val_acc: 0.5771\n",
      "Epoch 30/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8418 - acc: 0.6477 - val_loss: 0.9800 - val_acc: 0.5829\n",
      "Epoch 31/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8344 - acc: 0.6445 - val_loss: 0.9840 - val_acc: 0.5886\n",
      "Epoch 32/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.8564 - acc: 0.6458 - val_loss: 0.9899 - val_acc: 0.5771\n",
      "Epoch 33/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8281 - acc: 0.6458 - val_loss: 0.9944 - val_acc: 0.6000\n",
      "Epoch 34/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8413 - acc: 0.6503 - val_loss: 0.9907 - val_acc: 0.5943\n",
      "Epoch 35/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.8260 - acc: 0.6688 - val_loss: 0.9919 - val_acc: 0.6000\n",
      "Epoch 36/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.8037 - acc: 0.6777 - val_loss: 0.9888 - val_acc: 0.5943\n",
      "Epoch 37/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.8057 - acc: 0.6548 - val_loss: 0.9846 - val_acc: 0.5829\n",
      "Epoch 38/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.8197 - acc: 0.6688 - val_loss: 0.9871 - val_acc: 0.5886\n",
      "Epoch 39/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7959 - acc: 0.6854 - val_loss: 0.9984 - val_acc: 0.5886\n",
      "Epoch 40/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.8189 - acc: 0.6643 - val_loss: 1.0010 - val_acc: 0.5943\n",
      "Epoch 41/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7956 - acc: 0.6720 - val_loss: 1.0134 - val_acc: 0.5886\n",
      "Epoch 42/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7991 - acc: 0.6624 - val_loss: 1.0107 - val_acc: 0.5829\n",
      "Epoch 43/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7854 - acc: 0.6688 - val_loss: 0.9964 - val_acc: 0.5943\n",
      "Epoch 44/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7954 - acc: 0.6739 - val_loss: 1.0081 - val_acc: 0.5771\n",
      "Epoch 45/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.7829 - acc: 0.6739 - val_loss: 1.0057 - val_acc: 0.5829\n",
      "Epoch 46/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7688 - acc: 0.6892 - val_loss: 1.0203 - val_acc: 0.5771\n",
      "Epoch 47/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7813 - acc: 0.6886 - val_loss: 1.0102 - val_acc: 0.5829\n",
      "Epoch 48/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7634 - acc: 0.6771 - val_loss: 1.0129 - val_acc: 0.5829\n",
      "Epoch 49/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7602 - acc: 0.7007 - val_loss: 1.0307 - val_acc: 0.5771\n",
      "Epoch 50/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.7660 - acc: 0.6962 - val_loss: 1.0281 - val_acc: 0.5714\n",
      "Epoch 51/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7505 - acc: 0.6771 - val_loss: 1.0151 - val_acc: 0.5886\n",
      "Epoch 52/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7707 - acc: 0.6943 - val_loss: 1.0134 - val_acc: 0.5771\n",
      "Epoch 53/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7398 - acc: 0.6816 - val_loss: 1.0137 - val_acc: 0.6000\n",
      "Epoch 54/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7688 - acc: 0.6879 - val_loss: 1.0131 - val_acc: 0.5886\n",
      "Epoch 55/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.7681 - acc: 0.6956 - val_loss: 1.0173 - val_acc: 0.5829\n",
      "Epoch 56/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7397 - acc: 0.7007 - val_loss: 1.0352 - val_acc: 0.5714\n",
      "Epoch 57/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.7441 - acc: 0.7039 - val_loss: 1.0177 - val_acc: 0.5829\n",
      "Epoch 58/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.7368 - acc: 0.6969 - val_loss: 1.0275 - val_acc: 0.5829\n",
      "Epoch 59/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7550 - acc: 0.6899 - val_loss: 1.0240 - val_acc: 0.5771\n",
      "Epoch 60/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7322 - acc: 0.7218 - val_loss: 1.0189 - val_acc: 0.6000\n",
      "Epoch 61/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7524 - acc: 0.7013 - val_loss: 1.0306 - val_acc: 0.6057\n",
      "Epoch 62/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.7580 - acc: 0.6905 - val_loss: 1.0268 - val_acc: 0.5943\n",
      "Epoch 63/200\n",
      "1567/1567 [==============================] - 0s 73us/step - loss: 0.7300 - acc: 0.7052 - val_loss: 1.0404 - val_acc: 0.5943\n",
      "Epoch 64/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.7212 - acc: 0.7058 - val_loss: 1.0208 - val_acc: 0.6057\n",
      "Epoch 65/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.7409 - acc: 0.7007 - val_loss: 1.0363 - val_acc: 0.6000\n",
      "Epoch 66/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.7186 - acc: 0.7096 - val_loss: 1.0480 - val_acc: 0.5771\n",
      "Epoch 67/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.7160 - acc: 0.7026 - val_loss: 1.0373 - val_acc: 0.5943\n",
      "Epoch 68/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.7099 - acc: 0.7077 - val_loss: 1.0318 - val_acc: 0.6000\n",
      "Epoch 69/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6944 - acc: 0.7396 - val_loss: 1.0370 - val_acc: 0.6000\n",
      "Epoch 70/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.6971 - acc: 0.7192 - val_loss: 1.0495 - val_acc: 0.5943\n",
      "Epoch 71/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.7064 - acc: 0.7090 - val_loss: 1.0455 - val_acc: 0.5943\n",
      "Epoch 72/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.7097 - acc: 0.6924 - val_loss: 1.0395 - val_acc: 0.6057\n",
      "Epoch 73/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6898 - acc: 0.7224 - val_loss: 1.0470 - val_acc: 0.6000\n",
      "Epoch 74/200\n",
      "1567/1567 [==============================] - 0s 69us/step - loss: 0.6854 - acc: 0.7218 - val_loss: 1.0402 - val_acc: 0.6000\n",
      "Epoch 75/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6909 - acc: 0.7147 - val_loss: 1.0451 - val_acc: 0.5943\n",
      "Epoch 76/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6896 - acc: 0.7205 - val_loss: 1.0452 - val_acc: 0.6000\n",
      "Epoch 77/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6736 - acc: 0.7301 - val_loss: 1.0374 - val_acc: 0.6000\n",
      "Epoch 78/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6759 - acc: 0.7390 - val_loss: 1.0593 - val_acc: 0.5886\n",
      "Epoch 79/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6992 - acc: 0.7301 - val_loss: 1.0612 - val_acc: 0.5886\n",
      "Epoch 80/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6747 - acc: 0.7288 - val_loss: 1.0578 - val_acc: 0.5771\n",
      "Epoch 81/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6805 - acc: 0.7358 - val_loss: 1.0569 - val_acc: 0.5829\n",
      "Epoch 82/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6867 - acc: 0.7275 - val_loss: 1.0620 - val_acc: 0.5829\n",
      "Epoch 83/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6648 - acc: 0.7301 - val_loss: 1.0697 - val_acc: 0.5886\n",
      "Epoch 84/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6632 - acc: 0.7339 - val_loss: 1.0770 - val_acc: 0.5771\n",
      "Epoch 85/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6589 - acc: 0.7288 - val_loss: 1.0649 - val_acc: 0.5829\n",
      "Epoch 86/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6585 - acc: 0.7313 - val_loss: 1.0726 - val_acc: 0.5829\n",
      "Epoch 87/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6512 - acc: 0.7473 - val_loss: 1.0675 - val_acc: 0.5886\n",
      "Epoch 88/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6753 - acc: 0.7205 - val_loss: 1.0873 - val_acc: 0.5657\n",
      "Epoch 89/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.6566 - acc: 0.7358 - val_loss: 1.0968 - val_acc: 0.5829\n",
      "Epoch 90/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6390 - acc: 0.7403 - val_loss: 1.0852 - val_acc: 0.5771\n",
      "Epoch 91/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6477 - acc: 0.7473 - val_loss: 1.0929 - val_acc: 0.5771\n",
      "Epoch 92/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.6399 - acc: 0.7575 - val_loss: 1.0989 - val_acc: 0.5829\n",
      "Epoch 93/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6437 - acc: 0.7428 - val_loss: 1.0859 - val_acc: 0.5943\n",
      "Epoch 94/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.6451 - acc: 0.7409 - val_loss: 1.0813 - val_acc: 0.6000\n",
      "Epoch 95/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6404 - acc: 0.7390 - val_loss: 1.0740 - val_acc: 0.6000\n",
      "Epoch 96/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6545 - acc: 0.7243 - val_loss: 1.0707 - val_acc: 0.6000\n",
      "Epoch 97/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6279 - acc: 0.7594 - val_loss: 1.0824 - val_acc: 0.6000\n",
      "Epoch 98/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.6316 - acc: 0.7607 - val_loss: 1.1085 - val_acc: 0.5943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.6219 - acc: 0.7537 - val_loss: 1.1080 - val_acc: 0.5771\n",
      "Epoch 100/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6532 - acc: 0.7562 - val_loss: 1.1104 - val_acc: 0.5943\n",
      "Epoch 101/200\n",
      "1567/1567 [==============================] - 0s 71us/step - loss: 0.6006 - acc: 0.7556 - val_loss: 1.1043 - val_acc: 0.6000\n",
      "Epoch 102/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6603 - acc: 0.7320 - val_loss: 1.1066 - val_acc: 0.5943\n",
      "Epoch 103/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.6293 - acc: 0.7409 - val_loss: 1.1121 - val_acc: 0.5829\n",
      "Epoch 104/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6116 - acc: 0.7588 - val_loss: 1.1155 - val_acc: 0.5943\n",
      "Epoch 105/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.6034 - acc: 0.7664 - val_loss: 1.1141 - val_acc: 0.6057\n",
      "Epoch 106/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.6081 - acc: 0.7518 - val_loss: 1.1149 - val_acc: 0.5886\n",
      "Epoch 107/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6017 - acc: 0.7632 - val_loss: 1.1203 - val_acc: 0.5829\n",
      "Epoch 108/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.6005 - acc: 0.7543 - val_loss: 1.1075 - val_acc: 0.5829\n",
      "Epoch 109/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5920 - acc: 0.7671 - val_loss: 1.1211 - val_acc: 0.5714\n",
      "Epoch 110/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.6030 - acc: 0.7671 - val_loss: 1.1069 - val_acc: 0.5829\n",
      "Epoch 111/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.6111 - acc: 0.7715 - val_loss: 1.1046 - val_acc: 0.5943\n",
      "Epoch 112/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5894 - acc: 0.7715 - val_loss: 1.1144 - val_acc: 0.5886\n",
      "Epoch 113/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.5957 - acc: 0.7652 - val_loss: 1.1255 - val_acc: 0.6000\n",
      "Epoch 114/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5840 - acc: 0.7664 - val_loss: 1.1251 - val_acc: 0.5829\n",
      "Epoch 115/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5983 - acc: 0.7594 - val_loss: 1.1249 - val_acc: 0.5771\n",
      "Epoch 116/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5953 - acc: 0.7715 - val_loss: 1.1142 - val_acc: 0.5771\n",
      "Epoch 117/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5872 - acc: 0.7677 - val_loss: 1.1226 - val_acc: 0.5771\n",
      "Epoch 118/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.6103 - acc: 0.7575 - val_loss: 1.1180 - val_acc: 0.5714\n",
      "Epoch 119/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5801 - acc: 0.7817 - val_loss: 1.1182 - val_acc: 0.5543\n",
      "Epoch 120/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5739 - acc: 0.7709 - val_loss: 1.1102 - val_acc: 0.5543\n",
      "Epoch 121/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5851 - acc: 0.7811 - val_loss: 1.1301 - val_acc: 0.5657\n",
      "Epoch 122/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.6040 - acc: 0.7594 - val_loss: 1.1385 - val_acc: 0.5714\n",
      "Epoch 123/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5816 - acc: 0.7715 - val_loss: 1.1288 - val_acc: 0.5771\n",
      "Epoch 124/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5619 - acc: 0.7754 - val_loss: 1.1391 - val_acc: 0.5886\n",
      "Epoch 125/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5803 - acc: 0.7671 - val_loss: 1.1466 - val_acc: 0.5829\n",
      "Epoch 126/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5814 - acc: 0.7703 - val_loss: 1.1470 - val_acc: 0.5829\n",
      "Epoch 127/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5539 - acc: 0.7792 - val_loss: 1.1407 - val_acc: 0.5829\n",
      "Epoch 128/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5353 - acc: 0.7958 - val_loss: 1.1413 - val_acc: 0.5771\n",
      "Epoch 129/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5679 - acc: 0.7779 - val_loss: 1.1574 - val_acc: 0.5829\n",
      "Epoch 130/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5654 - acc: 0.7837 - val_loss: 1.1485 - val_acc: 0.5714\n",
      "Epoch 131/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.5504 - acc: 0.7913 - val_loss: 1.1555 - val_acc: 0.5886\n",
      "Epoch 132/200\n",
      "1567/1567 [==============================] - 0s 48us/step - loss: 0.5489 - acc: 0.7849 - val_loss: 1.1531 - val_acc: 0.5771\n",
      "Epoch 133/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.5615 - acc: 0.7830 - val_loss: 1.1532 - val_acc: 0.5829\n",
      "Epoch 134/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.5725 - acc: 0.7824 - val_loss: 1.1638 - val_acc: 0.5886\n",
      "Epoch 135/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5616 - acc: 0.7735 - val_loss: 1.1617 - val_acc: 0.5829\n",
      "Epoch 136/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5667 - acc: 0.7849 - val_loss: 1.1627 - val_acc: 0.5714\n",
      "Epoch 137/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5599 - acc: 0.7888 - val_loss: 1.1716 - val_acc: 0.5714\n",
      "Epoch 138/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5524 - acc: 0.7862 - val_loss: 1.1647 - val_acc: 0.5714\n",
      "Epoch 139/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5494 - acc: 0.7869 - val_loss: 1.1549 - val_acc: 0.5771\n",
      "Epoch 140/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5377 - acc: 0.8015 - val_loss: 1.1530 - val_acc: 0.5657\n",
      "Epoch 141/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.5238 - acc: 0.7920 - val_loss: 1.1505 - val_acc: 0.5714\n",
      "Epoch 142/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5501 - acc: 0.7843 - val_loss: 1.1622 - val_acc: 0.5771\n",
      "Epoch 143/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5532 - acc: 0.7907 - val_loss: 1.1662 - val_acc: 0.5714\n",
      "Epoch 144/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5693 - acc: 0.7869 - val_loss: 1.1573 - val_acc: 0.5714\n",
      "Epoch 145/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5518 - acc: 0.7830 - val_loss: 1.1704 - val_acc: 0.5657\n",
      "Epoch 146/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5253 - acc: 0.8015 - val_loss: 1.1652 - val_acc: 0.5600\n",
      "Epoch 147/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5127 - acc: 0.7958 - val_loss: 1.1647 - val_acc: 0.5771\n",
      "Epoch 148/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5178 - acc: 0.7996 - val_loss: 1.1681 - val_acc: 0.5771\n",
      "Epoch 149/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5489 - acc: 0.7811 - val_loss: 1.1718 - val_acc: 0.5829\n",
      "Epoch 150/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5152 - acc: 0.8028 - val_loss: 1.1736 - val_acc: 0.5771\n",
      "Epoch 151/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5107 - acc: 0.8034 - val_loss: 1.1729 - val_acc: 0.5886\n",
      "Epoch 152/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5384 - acc: 0.7971 - val_loss: 1.1684 - val_acc: 0.5829\n",
      "Epoch 153/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5127 - acc: 0.8066 - val_loss: 1.1735 - val_acc: 0.5829\n",
      "Epoch 154/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5260 - acc: 0.7971 - val_loss: 1.1861 - val_acc: 0.5771\n",
      "Epoch 155/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5388 - acc: 0.7875 - val_loss: 1.1937 - val_acc: 0.5657\n",
      "Epoch 156/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.5202 - acc: 0.8034 - val_loss: 1.1896 - val_acc: 0.5714\n",
      "Epoch 157/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.5329 - acc: 0.8054 - val_loss: 1.1745 - val_acc: 0.5829\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.5271 - acc: 0.8079 - val_loss: 1.1928 - val_acc: 0.5714\n",
      "Epoch 159/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5273 - acc: 0.7926 - val_loss: 1.1961 - val_acc: 0.5771\n",
      "Epoch 160/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4988 - acc: 0.8117 - val_loss: 1.2028 - val_acc: 0.5714\n",
      "Epoch 161/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4911 - acc: 0.8086 - val_loss: 1.2023 - val_acc: 0.5714\n",
      "Epoch 162/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.5240 - acc: 0.8066 - val_loss: 1.2087 - val_acc: 0.5714\n",
      "Epoch 163/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4839 - acc: 0.8156 - val_loss: 1.2011 - val_acc: 0.5657\n",
      "Epoch 164/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.5049 - acc: 0.8022 - val_loss: 1.2150 - val_acc: 0.5543\n",
      "Epoch 165/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4935 - acc: 0.8181 - val_loss: 1.2190 - val_acc: 0.5714\n",
      "Epoch 166/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.5012 - acc: 0.8003 - val_loss: 1.2201 - val_acc: 0.5714\n",
      "Epoch 167/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.5097 - acc: 0.8117 - val_loss: 1.1993 - val_acc: 0.5771\n",
      "Epoch 168/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4861 - acc: 0.8105 - val_loss: 1.2003 - val_acc: 0.5714\n",
      "Epoch 169/200\n",
      "1567/1567 [==============================] - 0s 56us/step - loss: 0.4838 - acc: 0.8194 - val_loss: 1.2033 - val_acc: 0.5829\n",
      "Epoch 170/200\n",
      "1567/1567 [==============================] - 0s 58us/step - loss: 0.4660 - acc: 0.8264 - val_loss: 1.2146 - val_acc: 0.5714\n",
      "Epoch 171/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4948 - acc: 0.8028 - val_loss: 1.1995 - val_acc: 0.5657\n",
      "Epoch 172/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.5058 - acc: 0.8130 - val_loss: 1.2020 - val_acc: 0.5543\n",
      "Epoch 173/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4967 - acc: 0.8117 - val_loss: 1.2178 - val_acc: 0.5600\n",
      "Epoch 174/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4893 - acc: 0.8175 - val_loss: 1.2137 - val_acc: 0.5600\n",
      "Epoch 175/200\n",
      "1567/1567 [==============================] - 0s 55us/step - loss: 0.4789 - acc: 0.8162 - val_loss: 1.1990 - val_acc: 0.5657\n",
      "Epoch 176/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4475 - acc: 0.8283 - val_loss: 1.2060 - val_acc: 0.5657\n",
      "Epoch 177/200\n",
      "1567/1567 [==============================] - 0s 61us/step - loss: 0.4754 - acc: 0.8264 - val_loss: 1.2162 - val_acc: 0.5714\n",
      "Epoch 178/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4871 - acc: 0.8047 - val_loss: 1.2375 - val_acc: 0.5657\n",
      "Epoch 179/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4636 - acc: 0.8194 - val_loss: 1.2354 - val_acc: 0.5714\n",
      "Epoch 180/200\n",
      "1567/1567 [==============================] - 0s 49us/step - loss: 0.4861 - acc: 0.8086 - val_loss: 1.2331 - val_acc: 0.5600\n",
      "Epoch 181/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4670 - acc: 0.8226 - val_loss: 1.2335 - val_acc: 0.5600\n",
      "Epoch 182/200\n",
      "1567/1567 [==============================] - 0s 64us/step - loss: 0.4520 - acc: 0.8309 - val_loss: 1.2409 - val_acc: 0.5600\n",
      "Epoch 183/200\n",
      "1567/1567 [==============================] - 0s 50us/step - loss: 0.4480 - acc: 0.8347 - val_loss: 1.2390 - val_acc: 0.5714\n",
      "Epoch 184/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4555 - acc: 0.8226 - val_loss: 1.2506 - val_acc: 0.5657\n",
      "Epoch 185/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4780 - acc: 0.8200 - val_loss: 1.2432 - val_acc: 0.5714\n",
      "Epoch 186/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4579 - acc: 0.8245 - val_loss: 1.2587 - val_acc: 0.5771\n",
      "Epoch 187/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4686 - acc: 0.8200 - val_loss: 1.2610 - val_acc: 0.5486\n",
      "Epoch 188/200\n",
      "1567/1567 [==============================] - 0s 51us/step - loss: 0.4692 - acc: 0.8188 - val_loss: 1.2488 - val_acc: 0.5771\n",
      "Epoch 189/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.4520 - acc: 0.8322 - val_loss: 1.2528 - val_acc: 0.5714\n",
      "Epoch 190/200\n",
      "1567/1567 [==============================] - 0s 57us/step - loss: 0.4802 - acc: 0.8092 - val_loss: 1.2593 - val_acc: 0.5600\n",
      "Epoch 191/200\n",
      "1567/1567 [==============================] - 0s 52us/step - loss: 0.4654 - acc: 0.8207 - val_loss: 1.2664 - val_acc: 0.5600\n",
      "Epoch 192/200\n",
      "1567/1567 [==============================] - 0s 53us/step - loss: 0.4591 - acc: 0.8264 - val_loss: 1.2594 - val_acc: 0.5771\n",
      "Epoch 193/200\n",
      "1567/1567 [==============================] - 0s 54us/step - loss: 0.4474 - acc: 0.8251 - val_loss: 1.2509 - val_acc: 0.5600\n",
      "Epoch 194/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.4504 - acc: 0.8290 - val_loss: 1.2736 - val_acc: 0.5429\n",
      "Epoch 195/200\n",
      "1567/1567 [==============================] - 0s 72us/step - loss: 0.4448 - acc: 0.8341 - val_loss: 1.2826 - val_acc: 0.5657\n",
      "Epoch 196/200\n",
      "1567/1567 [==============================] - 0s 59us/step - loss: 0.4402 - acc: 0.8309 - val_loss: 1.2678 - val_acc: 0.5771\n",
      "Epoch 197/200\n",
      "1567/1567 [==============================] - 0s 65us/step - loss: 0.4056 - acc: 0.8437 - val_loss: 1.2679 - val_acc: 0.5657\n",
      "Epoch 198/200\n",
      "1567/1567 [==============================] - 0s 62us/step - loss: 0.4431 - acc: 0.8379 - val_loss: 1.2708 - val_acc: 0.5657\n",
      "Epoch 199/200\n",
      "1567/1567 [==============================] - 0s 63us/step - loss: 0.4610 - acc: 0.8220 - val_loss: 1.2724 - val_acc: 0.5829\n",
      "Epoch 200/200\n",
      "1567/1567 [==============================] - 0s 60us/step - loss: 0.4489 - acc: 0.8239 - val_loss: 1.2685 - val_acc: 0.5714\n",
      "193/193 [==============================] - 0s 52us/step\n",
      "1742/1742 [==============================] - 0s 41us/step\n",
      "Train on 1741 samples, validate on 194 samples\n",
      "Epoch 1/100\n",
      "1741/1741 [==============================] - 11s 6ms/step - loss: 1.4654 - acc: 0.3004 - val_loss: 1.1673 - val_acc: 0.5464\n",
      "Epoch 2/100\n",
      "1741/1741 [==============================] - 0s 97us/step - loss: 1.2656 - acc: 0.4308 - val_loss: 1.0644 - val_acc: 0.5928\n",
      "Epoch 3/100\n",
      "1741/1741 [==============================] - 0s 102us/step - loss: 1.1973 - acc: 0.4658 - val_loss: 1.0182 - val_acc: 0.6443\n",
      "Epoch 4/100\n",
      "1741/1741 [==============================] - 0s 98us/step - loss: 1.1296 - acc: 0.4859 - val_loss: 0.9958 - val_acc: 0.6392\n",
      "Epoch 5/100\n",
      "1741/1741 [==============================] - 0s 101us/step - loss: 1.1123 - acc: 0.5089 - val_loss: 0.9670 - val_acc: 0.6443\n",
      "Epoch 6/100\n",
      "1741/1741 [==============================] - 0s 111us/step - loss: 1.0809 - acc: 0.5370 - val_loss: 0.9506 - val_acc: 0.6495\n",
      "Epoch 7/100\n",
      "1741/1741 [==============================] - 0s 100us/step - loss: 1.0546 - acc: 0.5353 - val_loss: 0.9377 - val_acc: 0.6701\n",
      "Epoch 8/100\n",
      "1741/1741 [==============================] - 0s 108us/step - loss: 1.0351 - acc: 0.5526 - val_loss: 0.9299 - val_acc: 0.6701\n",
      "Epoch 9/100\n",
      "1741/1741 [==============================] - 0s 98us/step - loss: 1.0413 - acc: 0.5526 - val_loss: 0.9195 - val_acc: 0.6856\n",
      "Epoch 10/100\n",
      "1741/1741 [==============================] - 0s 103us/step - loss: 1.0337 - acc: 0.5635 - val_loss: 0.9106 - val_acc: 0.6907\n",
      "Epoch 11/100\n",
      "1741/1741 [==============================] - 0s 107us/step - loss: 0.9959 - acc: 0.5727 - val_loss: 0.9075 - val_acc: 0.6856\n",
      "Epoch 12/100\n",
      "1741/1741 [==============================] - 0s 104us/step - loss: 0.9920 - acc: 0.5778 - val_loss: 0.9008 - val_acc: 0.7010\n",
      "Epoch 13/100\n",
      "1741/1741 [==============================] - 0s 102us/step - loss: 0.9738 - acc: 0.5732 - val_loss: 0.8957 - val_acc: 0.6804\n",
      "Epoch 14/100\n",
      "1741/1741 [==============================] - 0s 102us/step - loss: 0.9881 - acc: 0.5859 - val_loss: 0.8917 - val_acc: 0.7113\n",
      "Epoch 15/100\n",
      "1741/1741 [==============================] - 0s 101us/step - loss: 0.9653 - acc: 0.5801 - val_loss: 0.8902 - val_acc: 0.7165\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741/1741 [==============================] - 0s 103us/step - loss: 0.9496 - acc: 0.6020 - val_loss: 0.8893 - val_acc: 0.6959\n",
      "Epoch 17/100\n",
      "1741/1741 [==============================] - 0s 99us/step - loss: 0.9696 - acc: 0.5870 - val_loss: 0.8907 - val_acc: 0.7165\n",
      "Epoch 18/100\n",
      "1741/1741 [==============================] - 0s 105us/step - loss: 0.9450 - acc: 0.5939 - val_loss: 0.8863 - val_acc: 0.6856\n",
      "Epoch 19/100\n",
      "1741/1741 [==============================] - 0s 98us/step - loss: 0.9452 - acc: 0.5997 - val_loss: 0.8903 - val_acc: 0.6959\n",
      "Epoch 20/100\n",
      "1741/1741 [==============================] - 0s 99us/step - loss: 0.9425 - acc: 0.5997 - val_loss: 0.8909 - val_acc: 0.6959\n",
      "Epoch 21/100\n",
      "1741/1741 [==============================] - 0s 99us/step - loss: 0.9207 - acc: 0.6157 - val_loss: 0.8816 - val_acc: 0.7010\n",
      "Epoch 22/100\n",
      "1741/1741 [==============================] - 0s 96us/step - loss: 0.9248 - acc: 0.6014 - val_loss: 0.8806 - val_acc: 0.6907\n",
      "Epoch 23/100\n",
      "1741/1741 [==============================] - 0s 98us/step - loss: 0.9110 - acc: 0.6134 - val_loss: 0.8819 - val_acc: 0.6856\n",
      "Epoch 24/100\n",
      "1741/1741 [==============================] - 0s 111us/step - loss: 0.9009 - acc: 0.6335 - val_loss: 0.8822 - val_acc: 0.6907\n",
      "Epoch 25/100\n",
      "1741/1741 [==============================] - 0s 153us/step - loss: 0.9129 - acc: 0.6106 - val_loss: 0.8833 - val_acc: 0.6856\n",
      "Epoch 26/100\n",
      "1741/1741 [==============================] - 0s 153us/step - loss: 0.9000 - acc: 0.6146 - val_loss: 0.8831 - val_acc: 0.6804\n",
      "Epoch 27/100\n",
      "1741/1741 [==============================] - 0s 102us/step - loss: 0.9041 - acc: 0.6232 - val_loss: 0.8843 - val_acc: 0.6804\n",
      "Epoch 28/100\n",
      "1741/1741 [==============================] - 0s 112us/step - loss: 0.9120 - acc: 0.6198 - val_loss: 0.8803 - val_acc: 0.6753\n",
      "Epoch 29/100\n",
      "1741/1741 [==============================] - 0s 132us/step - loss: 0.8840 - acc: 0.6376 - val_loss: 0.8810 - val_acc: 0.6804\n",
      "Epoch 30/100\n",
      "1741/1741 [==============================] - 0s 141us/step - loss: 0.8872 - acc: 0.6358 - val_loss: 0.8802 - val_acc: 0.6753\n",
      "Epoch 31/100\n",
      "1741/1741 [==============================] - 0s 123us/step - loss: 0.8801 - acc: 0.6284 - val_loss: 0.8784 - val_acc: 0.6856\n",
      "Epoch 32/100\n",
      "1741/1741 [==============================] - 0s 110us/step - loss: 0.8883 - acc: 0.6163 - val_loss: 0.8799 - val_acc: 0.6753\n",
      "Epoch 33/100\n",
      "1741/1741 [==============================] - 0s 105us/step - loss: 0.8680 - acc: 0.6312 - val_loss: 0.8767 - val_acc: 0.6753\n",
      "Epoch 34/100\n",
      "1741/1741 [==============================] - 0s 127us/step - loss: 0.8679 - acc: 0.6502 - val_loss: 0.8800 - val_acc: 0.6856\n",
      "Epoch 35/100\n",
      "1741/1741 [==============================] - 0s 150us/step - loss: 0.8595 - acc: 0.6422 - val_loss: 0.8837 - val_acc: 0.6804\n",
      "Epoch 36/100\n",
      "1741/1741 [==============================] - 0s 120us/step - loss: 0.8896 - acc: 0.6312 - val_loss: 0.8828 - val_acc: 0.6753\n",
      "Epoch 37/100\n",
      "1741/1741 [==============================] - 0s 103us/step - loss: 0.8654 - acc: 0.6358 - val_loss: 0.8813 - val_acc: 0.6907\n",
      "Epoch 38/100\n",
      "1741/1741 [==============================] - 0s 122us/step - loss: 0.8575 - acc: 0.6462 - val_loss: 0.8803 - val_acc: 0.6701\n",
      "Epoch 39/100\n",
      "1741/1741 [==============================] - 0s 110us/step - loss: 0.8590 - acc: 0.6358 - val_loss: 0.8886 - val_acc: 0.6753\n",
      "Epoch 40/100\n",
      "1741/1741 [==============================] - 0s 95us/step - loss: 0.8581 - acc: 0.6462 - val_loss: 0.8817 - val_acc: 0.6546\n",
      "Epoch 41/100\n",
      "1741/1741 [==============================] - 0s 98us/step - loss: 0.8504 - acc: 0.6496 - val_loss: 0.8842 - val_acc: 0.6804\n",
      "Epoch 42/100\n",
      "1741/1741 [==============================] - 0s 97us/step - loss: 0.8427 - acc: 0.6623 - val_loss: 0.8807 - val_acc: 0.6856\n",
      "Epoch 43/100\n",
      "1741/1741 [==============================] - 0s 100us/step - loss: 0.8327 - acc: 0.6565 - val_loss: 0.8804 - val_acc: 0.6804\n",
      "Epoch 44/100\n",
      "1741/1741 [==============================] - 0s 103us/step - loss: 0.8458 - acc: 0.6439 - val_loss: 0.8841 - val_acc: 0.6753\n",
      "Epoch 45/100\n",
      "1741/1741 [==============================] - 0s 101us/step - loss: 0.8254 - acc: 0.6571 - val_loss: 0.8816 - val_acc: 0.6907\n",
      "Epoch 46/100\n",
      "1741/1741 [==============================] - 0s 108us/step - loss: 0.8291 - acc: 0.6468 - val_loss: 0.8819 - val_acc: 0.6753\n",
      "Epoch 47/100\n",
      "1741/1741 [==============================] - 0s 114us/step - loss: 0.8264 - acc: 0.6634 - val_loss: 0.8792 - val_acc: 0.6856\n",
      "Epoch 48/100\n",
      "1741/1741 [==============================] - 0s 118us/step - loss: 0.8371 - acc: 0.6657 - val_loss: 0.8797 - val_acc: 0.6856\n",
      "Epoch 49/100\n",
      "1741/1741 [==============================] - 0s 114us/step - loss: 0.8090 - acc: 0.6703 - val_loss: 0.8860 - val_acc: 0.6804\n",
      "Epoch 50/100\n",
      "1741/1741 [==============================] - 0s 105us/step - loss: 0.8188 - acc: 0.6571 - val_loss: 0.8886 - val_acc: 0.6804\n",
      "Epoch 51/100\n",
      "1741/1741 [==============================] - 0s 109us/step - loss: 0.7997 - acc: 0.6686 - val_loss: 0.8837 - val_acc: 0.6856\n",
      "Epoch 52/100\n",
      "1741/1741 [==============================] - 0s 102us/step - loss: 0.8074 - acc: 0.6858 - val_loss: 0.8845 - val_acc: 0.6804\n",
      "Epoch 53/100\n",
      "1741/1741 [==============================] - 0s 104us/step - loss: 0.8000 - acc: 0.6743 - val_loss: 0.8855 - val_acc: 0.6753\n",
      "Epoch 54/100\n",
      "1741/1741 [==============================] - 0s 109us/step - loss: 0.8299 - acc: 0.6582 - val_loss: 0.8899 - val_acc: 0.6804\n",
      "Epoch 55/100\n",
      "1741/1741 [==============================] - 0s 109us/step - loss: 0.8046 - acc: 0.6841 - val_loss: 0.8921 - val_acc: 0.6753\n",
      "Epoch 56/100\n",
      "1741/1741 [==============================] - 0s 100us/step - loss: 0.8099 - acc: 0.6686 - val_loss: 0.8945 - val_acc: 0.6856\n",
      "Epoch 57/100\n",
      "1741/1741 [==============================] - 0s 99us/step - loss: 0.8127 - acc: 0.6749 - val_loss: 0.8865 - val_acc: 0.6753\n",
      "Epoch 58/100\n",
      "1741/1741 [==============================] - 0s 109us/step - loss: 0.7987 - acc: 0.6738 - val_loss: 0.8899 - val_acc: 0.6701\n",
      "Epoch 59/100\n",
      "1741/1741 [==============================] - 0s 104us/step - loss: 0.7940 - acc: 0.6709 - val_loss: 0.8875 - val_acc: 0.6856\n",
      "Epoch 60/100\n",
      "1741/1741 [==============================] - 0s 107us/step - loss: 0.8086 - acc: 0.6709 - val_loss: 0.8872 - val_acc: 0.6804\n",
      "Epoch 61/100\n",
      "1741/1741 [==============================] - 0s 119us/step - loss: 0.7814 - acc: 0.6749 - val_loss: 0.8940 - val_acc: 0.6701\n",
      "Epoch 62/100\n",
      "1741/1741 [==============================] - 0s 113us/step - loss: 0.7920 - acc: 0.6766 - val_loss: 0.8865 - val_acc: 0.6701\n",
      "Epoch 63/100\n",
      "1741/1741 [==============================] - 0s 108us/step - loss: 0.7727 - acc: 0.6726 - val_loss: 0.9050 - val_acc: 0.6546\n",
      "Epoch 64/100\n",
      "1741/1741 [==============================] - 0s 103us/step - loss: 0.7828 - acc: 0.6766 - val_loss: 0.9021 - val_acc: 0.6649\n",
      "Epoch 65/100\n",
      "1741/1741 [==============================] - 0s 112us/step - loss: 0.7864 - acc: 0.6824 - val_loss: 0.8997 - val_acc: 0.6649\n",
      "Epoch 66/100\n",
      "1741/1741 [==============================] - 0s 114us/step - loss: 0.7935 - acc: 0.6743 - val_loss: 0.8967 - val_acc: 0.6753\n",
      "Epoch 67/100\n",
      "1741/1741 [==============================] - 0s 108us/step - loss: 0.7905 - acc: 0.6766 - val_loss: 0.8939 - val_acc: 0.6753\n",
      "Epoch 68/100\n",
      "1741/1741 [==============================] - 0s 103us/step - loss: 0.7630 - acc: 0.6898 - val_loss: 0.8941 - val_acc: 0.6701\n",
      "Epoch 69/100\n",
      "1741/1741 [==============================] - 0s 109us/step - loss: 0.7749 - acc: 0.6950 - val_loss: 0.8930 - val_acc: 0.6753\n",
      "Epoch 70/100\n",
      "1741/1741 [==============================] - 0s 109us/step - loss: 0.7607 - acc: 0.6881 - val_loss: 0.8903 - val_acc: 0.6701\n",
      "Epoch 71/100\n",
      "1741/1741 [==============================] - 0s 114us/step - loss: 0.7582 - acc: 0.6904 - val_loss: 0.8912 - val_acc: 0.6753\n",
      "Epoch 72/100\n",
      "1741/1741 [==============================] - 0s 111us/step - loss: 0.7676 - acc: 0.6864 - val_loss: 0.8955 - val_acc: 0.6804\n",
      "Epoch 73/100\n",
      "1741/1741 [==============================] - 0s 120us/step - loss: 0.7470 - acc: 0.6950 - val_loss: 0.8978 - val_acc: 0.6649\n",
      "Epoch 74/100\n",
      "1741/1741 [==============================] - 0s 117us/step - loss: 0.7631 - acc: 0.6950 - val_loss: 0.9056 - val_acc: 0.6598\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741/1741 [==============================] - 0s 113us/step - loss: 0.7567 - acc: 0.6910 - val_loss: 0.8972 - val_acc: 0.6959\n",
      "Epoch 76/100\n",
      "1741/1741 [==============================] - 0s 104us/step - loss: 0.7503 - acc: 0.6875 - val_loss: 0.9058 - val_acc: 0.6598\n",
      "Epoch 77/100\n",
      "1741/1741 [==============================] - 0s 120us/step - loss: 0.7588 - acc: 0.6887 - val_loss: 0.9102 - val_acc: 0.6804\n",
      "Epoch 78/100\n",
      "1741/1741 [==============================] - 0s 111us/step - loss: 0.7548 - acc: 0.6927 - val_loss: 0.8950 - val_acc: 0.6959\n",
      "Epoch 79/100\n",
      "1741/1741 [==============================] - 0s 105us/step - loss: 0.7337 - acc: 0.6962 - val_loss: 0.9088 - val_acc: 0.6598\n",
      "Epoch 80/100\n",
      "1741/1741 [==============================] - 0s 102us/step - loss: 0.7521 - acc: 0.6944 - val_loss: 0.9103 - val_acc: 0.6804\n",
      "Epoch 81/100\n",
      "1741/1741 [==============================] - 0s 100us/step - loss: 0.7284 - acc: 0.7088 - val_loss: 0.9088 - val_acc: 0.6701\n",
      "Epoch 82/100\n",
      "1741/1741 [==============================] - 0s 101us/step - loss: 0.7512 - acc: 0.6835 - val_loss: 0.9136 - val_acc: 0.6598\n",
      "Epoch 83/100\n",
      "1741/1741 [==============================] - 0s 95us/step - loss: 0.7419 - acc: 0.6881 - val_loss: 0.9185 - val_acc: 0.6701\n",
      "Epoch 84/100\n",
      "1741/1741 [==============================] - 0s 101us/step - loss: 0.7363 - acc: 0.6910 - val_loss: 0.9175 - val_acc: 0.6546\n",
      "Epoch 85/100\n",
      "1741/1741 [==============================] - 0s 96us/step - loss: 0.7494 - acc: 0.6898 - val_loss: 0.9150 - val_acc: 0.6701\n",
      "Epoch 86/100\n",
      "1741/1741 [==============================] - 0s 96us/step - loss: 0.7165 - acc: 0.7151 - val_loss: 0.9141 - val_acc: 0.6649\n",
      "Epoch 87/100\n",
      "1741/1741 [==============================] - 0s 96us/step - loss: 0.7431 - acc: 0.6939 - val_loss: 0.9169 - val_acc: 0.6546\n",
      "Epoch 88/100\n",
      "1741/1741 [==============================] - 0s 103us/step - loss: 0.7138 - acc: 0.7036 - val_loss: 0.9174 - val_acc: 0.6546\n",
      "Epoch 89/100\n",
      "1741/1741 [==============================] - 0s 95us/step - loss: 0.7051 - acc: 0.7151 - val_loss: 0.9164 - val_acc: 0.6701\n",
      "Epoch 90/100\n",
      "1741/1741 [==============================] - 0s 103us/step - loss: 0.7372 - acc: 0.6979 - val_loss: 0.9204 - val_acc: 0.6598\n",
      "Epoch 91/100\n",
      "1741/1741 [==============================] - 0s 100us/step - loss: 0.7250 - acc: 0.7099 - val_loss: 0.9217 - val_acc: 0.6701\n",
      "Epoch 92/100\n",
      "1741/1741 [==============================] - 0s 101us/step - loss: 0.7307 - acc: 0.7065 - val_loss: 0.9249 - val_acc: 0.6495\n",
      "Epoch 93/100\n",
      "1741/1741 [==============================] - 0s 99us/step - loss: 0.7324 - acc: 0.6979 - val_loss: 0.9291 - val_acc: 0.6753\n",
      "Epoch 94/100\n",
      "1741/1741 [==============================] - 0s 106us/step - loss: 0.7163 - acc: 0.7163 - val_loss: 0.9308 - val_acc: 0.6753\n",
      "Epoch 95/100\n",
      "1741/1741 [==============================] - 0s 96us/step - loss: 0.7302 - acc: 0.6979 - val_loss: 0.9235 - val_acc: 0.6804\n",
      "Epoch 96/100\n",
      "1741/1741 [==============================] - 0s 100us/step - loss: 0.7128 - acc: 0.7140 - val_loss: 0.9234 - val_acc: 0.6598\n",
      "Epoch 97/100\n",
      "1741/1741 [==============================] - 0s 100us/step - loss: 0.7015 - acc: 0.7140 - val_loss: 0.9225 - val_acc: 0.6701\n",
      "Epoch 98/100\n",
      "1741/1741 [==============================] - 0s 102us/step - loss: 0.7000 - acc: 0.7168 - val_loss: 0.9198 - val_acc: 0.6649\n",
      "Epoch 99/100\n",
      "1741/1741 [==============================] - 0s 122us/step - loss: 0.6879 - acc: 0.7157 - val_loss: 0.9306 - val_acc: 0.6495\n",
      "Epoch 100/100\n",
      "1741/1741 [==============================] - 0s 119us/step - loss: 0.7092 - acc: 0.6996 - val_loss: 0.9323 - val_acc: 0.6546\n",
      "{'epochs': 100, 'validation_split': 0.1, 'optimizer': 'adam', 'batch_size': 64}\n",
      "Accuracy: 58.40\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "keras_classifier = KerasClassifier(build_fn=build_ann)\n",
    "parameters = {'batch_size': [64, 128],\n",
    "              'epochs': [100, 150, 200],\n",
    "              'optimizer': ['adam', 'nadam'],\n",
    "              'validation_split': [0.1]}\n",
    "grid_search = GridSearchCV(estimator = keras_classifier,\n",
    "                           param_grid = parameters,\n",
    "                           cv = 10,\n",
    "                           verbose=False)\n",
    "grid_search = grid_search.fit(X_nn, y_nn)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print(best_parameters)\n",
    "print('Accuracy: %0.2f' % (best_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Extra Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model performs on an extra test set defined by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shapes...\n",
      "X shape (20, 310)\n",
      "y shape (20, 4)\n",
      "Accuracy score 0.45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4ab3cc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "We are going to use the encoder and the scaler previously trained\n",
    "'''\n",
    "y_extra_test_nn = np_utils.to_categorical(encoder.transform(y_extra_test))\n",
    "\n",
    "X_extra_test_nn = sc.transform(X_extra_test)\n",
    "print('New shapes...')\n",
    "print('X shape',X_extra_test_nn.shape)\n",
    "print('y shape',y_extra_test_nn.shape)\n",
    "\n",
    "y_pred = classifier.predict(X_extra_test_nn)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_true = np.argmax(y_extra_test_nn,axis=1)\n",
    "filename = './Advanced_feature_engineering_pictures/CM_ANN_extra.png'\n",
    "plot_confusion_matrix(y_true, y_pred,'ANN Extra test - confusion matrix', emotion_labels, filename)\n",
    "\n",
    "print('Accuracy score', round(accuracy_score(y_pred,y_true),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:43:04.701045Z",
     "start_time": "2018-06-05T20:43:04.669847Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_lr = encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_lr, test_size = 0.1, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:43:05.157176Z",
     "start_time": "2018-06-05T20:43:05.143581Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def evaluate_logreg(X_train, y_train, X_test):\n",
    "    classifier = LogisticRegression(penalty='l2', dual=False, C=25,\n",
    "                                    solver='newton-cg', multi_class='multinomial', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:43:14.890941Z",
     "start_time": "2018-06-05T20:43:06.096099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 10-fold validation: 0.5787 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "classifier = LogisticRegression(penalty='l2', dual=False, C=25,\n",
    "                                    solver='newton-cg', multi_class='multinomial', random_state=0)\n",
    "scores = cross_val_score(classifier, X, y_lr, cv=10)\n",
    "print('Accuracy for 10-fold validation: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:28:00.490846Z",
     "start_time": "2018-06-05T20:27:57.977253Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "y_pred = evaluate_logreg(X_train, y_train, X_test)\n",
    "print('Accuracy:', round(accuracy_score(y_test, y_pred),2))\n",
    "filename = '../Report/chapters/chapter5/images/join/CM_LR.png'\n",
    "plot_confusion_matrix(y_test, y_pred, 'Logistic regression - confusion matrix', emotion_labels, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Extra Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T13:31:58.248593Z",
     "start_time": "2018-05-03T13:31:57.143904Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2db69b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = evaluate_logreg(X_train, y_train, X_extra_test)\n",
    "y_true = encoder.transform(y_extra_test)\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "filename = './Advanced_feature_engineering_pictures/CM_LR_extra.png'\n",
    "plot_confusion_matrix(y_true, y_pred, 'Logistic regression Extra Test - confusion matrix', emotion_labels, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:43:25.038968Z",
     "start_time": "2018-06-05T20:43:25.029991Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_svm = encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_svm, test_size = 0.1, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:43:41.544957Z",
     "start_time": "2018-06-05T20:43:27.833446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 10-fold validation: 0.5804 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Cross validation\n",
    "clf = SVC(C=220, kernel='rbf')\n",
    "scores = cross_val_score(clf, X, y_svm, cv=10)\n",
    "print('Accuracy for 10-fold validation: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T15:12:57.576228Z",
     "start_time": "2018-05-15T15:12:54.535156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a202d3cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = SVC(C=220, kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', round(accuracy_score(y_test, y_pred),2))\n",
    "filename = '../Report/chapters/chapter5/images/join/CM_SVM.png'\n",
    "plot_confusion_matrix(y_test, y_pred, 'SVM - confusion matrix', emotion_labels, filename )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "svm_clf = SVC(C=220, kernel='rbf')\n",
    "scores = cross_val_score(svm_clf, X_vect, y, cv=10)\n",
    "    print('Accuracy for 10-fold validation: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Extra Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:58:39.760898Z",
     "start_time": "2018-05-15T14:58:39.441641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a337cd048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_extra_test)\n",
    "y_true = encoder.transform(y_extra_test)\n",
    "print('Accuracy:', round(accuracy_score(y_true, y_pred),2))\n",
    "filename = './Advanced_feature_engineering_pictures/CM_SVM_extra.png'\n",
    "plot_confusion_matrix(y_true, y_pred, 'SVM Extra test - confusion matrix', emotion_labels, filename )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eXtreme Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:43:55.801431Z",
     "start_time": "2018-06-05T20:43:55.789855Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_xgb = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T20:46:32.810872Z",
     "start_time": "2018-06-05T20:43:56.448710Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 10-fold validation: 0.5689 (+/- 0.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Cross validation\n",
    "clf = xgboost.XGBClassifier()\n",
    "scores = cross_val_score(clf, X, y_xgb, cv=10)\n",
    "print('Accuracy for 10-fold validation: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:53:22.059186Z",
     "start_time": "2018-05-15T14:53:14.359306Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saragiammusso/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.70%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a203ff048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_xgb, test_size = 0.1, random_state = 13)\n",
    "xgb = xgboost.XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "filename = '../Report/chapters/chapter5/images/join/CM_XGB.png'\n",
    "plot_confusion_matrix(y_test, y_pred, 'XGB - confusion matrix', emotion_labels, filename )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Extra Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saragiammusso/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x101dda400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = xgb.predict(X_extra_test)\n",
    "y_true = encoder.transform(y_extra_test)\n",
    "print('Accuracy:', round(accuracy_score(y_true, y_pred),2))\n",
    "filename = './Advanced_feature_engineering_pictures/CM_XGB_extra.png'\n",
    "plot_confusion_matrix(y_true, y_pred, 'XGB Extra test - confusion matrix', emotion_labels, filename )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "316px",
    "width": "287px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
