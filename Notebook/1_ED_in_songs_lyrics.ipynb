{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Patterns in Music Playlists\n",
    "\n",
    "Please refer to [https://github.com/sgiammy/emotion-patterns-in-music-playlists](https://github.com/sgiammy/emotion-patterns-in-music-playlists) for more details on the code which is used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:28:56.350552Z",
     "start_time": "2018-06-05T17:28:54.975582Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_color_codes()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.datasets import load_dataset_from_path, split_train_validation\n",
    "\n",
    "lyrics_path = './ml_lyrics'\n",
    "\n",
    "emotion_labels = ['happy', 'sad', 'relaxed', 'angry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:28:57.545015Z",
     "start_time": "2018-06-05T17:28:57.535246Z"
    }
   },
   "outputs": [],
   "source": [
    "def parameters_grid_search(classifier, params, x, y, cv=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Grid Search to find best parameters for a certain classifier whose\n",
    "    performances are evaluated using cross-validation\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(classifier(), params, cv=cv, n_jobs=-1, verbose=verbose)\n",
    "    gs.fit(x, y)    \n",
    "    return (gs.best_estimator_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has the sole purpose of providing us an experimental environment in which we can quickly test our intuitions and ideas. Therefore, even though for the final algorithm we plan to use FastText, for this notebook we will use a pre-trainend language model available in spaCy containing 685k unique vectors trained on Common Crawl.\n",
    "\n",
    "For more information about the language model we are using, please refer to [this](https://spacy.io/models/en) link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:29:10.225390Z",
     "start_time": "2018-06-05T17:28:58.761590Z"
    }
   },
   "outputs": [],
   "source": [
    "# For this notebook we will use a simple spacy vocabulary\n",
    "# because we just need to do some experiments\n",
    "spacy_lang = 'en_core_web_lg'\n",
    "nlp = spacy.load(spacy_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:29:14.788370Z",
     "start_time": "2018-06-05T17:29:10.228526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The language model we are using has some issues with stop words.\n",
    "# Basically we need to grab stopwords from the 'en' language model\n",
    "# and add them back to the model we are using.\n",
    "# https://github.com/explosion/spaCy/issues/922\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Emotion-Patterns-in-Music-Playlists\" data-toc-modified-id=\"Emotion-Patterns-in-Music-Playlists-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Emotion Patterns in Music Playlists</a></span></li><li><span><a href=\"#The-Dataset:-MoodyLyrics\" data-toc-modified-id=\"The-Dataset:-MoodyLyrics-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The Dataset: MoodyLyrics</a></span></li><li><span><a href=\"#Lyrics-Preprocessing\" data-toc-modified-id=\"Lyrics-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Lyrics Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lyrics-Download\" data-toc-modified-id=\"Lyrics-Download-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Lyrics Download</a></span></li><li><span><a href=\"#Stopwords-deletion\" data-toc-modified-id=\"Stopwords-deletion-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stopwords deletion</a></span></li><li><span><a href=\"#Preprocessing-Function\" data-toc-modified-id=\"Preprocessing-Function-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Preprocessing Function</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Principal-Component-Analysis\" data-toc-modified-id=\"Principal-Component-Analysis-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Principal Component Analysis</a></span></li><li><span><a href=\"#Feature-Engineering-Function\" data-toc-modified-id=\"Feature-Engineering-Function-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Feature Engineering Function</a></span></li></ul></li><li><span><a href=\"#Classifiers-on-Lyrics-Content\" data-toc-modified-id=\"Classifiers-on-Lyrics-Content-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classifiers on Lyrics Content</a></span><ul class=\"toc-item\"><li><span><a href=\"#Supervised-K-Means\" data-toc-modified-id=\"Supervised-K-Means-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Supervised K-Means</a></span></li><li><span><a href=\"#k-Nearest-Neighbour\" data-toc-modified-id=\"k-Nearest-Neighbour-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>k-Nearest Neighbour</a></span></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>SVM</a></span></li><li><span><a href=\"#Gradient-Boost\" data-toc-modified-id=\"Gradient-Boost-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Gradient Boost</a></span></li><li><span><a href=\"#Artifical-Neural-Network\" data-toc-modified-id=\"Artifical-Neural-Network-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Artifical Neural Network</a></span></li></ul></li><li><span><a href=\"#What-if-we-just-consider-the-song-title?\" data-toc-modified-id=\"What-if-we-just-consider-the-song-title?-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>What if we just consider the song title?</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM-with-title-only\" data-toc-modified-id=\"SVM-with-title-only-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>SVM with title only</a></span></li><li><span><a href=\"#Gradient-Boost-with-title-only\" data-toc-modified-id=\"Gradient-Boost-with-title-only-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Gradient Boost with title only</a></span></li><li><span><a href=\"#Artificial-Neural-Network-with-title-only\" data-toc-modified-id=\"Artificial-Neural-Network-with-title-only-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Artificial Neural Network with title only</a></span></li></ul></li><li><span><a href=\"#Song-Content-+-Song-Title\" data-toc-modified-id=\"Song-Content-+-Song-Title-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Song Content + Song Title</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>SVM</a></span></li><li><span><a href=\"#Artificial-Neural-Network\" data-toc-modified-id=\"Artificial-Neural-Network-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Artificial Neural Network</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset: MoodyLyrics\n",
    "\n",
    "In the followign section we will provide some interesting statistics on the dataset we will be using throughtout the notebook: MoodyLyrics.<br>\n",
    "ModdyLyrics has the following header: <br>\n",
    "*<Index, Artist, Song, Emotion>*\n",
    "where:\n",
    "<ul>\n",
    "    <li>*Index*: monotonically increasing ID</li>\n",
    "    <li>*Artist*: name of the artist</li>\n",
    "    <li>*Song*: title of the song</li>\n",
    "    <li>*Emotion*: one between ['happy', 'sad', 'relaxed', 'angry']</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's see how many rows we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:45:44.784274Z",
     "start_time": "2018-06-05T17:45:44.758023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2595, 4)\n"
     ]
    }
   ],
   "source": [
    "moodyLyricsDF = pd.read_csv('./datasets/MoodyLyrics.csv')\n",
    "print(moodyLyricsDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 2596 rows, but the first one of course is the header. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MoodyLyrics we have 4 different emotion labels for our songs: happy, sad, relaxed and angry. Let's see how those 4 classes are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:30:07.439617Z",
     "start_time": "2018-06-05T17:30:07.395749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2595</td>\n",
       "      <td>2595</td>\n",
       "      <td>2595</td>\n",
       "      <td>2595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2595</td>\n",
       "      <td>1672</td>\n",
       "      <td>2229</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ML737</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Silent Night</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>24</td>\n",
       "      <td>819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Index       Artist          Song Emotion\n",
       "count    2595         2595          2595    2595\n",
       "unique   2595         1672          2229       4\n",
       "top     ML737  The Beatles  Silent Night   happy\n",
       "freq        1           52            24     819"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moodyLyricsDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice something weird! Some songs, such as \"Silent Night\" appear more than once.<br>\n",
    "Let's check if this is because there are duplicated rows or just because there are songs with the same title but different artist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T15:41:06.242246Z",
     "start_time": "2018-05-27T15:41:06.198735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2509, 3)\n",
      "(82, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>ML654</td>\n",
       "      <td>Akon</td>\n",
       "      <td>Don't Matter</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>ML962</td>\n",
       "      <td>Akon</td>\n",
       "      <td>Lonely</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>ML2418</td>\n",
       "      <td>Akon</td>\n",
       "      <td>Lonely</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>ML2539</td>\n",
       "      <td>Akon</td>\n",
       "      <td>Don't Matter</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index Artist          Song Emotion\n",
       "653    ML654   Akon  Don't Matter   angry\n",
       "961    ML962   Akon        Lonely     sad\n",
       "2417  ML2418   Akon        Lonely     sad\n",
       "2538  ML2539   Akon  Don't Matter   angry"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(moodyLyricsDF.Song.value_counts()[:10])\n",
    "duplicatedCheck = moodyLyricsDF.groupby(['Artist','Song']).size().reset_index(name='count')\n",
    "print(duplicatedCheck.shape)\n",
    "duplicatedRows = duplicatedCheck [(duplicatedCheck ['count']>1)]\n",
    "print(duplicatedRows.shape)\n",
    "moodyLyricsDF[(moodyLyricsDF['Artist']=='Akon')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 82 duplicated rows. <br>\n",
    "We now group by 'Artist' and 'Song', and we label each song with the most frequent emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T08:55:39.741672Z",
     "start_time": "2018-04-16T08:55:38.303602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2509, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Akon</td>\n",
       "      <td>Don't Matter</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Akon</td>\n",
       "      <td>Lonely</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Artist          Song Emotion\n",
       "34   Akon  Don't Matter   angry\n",
       "35   Akon        Lonely     sad"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moodyLyricsDF = moodyLyricsDF.groupby(['Artist','Song'],as_index=False)['Emotion'].agg(lambda x:x.value_counts().index[0])\n",
    "print(moodyLyricsDF.shape)\n",
    "moodyLyricsDF[(moodyLyricsDF['Artist']==\"Akon\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T15:46:08.649771Z",
     "start_time": "2018-05-27T15:46:08.381411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy      819\n",
      "sad        604\n",
      "relaxed    597\n",
      "angry      575\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotionDistribution = moodyLyricsDF.Emotion.value_counts()\n",
    "print(emotionDistribution)\n",
    "ax = sns.barplot(x=np.array(range(4)),y=emotionDistribution)\n",
    "ax.set_title('Emotion distribution',fontsize=20)\n",
    "ax.set_xticklabels(emotion_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our lyrics are quite balanced among the 4 different classes. Only the happy class has some more lyrics with respect to the other threes. However we believe that it is not going to be a problem as the different between happy and other lyrics should not be too relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the cleaned dataset into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T08:55:41.279957Z",
     "start_time": "2018-04-16T08:55:41.258075Z"
    }
   },
   "outputs": [],
   "source": [
    "moodyLyricsDF.to_csv('./datasets/moodylyrics_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Preprocessing\n",
    "\n",
    "Now we will move on analyzing several techniques for lyrics preprocessing, eventually evaluating their effects and their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the lyrics from lyricwikia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:04:53.614882Z",
     "start_time": "2018-04-11T21:04:53.352578Z"
    }
   },
   "outputs": [],
   "source": [
    "import lyricwikia\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from utils.progress import progress\n",
    "\n",
    "def download_lyrics(input,output,skipHeader=True):\n",
    "\n",
    "    def songs_count(path):\n",
    "        with open(path) as f:\n",
    "            count = len(f.readlines()) - 1\n",
    "            if skipHeader:\n",
    "                count -= 1\n",
    "            return count\n",
    "\n",
    "    # Generator function which reads the lyrics from a csv file line by line\n",
    "    def lyric_entries_generator(path):\n",
    "        with open(path) as lp:\n",
    "            l = lp.readline()\n",
    "            if skipHeader:\n",
    "                l = lp.readline()\n",
    "            while l:\n",
    "                yield l.rstrip().split(',')\n",
    "                l = lp.readline()\n",
    "\n",
    "    def create_output_dir(path):\n",
    "        if os.path.exists(path) and os.path.isdir(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "    LOG_FILE = '.'.join([input, 'log'])\n",
    "    try:\n",
    "        os.remove(os.path.join('.', LOG_FILE))\n",
    "    except OSError:\n",
    "      # Log file did not exists...not too bad\n",
    "        pass\n",
    "\n",
    "    def err(msg):\n",
    "        with open(os.path.join('.', LOG_FILE), 'a') as log:\n",
    "            log.write(msg)\n",
    "            log.write('\\n')\n",
    "\n",
    "    def download_lyric(song):\n",
    "        try:\n",
    "            lyric = lyricwikia.get_lyrics(song[1], song[2])\n",
    "            filename = '_'.join([song[3], song[1], song[2]])\n",
    "            filename = filename.replace('/', '-') # The '/' should never appear\n",
    "            with open(os.path.join(output, filename), 'w') as sfile:\n",
    "                sfile.write(lyric)\n",
    "                return True\n",
    "        except lyricwikia.LyricsNotFound:\n",
    "            err('Could not download {}'.format(song))\n",
    "            return False\n",
    "\n",
    "\n",
    "    # Get the number of songs we are going to download\n",
    "    totalSongs = songs_count(input)\n",
    "\n",
    "    # Create output directory\n",
    "    create_output_dir(output)\n",
    "\n",
    "    # Download songs\n",
    "    count = 0 \n",
    "    errCount = 0\n",
    "    for lyric in lyric_entries_generator(input):\n",
    "        progress(count, totalSongs, 'Errors encountered: {}'.format(errCount))\n",
    "        if not download_lyric(lyric):\n",
    "            errCount += 1\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:12:41.509559Z",
     "start_time": "2018-04-11T21:12:41.503423Z"
    }
   },
   "outputs": [],
   "source": [
    "inputCsv = './datasets/moodylyrics_cleaned.csv'\n",
    "outputDir = './ml_lyrics'\n",
    "if os.path.exists(outputDir) and os.path.isdir(outputDir):\n",
    "    pass\n",
    "else:\n",
    "    download_lyrics(inputCsv,outputDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many lyrics we have actually downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:30:18.550698Z",
     "start_time": "2018-06-05T17:30:18.467027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2447\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -1 ml_lyrics/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's analyse the emotion distribution considering only the song for which we have downloaded the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T15:44:00.818126Z",
     "start_time": "2018-05-27T15:44:00.800833Z"
    }
   },
   "outputs": [],
   "source": [
    "count = dict(list(zip(emotion_labels, [0 for x in range(len(emotion_labels))])))\n",
    "\n",
    "# Traverse the dataset directory\n",
    "for root, dirs, files in os.walk(lyrics_path):\n",
    "    for f in files:\n",
    "        fields = f.split('_')\n",
    "        if len(fields) > 0:\n",
    "            count[fields[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T15:44:07.020286Z",
     "start_time": "2018-05-27T15:44:06.674179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEUCAYAAACCr4kEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAIABJREFUeJzt3XlclXX+///HUSQxAcGQ40KLFmCokLmAmuZRxDXBNMfSyaWfTankkpMtph9LLcdKrcbk41Q6TmYuoJNNouiIu5ZabjVjaoJ1DoogiMp6/f7ow/lKIKBCV+Dzfrt5u8n72l7vN9c5z3Mt58JiGIaBiIiICWqYXYCIiNy6FEIiImIahZCIiJhGISQiIqZRCImIiGkUQiIiYhqFkFSaU6dOMWbMGDp27EhAQABt2rQpdf7k5GQCAgKYMmVKpda1Zs0aAgICWLNmTaVupzyWLl1K7969adWqFQEBAXz88cdml2SqPXv2EBAQwLvvvmt2Kb9LAQEBDBs2zOwyKpSL2QVUtoULFzJv3jwA/vWvf9G0aVOTKzJPcnIy3bp1IyoqijfeeKNSt5Wfn8+YMWP48ccf6d+/P1arldtuu61St1nVrF+/npkzZ3L//ffz5JNP4urqSkhIiNlliYlsNhsAmzdvNrmS3061DiHDMFi5ciUWi8X5/xdeeMHssm4JycnJHD9+nMcee4zXXnvN7HKKCA8PJzg4mAYNGphax5YtWwD44IMP8PX1NbUWqRq++OIL3NzczC6jQlXr03Hbt2/nzJkzREVF4ePjQ2xsLDk5OWaXdUtISUkBMP2NviTu7u40a9YMd3d3U+soHCMFkJRXs2bNaNSokdllVKhqHUIrV64EYNCgQfTr14+0tDQ2bdpU4rzvvvsuAQEB7Nmzh88//5wBAwYQHBxMp06dmD17tjO8du3axbBhw2jdujVt27Zl8uTJpKWllbjOw4cPM27cOMLCwmjRogVdu3Zl+vTpzjefqw0bNoyAgIAS13Otaxg2mw2bzcalS5d48803efjhh2nRogXh4eHExMRw9ROZ3n33Xbp16wZAbGwsAQEBzn/lvTZS3v4EBAQwdOhQAN577z3ndm7kPP/EiRMJCAhg7969JU7fsGEDAQEBzJgxw9lWOJY5OTm89957RERE0KJFC+e1ptKuCdntdl5//XV69OhBq1ataNeuHQMHDuT9998vMt93333HxIkTsdlstGjRgtDQUKKiopg5cya5ubml9unqfQ0o8ru42q5duxg1ahTt2rWjRYsWREREMHfuXDIzM4uts6w+l+WHH35gypQpdOnShRYtWtChQwcmTZrEiRMnis178uRJ5s6dy4ABAwgNDXXuC1OnTsVut19zG9u3b+dPf/qTc//p0qULzzzzDDt37ixx/mPHjjF69GjatGlDcHAwQ4cOZf/+/eXqz9W++eYboqOj6dixo3O7r776Kg6Ho9i8heOYm5vLe++9R/fu3WnZsiURERF89tlnzvmWL19Ov379aNWqFZ07d2bBggUUFBSUuP0vvviCJ554ggcffJBWrVrRr18/Fi1aVOQDceG1sDNnznDmzJki+8TVv8NrXRPKzMzkrbfeIiIigpYtW9K2bVtGjRpV4thefd2tosb4ZlTb03Hnzp1j8+bN3H333bRu3Zq6devy4YcfsmLFCnr37n3N5ZYtW0ZiYiLdu3enXbt27Nixg48//pgLFy7QrVs3JkyYwMMPP8zgwYM5cOAA69atIy0tjcWLFxdZz5YtWxg3bhwAERERNGrUiCNHjrB8+XISEhL45JNP8PPzu+l+5ubmMmrUKFJSUujcuTM1a9Zk06ZNvPXWW+Tk5DB27FgA2rVrxx//+EeWLl1KYGAg3bt3d66jefPmZW7nevozduxYzpw5Q2xsLO3ataNdu3bOGq7XkCFDWL9+PStWrChx+RUrVgDwhz/8odi06OhoDh06ROfOnenevTv169cvdVuHDh3iqaeeIj09nbZt2xIeHs6VK1c4fvw47733HmPGjAF+CaDHHnsMi8WCzWajSZMmXLx4kdOnT7N8+XLGjx9PrVq1rrmddu3aMXbsWGJjYzlz5ozzd3S1Tz/9lOnTp+Pm5kbPnj2pX78+e/fu5X//93/ZsmULy5cvx8PD46b7DJCYmMi4cePIy8uja9eu3HnnnTgcDuLj4/n3v//N0qVLCQoKcs6/ceNGPv30U9q3b0/r1q2pVasW//3vf1m5ciVbtmxh9erVxY7uFixYwPvvv0+dOnXo3r07DRs2JCUlxfka6tChQ5H5Dx8+zOLFiwkJCWHQoEH89NNPxMfHM3z4cOLi4sp9bXfVqlW8+uqruLq6YrPZsFqt/Pjjj6xcuZLNmzfz2WeflXhkMXHiRL755hu6dOmCi4sLGzZsYOrUqbi4uPD9998TFxfHww8/TGhoKJs3b+b999+ndu3ajB49ush63n77bRYtWoSXlxd9+/alTp06bNu2jbfffpvt27fzt7/9DVdXVxo3bszYsWNZsmQJAE8++aRzHWW9PjMyMhgyZAjHjx+nZcuWPPnkk6SlpfGvf/2LkSNHMn369BJfHxU1xjfNqKYWLVpk+Pv7Gx988IGzLSoqyggICDBOnTpVbP4FCxYY/v7+RuvWrY3jx48727Ozs43evXsbgYGBRrt27Yw9e/Y4p+Xn5xvDhw83/P39jaNHjzrbL168aLRr184IDAw09u3bV2JdI0aMKNI+dOhQw9/fv8S+rF692vD39zdWr15dpL1r166Gv7+/8dRTTxmXL192tp87d8548MEHjQcffNDIyclxticlJRn+/v7GCy+8UOJ2ruVG+rN7927D39/fWLBgQbm3c636+vTpY7Ro0cI4f/58kfbTp08bAQEBxuDBg4u0F45l3759jdTU1GLbKWk8s7OzneO5bt26Ysv8/PPPzv/Pnj3b8Pf3NzZu3FhsvvT0dCM/P79c/b3W7zw5OdkICgoyHnjggSL7omEYxrRp0wx/f3/jlVdeKXFd1+rztaSnpxtt2rQx2rVrZ/z3v/8tMu377783QkJCjMjIyCLtdrvdyM7OLraubdu2GYGBgcarr75arN3f39+w2WyG3W4vttzVY1u435S0vy9fvtzw9/c3pk2bVq6+nThxwggKCjK6d+9ebLs7d+40AgMDjWeffbZIe+E4DhgwwLhw4YKz/fTp00ZQUJDRpk0bo2vXrkXWd+HCBaNdu3ZG+/btjdzcXGf7/v37DX9/f6NLly5GSkqKsz03N9d4+umnDX9/f2PhwoVFtt+1a1eja9eu1+yTv7+/MXTo0CJtU6dONfz9/Y2pU6caBQUFzvaTJ08arVu3NoKCgoykpCRne0WOcUWolqfjjP+7CaFGjRpERkY62wcMGIBhGEUOq39t2LBhNGvWzPmzq6srvXr1oqCggC5duhT5NF6jRg0eeeQR4JdPx4USEhJIT0+nd+/exW5LHjlyJI0bN2bHjh389NNPN91XgFdeeYXatWs7f65fvz7dunUjMzOTkydP3vT6f+v+/NqQIUPIyckhNja2SPtnn32GYRglfsoDeO655/D29i7XNrZs2cKZM2ew2Wz069ev2HSr1Vqs7eoxL+Tp6UmNGjf3slq3bh25ubkMHTq0yL4IMGHCBG6//XbWrl1b4vXN6+kzQFxcHBkZGURHR3PvvfcWmebv78+gQYM4evQox48fd7b7+vri6upabF2dOnXi3nvvZfv27UXaly1bBsCUKVNKvP5V0ti2bt2aAQMGFGl79NFHcXFx4dtvvy1X35YvX05ubi4vv/xyse2GhYVhs9nYsmULFy9eLLbs888/X+RI08/Pj9atW5ORkcGzzz5bZH0eHh7YbDbS0tKKnOJbvXo1AM888ww+Pj7OdhcXF1544QVq1KjhvGRwo3Jycli3bh116tRh4sSJWCwW57S7776bYcOGkZubS1xcXLFlK2KMK0K1PB23e/duTp8+TadOnYrsLH379uWNN94gNjb2mqdMWrRoUaytcB1Xn5L49bSrz4UfPXoUgNDQ0GLzu7i40LZtW86cOcPRo0dv+iKju7s7d911V7H2whd2RkbGTa0fftv+lKR///7MnTuXFStWMHLkSOCX05CxsbF4enrSq1evEpdr1apVubdx8OBBADp37lzmvL1792bp0qWMGTOGiIgIOnToQOvWrbnzzjvLvb3SlDbenp6e3H///ezbt48TJ04QGBhYZPr19Bn+X7+/++67Eq/ZnTp1CvjlmlFhSBmGwbp164iNjeW7774jIyOD/Px85zK/fl0dPHgQi8XCQw89VO66Snod1qpVi/r165d7ny7s2969ezl06FCx6ampqeTn53Pq1Kli2ytp+4U32ZQ2zW6307hxY6D03+M999yD1WolOTmZzMzMG75J5uTJk1y+fJnWrVtTr169YtNDQ0NZuHAhx44dKzatIsa4IlTLECq8TvDrlK9Xrx42m40NGzaQkJBAz549iy1b0s5Qs2bNMqfl5eU52wovHF/96edqhe0lXWC+XiVdF4BfwgEo8uZwo37L/pSkbt26PPLII3z66afs3r3beR7+7NmzPPnkk9f8/tG16i1JYe3luVOtVatW/OMf/+CDDz5gw4YNrF27FvjljWXs2LH07du33NstrZayxrukN4rr6TNAeno6QKlnBwAuXbrk/P/s2bNZsmQJPj4+zg96hUeFhde5rpaZmYmnp2eJR47XUtp+fa0bAH6tsG9/+9vfSp3v6r4VKum1Xviaqlu37jWnXe/7wE8//URGRsYNh9DN7CsVMcYVodqF0Pnz5513wE2cOJGJEyeWON9nn31WYghVhMId6uzZsyVOL2y/escrPIzOy8tz7tCFfstPJSW5kf5UtCFDhvDpp5+yYsUKQkNDnR80Bg8efM1lrj41UZbC2ku6Y6okDzzwgPMOp8OHD7Nt2zaWLVvGpEmT8Pb2Lnah/XoU1nLu3Dnuu+++YtNLG+/r6fPV61i7dm2xo6qSpKam8ve//x1/f3+WL19e7A35888/L3Eb6enpXLly5bqC6GYV1vb111+XGByV7erfY0lHyRXxurl6GyX5LV6bN6vaXROKjY0lNzeXoKAgBg4cWOI/b29vdu7cSVJSUqXUUHg3S0m3Fefl5fHVV18BcP/99zvbPT09Afj555+LLXP48OEKqavwqO16j45upD8VLTAwkNatW7Nx40a++eYbdu7cSdu2bYtdM7lRhU8qSExMvK7lXF1dad26Nc899xwvv/wy8Ms1tJtRON6Ft3BfLSMjg2PHjnHbbbdVSN+Dg4OBX96oyyMpKYmCggI6duxY7I3dbreTnJxcbJmQkBAMw2Dbtm03Xe/1KPydFu6fv7XSfo8//vgjdrudJk2aFDkiqVGjxnW9Pu+55x7c3Nycp0V/rXDblfnavFnVLoQKTytMnz6dmTNnlvhv8ODBGIbBqlWrKqWG7t27U69ePdavX+88L11oyZIlJCcn06FDhyLXT1q2bAlQ7ELlrl27WL9+fYXU5eHhgcViKTHoSnMj/akMQ4YMITc3l3HjxpV6Q8KN6Nq1K40bN2bz5s0lfpq/+prf/v37uXLlSrF5UlNTgZJvWLgejzzyCLVq1WLZsmX8+OOPRabNnz+fixcv8sgjj5R4c8D1GjBgAB4eHrz33nslXowuKCgo8iZaeL3j66+/LvJmmZWVxSuvvFLkdFShwu+MvfHGGyUeaZb36PN6PfHEE9SqVYvZs2eXeINOTk5OpQbUo48+Cvzy6LDz58872/Pz83nzzTcpKChg4MCBRZapV68e58+fL3H/Komrqyv9+vUjKyuL+fPnF5l2+vRp/v73v1OrVi369+9/k72pPNXqdNyePXs4deoU/v7+pV6gHThwIB988AGrV69m3LhxxU5/3azbb7+dmTNnMn78eIYOHUrPnj2d36vZvn07Pj4+Rb5cCb/ssH/7299YtGgR3333Hc2aNePUqVNs27aN8PBwNmzYUCF1BQcH89VXXzFp0iTuueceatSogc1mK/VUzI30pzL07NmT2bNn43A48PLyokePHhW2bldXV+bPn8+oUaOYNGkSK1asIDg4mOzsbE6cOMGuXbucF5oXL17M7t27adOmDU2aNKFOnTocP36cxMREPD09Sz1FWB5NmjThxRdfZMaMGURFRdGrVy+8vb3Zt28fBw4coGnTpjz//PMV0W28vLxYsGABY8aM4bHHHiMsLIx7770Xi8WC3W7nwIEDpKenOy/s+/j40KdPH9avX09kZCQdO3YkMzOTnTt34urqSvPmzYtdBO/UqRPPPPMMCxcupFevXs7vCZ07d46vv/6akJCQSnmWYbNmzZg5cyYvv/wyffv25aGHHuLuu+8mLy+Pn376ia+//hovLy++/PLLCt82/HL32VNPPcXixYvp27cvERERuLm5sW3bNv7zn//w4IMPMmrUqCLLhIWFOb+v1qZNG1xdXQkMDHQ+U64kkyZN4quvvmLZsmUcOnSI9u3bO78nlJWVxdSpUyvkO4mVpVqFUOFR0KBBg0qdr0mTJnTo0IEdO3awZcsWwsPDK7yW7t2788knn7Bo0SK2b9/OxYsXueOOO/jDH/5Q7BZP+OW26mXLljFnzhz27dvHvn37aNGiBR9++CHJyckVEkIAc+bMYfbs2Wzfvp3169djGAZWq7XM6wHX25/KUPipb8mSJURFRVXIkcDVWrZsSVxcHDExMSQmJnLgwAFuv/127rzzTqKjo53zPf7443h6evLNN984jwh8fX15/PHHGTFihPNo4WY88cQT3HXXXXz44YfEx8dz+fJlGjZsyKhRo/jTn/50zYvKNyIsLIx169bx4Ycfsn37dr766itq1apFgwYNCA0NJSIiosj8M2fOxM/Pjy+++IJ//OMfeHt7Y7PZiI6OLjJOVxs/fjwPPPAAS5cu5d///jeXLl2ifv36tGjRolI/pffv35/AwEA++ugj9uzZw/bt26lTpw4NGjQgIiLimndWVpTJkydz//33s2zZMuLi4sjLy+POO+9k/PjxjBw5stg+/Mwzz5CRkcGWLVvYv38/+fn5REVFlRpC9erVY8WKFSxatIiNGzfy0UcfUbt2bVq1asWoUaPo1KlTpfbxZlkM46pnu4j8zg0bNox9+/bx5Zdfcvfdd5tdjojcpGp3TUiqr2+//Za9e/fSqVMnBZBINVGtTsdJ9fTJJ5/gcDhYs2YNNWrUuOYpHxGpesp1Ou7jjz92/l0ef39/Zs+eTUpKChMnTiQ9PZ2goCDmzJmDq6srOTk5/PnPf+bIkSPUq1ePd955hyZNmvwWfZFqymazYbfb8fPzY+zYsSU+VkdEqqYyQ8jhcDBkyBC++OILateuzXPPPUeXLl3YunUrPXr0oE+fPrz66qsEBgby+OOP849//IPvv/+eGTNmsH79ejZu3Oj8y6YiIiJXK9c1ofz8fK5cuUJeXh5XrlzBx8eH3bt3O++aiYqKcn5Bb/PmzURFRQG/PPJ/165d6N4HEREpSZkh5Ovry8iRI+natSudOnWibt26BAUF4eHh4fx+jdVqdX7hzOFw0LBhQ+CXZxC5u7tf84++FcrLu/nnm4mISNVT5o0JFy5cICEhgYSEBNzd3Xnuuecq/PEbaWnFHyAoIiLVg4/PtZ9dV+aR0M6dO2nSpAne3t7UqlWLHj16sH//fjIyMpyP6LDb7c4vK/r6+jofC5OXl0dmZiZeXl4V0Q8REalmygyhRo0a8c0333D58mUMw2DXrl3ce++9tG/f3vkt/tjYWOc3em02m/OPj23YsIHQ0NDrfrKviIjcGsp1i/aCBQv44osvcHFxoXnz5sycOROHw8GECRO4cOECzZs3Z+7cubi6upKdnc3kyZM5duwYnp6evPPOO2U+t+js2cr5OzQiImK+0k7H/S4e26MQEhGpvm7qmpCIiEhlUQiJiIhpFEIiImIahZCIiJhGT9EW+Z3aN0lPCy+vtm8tMLsEuUE6EhIREdMohERExDQKIRERMY1CSERETKMQEhER0yiERETENAohERExjUJIRERMoxASERHTKIRERMQ0CiERETGNQkhEREyjEBIREdMohERExDRl/imHEydOMGHCBOfPSUlJREdHExkZyYQJEzhz5gyNGzdm3rx5eHp6YhgGM2fOZOvWrdSuXZs33niDoKCgSu2EiIhUTWUeCTVt2pS1a9eydu1a1qxZg5ubG+Hh4cTExBAWFkZ8fDxhYWHExMQAkJiYyKlTp4iPj+e1115j+vTpld0HERGpoq7rdNyuXbvw8/OjcePGJCQkEBkZCUBkZCSbNm0CcLZbLBZCQkLIyMggJSWl4isXEZEq77r+sur69evp27cvAKmpqTRo0AAAHx8fUlNTAXA4HFitVucyVqsVh8PhnLckXl51cHGped3Fi4gA+Pi4m12C3KByh1BOTg6bN29m0qRJxaZZLBYsFssNF5GWdumGlxUROXs20+wSpBSlfUgo9+m4xMREgoKCuOOOOwCoX7++8zRbSkoK3t7eAPj6+mK3253L2e12fH19b6hwERGp3sodQuvXr6dPnz7On202G3FxcQDExcXRrVu3Iu2GYXDw4EHc3d1LPRUnIiK3rnKF0KVLl9i5cyc9evRwto0ePZodO3bQo0cPdu7cyejRowHo0qULfn5+hIeHM3XqVKZNm1Y5lYuISJVnMQzDMLsInc8VKW7fpGizS6gy2r61wOwSpBQVck1IRESkoimERETENAohERExjUJIRERMoxASERHTKIRERMQ0CiERETGNQkhEREyjEBIREdMohERExDQKIRERMY1CSERETKMQEhER0yiERETENAohERExjUJIRERMoxASERHTKIRERMQ0CiERETFNuUIoIyOD6OhoevbsSa9evThw4ADp6emMGDGCHj16MGLECC5cuACAYRi8/vrrhIeH069fP44cOVKpHRARkaqrXCE0c+ZMHnroIb788kvWrl1Ls2bNiImJISwsjPj4eMLCwoiJiQEgMTGRU6dOER8fz2uvvcb06dMrs34REanCygyhzMxM9u3bx8CBAwFwdXXFw8ODhIQEIiMjAYiMjGTTpk0AznaLxUJISAgZGRmkpKRUYhdERKSqcilrhuTkZLy9vXnxxRf57rvvCAoK4uWXXyY1NZUGDRoA4OPjQ2pqKgAOhwOr1epc3mq14nA4nPOWxMurDi4uNW+2LyJyi/LxcTe7BLlBZYZQXl4eR48eZerUqQQHB/P66687T70VslgsWCyWGy4iLe3SDS8rInL2bKbZJUgpSvuQUObpOKvVitVqJTg4GICePXty9OhR6tev7zzNlpKSgre3NwC+vr7Y7Xbn8na7HV9f35vqgIiIVE9lhpCPjw9Wq5UTJ04AsGvXLpo1a4bNZiMuLg6AuLg4unXrBuBsNwyDgwcP4u7uXuqpOBERuXWVeToOYOrUqTz//PPk5ubi5+fH7NmzKSgoYPz48axatYpGjRoxb948ALp06cLWrVsJDw/Hzc2NWbNmVWoHRESk6rIYhmGYXYTO54oUt29StNklVBlt31pgdglSipu6JiQiIlJZynU6TkTkVvG/8740u4Qq4/8b3/Om16EjIRERMY1CSERETKMQEhER0+iakJRp8uevmF1ClfGXvq+bXYJIlaIjIRERMY1CSERETKMQEhER01SZa0LP/WWd2SVUKfMnP2J2CSIiZdKRkIiImEYhJCIiplEIiYiIaRRCIiJiGoWQiIiYRiEkIiKmUQiJiIhpFEIiImIahZCIiJimXE9MsNls3H777dSoUYOaNWuyZs0a0tPTmTBhAmfOnKFx48bMmzcPT09PDMNg5syZbN26ldq1a/PGG28QFBRU2f0QEZEqqNxHQkuWLGHt2rWsWbMGgJiYGMLCwoiPjycsLIyYmBgAEhMTOXXqFPHx8bz22mtMnz69UgoXEZGq74ZPxyUkJBAZGQlAZGQkmzZtKtJusVgICQkhIyODlJSUiqlWRESqlXI/wHTUqFFYLBYGDx7M4MGDSU1NpUGDBgD4+PiQmpoKgMPhwGq1OpezWq04HA7nvCXx8qqDi0vNG+2DlMDHx93sEm5JGndzaNzNURHjXq4QWr58Ob6+vqSmpjJixAiaNm1aZLrFYsFisdxwEWlpl254WSnZ2bOZZpdwS9K4m0Pjbo7yjntpYVWu03G+vr4A1K9fn/DwcL799lvq16/vPM2WkpKCt7e3c1673e5c1m63O5cXERG5WpkhdOnSJS5evOj8/44dO7jvvvuw2WzExcUBEBcXR7du3QCc7YZhcPDgQdzd3Us9FSciIreuMk/HpaamMmbMGADy8/Pp27cvnTt3pmXLlowfP55Vq1bRqFEj5s2bB0CXLl3YunUr4eHhuLm5MWvWrMrtgYiIVFllhpCfnx/r1hX/q6ZeXl4sWbKkWLvFYmHatGkVU52IiFRremKCiIiYRiEkIiKmUQiJiIhpFEIiImIahZCIiJhGISQiIqZRCImIiGkUQiIiYhqFkIiImEYhJCIiplEIiYiIaRRCIiJiGoWQiIiYRiEkIiKmUQiJiIhpFEIiImIahZCIiJhGISQiIqZRCImIiGnKHUL5+flERkby9NNPA5CUlMSgQYMIDw9n/Pjx5OTkAJCTk8P48eMJDw9n0KBBJCcnV07lIiJS5ZU7hJYuXUqzZs2cP8+dO5fhw4ezceNGPDw8WLVqFQArV67Ew8ODjRs3Mnz4cObOnVvxVYuISLVQrhCy2+38+9//ZuDAgQAYhsHu3buJiIgAICoqioSEBAA2b95MVFQUABEREezatQvDMCqjdhERqeJcyjPTrFmzmDx5MllZWQCkpaXh4eGBi8svi1utVhwOBwAOh4OGDRv+snIXF9zd3UlLS8Pb2/ua6/fyqoOLS82b6ogU5ePjbnYJtySNuzk07uaoiHEvM4S2bNmCt7c3LVq0YM+ePTe9wZKkpV2qlPXeys6ezTS7hFuSxt0cGndzlHfcSwurMkNo//79bN68mcTERLKzs7l48SIzZ84kIyODvLw8XFxcsNvt+Pr6AuDr68vPP/+M1WolLy+PzMxMvLy8ytklERG5lZR5TWjSpEkkJiayefNm3n77bUJDQ3nrrbdo3749GzZsACBEIeXhAAAQtklEQVQ2NhabzQaAzWYjNjYWgA0bNhAaGorFYqnELoiISFV1w98Tmjx5Mh999BHh4eGkp6czaNAgAAYOHEh6ejrh4eF89NFHPP/88xVWrIiIVC/lujGhUPv27Wnfvj0Afn5+ztuyr3bbbbexYMGCiqlORESqNT0xQURETKMQEhER0yiERETENAohERExjUJIRERMoxASERHTKIRERMQ0CiERETGNQkhEREyjEBIREdMohERExDQKIRERMY1CSERETKMQEhER0yiERETENAohERExjUJIRERMoxASERHTlPnnvbOzs3niiSfIyckhPz+fiIgIoqOjSUpKYuLEiaSnpxMUFMScOXNwdXUlJyeHP//5zxw5coR69erxzjvv0KRJk9+iLyIiUsWUeSTk6urKkiVLWLduHXFxcWzbto2DBw8yd+5chg8fzsaNG/Hw8GDVqlUArFy5Eg8PDzZu3Mjw4cOZO3dupXdCRESqpjJDyGKxcPvttwOQl5dHXl4eFouF3bt3ExERAUBUVBQJCQkAbN68maioKAAiIiLYtWsXhmFUVv0iIlKFleuaUH5+Pv3796dDhw506NABPz8/PDw8cHH55Wye1WrF4XAA4HA4aNiwIQAuLi64u7uTlpZWSeWLiEhVVuY1IYCaNWuydu1aMjIyGDNmDCdOnKjQIry86uDiUrNC13mr8/FxN7uEW5LG3Rwad3NUxLiXK4QKeXh40L59ew4ePEhGRgZ5eXm4uLhgt9vx9fUFwNfXl59//hmr1UpeXh6ZmZl4eXmVut60tEs33gMp0dmzmWaXcEvSuJtD426O8o57aWFV5um48+fPk5GRAcCVK1fYuXMnzZo1o3379mzYsAGA2NhYbDYbADabjdjYWAA2bNhAaGgoFoulXIWKiMitpcwjoZSUFKZMmUJ+fj6GYdCzZ0+6du3Kvffey4QJE5g3bx7Nmzdn0KBBAAwcOJDJkycTHh6Op6cn77zzTqV3QkREqqYyQygwMJC4uLhi7X5+fs7bsq922223sWDBgoqpTkREqjU9MUFEREyjEBIREdMohERExDQKIRERMY1CSERETKMQEhER0yiERETENAohERExjUJIRERMoxASERHTKIRERMQ0CiERETGNQkhEREyjEBIREdMohERExDQKIRERMY1CSERETKMQEhER0yiERETENGWG0M8//8ywYcPo3bs3ffr0YcmSJQCkp6czYsQIevTowYgRI7hw4QIAhmHw+uuvEx4eTr9+/Thy5Ejl9kBERKqsMkOoZs2aTJkyhS+++IIVK1bwySefcPz4cWJiYggLCyM+Pp6wsDBiYmIASExM5NSpU8THx/Paa68xffr0yu6DiIhUUWWGUIMGDQgKCgKgbt26NG3aFIfDQUJCApGRkQBERkayadMmAGe7xWIhJCSEjIwMUlJSKrELIiJSVV3XNaHk5GSOHTtGcHAwqampNGjQAAAfHx9SU1MBcDgcWK1W5zJWqxWHw1GBJYuISHXhUt4Zs7KyiI6O5qWXXqJu3bpFplksFiwWyw0X4eVVBxeXmje8vBTn4+Nudgm3JI27OTTu5qiIcS9XCOXm5hIdHU2/fv3o0aMHAPXr1yclJYUGDRqQkpKCt7c3AL6+vtjtdueydrsdX1/fUteflnbpRuuXazh7NtPsEm5JGndzaNzNUd5xLy2syjwdZxgGL7/8Mk2bNmXEiBHOdpvNRlxcHABxcXF069atSLthGBw8eBB3d3fnaTsREZGrlXkk9PXXX7N27Vr8/f3p378/ABMnTmT06NGMHz+eVatW0ahRI+bNmwdAly5d2Lp1K+Hh4bi5uTFr1qzK7YGIiFRZZYZQmzZt+P7770ucVvidoatZLBamTZt285WJiEi1pycmiIiIaRRCIiJiGoWQiIiYRiEkIiKmUQiJiIhpFEIiImIahZCIiJhGISQiIqZRCImIiGkUQiIiYhqFkIiImEYhJCIiplEIiYiIaRRCIiJiGoWQiIiYRiEkIiKmUQiJiIhpFEIiImIahZCIiJimzBB68cUXCQsLo2/fvs629PR0RowYQY8ePRgxYgQXLlwAwDAMXn/9dcLDw+nXrx9HjhypvMpFRKTKKzOEBgwYwOLFi4u0xcTEEBYWRnx8PGFhYcTExACQmJjIqVOniI+P57XXXmP69OmVUrSIiFQPZYZQ27Zt8fT0LNKWkJBAZGQkAJGRkWzatKlIu8ViISQkhIyMDFJSUiqhbBERqQ5u6JpQamoqDRo0AMDHx4fU1FQAHA4HVqvVOZ/VasXhcFRAmSIiUh253OwKLBYLFovlptbh5VUHF5eaN1uKXMXHx93sEm5JGndzaNzNURHjfkMhVL9+fVJSUmjQoAEpKSl4e3sD4Ovri91ud85nt9vx9fUtc31paZdupAwpxdmzmWaXcEvSuJtD426O8o57aWF1Q6fjbDYbcXFxAMTFxdGtW7ci7YZhcPDgQdzd3Z2n7URERH6tzCOhiRMnsnfvXtLS0ujcuTPjxo1j9OjRjB8/nlWrVtGoUSPmzZsHQJcuXdi6dSvh4eG4ubkxa9asSu+AiIhUXWWG0Ntvv11i+5IlS4q1WSwWpk2bdvNViYjILUFPTBAREdMohERExDQKIRERMY1CSERETKMQEhER0yiERETENAohERExjUJIRERMoxASERHTKIRERMQ0CiERETGNQkhEREyjEBIREdMohERExDQKIRERMY1CSERETKMQEhER0yiERETENAohERExTaWEUGJiIhEREYSHhxMTE1MZmxARkWqgwkMoPz+fGTNmsHjxYtavX8/nn3/O8ePHK3ozIiJSDVR4CH377bfcdddd+Pn54erqSp8+fUhISKjozYiISDVQ4SHkcDiwWq3On319fXE4HBW9GRERqQZczC4AwMfHvcx5PpnzxG9QiZTk4xHzzS7hltR76Udml3BLemnmILNLuKVU+JGQr68vdrvd+bPD4cDX17eiNyMiItVAhYdQy5YtOXXqFElJSeTk5LB+/XpsNltFb0ZERKqBCj8d5+LiwquvvspTTz1Ffn4+jz76KPfdd19Fb0ZERKoBi2EYhtlFiIjIrUlPTBAREdMohERExDQKIfldGjZsGIcOHaq09e/Zs4enn3660tb/e5ecnEzfvn3NLkNuUnX4PSqEKlFeXp7ZJfyuGYZBQUGB2WWIiIl+F19W/b149tlnsdvtZGdn88c//pHBgwfzwAMP8Mc//pEtW7ZQu3Zt/vrXv3LHHXdw+vRpnn/+eS5fvozNZmPp0qUcOHCAPXv2MH/+fDw8PDh58iS9e/fG09OT4cOHA/DOO+/g7e3Nk08+aW5nTZKcnMyoUaMIDg7myJEjPPXUU3z66afk5OTg5+fH7Nmzuf3224ssM23aNA4dOkR2djYRERFER0eTmZnJwIEDWbhwIU2bNmXixImEhoby2GOPsX37dt59991i60xMTGTWrFm4ubnx4IMPmjQCvx/5+fm88sorHDhwAF9fX/7617+ybt06VqxYQW5uLnfddRdz5szBzc2NKVOm4OrqyuHDh8nKymLKlCl07dqVNWvWsHHjRi5evIjD4eCRRx5h7NixzJ8/X/v9dbh06RLjx4/HbrdTUFDAs88+y4kTJ9iyZQvZ2dk88MADzJgxA4vFwuHDh3nppZcA6Nixo8mVVwBDnNLS0gzDMIzLly8bffr0Mc6fP2/4+/sbCQkJhmEYxptvvmm8//77hmEYxujRo41//vOfhmEYxieffGKEhIQYhmEYu3fvNoKDg43Tp08bhmEYSUlJRmRkpGEYhpGfn29069bNOH/+/G/ar9+TpKQkIyAgwDhw4ICRmppqPP7440ZWVpZhGIaxaNEi49133zUMwzCGDh1qfPvtt4Zh/L/fS15enjF06FDj2LFjhmEYxvbt243HHnvM+Pzzz42RI0cahmFcc51XrlwxOnfubJw8edIoKCgwoqOjjdGjR/+mff89SUpKMpo3b24cPXrUMAzDiI6ONuLi4orsm2+//baxdOlSwzAM44UXXjBGjhxp5OfnGydPnjQeeugh48qVK8bq1auNjh07GufPn3e+br799lvt99fpyy+/NF5++WXnzxkZGc793jAM4/nnn3e+D/Xt29fYu3evYRiG8cYbbxh9+vT5bYutYDoSusrf//53Nm7cCMDPP//Mjz/+SK1atejatSsALVq0YMeOHQAcPHiQ999/H4B+/foxZ84c53patmyJn58fAE2aNKFevXocPXqUc+fOcf/99+Pl5fVbdut3p1GjRoSEhLBlyxaOHz/OkCFDAMjNzSUkJKTY/P/617/47LPPyMvL4+zZs/zwww8EBgbSsWNHvvzyS2bMmMHatWsB+Oabb0pc54kTJ2jSpAl33303AI888gifffbZb9Ph36kmTZrQvHlzAIKCgjhz5gz//e9/mTdvHpmZmWRlZdGpUyfn/L169aJGjRrcfffd+Pn5ceLECQA6dOjg3KfDw8P5+uuvGT58uPb76+Dv78+bb77JX/7yF7p27UqbNm3YsGEDixcv5sqVK6Snp3PffffRpk0bMjMzadu2LQD9+/dn27ZtJld/cxRC/2fPnj3s3LmTFStW4ObmxrBhw8jOzqZWrVpYLBYAatSoQX5+fpnrqlOnTpGfBw0axJo1azh37hyPPvpopdRflRSOj2EYdOzYkbfffvua8yYlJfHhhx+yatUqPD09mTJlCtnZ2QAUFBTwww8/ULt2bS5cuIDVar3mOo8dO1Z5HaqiXF1dnf+vWbMm2dnZTJkyhb/+9a8EBgayZs0a9u7d65yn8HXw65+v1a79vvzuuece1qxZw9atW5k3bx6hoaF88sknrF69moYNG/Luu+869/vqRjcm/J/MzEw8PT1xc3Pjhx9+4ODBg6XOHxwcTHx8PADr168vdd7u3buzbds2Dh06VOST5a0uJCSE/fv38+OPPwK/nBc/efJkkXmysrJwc3PD3d2dc+fOkZiY6Jz28ccf06xZM9566y1efPFF51FPSets2rQpZ86c4fTp00DZv7NbVVZWFj4+PuTm5vLPf/6zyLQvv/ySgoICTp8+TVJSEvfccw8AO3bsID09nStXrrBp0yZat24NaL+/Hg6HAzc3N/r378+oUaM4evQoAF5eXmRlZbFhwwYAPDw8cHd356uvvgIo9juqinQk9H86d+7Mp59+Sq9evbjnnntKPC10tZdeeonJkyezcOFCHnroIerWrXvNeV1dXWnfvj0eHh7UrFmzokuvsry9vZk9ezYTJ04kJycHgPHjxzvf3AACAwO5//776dWrF1ar1fkGd+LECVauXMnKlSupW7cubdu2ZeHChURHR19znTNmzGD06NHOGxOysrJ++07/zj333HMMGjQIb29vgoODi4xRw4YNGThwIFlZWfzP//wPt912GwCtWrVi3LhxzhsTWrZsCWi/vx7/+c9/mDNnDjVq1MDFxYXp06ezadMm+vbtyx133OEcU4DZs2fz0ksvYbFYqsWNCXpszw26fPkytWvXxmKxOP+C7MKFC0uct6CggKioKObPn++8JiFSlUyZMoWHH36Ynj17Fmlfs2YNhw8f5tVXXy22jPZ7KQ8dCd2gI0eOMGPGDAzDwMPDg1mzZpU43/Hjx3n66acJDw/XC1FuGdrvpbx0JCQiIqbRjQkiImIahZCIiJhGISQiIqZRCImIiGkUQiIiYhqFkIiImOb/Bx31NxSjNcZxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the obtained counts\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "#fig.set_size_inches(10, 7)\n",
    "ax = sns.barplot(x=list(count.keys()), y=list(count.values()), ax=ax)\n",
    "ax.set_title('Amount of lyrics for each emotion',fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the emotion distribution looks alike the previous one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords deletion\n",
    "\n",
    "In one of our meetings for our project an interesting question was raised: does it really make sense to remove stopwords from lyrics? Would we have enough words after removing stopwords? Let's see what is the percentage change in the amount of words in our lyrics after removing stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:30:24.394271Z",
     "start_time": "2018-06-05T17:30:24.388931Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    tks = list(filter(lambda tk: not tk.is_stop, doc))\n",
    "    return spacy.tokens.Doc(nlp.vocab, words=[tk.text for tk in tks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:34:37.594028Z",
     "start_time": "2018-06-05T17:30:26.260594Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Lyric  Word_Count  \\\n",
      "0  /home/mario/dev/emotion-patterns-in-music-play...         161   \n",
      "1  /home/mario/dev/emotion-patterns-in-music-play...        1151   \n",
      "2  /home/mario/dev/emotion-patterns-in-music-play...         391   \n",
      "3  /home/mario/dev/emotion-patterns-in-music-play...         297   \n",
      "4  /home/mario/dev/emotion-patterns-in-music-play...         759   \n",
      "\n",
      "   Word_Count_After  Percentage_Change  \n",
      "0                85          47.204969  \n",
      "1               562          51.172893  \n",
      "2               245          37.340153  \n",
      "3               186          37.373737  \n",
      "4               417          45.059289  \n"
     ]
    }
   ],
   "source": [
    "paths = load_dataset_from_path(lyrics_path)['Lyric_Path'].as_matrix()\n",
    "\n",
    "# Build a dataframe with the following schema:\n",
    "# <Song, word_count, words_after_stopwords_removal, percentage_change>\n",
    "rows = list()\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        doc = nlp(f.read())\n",
    "        n_words_before = len(doc)\n",
    "        doc = remove_stopwords(doc)\n",
    "        n_words_after = len(doc)\n",
    "        perc = (n_words_before - n_words_after) / n_words_before * 100\n",
    "        row = (path, n_words_before, n_words_after, perc)\n",
    "        rows.append(row)\n",
    "\n",
    "# Create a dataframe with the found information\n",
    "df = pd.DataFrame(rows, columns=['Lyric', 'Word_Count', \n",
    "                  'Word_Count_After', 'Percentage_Change'])\n",
    "print(df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T16:07:36.012403Z",
     "start_time": "2018-05-27T16:07:35.904791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of change in lyrics after removing stopwords:\n",
      "                0     1\n",
      "0            < 25   225\n",
      "1  >= 25 and < 30    74\n",
      "2  >= 30 and < 40   495\n",
      "3  >= 40 and < 50  1009\n",
      "4  >= 50 and < 60   585\n",
      "5  >= 60 and < 75    59\n",
      "6           >= 75     0\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics\n",
    "percs = [ 25, 30, 40, 50, 60, 75 ]\n",
    "print('Percentage of change in lyrics after removing stopwords:')\n",
    "it = enumerate(percs)\n",
    "plt_data = list()\n",
    "for (i, perc) in it:\n",
    "    if i == 0: \n",
    "        count = len(df[df['Percentage_Change'] < perc])\n",
    "        #print(' - < {}:\\t\\t\\t{}'.format(perc, count))\n",
    "        plt_data.append(('< {}'.format(perc), count))\n",
    "    elif i == len(percs) - 1:\n",
    "        prev_p = percs[i-1]\n",
    "        count = len(df[(df['Percentage_Change'] >= prev_p) & (df['Percentage_Change'] < perc)])\n",
    "        #print(' - between {} and {}:\\t{}'.format(prev_p, perc, count))\n",
    "        plt_data.append(('>= {} and < {}'.format(prev_p, perc, count), count))\n",
    "        \n",
    "        count = len(df[df['Percentage_Change'] >= perc])\n",
    "        #print(' - >= {}:\\t\\t\\t{}'.format(perc, count))\n",
    "        plt_data.append(('>= {}'.format(perc), count))\n",
    "    else:\n",
    "        prev_p = percs[i-1]\n",
    "        count = len(df[(df['Percentage_Change'] >= prev_p) & (df['Percentage_Change'] < perc)])\n",
    "        #print(' - between {} and {}:\\t{}'.format(prev_p, perc, count))\n",
    "        plt_data.append(('>= {} and < {}'.format(prev_p, perc, count), count))\n",
    "\n",
    "pltDf = pd.DataFrame(plt_data)\n",
    "print(pltDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T16:13:33.540608Z",
     "start_time": "2018-05-27T16:13:33.215993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 7)\n",
    "ax = sns.barplot(pltDf[0], pltDf[1], ax=ax)\n",
    "plt.title('Percentage change in number of words after stopwords pruning', weight='bold', fontsize=20)\n",
    "plt.xticks(rotation=30)\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above plot, there is a minimum (but still relevant to our purpose) impact on lyrics, which still have a large number of words to be analyzed. Very few songs reduced their size of more then the 50% and for those songs we may probably have some issues. However this is a really small amount of lyrics that does not prevent us from saying that stopword deletion can be used as a good pre-processing technique for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function\n",
    "Now that we have defined which kind of preprocessing is good to apply to the text of our songs, let's define a function which will be used throughout the notebook to perform lyrics preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:34:37.609903Z",
     "start_time": "2018-06-05T17:34:37.598943Z"
    }
   },
   "outputs": [],
   "source": [
    "def doc_preprocess(doc):\n",
    "    d = remove_stopwords(doc)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "We will now move on trying to extract some interesting feature from our dataset. \n",
    "\n",
    "An important thing we need to state before starting analyzing this section, is that, by using spaCy's language model, each document has a word vector of length 300. SpaCy's language model has vectors of length 300 because it uses [GloVe](https://nlp.stanford.edu/projects/glove/) to train its models which is based on word2vec and assigns vectors of length 300 to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "As we already said, the language model we are usin assigns word vectors of length 300 to documents and words. However this dimension may be too huge for our problem. In fact it is possible that, by performing some dimensionality reduction, we could still obtain good results from our model.\n",
    "\n",
    "First of all we will perform PCA with 2 components, just because we want to visualize the effect of dimensionality reduction on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T16:11:12.935960Z",
     "start_time": "2018-05-27T16:07:36.379916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read dataset into pandas dataframe\n",
    "pca_dataset = load_dataset_from_path(lyrics_path)\n",
    "\n",
    "# Turn emotion labels into numerical features\n",
    "mapping = dict(zip(emotion_labels, range(len(emotion_labels)))) # { 'happy' : 0, 'sad': 1, 'relaxed': 2, 'angry': 3 }\n",
    "pca_dataset['Emotion'] = pca_dataset['Emotion'].map(mapping)\n",
    "\n",
    "# Make the dataset to follow this scema:\n",
    "# <Lyric_Path, Emotion, Vector, Vector_Norm>\n",
    "rows = list()\n",
    "pca_dataset['Vector'] = np.nan\n",
    "for index, row in pca_dataset.iterrows():\n",
    "    lyric = row['Lyric_Path']\n",
    "    emotion = row['Emotion']\n",
    "    with open(lyric, 'r') as lyric_file: \n",
    "        doc = nlp(lyric_file.read())\n",
    "        doc = doc_preprocess(doc)\n",
    "        # Consider only those vectors with the same length\n",
    "        # This will be avoided when we will have proper PCA\n",
    "        if len(doc.vector) == 300:\n",
    "            rows.append((\n",
    "                emotion, doc.vector\n",
    "            ))\n",
    "pca_dataset = pd.DataFrame(rows, columns=['Emotion', 'Vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T16:11:12.985612Z",
     "start_time": "2018-05-27T16:11:12.938350Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_X_vect = pca_dataset['Vector'].as_matrix()\n",
    "pca_X_vect = np.array([np.array(x) for x in pca_X_vect])\n",
    "pca_y = pca_dataset['Emotion'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T16:11:13.010359Z",
     "start_time": "2018-05-27T16:11:12.994333Z"
    }
   },
   "outputs": [],
   "source": [
    "def pca(n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(pca_X_vect)\n",
    "    return pca.transform(pca_X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T16:11:13.155296Z",
     "start_time": "2018-05-27T16:11:13.012804Z"
    }
   },
   "outputs": [],
   "source": [
    "components = pca(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T16:11:13.188786Z",
     "start_time": "2018-05-27T16:11:13.167488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put reduced components and labels together for plotting\n",
    "comps = list(zip(components, pca_y))\n",
    "pca_df = pd.DataFrame(comps, columns=['Vector', 'Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T16:12:56.696940Z",
     "start_time": "2018-05-27T16:12:56.328640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot points for each class\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 7)\n",
    "for i in range(4):\n",
    "    emo_df = pca_df[pca_df['Emotion'] == i]\n",
    "    x = emo_df['Vector'].as_matrix()\n",
    "    x = np.array([np.array(k) for k in x])\n",
    "    plt.scatter(x[:,0], x[:,1])\n",
    "plt.legend(emotion_labels)\n",
    "plt.title('PCA with 2 components', weight='bold', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we were not expecting to obtain fantastic results. However we believed that doing a PCA with two components could have been great, especially because of the arousal-valence based songs classification model which was used in MoodyLyrics. \n",
    "\n",
    "We will abandon PCA for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Function\n",
    "Now that we have defined which kind of feature engineering we want to, let's define a function which will be used throughout the notebook to perform the desired operations on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:34:37.617806Z",
     "start_time": "2018-06-05T17:34:37.612877Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: we still have to properly define which feature \n",
    "# engineering we want to perform\n",
    "def feature_engineer(dataset):\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers on Lyrics Content\n",
    "In the next sections we will provide the implementation of several classification algorithms we used. Those classifiers work on the 300-shaped vector assigned by our language model to the lyrics of each song.\n",
    "\n",
    "First, let's prepare the data for those classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:34:37.703682Z",
     "start_time": "2018-06-05T17:34:37.624635Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(lyrics_path):\n",
    "    # Read dataset into pandas dataframe\n",
    "    dataset = load_dataset_from_path(lyrics_path)\n",
    "\n",
    "    # Turn emotion labels into numerical features\n",
    "    mapping = dict(zip(emotion_labels, range(len(emotion_labels)))) # { 'happy' : 0, 'sad': 1, 'relaxed': 2, 'angry': 3 }\n",
    "    dataset['Emotion'] = dataset['Emotion'].map(mapping)\n",
    "\n",
    "    # Make the dataset to follow this scema:\n",
    "    # <Lyric_Path, Emotion, Vector, Vector_Norm>\n",
    "    rows = list()\n",
    "    dataset['Vector'] = np.nan\n",
    "    dataset['Vector_Norm'] = np.nan\n",
    "    for index, row in dataset.iterrows():\n",
    "        lyric = row['Lyric_Path']\n",
    "        emotion = row['Emotion']\n",
    "        with open(lyric, 'r') as lyric_file: \n",
    "            doc = nlp(lyric_file.read())\n",
    "            doc = doc_preprocess(doc) # Preprocessing step\n",
    "            # Consider only those vectors with the same length\n",
    "            # This will be avoided when we will have proper PCA\n",
    "            if len(doc.vector) == 300:\n",
    "                rows.append((\n",
    "                    lyric,\n",
    "                    emotion, doc.vector,\n",
    "                    doc.vector_norm\n",
    "                ))\n",
    "    dataset = pd.DataFrame(rows, columns=['Lyric_Path', 'Emotion', 'Vector', 'Vector_Norm'])\n",
    "    #dataset = feature_engineer(dataset) # Do feature engineering\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:05:18.855496Z",
     "start_time": "2018-06-05T18:00:30.072200Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric_Path</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Vector</th>\n",
       "      <th>Vector_Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.05747938, 0.18501352, -0.14104907, -0.04625...</td>\n",
       "      <td>2.859903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.013439501, 0.17781271, -0.1585186, -0.0208...</td>\n",
       "      <td>2.633056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.030751104, 0.31046307, -0.15142494, -0.013...</td>\n",
       "      <td>3.019833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.062542394, 0.08743761, -0.08960157, -0.0961...</td>\n",
       "      <td>2.446515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/mario/dev/emotion-patterns-in-music-play...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.0777918, 0.1709495, -0.16516578, -0.032506...</td>\n",
       "      <td>2.737937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Lyric_Path  Emotion  \\\n",
       "0  /home/mario/dev/emotion-patterns-in-music-play...        3   \n",
       "1  /home/mario/dev/emotion-patterns-in-music-play...        0   \n",
       "2  /home/mario/dev/emotion-patterns-in-music-play...        0   \n",
       "3  /home/mario/dev/emotion-patterns-in-music-play...        1   \n",
       "4  /home/mario/dev/emotion-patterns-in-music-play...        3   \n",
       "\n",
       "                                              Vector  Vector_Norm  \n",
       "0  [0.05747938, 0.18501352, -0.14104907, -0.04625...     2.859903  \n",
       "1  [-0.013439501, 0.17781271, -0.1585186, -0.0208...     2.633056  \n",
       "2  [-0.030751104, 0.31046307, -0.15142494, -0.013...     3.019833  \n",
       "3  [0.062542394, 0.08743761, -0.08960157, -0.0961...     2.446515  \n",
       "4  [-0.0777918, 0.1709495, -0.16516578, -0.032506...     2.737937  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(lyrics_path)\n",
    "# Show some dataset's values\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:39:15.633335Z",
     "start_time": "2018-06-05T17:39:15.592241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare array for sklearn classifiers\n",
    "X_vect = dataset['Vector'].as_matrix().T\n",
    "X_vect = np.array([np.array(x) for x in X_vect])\n",
    "X_norm = dataset['Vector_Norm'].as_matrix()\n",
    "y = dataset['Emotion'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:39:15.641054Z",
     "start_time": "2018-06-05T17:39:15.636427Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2447, 300)\n",
      "(2447,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_vect))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide a function to perform some additional tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:48:51.851065Z",
     "start_time": "2018-04-16T09:48:51.694047Z"
    }
   },
   "outputs": [],
   "source": [
    "import lyricwikia\n",
    "\n",
    "def extra_test(classifier):\n",
    "    songs = [\n",
    "        ('Bobby McFerrin', 'Don\\'t Worry, Be Happy', 'happy'),\n",
    "        ('Queen', 'Don\\'t Stop me Now', 'happy'),\n",
    "        ('Pharrell Williams', 'Happy', 'happy'),\n",
    "        ('The Monkees', 'I\\'m a believer', 'happy'),\n",
    "        \n",
    "        ('R.E.M.', 'Everybody Hurts', 'sad'),\n",
    "        ('Adele', 'Someone Like You', 'sad'),\n",
    "        ('Pink Floyd', 'Wish you were here', 'sad'),\n",
    "        ('Johnny Cash', 'Hurt', 'sad'),\n",
    "        ('Nirvana', 'Smells like teen spirit', 'sad'),\n",
    "        \n",
    "        ('Rage Against the Machine', 'Killing in the name', 'angry'),\n",
    "        ('Kanye West', 'Stronger', 'angry'),\n",
    "        ('Smash Mouth', 'All Star', 'angry'),\n",
    "        ('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry'),\n",
    "        \n",
    "        ('Blur', 'Song 2', 'relaxed') # I'm not quite confident about this labeling\n",
    "    ]\n",
    "\n",
    "    count_correct = 0\n",
    "    for s in songs:\n",
    "        # Download the lyric\n",
    "        lyric = lyricwikia.get_lyrics(s[0], s[1])\n",
    "        # Convert lyric to spacy Doc and preproces it\n",
    "        doc = nlp(lyric)\n",
    "        doc = doc_preprocess(doc)\n",
    "        # Classify\n",
    "        vect = np.array([doc.vector])\n",
    "        label = classifier.predict(vect)\n",
    "        if emotion_labels[label[0]] == s[2]:\n",
    "            count_correct += 1\n",
    "        print(s, '->', emotion_labels[label[0]], '(was supposed to be {})'.format(s[2]))\n",
    "    print('We got {} predictions our of {} songs'.format(count_correct, len(songs)))\n",
    "    print('Accuracy: %0.2f' % (count_correct / len(songs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised K-Means\n",
    "This is the first and the easieast classifier idea we came up. We used this idea just to verify what we could do and if we were on the right path. \n",
    "\n",
    "The name we gave to this classifier could sound ambiguous but we believe that, once its functioning is explained, the name will sound more decent.\n",
    "\n",
    "The idea behind this classifier is quite simple. The first thing we do is to compute the centroids for each emotion class. Then, to classify a lyric, we simply compare its word vector norm to the 4 centroids. At the end we will assign our lyric with the label of the closest \"cluster\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:40:02.719222Z",
     "start_time": "2018-04-11T09:39:36.344927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31\n"
     ]
    }
   ],
   "source": [
    "from classifier.LyricsSupervisedKMeans import LyricsSupervisedKMeans\n",
    "\n",
    "clf = LyricsSupervisedKMeans()\n",
    "clf.set_lang(nlp)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "trainDf, testDf = split_train_validation(dataset)\n",
    "\n",
    "X_sup_kmeans_train = trainDf['Vector_Norm'].as_matrix().T\n",
    "X_sup_kmeans_train = np.array([np.array(x) for x in X_sup_kmeans_train])\n",
    "\n",
    "y_sup_kmeans_train = trainDf['Emotion'].as_matrix()\n",
    "\n",
    "# Train our model\n",
    "clf.train(X_sup_kmeans_train, y_sup_kmeans_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "acc = clf.score(testDf)\n",
    "print('Accuracy: %0.2f' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we did not even implement cross-validation. We just split the dataset into a training set (90%) and a test set (10%) and we evaluated the classification accuracy on that test set. We decided not to implement cross-validation for this classifier as we believe it is not worth to spend time on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were expecting poor results from this classifier but at least it served to the purpose of giving us a hint of which direction to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbour\n",
    "We will now build a k-NN model which is basically a generalization of what we called \"Supervised K-Means\". We will evaluate our model for several different k values. The parameters passed to our model are quite self-explicative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:37:07.317982Z",
     "start_time": "2018-04-11T09:36:41.271097Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1: 0.81 (+/- 0.04)\n",
      "Accuracy for k=3: 0.81 (+/- 0.05)\n",
      "Accuracy for k=5: 0.81 (+/- 0.05)\n",
      "Accuracy for k=7: 0.81 (+/- 0.05)\n",
      "Accuracy for k=9: 0.82 (+/- 0.05)\n",
      "Accuracy for k=11: 0.81 (+/- 0.04)\n",
      "Accuracy for k=13: 0.82 (+/- 0.04)\n",
      "Accuracy for k=15: 0.81 (+/- 0.04)\n",
      "Accuracy for k=17: 0.81 (+/- 0.06)\n",
      "Accuracy for k=19: 0.81 (+/- 0.06)\n",
      "Accuracy for k=21: 0.81 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ks = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "\n",
    "for k in ks:\n",
    "    # Build model\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, algorithm='auto', \n",
    "                           metric='euclidean', n_jobs=-1)\n",
    "    # Evaluate accuracy\n",
    "    scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "    print('Accuracy for k=%d: %0.2f (+/- %0.2f)' % (k, scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-NN algorithm produced unexpectedly good results. As we can see the algorithm never reaches an accuracy value higher than 82%. From our observations we can conclude that `k=9` could be an good parameter for that model.\n",
    "\n",
    "An interesting thing we noticed while running our experiments is that, when we read our dataset without preprocessing our lyrics, the accuracy was around 10% lower.\n",
    "\n",
    "Let's do some more tests and see if this classifier is really that good. We will now try to classify some very popular songs which were labelled according to our personal tastes and to [IBM Tone Analyzer](https://tone-analyzer-demo.ng.bluemix.net/?cm_mc_uid=56761301373215210511228&cm_mc_sid_50200000=91001461523311389617&cm_mc_sid_52640000=29317781523311389622) ones. Those songs we are trying to classify are not available in MoodyLyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T09:38:22.907601Z",
     "start_time": "2018-04-11T09:38:13.799310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> sad (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> angry (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> sad (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> angry (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> angry (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> sad (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> relaxed (was supposed to be relaxed)\n",
      "We got 9 predictions our of 14 songs\n",
      "Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=9, algorithm='auto', \n",
    "                           metric='euclidean', n_jobs=-1)\n",
    "clf.fit(X_vect, y)\n",
    "extra_test(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results are encouraging and make us believe that we can certainly do even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "We will now train a Support Vector Machine using the algorithm available in sklearn. For this first experiment we did not change the default parameters of the SVM algorithm provided in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:15:16.369778Z",
     "start_time": "2018-04-09T21:14:50.158328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build model\n",
    "clf = SVC()\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can improve our model's accuracy just my properly tuning our parameters. We will perform a [Grid Search](http://scikit-learn.org/stable/modules/grid_search.html) to properly tune our model using a cross validation approach to evaluating accuracy. We will operate on both the kernel function and the penalty parameter C.\n",
    "\n",
    "Beware that the below cell takes quite a long time to run as the `SVC` train function complexity is more than quadratic and, having to run it several times, is quite expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:22:15.533515Z",
     "start_time": "2018-04-09T21:15:21.742694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 29 candidates, totalling 290 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 290 out of 290 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'C': 10, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['linear'], 'C': [ 0.01, 0.05, 1, 10, 100 ]},\n",
    "    { 'kernel': ['rbf', 'sigmoid'], 'C': [ 0.01, 0.05, 0.1, 0.3, 0.8, 1, 3, 10, 50, 100, 150, 200 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params = parameters_grid_search(SVC, params, X_vect, y, verbose=1)\n",
    "print('Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:22:26.843539Z",
     "start_time": "2018-04-09T21:22:20.992884Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm_best, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly we were not expecting the SVM model to be so good. Let's see how our optimal SVM performs on the extra test set as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T22:14:54.516665Z",
     "start_time": "2018-04-09T22:14:36.130397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> angry (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> angry (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> angry (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> angry (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> sad (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> sad (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> happy (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 3 predictions our of 14 songs\n",
      "Accuracy: 0.21\n"
     ]
    }
   ],
   "source": [
    "extra_test(svm_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not properly what we would expect from a predictor having 91% accuracy. Probably (you don't say?) we are overfitting. In fact, we would get better results even by using a random classifier instead of our \"optimal\" SVM.\n",
    "\n",
    "One interesting thing we can notice is that a \"Don't Worry, Be Happy\" is labelled as angry. However it is quite explicit from the title (and also from the lyrics) that the song is about happiness. This obeservation suggests us that we should probably be considering also the song's title when computing our word vectors.\n",
    "\n",
    "Let's now move on to some ensemble methods which are supposed to be even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function ([reference link](https://en.wikipedia.org/wiki/Gradient_boosting)).\n",
    "\n",
    "Since we already obtained great results using our SVM model, we will omit any grid search for parameters tuning on this model. In fact, a grid search on a Gradient Boosting Classifier would be quite expensive and would slow down our experiments a lot. Instead, some manual tuning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:26:35.695954Z",
     "start_time": "2018-04-09T21:22:32.508973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Build model\n",
    "clf = GradientBoostingClassifier(learning_rate=0.7, n_estimators=200)\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several attempts we did never succeed in obtaining higher accuracy values w.r.t. our SVM model. Therefore, given the slow training time for the GradientBoostingClassifier algorithm, we will keep using the SVM model.\n",
    "\n",
    "However, let's have a look on Gradient Boost performances on our \"extra\" test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T21:57:55.163898Z",
     "start_time": "2018-04-09T21:57:26.262125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.7, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_vect, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T22:16:54.795941Z",
     "start_time": "2018-04-09T22:16:43.952136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> angry (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> angry (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> angry (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> sad (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> angry (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> angry (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> angry (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> sad (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> angry (was supposed to be relaxed)\n",
      "We got 6 predictions our of 14 songs\n",
      "Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "extra_test(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thst's definitivelly better but we still need to improve a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Neural Network\n",
    "Additionally to what we have already done we will now try to tune, train and evaluate an artificial neural network model with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:39:15.649592Z",
     "start_time": "2018-06-05T17:39:15.643591Z"
    }
   },
   "outputs": [],
   "source": [
    "#1 Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:39:17.157835Z",
     "start_time": "2018-06-05T17:39:15.652280Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# y_nn should be a vector (len(X_vect),4), with a 1 in the right class\n",
    "from keras.utils import np_utils\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "y_nn = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:39:17.177878Z",
     "start_time": "2018-06-05T17:39:17.160420Z"
    }
   },
   "outputs": [],
   "source": [
    "#2 Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# we need to scale because we don't want one feature to predomine the others\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:39:17.217344Z",
     "start_time": "2018-06-05T17:39:17.180222Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1 Importing the Keras libraries and packages\n",
    "import keras\n",
    "# Sequential module is required to initialize our ANN\n",
    "from keras.models import Sequential\n",
    "# Dense module is required to create the layers\n",
    "from keras.layers import Dense, Dropout\n",
    "    \n",
    "def build_ann(optimizer='adam', input_size=300):\n",
    "    classifier = Sequential()\n",
    "    #2 Adding first hidden layer\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'random_normal', activation = 'sigmoid', input_dim = input_size))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    # Adding second hidden layer\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    # Adding output layer\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'random_normal', activation = 'softmax'))\n",
    "\n",
    "    #3 Compiling the ANN\n",
    "    classifier.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:14:47.269277Z",
     "start_time": "2018-04-11T13:14:41.410764Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1962/1962 [==============================] - 0s 199us/step - loss: 1.3823 - acc: 0.3012\n",
      "Epoch 2/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 1.2945 - acc: 0.4154\n",
      "Epoch 3/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 1.1512 - acc: 0.5377\n",
      "Epoch 4/50\n",
      "1962/1962 [==============================] - 0s 56us/step - loss: 0.9740 - acc: 0.6055\n",
      "Epoch 5/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.8528 - acc: 0.6519\n",
      "Epoch 6/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.7660 - acc: 0.7130\n",
      "Epoch 7/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.6951 - acc: 0.7543\n",
      "Epoch 8/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.6213 - acc: 0.7803\n",
      "Epoch 9/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.5755 - acc: 0.8109\n",
      "Epoch 10/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.5100 - acc: 0.8318\n",
      "Epoch 11/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.4745 - acc: 0.8486\n",
      "Epoch 12/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.4406 - acc: 0.8527\n",
      "Epoch 13/50\n",
      "1962/1962 [==============================] - 0s 60us/step - loss: 0.4139 - acc: 0.8665\n",
      "Epoch 14/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.3953 - acc: 0.8726\n",
      "Epoch 15/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.3587 - acc: 0.8818\n",
      "Epoch 16/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.3432 - acc: 0.8930\n",
      "Epoch 17/50\n",
      "1962/1962 [==============================] - 0s 53us/step - loss: 0.3293 - acc: 0.8976\n",
      "Epoch 18/50\n",
      "1962/1962 [==============================] - 0s 57us/step - loss: 0.3127 - acc: 0.8991\n",
      "Epoch 19/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.3098 - acc: 0.9006\n",
      "Epoch 20/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.3080 - acc: 0.8950\n",
      "Epoch 21/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2885 - acc: 0.9042\n",
      "Epoch 22/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.2715 - acc: 0.9103\n",
      "Epoch 23/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2692 - acc: 0.9128\n",
      "Epoch 24/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.2673 - acc: 0.9164\n",
      "Epoch 25/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.2523 - acc: 0.9169\n",
      "Epoch 26/50\n",
      "1962/1962 [==============================] - 0s 52us/step - loss: 0.2416 - acc: 0.9286\n",
      "Epoch 27/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.2272 - acc: 0.9251\n",
      "Epoch 28/50\n",
      "1962/1962 [==============================] - 0s 44us/step - loss: 0.2322 - acc: 0.9185\n",
      "Epoch 29/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.2254 - acc: 0.9256\n",
      "Epoch 30/50\n",
      "1962/1962 [==============================] - 0s 51us/step - loss: 0.2307 - acc: 0.9235\n",
      "Epoch 31/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.2202 - acc: 0.9297\n",
      "Epoch 32/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2059 - acc: 0.9373\n",
      "Epoch 33/50\n",
      "1962/1962 [==============================] - 0s 53us/step - loss: 0.2096 - acc: 0.9358\n",
      "Epoch 34/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.2119 - acc: 0.9276\n",
      "Epoch 35/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.2090 - acc: 0.9348\n",
      "Epoch 36/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.2114 - acc: 0.9327\n",
      "Epoch 37/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1957 - acc: 0.9353\n",
      "Epoch 38/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.2027 - acc: 0.9353\n",
      "Epoch 39/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1775 - acc: 0.9383\n",
      "Epoch 40/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.1895 - acc: 0.9343\n",
      "Epoch 41/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1883 - acc: 0.9429\n",
      "Epoch 42/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.1765 - acc: 0.9501\n",
      "Epoch 43/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.1676 - acc: 0.9485\n",
      "Epoch 44/50\n",
      "1962/1962 [==============================] - 0s 48us/step - loss: 0.1706 - acc: 0.9424\n",
      "Epoch 45/50\n",
      "1962/1962 [==============================] - 0s 42us/step - loss: 0.1737 - acc: 0.9439\n",
      "Epoch 46/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.1677 - acc: 0.9490\n",
      "Epoch 47/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.1596 - acc: 0.9465\n",
      "Epoch 48/50\n",
      "1962/1962 [==============================] - 0s 50us/step - loss: 0.1670 - acc: 0.9465\n",
      "Epoch 49/50\n",
      "1962/1962 [==============================] - 0s 47us/step - loss: 0.1608 - acc: 0.9501\n",
      "Epoch 50/50\n",
      "1962/1962 [==============================] - 0s 49us/step - loss: 0.1721 - acc: 0.9460\n"
     ]
    }
   ],
   "source": [
    "classifier = build_ann('adam')\n",
    "classifier.fit(X_train, y_nn, batch_size = 64, epochs = 50)\n",
    "\n",
    "#1 Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred1 = np.argmax(y_pred,axis=1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "y_nn_pred = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:16:21.039165Z",
     "start_time": "2018-04-11T13:16:21.033560Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred1, y_nn_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:16:30.264689Z",
     "start_time": "2018-04-11T13:16:30.258757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.87\n"
     ]
    }
   ],
   "source": [
    "accuracy = (sum([cm[i,i] for i in range(len(cm))])) / len(y_nn_pred)\n",
    "print('Accuracy: %0.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:40:32.096760Z",
     "start_time": "2018-06-05T17:39:48.148002Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1761/1761 [==============================] - 0s 192us/step - loss: 1.3769 - acc: 0.2959\n",
      "Epoch 2/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 1.3503 - acc: 0.3566\n",
      "Epoch 3/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 1.2955 - acc: 0.4299\n",
      "Epoch 4/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 1.2238 - acc: 0.5060\n",
      "Epoch 5/50\n",
      "1761/1761 [==============================] - 0s 43us/step - loss: 1.1140 - acc: 0.5508\n",
      "Epoch 6/50\n",
      "1761/1761 [==============================] - 0s 44us/step - loss: 1.0167 - acc: 0.5781\n",
      "Epoch 7/50\n",
      "1761/1761 [==============================] - 0s 38us/step - loss: 0.9273 - acc: 0.6036\n",
      "Epoch 8/50\n",
      "1761/1761 [==============================] - 0s 36us/step - loss: 0.8705 - acc: 0.6468\n",
      "Epoch 9/50\n",
      "1761/1761 [==============================] - 0s 43us/step - loss: 0.8191 - acc: 0.6587\n",
      "Epoch 10/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.7799 - acc: 0.6803\n",
      "Epoch 11/50\n",
      "1761/1761 [==============================] - 0s 44us/step - loss: 0.7346 - acc: 0.7115\n",
      "Epoch 12/50\n",
      "1761/1761 [==============================] - 0s 42us/step - loss: 0.6946 - acc: 0.7411\n",
      "Epoch 13/50\n",
      "1761/1761 [==============================] - 0s 34us/step - loss: 0.6683 - acc: 0.7405\n",
      "Epoch 14/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.6147 - acc: 0.7672\n",
      "Epoch 15/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.5931 - acc: 0.7990\n",
      "Epoch 16/50\n",
      "1761/1761 [==============================] - 0s 47us/step - loss: 0.5617 - acc: 0.8035\n",
      "Epoch 17/50\n",
      "1761/1761 [==============================] - 0s 41us/step - loss: 0.5330 - acc: 0.8245\n",
      "Epoch 18/50\n",
      "1761/1761 [==============================] - 0s 40us/step - loss: 0.5037 - acc: 0.8268\n",
      "Epoch 19/50\n",
      "1761/1761 [==============================] - 0s 37us/step - loss: 0.4734 - acc: 0.8399\n",
      "Epoch 20/50\n",
      "1761/1761 [==============================] - 0s 38us/step - loss: 0.4462 - acc: 0.8603\n",
      "Epoch 21/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.4256 - acc: 0.8654\n",
      "Epoch 22/50\n",
      "1761/1761 [==============================] - 0s 45us/step - loss: 0.4205 - acc: 0.8518\n",
      "Epoch 23/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.3906 - acc: 0.8836\n",
      "Epoch 24/50\n",
      "1761/1761 [==============================] - 0s 40us/step - loss: 0.3750 - acc: 0.8830\n",
      "Epoch 25/50\n",
      "1761/1761 [==============================] - 0s 40us/step - loss: 0.3681 - acc: 0.8836\n",
      "Epoch 26/50\n",
      "1761/1761 [==============================] - 0s 40us/step - loss: 0.3442 - acc: 0.8859\n",
      "Epoch 27/50\n",
      "1761/1761 [==============================] - 0s 42us/step - loss: 0.3412 - acc: 0.8978\n",
      "Epoch 28/50\n",
      "1761/1761 [==============================] - 0s 46us/step - loss: 0.3275 - acc: 0.8949\n",
      "Epoch 29/50\n",
      "1761/1761 [==============================] - 0s 41us/step - loss: 0.3161 - acc: 0.8989\n",
      "Epoch 30/50\n",
      "1761/1761 [==============================] - 0s 45us/step - loss: 0.3088 - acc: 0.9018\n",
      "Epoch 31/50\n",
      "1761/1761 [==============================] - 0s 41us/step - loss: 0.2998 - acc: 0.9091\n",
      "Epoch 32/50\n",
      "1761/1761 [==============================] - 0s 40us/step - loss: 0.2922 - acc: 0.9080\n",
      "Epoch 33/50\n",
      "1761/1761 [==============================] - 0s 41us/step - loss: 0.2833 - acc: 0.9091\n",
      "Epoch 34/50\n",
      "1761/1761 [==============================] - 0s 42us/step - loss: 0.2740 - acc: 0.9148\n",
      "Epoch 35/50\n",
      "1761/1761 [==============================] - 0s 44us/step - loss: 0.2755 - acc: 0.9143\n",
      "Epoch 36/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.2598 - acc: 0.9267\n",
      "Epoch 37/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.2644 - acc: 0.9160\n",
      "Epoch 38/50\n",
      "1761/1761 [==============================] - 0s 45us/step - loss: 0.2646 - acc: 0.9211\n",
      "Epoch 39/50\n",
      "1761/1761 [==============================] - 0s 38us/step - loss: 0.2413 - acc: 0.9267\n",
      "Epoch 40/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.2316 - acc: 0.9211\n",
      "Epoch 41/50\n",
      "1761/1761 [==============================] - 0s 34us/step - loss: 0.2491 - acc: 0.9182\n",
      "Epoch 42/50\n",
      "1761/1761 [==============================] - 0s 42us/step - loss: 0.2360 - acc: 0.9256\n",
      "Epoch 43/50\n",
      "1761/1761 [==============================] - 0s 43us/step - loss: 0.2224 - acc: 0.9324\n",
      "Epoch 44/50\n",
      "1761/1761 [==============================] - 0s 63us/step - loss: 0.2214 - acc: 0.9330\n",
      "Epoch 45/50\n",
      "1761/1761 [==============================] - 0s 50us/step - loss: 0.2180 - acc: 0.9284\n",
      "Epoch 46/50\n",
      "1761/1761 [==============================] - 0s 43us/step - loss: 0.2110 - acc: 0.9330\n",
      "Epoch 47/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.2171 - acc: 0.9375\n",
      "Epoch 48/50\n",
      "1761/1761 [==============================] - 0s 54us/step - loss: 0.2011 - acc: 0.9421\n",
      "Epoch 49/50\n",
      "1761/1761 [==============================] - 0s 46us/step - loss: 0.2045 - acc: 0.9307\n",
      "Epoch 50/50\n",
      "1761/1761 [==============================] - 0s 48us/step - loss: 0.2060 - acc: 0.9387\n",
      "Epoch 1/50\n",
      "1761/1761 [==============================] - 0s 239us/step - loss: 1.3800 - acc: 0.3010\n",
      "Epoch 2/50\n",
      "1761/1761 [==============================] - 0s 46us/step - loss: 1.3497 - acc: 0.3390\n",
      "Epoch 3/50\n",
      "1761/1761 [==============================] - 0s 38us/step - loss: 1.3156 - acc: 0.3890\n",
      "Epoch 4/50\n",
      "1761/1761 [==============================] - 0s 42us/step - loss: 1.2447 - acc: 0.4770\n",
      "Epoch 5/50\n",
      "1761/1761 [==============================] - 0s 41us/step - loss: 1.1507 - acc: 0.5417\n",
      "Epoch 6/50\n",
      "1761/1761 [==============================] - 0s 51us/step - loss: 1.0393 - acc: 0.5877\n",
      "Epoch 7/50\n",
      "1761/1761 [==============================] - 0s 46us/step - loss: 0.9436 - acc: 0.6224\n",
      "Epoch 8/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.8742 - acc: 0.6462\n",
      "Epoch 9/50\n",
      "1761/1761 [==============================] - 0s 36us/step - loss: 0.8192 - acc: 0.7024\n",
      "Epoch 10/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.7634 - acc: 0.7246\n",
      "Epoch 11/50\n",
      "1761/1761 [==============================] - 0s 41us/step - loss: 0.7036 - acc: 0.7666\n",
      "Epoch 12/50\n",
      "1761/1761 [==============================] - 0s 40us/step - loss: 0.6604 - acc: 0.7853\n",
      "Epoch 13/50\n",
      "1761/1761 [==============================] - 0s 50us/step - loss: 0.6084 - acc: 0.7910\n",
      "Epoch 14/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.5803 - acc: 0.8274\n",
      "Epoch 15/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.5357 - acc: 0.8399\n",
      "Epoch 16/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.5102 - acc: 0.8421\n",
      "Epoch 17/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.4684 - acc: 0.8580\n",
      "Epoch 18/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.4429 - acc: 0.8614\n",
      "Epoch 19/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.4180 - acc: 0.8711\n",
      "Epoch 20/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.4001 - acc: 0.8728\n",
      "Epoch 21/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3975 - acc: 0.8819\n",
      "Epoch 22/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.3703 - acc: 0.8864\n",
      "Epoch 23/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.3561 - acc: 0.8927\n",
      "Epoch 24/50\n",
      "1761/1761 [==============================] - 0s 46us/step - loss: 0.3439 - acc: 0.8984\n",
      "Epoch 25/50\n",
      "1761/1761 [==============================] - 0s 92us/step - loss: 0.3345 - acc: 0.8932\n",
      "Epoch 26/50\n",
      "1761/1761 [==============================] - 0s 77us/step - loss: 0.3227 - acc: 0.8984\n",
      "Epoch 27/50\n",
      "1761/1761 [==============================] - 0s 35us/step - loss: 0.3140 - acc: 0.9001\n",
      "Epoch 28/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3227 - acc: 0.8984\n",
      "Epoch 29/50\n",
      "1761/1761 [==============================] - 0s 45us/step - loss: 0.2914 - acc: 0.9125\n",
      "Epoch 30/50\n",
      "1761/1761 [==============================] - 0s 41us/step - loss: 0.2958 - acc: 0.9108\n",
      "Epoch 31/50\n",
      "1761/1761 [==============================] - 0s 40us/step - loss: 0.2848 - acc: 0.9137\n",
      "Epoch 32/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.2848 - acc: 0.9216\n",
      "Epoch 33/50\n",
      "1761/1761 [==============================] - 0s 47us/step - loss: 0.2802 - acc: 0.9091\n",
      "Epoch 34/50\n",
      "1761/1761 [==============================] - 0s 34us/step - loss: 0.2742 - acc: 0.9137\n",
      "Epoch 35/50\n",
      "1761/1761 [==============================] - 0s 39us/step - loss: 0.2689 - acc: 0.9182\n",
      "Epoch 36/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2647 - acc: 0.9143\n",
      "Epoch 37/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2558 - acc: 0.9137\n",
      "Epoch 38/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2438 - acc: 0.9279\n",
      "Epoch 39/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2429 - acc: 0.9262\n",
      "Epoch 40/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2375 - acc: 0.9228\n",
      "Epoch 41/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.2486 - acc: 0.9165\n",
      "Epoch 42/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2308 - acc: 0.9267\n",
      "Epoch 43/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2303 - acc: 0.9284\n",
      "Epoch 44/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2102 - acc: 0.9358\n",
      "Epoch 45/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2235 - acc: 0.9313\n",
      "Epoch 46/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.2152 - acc: 0.9353\n",
      "Epoch 47/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2104 - acc: 0.9398\n",
      "Epoch 48/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2109 - acc: 0.9415\n",
      "Epoch 49/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2087 - acc: 0.9375\n",
      "Epoch 50/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2104 - acc: 0.9375\n",
      "Epoch 1/50\n",
      "1761/1761 [==============================] - 0s 242us/step - loss: 1.3926 - acc: 0.2760\n",
      "Epoch 2/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 1.3499 - acc: 0.3606\n",
      "Epoch 3/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 1.3093 - acc: 0.3873\n",
      "Epoch 4/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 1.2361 - acc: 0.4821\n",
      "Epoch 5/50\n",
      "1761/1761 [==============================] - 0s 49us/step - loss: 1.1387 - acc: 0.5446\n",
      "Epoch 6/50\n",
      "1761/1761 [==============================] - 0s 37us/step - loss: 1.0365 - acc: 0.5758\n",
      "Epoch 7/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.9481 - acc: 0.6031\n",
      "Epoch 8/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.8855 - acc: 0.6093\n",
      "Epoch 9/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.8361 - acc: 0.6525\n",
      "Epoch 10/50\n",
      "1761/1761 [==============================] - 0s 33us/step - loss: 0.7837 - acc: 0.6860\n",
      "Epoch 11/50\n",
      "1761/1761 [==============================] - 0s 44us/step - loss: 0.7585 - acc: 0.6990\n",
      "Epoch 12/50\n",
      "1761/1761 [==============================] - 0s 58us/step - loss: 0.7131 - acc: 0.7342\n",
      "Epoch 13/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.6775 - acc: 0.7388\n",
      "Epoch 14/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.6584 - acc: 0.7581\n",
      "Epoch 15/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.6112 - acc: 0.7973\n",
      "Epoch 16/50\n",
      "1761/1761 [==============================] - 0s 36us/step - loss: 0.5765 - acc: 0.8120\n",
      "Epoch 17/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.5463 - acc: 0.8223\n",
      "Epoch 18/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.5187 - acc: 0.8387\n",
      "Epoch 19/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.4887 - acc: 0.8365\n",
      "Epoch 20/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.4721 - acc: 0.8529\n",
      "Epoch 21/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.4330 - acc: 0.8666\n",
      "Epoch 22/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.4298 - acc: 0.8558\n",
      "Epoch 23/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.4015 - acc: 0.8773\n",
      "Epoch 24/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3963 - acc: 0.8779\n",
      "Epoch 25/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.3783 - acc: 0.8842\n",
      "Epoch 26/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.3591 - acc: 0.8949\n",
      "Epoch 27/50\n",
      "1761/1761 [==============================] - 0s 42us/step - loss: 0.3457 - acc: 0.8870\n",
      "Epoch 28/50\n",
      "1761/1761 [==============================] - 0s 42us/step - loss: 0.3253 - acc: 0.8989\n",
      "Epoch 29/50\n",
      "1761/1761 [==============================] - 0s 33us/step - loss: 0.3179 - acc: 0.9006\n",
      "Epoch 30/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3138 - acc: 0.9018\n",
      "Epoch 31/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3228 - acc: 0.8904\n",
      "Epoch 32/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3022 - acc: 0.9074\n",
      "Epoch 33/50\n",
      "1761/1761 [==============================] - 0s 42us/step - loss: 0.2957 - acc: 0.9091\n",
      "Epoch 34/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2786 - acc: 0.9035\n",
      "Epoch 35/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.2629 - acc: 0.9165\n",
      "Epoch 36/50\n",
      "1761/1761 [==============================] - 0s 38us/step - loss: 0.2605 - acc: 0.9194\n",
      "Epoch 37/50\n",
      "1761/1761 [==============================] - 0s 40us/step - loss: 0.2632 - acc: 0.9182\n",
      "Epoch 38/50\n",
      "1761/1761 [==============================] - 0s 45us/step - loss: 0.2522 - acc: 0.9239\n",
      "Epoch 39/50\n",
      "1761/1761 [==============================] - 0s 33us/step - loss: 0.2567 - acc: 0.9108\n",
      "Epoch 40/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.2283 - acc: 0.9273\n",
      "Epoch 41/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.2468 - acc: 0.9228\n",
      "Epoch 42/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2381 - acc: 0.9302\n",
      "Epoch 43/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.2340 - acc: 0.9347\n",
      "Epoch 44/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2412 - acc: 0.9267\n",
      "Epoch 45/50\n",
      "1761/1761 [==============================] - 0s 33us/step - loss: 0.2154 - acc: 0.9347\n",
      "Epoch 46/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.2303 - acc: 0.9233\n",
      "Epoch 47/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2161 - acc: 0.9307\n",
      "Epoch 48/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2031 - acc: 0.9324\n",
      "Epoch 49/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.2014 - acc: 0.9381\n",
      "Epoch 50/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2180 - acc: 0.9364\n",
      "Epoch 1/50\n",
      "1761/1761 [==============================] - 0s 243us/step - loss: 1.3900 - acc: 0.2788\n",
      "Epoch 2/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 1.3548 - acc: 0.3504\n",
      "Epoch 3/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 1.3023 - acc: 0.4196\n",
      "Epoch 4/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 1.2177 - acc: 0.4986\n",
      "Epoch 5/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 1.1230 - acc: 0.5514\n",
      "Epoch 6/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 1.0160 - acc: 0.5985\n",
      "Epoch 7/50\n",
      "1761/1761 [==============================] - 0s 33us/step - loss: 0.9406 - acc: 0.6184\n",
      "Epoch 8/50\n",
      "1761/1761 [==============================] - 0s 34us/step - loss: 0.8617 - acc: 0.6553\n",
      "Epoch 9/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.8041 - acc: 0.7070\n",
      "Epoch 10/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.7617 - acc: 0.7132\n",
      "Epoch 11/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.7227 - acc: 0.7342\n",
      "Epoch 12/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.6776 - acc: 0.7541\n",
      "Epoch 13/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.6353 - acc: 0.7853\n",
      "Epoch 14/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.5960 - acc: 0.7995\n",
      "Epoch 15/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.5661 - acc: 0.8177\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.5416 - acc: 0.8262\n",
      "Epoch 17/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.5104 - acc: 0.8313\n",
      "Epoch 18/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.4678 - acc: 0.8484\n",
      "Epoch 19/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.4533 - acc: 0.8518\n",
      "Epoch 20/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.4338 - acc: 0.8631\n",
      "Epoch 21/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3943 - acc: 0.8734\n",
      "Epoch 22/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3934 - acc: 0.8762\n",
      "Epoch 23/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3678 - acc: 0.8921\n",
      "Epoch 24/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3639 - acc: 0.8830\n",
      "Epoch 25/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3248 - acc: 0.8927\n",
      "Epoch 26/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3227 - acc: 0.9006\n",
      "Epoch 27/50\n",
      "1761/1761 [==============================] - 0s 34us/step - loss: 0.3218 - acc: 0.9035\n",
      "Epoch 28/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3127 - acc: 0.9069\n",
      "Epoch 29/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2962 - acc: 0.9080\n",
      "Epoch 30/50\n",
      "1761/1761 [==============================] - 0s 26us/step - loss: 0.2975 - acc: 0.9120\n",
      "Epoch 31/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2909 - acc: 0.9057\n",
      "Epoch 32/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2783 - acc: 0.9114\n",
      "Epoch 33/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2672 - acc: 0.9216\n",
      "Epoch 34/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2483 - acc: 0.9131\n",
      "Epoch 35/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2502 - acc: 0.9239\n",
      "Epoch 36/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2626 - acc: 0.9143\n",
      "Epoch 37/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.2292 - acc: 0.9313\n",
      "Epoch 38/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2335 - acc: 0.9313\n",
      "Epoch 39/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2240 - acc: 0.9336\n",
      "Epoch 40/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2321 - acc: 0.9296\n",
      "Epoch 41/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2190 - acc: 0.9341\n",
      "Epoch 42/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2076 - acc: 0.9313\n",
      "Epoch 43/50\n",
      "1761/1761 [==============================] - 0s 26us/step - loss: 0.2139 - acc: 0.9296\n",
      "Epoch 44/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.1977 - acc: 0.9421\n",
      "Epoch 45/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.1987 - acc: 0.9421\n",
      "Epoch 46/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.1938 - acc: 0.9387\n",
      "Epoch 47/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.1950 - acc: 0.9336\n",
      "Epoch 48/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.1952 - acc: 0.9370\n",
      "Epoch 49/50\n",
      "1761/1761 [==============================] - 0s 26us/step - loss: 0.1792 - acc: 0.9466\n",
      "Epoch 50/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.1826 - acc: 0.9472\n",
      "Epoch 1/50\n",
      "1761/1761 [==============================] - 0s 262us/step - loss: 1.3850 - acc: 0.2834\n",
      "Epoch 2/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 1.3360 - acc: 0.3640\n",
      "Epoch 3/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 1.2907 - acc: 0.4276\n",
      "Epoch 4/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 1.2198 - acc: 0.4878\n",
      "Epoch 5/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 1.1062 - acc: 0.5514\n",
      "Epoch 6/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 1.0129 - acc: 0.5826\n",
      "Epoch 7/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.9216 - acc: 0.6315\n",
      "Epoch 8/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.8589 - acc: 0.6422\n",
      "Epoch 9/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.8158 - acc: 0.6746\n",
      "Epoch 10/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.7525 - acc: 0.7081\n",
      "Epoch 11/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.7155 - acc: 0.7388\n",
      "Epoch 12/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.6770 - acc: 0.7553\n",
      "Epoch 13/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.6388 - acc: 0.7751\n",
      "Epoch 14/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.5936 - acc: 0.8035\n",
      "Epoch 15/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.5596 - acc: 0.8189\n",
      "Epoch 16/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.5322 - acc: 0.8228\n",
      "Epoch 17/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.4958 - acc: 0.8382\n",
      "Epoch 18/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.4752 - acc: 0.8478\n",
      "Epoch 19/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.4430 - acc: 0.8637\n",
      "Epoch 20/50\n",
      "1761/1761 [==============================] - 0s 35us/step - loss: 0.4281 - acc: 0.8683\n",
      "Epoch 21/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.4077 - acc: 0.8739\n",
      "Epoch 22/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3820 - acc: 0.8779\n",
      "Epoch 23/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.3722 - acc: 0.8864\n",
      "Epoch 24/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3667 - acc: 0.8853\n",
      "Epoch 25/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3622 - acc: 0.8932\n",
      "Epoch 26/50\n",
      "1761/1761 [==============================] - 0s 26us/step - loss: 0.3318 - acc: 0.8938\n",
      "Epoch 27/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3185 - acc: 0.9057\n",
      "Epoch 28/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3069 - acc: 0.9074\n",
      "Epoch 29/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3051 - acc: 0.9120\n",
      "Epoch 30/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2944 - acc: 0.9063\n",
      "Epoch 31/50\n",
      "1761/1761 [==============================] - 0s 26us/step - loss: 0.2759 - acc: 0.9131\n",
      "Epoch 32/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2829 - acc: 0.9131\n",
      "Epoch 33/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2692 - acc: 0.9233\n",
      "Epoch 34/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2628 - acc: 0.9171\n",
      "Epoch 35/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2611 - acc: 0.9233\n",
      "Epoch 36/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2536 - acc: 0.9177\n",
      "Epoch 37/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2487 - acc: 0.9222\n",
      "Epoch 38/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2444 - acc: 0.9177\n",
      "Epoch 39/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2328 - acc: 0.9279\n",
      "Epoch 40/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2304 - acc: 0.9341\n",
      "Epoch 41/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2115 - acc: 0.9398\n",
      "Epoch 42/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2388 - acc: 0.9279\n",
      "Epoch 43/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2329 - acc: 0.9307\n",
      "Epoch 44/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2194 - acc: 0.9370\n",
      "Epoch 45/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2126 - acc: 0.9387\n",
      "Epoch 46/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.1954 - acc: 0.9398\n",
      "Epoch 47/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2135 - acc: 0.9364\n",
      "Epoch 48/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.1902 - acc: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.1907 - acc: 0.9387\n",
      "Epoch 50/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2033 - acc: 0.9381\n",
      "Epoch 1/50\n",
      "1761/1761 [==============================] - 0s 271us/step - loss: 1.3902 - acc: 0.2709\n",
      "Epoch 2/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 1.3551 - acc: 0.3458\n",
      "Epoch 3/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 1.3044 - acc: 0.4083\n",
      "Epoch 4/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 1.2424 - acc: 0.4804\n",
      "Epoch 5/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 1.1530 - acc: 0.5503\n",
      "Epoch 6/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 1.0328 - acc: 0.5934\n",
      "Epoch 7/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.9432 - acc: 0.6190\n",
      "Epoch 8/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.8662 - acc: 0.6400\n",
      "Epoch 9/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.8137 - acc: 0.6826\n",
      "Epoch 10/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.7638 - acc: 0.7269\n",
      "Epoch 11/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.7300 - acc: 0.7280\n",
      "Epoch 12/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.6792 - acc: 0.7535\n",
      "Epoch 13/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.6420 - acc: 0.7740\n",
      "Epoch 14/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.6054 - acc: 0.8041\n",
      "Epoch 15/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.5680 - acc: 0.8189\n",
      "Epoch 16/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.5367 - acc: 0.8330\n",
      "Epoch 17/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.4917 - acc: 0.8478\n",
      "Epoch 18/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.4704 - acc: 0.8518\n",
      "Epoch 19/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.4320 - acc: 0.8768\n",
      "Epoch 20/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.4274 - acc: 0.8728\n",
      "Epoch 21/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.4113 - acc: 0.8694\n",
      "Epoch 22/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.3885 - acc: 0.8711\n",
      "Epoch 23/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.3657 - acc: 0.8898\n",
      "Epoch 24/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3356 - acc: 0.8989\n",
      "Epoch 25/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3393 - acc: 0.8887\n",
      "Epoch 26/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3282 - acc: 0.8949\n",
      "Epoch 27/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3165 - acc: 0.9018\n",
      "Epoch 28/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3145 - acc: 0.9120\n",
      "Epoch 29/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3126 - acc: 0.8938\n",
      "Epoch 30/50\n",
      "1761/1761 [==============================] - 0s 26us/step - loss: 0.2978 - acc: 0.9052\n",
      "Epoch 31/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.2859 - acc: 0.9069\n",
      "Epoch 32/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2873 - acc: 0.9108\n",
      "Epoch 33/50\n",
      "1761/1761 [==============================] - 0s 25us/step - loss: 0.2757 - acc: 0.9120\n",
      "Epoch 34/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2699 - acc: 0.9194\n",
      "Epoch 35/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2813 - acc: 0.9074\n",
      "Epoch 36/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2649 - acc: 0.9143\n",
      "Epoch 37/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2626 - acc: 0.9194\n",
      "Epoch 38/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2469 - acc: 0.9228\n",
      "Epoch 39/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2443 - acc: 0.9245\n",
      "Epoch 40/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2403 - acc: 0.9233\n",
      "Epoch 41/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2305 - acc: 0.9302\n",
      "Epoch 42/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2246 - acc: 0.9347\n",
      "Epoch 43/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2228 - acc: 0.9375\n",
      "Epoch 44/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2196 - acc: 0.9324\n",
      "Epoch 45/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2147 - acc: 0.9324\n",
      "Epoch 46/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2084 - acc: 0.9426\n",
      "Epoch 47/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2088 - acc: 0.9421\n",
      "Epoch 48/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2033 - acc: 0.9353\n",
      "Epoch 49/50\n",
      "1761/1761 [==============================] - 0s 32us/step - loss: 0.2021 - acc: 0.9438\n",
      "Epoch 50/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.1948 - acc: 0.9364\n",
      "Epoch 1/50\n",
      "1761/1761 [==============================] - 0s 271us/step - loss: 1.3796 - acc: 0.3021\n",
      "Epoch 2/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 1.3445 - acc: 0.3430\n",
      "Epoch 3/50\n",
      "1761/1761 [==============================] - 0s 34us/step - loss: 1.2984 - acc: 0.4191\n",
      "Epoch 4/50\n",
      "1761/1761 [==============================] - 0s 38us/step - loss: 1.2215 - acc: 0.5054\n",
      "Epoch 5/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 1.1271 - acc: 0.5247\n",
      "Epoch 6/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 1.0213 - acc: 0.5855\n",
      "Epoch 7/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.9361 - acc: 0.6218\n",
      "Epoch 8/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.8653 - acc: 0.6388\n",
      "Epoch 9/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.8102 - acc: 0.6672\n",
      "Epoch 10/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.7613 - acc: 0.6934\n",
      "Epoch 11/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.7256 - acc: 0.7155\n",
      "Epoch 12/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.6813 - acc: 0.7445\n",
      "Epoch 13/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.6397 - acc: 0.7740\n",
      "Epoch 14/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.6219 - acc: 0.7643\n",
      "Epoch 15/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.5781 - acc: 0.8001\n",
      "Epoch 16/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.5472 - acc: 0.8103\n",
      "Epoch 17/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.5087 - acc: 0.8302\n",
      "Epoch 18/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.4926 - acc: 0.8330\n",
      "Epoch 19/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.4672 - acc: 0.8444\n",
      "Epoch 20/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.4480 - acc: 0.8575\n",
      "Epoch 21/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.4314 - acc: 0.8654\n",
      "Epoch 22/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.4080 - acc: 0.8705\n",
      "Epoch 23/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.3902 - acc: 0.8762\n",
      "Epoch 24/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3609 - acc: 0.8842\n",
      "Epoch 25/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3669 - acc: 0.8876\n",
      "Epoch 26/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.3471 - acc: 0.8910\n",
      "Epoch 27/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3254 - acc: 0.8989\n",
      "Epoch 28/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.3231 - acc: 0.8949\n",
      "Epoch 29/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.3017 - acc: 0.9194\n",
      "Epoch 30/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3190 - acc: 0.8961\n",
      "Epoch 31/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3206 - acc: 0.8949\n",
      "Epoch 32/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.3055 - acc: 0.9023\n",
      "Epoch 33/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2933 - acc: 0.9080\n",
      "Epoch 34/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2861 - acc: 0.9063\n",
      "Epoch 35/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2783 - acc: 0.9074\n",
      "Epoch 36/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2734 - acc: 0.9046\n",
      "Epoch 37/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2524 - acc: 0.9165\n",
      "Epoch 38/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2585 - acc: 0.9188\n",
      "Epoch 39/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2500 - acc: 0.9239\n",
      "Epoch 40/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2431 - acc: 0.9284\n",
      "Epoch 41/50\n",
      "1761/1761 [==============================] - 0s 30us/step - loss: 0.2401 - acc: 0.9216\n",
      "Epoch 42/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2339 - acc: 0.9211\n",
      "Epoch 43/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2111 - acc: 0.9330\n",
      "Epoch 44/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.2466 - acc: 0.9233\n",
      "Epoch 45/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2252 - acc: 0.9250\n",
      "Epoch 46/50\n",
      "1761/1761 [==============================] - 0s 31us/step - loss: 0.2200 - acc: 0.9262\n",
      "Epoch 47/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.2204 - acc: 0.9330\n",
      "Epoch 48/50\n",
      "1761/1761 [==============================] - 0s 28us/step - loss: 0.1999 - acc: 0.9290\n",
      "Epoch 49/50\n",
      "1761/1761 [==============================] - 0s 27us/step - loss: 0.1980 - acc: 0.9324\n",
      "Epoch 50/50\n",
      "1761/1761 [==============================] - 0s 29us/step - loss: 0.2150 - acc: 0.9262\n",
      "Epoch 1/50\n",
      "1762/1762 [==============================] - 1s 298us/step - loss: 1.3893 - acc: 0.2900\n",
      "Epoch 2/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 1.3580 - acc: 0.3337\n",
      "Epoch 3/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 1.3234 - acc: 0.3814\n",
      "Epoch 4/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 1.2480 - acc: 0.4750\n",
      "Epoch 5/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 1.1591 - acc: 0.5318\n",
      "Epoch 6/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 1.0598 - acc: 0.5851\n",
      "Epoch 7/50\n",
      "1762/1762 [==============================] - 0s 38us/step - loss: 0.9720 - acc: 0.6067\n",
      "Epoch 8/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.8872 - acc: 0.6328\n",
      "Epoch 9/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.8239 - acc: 0.6589\n",
      "Epoch 10/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.7811 - acc: 0.6856\n",
      "Epoch 11/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.7242 - acc: 0.7191\n",
      "Epoch 12/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.6915 - acc: 0.7350\n",
      "Epoch 13/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.6478 - acc: 0.7537\n",
      "Epoch 14/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.6186 - acc: 0.7758\n",
      "Epoch 15/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.6052 - acc: 0.7747\n",
      "Epoch 16/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.5801 - acc: 0.7753\n",
      "Epoch 17/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.5514 - acc: 0.7917\n",
      "Epoch 18/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.5223 - acc: 0.8138\n",
      "Epoch 19/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.5004 - acc: 0.8173\n",
      "Epoch 20/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.4824 - acc: 0.8161\n",
      "Epoch 21/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.4554 - acc: 0.8490\n",
      "Epoch 22/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.4405 - acc: 0.8462\n",
      "Epoch 23/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.4118 - acc: 0.8632\n",
      "Epoch 24/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.4029 - acc: 0.8672\n",
      "Epoch 25/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.3896 - acc: 0.8700\n",
      "Epoch 26/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.3785 - acc: 0.8854\n",
      "Epoch 27/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.3565 - acc: 0.8820\n",
      "Epoch 28/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.3431 - acc: 0.8893\n",
      "Epoch 29/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.3294 - acc: 0.8950\n",
      "Epoch 30/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.3249 - acc: 0.8888\n",
      "Epoch 31/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.3272 - acc: 0.8916\n",
      "Epoch 32/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.3037 - acc: 0.8995\n",
      "Epoch 33/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2888 - acc: 0.9103\n",
      "Epoch 34/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2900 - acc: 0.9058\n",
      "Epoch 35/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2828 - acc: 0.9098\n",
      "Epoch 36/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2845 - acc: 0.9041\n",
      "Epoch 37/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.2625 - acc: 0.9205\n",
      "Epoch 38/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2639 - acc: 0.9120\n",
      "Epoch 39/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.2552 - acc: 0.9188\n",
      "Epoch 40/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.2305 - acc: 0.9251\n",
      "Epoch 41/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2395 - acc: 0.9268\n",
      "Epoch 42/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2467 - acc: 0.9234\n",
      "Epoch 43/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2276 - acc: 0.9217\n",
      "Epoch 44/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2106 - acc: 0.9285\n",
      "Epoch 45/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.2199 - acc: 0.9393\n",
      "Epoch 46/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2209 - acc: 0.9302\n",
      "Epoch 47/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.2118 - acc: 0.9370\n",
      "Epoch 48/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.1975 - acc: 0.9404\n",
      "Epoch 49/50\n",
      "1762/1762 [==============================] - 0s 53us/step - loss: 0.1990 - acc: 0.9302\n",
      "Epoch 50/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2042 - acc: 0.9296\n",
      "Epoch 1/50\n",
      "1762/1762 [==============================] - 1s 318us/step - loss: 1.3877 - acc: 0.2758\n",
      "Epoch 2/50\n",
      "1762/1762 [==============================] - 0s 40us/step - loss: 1.3558 - acc: 0.3411\n",
      "Epoch 3/50\n",
      "1762/1762 [==============================] - 0s 45us/step - loss: 1.3012 - acc: 0.4086\n",
      "Epoch 4/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 1.2384 - acc: 0.4722\n",
      "Epoch 5/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 1.1542 - acc: 0.5306\n",
      "Epoch 6/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 1.0506 - acc: 0.5641\n",
      "Epoch 7/50\n",
      "1762/1762 [==============================] - 0s 26us/step - loss: 0.9561 - acc: 0.5948\n",
      "Epoch 8/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.8834 - acc: 0.6447\n",
      "Epoch 9/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.8351 - acc: 0.6663\n",
      "Epoch 10/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.7747 - acc: 0.6947\n",
      "Epoch 11/50\n",
      "1762/1762 [==============================] - 0s 40us/step - loss: 0.7326 - acc: 0.7111\n",
      "Epoch 12/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.6952 - acc: 0.7316\n",
      "Epoch 13/50\n",
      "1762/1762 [==============================] - 0s 26us/step - loss: 0.6591 - acc: 0.7412\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.6170 - acc: 0.7633\n",
      "Epoch 15/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.5863 - acc: 0.7917\n",
      "Epoch 16/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.5446 - acc: 0.8161\n",
      "Epoch 17/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.5211 - acc: 0.8178\n",
      "Epoch 18/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.4798 - acc: 0.8462\n",
      "Epoch 19/50\n",
      "1762/1762 [==============================] - 0s 41us/step - loss: 0.4721 - acc: 0.8468\n",
      "Epoch 20/50\n",
      "1762/1762 [==============================] - 0s 39us/step - loss: 0.4481 - acc: 0.8536\n",
      "Epoch 21/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.4161 - acc: 0.8570\n",
      "Epoch 22/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.4058 - acc: 0.8700\n",
      "Epoch 23/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.3953 - acc: 0.8734\n",
      "Epoch 24/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.3751 - acc: 0.8774\n",
      "Epoch 25/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.3575 - acc: 0.8922\n",
      "Epoch 26/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.3402 - acc: 0.8899\n",
      "Epoch 27/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.3313 - acc: 0.8950\n",
      "Epoch 28/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.3122 - acc: 0.9030\n",
      "Epoch 29/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.3077 - acc: 0.9007\n",
      "Epoch 30/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2979 - acc: 0.9086\n",
      "Epoch 31/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2997 - acc: 0.9030\n",
      "Epoch 32/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2799 - acc: 0.9103\n",
      "Epoch 33/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.2793 - acc: 0.9120\n",
      "Epoch 34/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2730 - acc: 0.9120\n",
      "Epoch 35/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2687 - acc: 0.9120\n",
      "Epoch 36/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2526 - acc: 0.9217\n",
      "Epoch 37/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2469 - acc: 0.9228\n",
      "Epoch 38/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.2324 - acc: 0.9245\n",
      "Epoch 39/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.2500 - acc: 0.9194\n",
      "Epoch 40/50\n",
      "1762/1762 [==============================] - 0s 26us/step - loss: 0.2137 - acc: 0.9336\n",
      "Epoch 41/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.2225 - acc: 0.9279\n",
      "Epoch 42/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2206 - acc: 0.9398\n",
      "Epoch 43/50\n",
      "1762/1762 [==============================] - 0s 40us/step - loss: 0.2145 - acc: 0.9342\n",
      "Epoch 44/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.2146 - acc: 0.9313\n",
      "Epoch 45/50\n",
      "1762/1762 [==============================] - 0s 45us/step - loss: 0.2116 - acc: 0.9342\n",
      "Epoch 46/50\n",
      "1762/1762 [==============================] - 0s 44us/step - loss: 0.2125 - acc: 0.9319: 0s - loss: 0.2149 - acc: 0.932\n",
      "Epoch 47/50\n",
      "1762/1762 [==============================] - 0s 44us/step - loss: 0.2078 - acc: 0.9319\n",
      "Epoch 48/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2093 - acc: 0.9370\n",
      "Epoch 49/50\n",
      "1762/1762 [==============================] - 0s 45us/step - loss: 0.1962 - acc: 0.9432\n",
      "Epoch 50/50\n",
      "1762/1762 [==============================] - 0s 42us/step - loss: 0.1944 - acc: 0.9359\n",
      "Epoch 1/50\n",
      "1762/1762 [==============================] - 1s 438us/step - loss: 1.3822 - acc: 0.3121\n",
      "Epoch 2/50\n",
      "1762/1762 [==============================] - 0s 45us/step - loss: 1.3503 - acc: 0.3417\n",
      "Epoch 3/50\n",
      "1762/1762 [==============================] - 0s 40us/step - loss: 1.2996 - acc: 0.4030\n",
      "Epoch 4/50\n",
      "1762/1762 [==============================] - 0s 39us/step - loss: 1.2298 - acc: 0.4966\n",
      "Epoch 5/50\n",
      "1762/1762 [==============================] - 0s 43us/step - loss: 1.1188 - acc: 0.5460\n",
      "Epoch 6/50\n",
      "1762/1762 [==============================] - 0s 35us/step - loss: 1.0150 - acc: 0.5823\n",
      "Epoch 7/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.9285 - acc: 0.6203\n",
      "Epoch 8/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.8556 - acc: 0.6544\n",
      "Epoch 9/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.8100 - acc: 0.6720\n",
      "Epoch 10/50\n",
      "1762/1762 [==============================] - 0s 33us/step - loss: 0.7651 - acc: 0.6918\n",
      "Epoch 11/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.7304 - acc: 0.7123\n",
      "Epoch 12/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.6852 - acc: 0.7463\n",
      "Epoch 13/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.6486 - acc: 0.7514\n",
      "Epoch 14/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.6221 - acc: 0.7804\n",
      "Epoch 15/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.5945 - acc: 0.7860\n",
      "Epoch 16/50\n",
      "1762/1762 [==============================] - 0s 33us/step - loss: 0.5584 - acc: 0.8167\n",
      "Epoch 17/50\n",
      "1762/1762 [==============================] - 0s 37us/step - loss: 0.5252 - acc: 0.8218\n",
      "Epoch 18/50\n",
      "1762/1762 [==============================] - 0s 42us/step - loss: 0.5026 - acc: 0.8326\n",
      "Epoch 19/50\n",
      "1762/1762 [==============================] - 0s 37us/step - loss: 0.4745 - acc: 0.8547\n",
      "Epoch 20/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.4555 - acc: 0.8581\n",
      "Epoch 21/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.4419 - acc: 0.8558\n",
      "Epoch 22/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.4246 - acc: 0.8581\n",
      "Epoch 23/50\n",
      "1762/1762 [==============================] - 0s 31us/step - loss: 0.4045 - acc: 0.8706\n",
      "Epoch 24/50\n",
      "1762/1762 [==============================] - 0s 37us/step - loss: 0.3668 - acc: 0.8808\n",
      "Epoch 25/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.3811 - acc: 0.8797\n",
      "Epoch 26/50\n",
      "1762/1762 [==============================] - 0s 33us/step - loss: 0.3618 - acc: 0.8859\n",
      "Epoch 27/50\n",
      "1762/1762 [==============================] - 0s 40us/step - loss: 0.3414 - acc: 0.8978\n",
      "Epoch 28/50\n",
      "1762/1762 [==============================] - 0s 47us/step - loss: 0.3235 - acc: 0.9041\n",
      "Epoch 29/50\n",
      "1762/1762 [==============================] - 0s 50us/step - loss: 0.3307 - acc: 0.8927\n",
      "Epoch 30/50\n",
      "1762/1762 [==============================] - 0s 51us/step - loss: 0.3126 - acc: 0.8950\n",
      "Epoch 31/50\n",
      "1762/1762 [==============================] - 0s 39us/step - loss: 0.3055 - acc: 0.9041\n",
      "Epoch 32/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.2917 - acc: 0.9035\n",
      "Epoch 33/50\n",
      "1762/1762 [==============================] - 0s 35us/step - loss: 0.2857 - acc: 0.9052\n",
      "Epoch 34/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.2884 - acc: 0.9069\n",
      "Epoch 35/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2663 - acc: 0.9166\n",
      "Epoch 36/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.2587 - acc: 0.9171\n",
      "Epoch 37/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2539 - acc: 0.9171\n",
      "Epoch 38/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2457 - acc: 0.9279\n",
      "Epoch 39/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2411 - acc: 0.9217\n",
      "Epoch 40/50\n",
      "1762/1762 [==============================] - 0s 30us/step - loss: 0.2434 - acc: 0.9257\n",
      "Epoch 41/50\n",
      "1762/1762 [==============================] - 0s 27us/step - loss: 0.2263 - acc: 0.9274\n",
      "Epoch 42/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2396 - acc: 0.9268\n",
      "Epoch 43/50\n",
      "1762/1762 [==============================] - 0s 29us/step - loss: 0.2378 - acc: 0.9205\n",
      "Epoch 44/50\n",
      "1762/1762 [==============================] - 0s 40us/step - loss: 0.2312 - acc: 0.9222\n",
      "Epoch 45/50\n",
      "1762/1762 [==============================] - 0s 39us/step - loss: 0.2253 - acc: 0.9251\n",
      "Epoch 46/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2053 - acc: 0.9359\n",
      "Epoch 47/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2103 - acc: 0.9325\n",
      "Epoch 48/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.2048 - acc: 0.9364\n",
      "Epoch 49/50\n",
      "1762/1762 [==============================] - 0s 32us/step - loss: 0.1931 - acc: 0.9449\n",
      "Epoch 50/50\n",
      "1762/1762 [==============================] - 0s 28us/step - loss: 0.1940 - acc: 0.9404\n",
      "Epoch 1/50\n",
      "1957/1957 [==============================] - 1s 333us/step - loss: 1.3820 - acc: 0.2928\n",
      "Epoch 2/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 1.3384 - acc: 0.3792\n",
      "Epoch 3/50\n",
      "1957/1957 [==============================] - 0s 31us/step - loss: 1.2732 - acc: 0.4405\n",
      "Epoch 4/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 1.1614 - acc: 0.5084\n",
      "Epoch 5/50\n",
      "1957/1957 [==============================] - 0s 31us/step - loss: 1.0475 - acc: 0.5611\n",
      "Epoch 6/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.9480 - acc: 0.5800\n",
      "Epoch 7/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.8754 - acc: 0.6101\n",
      "Epoch 8/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.8050 - acc: 0.6817\n",
      "Epoch 9/50\n",
      "1957/1957 [==============================] - 0s 35us/step - loss: 0.7715 - acc: 0.6704\n",
      "Epoch 10/50\n",
      "1957/1957 [==============================] - 0s 38us/step - loss: 0.7130 - acc: 0.7138\n",
      "Epoch 11/50\n",
      "1957/1957 [==============================] - 0s 33us/step - loss: 0.6738 - acc: 0.7368\n",
      "Epoch 12/50\n",
      "1957/1957 [==============================] - 0s 31us/step - loss: 0.6367 - acc: 0.7634\n",
      "Epoch 13/50\n",
      "1957/1957 [==============================] - 0s 31us/step - loss: 0.6003 - acc: 0.7869\n",
      "Epoch 14/50\n",
      "1957/1957 [==============================] - 0s 33us/step - loss: 0.5661 - acc: 0.8038\n",
      "Epoch 15/50\n",
      "1957/1957 [==============================] - 0s 33us/step - loss: 0.5447 - acc: 0.8053\n",
      "Epoch 16/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.5081 - acc: 0.8258\n",
      "Epoch 17/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.4821 - acc: 0.8421\n",
      "Epoch 18/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.4552 - acc: 0.8498\n",
      "Epoch 19/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.4389 - acc: 0.8513\n",
      "Epoch 20/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.4247 - acc: 0.8508\n",
      "Epoch 21/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.4048 - acc: 0.8523\n",
      "Epoch 22/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.3880 - acc: 0.8743\n",
      "Epoch 23/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.3759 - acc: 0.8697\n",
      "Epoch 24/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.3534 - acc: 0.8896\n",
      "Epoch 25/50\n",
      "1957/1957 [==============================] - 0s 35us/step - loss: 0.3518 - acc: 0.8886\n",
      "Epoch 26/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.3320 - acc: 0.8958\n",
      "Epoch 27/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.3219 - acc: 0.8917\n",
      "Epoch 28/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.3026 - acc: 0.8958\n",
      "Epoch 29/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.3131 - acc: 0.8973\n",
      "Epoch 30/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2928 - acc: 0.9111\n",
      "Epoch 31/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.3076 - acc: 0.8968\n",
      "Epoch 32/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2889 - acc: 0.9065\n",
      "Epoch 33/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2770 - acc: 0.9136\n",
      "Epoch 34/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.2820 - acc: 0.9080\n",
      "Epoch 35/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2721 - acc: 0.9070\n",
      "Epoch 36/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2700 - acc: 0.9106\n",
      "Epoch 37/50\n",
      "1957/1957 [==============================] - 0s 31us/step - loss: 0.2509 - acc: 0.9203\n",
      "Epoch 38/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.2581 - acc: 0.9193\n",
      "Epoch 39/50\n",
      "1957/1957 [==============================] - 0s 33us/step - loss: 0.2510 - acc: 0.9157\n",
      "Epoch 40/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2376 - acc: 0.9198\n",
      "Epoch 41/50\n",
      "1957/1957 [==============================] - 0s 29us/step - loss: 0.2403 - acc: 0.9193\n",
      "Epoch 42/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2376 - acc: 0.9223\n",
      "Epoch 43/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.2335 - acc: 0.9218\n",
      "Epoch 44/50\n",
      "1957/1957 [==============================] - 0s 37us/step - loss: 0.2329 - acc: 0.9203\n",
      "Epoch 45/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2258 - acc: 0.9315\n",
      "Epoch 46/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2242 - acc: 0.9249\n",
      "Epoch 47/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.2139 - acc: 0.9325\n",
      "Epoch 48/50\n",
      "1957/1957 [==============================] - 0s 27us/step - loss: 0.2154 - acc: 0.9305\n",
      "Epoch 49/50\n",
      "1957/1957 [==============================] - 0s 31us/step - loss: 0.1968 - acc: 0.9382\n",
      "Epoch 50/50\n",
      "1957/1957 [==============================] - 0s 28us/step - loss: 0.2062 - acc: 0.9356\n",
      "Accuracy: 90.55\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "keras_classifier = KerasClassifier(build_fn=build_ann)\n",
    "parameters = {'batch_size': [128],\n",
    "              'epochs': [50],\n",
    "              'optimizer': ['adam']}\n",
    "grid_search = GridSearchCV(estimator = keras_classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print('Accuracy: %0.2f' % (best_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T10:05:50.598608Z",
     "start_time": "2018-04-16T10:05:50.547601Z"
    }
   },
   "outputs": [],
   "source": [
    "def extra_test_ann(classifier):\n",
    "    songs = [\n",
    "        ('Bobby McFerrin', 'Don\\'t Worry, Be Happy', 'happy'),\n",
    "        ('Queen', 'Don\\'t Stop me Now', 'happy'),\n",
    "        ('Pharrell Williams', 'Happy', 'happy'),\n",
    "        ('The Monkees', 'I\\'m a believer', 'happy'),\n",
    "        \n",
    "        ('R.E.M.', 'Everybody Hurts', 'sad'),\n",
    "        ('Adele', 'Someone Like You', 'sad'),\n",
    "        ('Pink Floyd', 'Wish you were here', 'sad'),\n",
    "        ('Johnny Cash', 'Hurt', 'sad'),\n",
    "        ('Nirvana', 'Smells like teen spirit', 'sad'),\n",
    "        \n",
    "        ('Rage Against the Machine', 'Killing in the name', 'angry'),\n",
    "        ('Kanye West', 'Stronger', 'angry'),\n",
    "        ('Smash Mouth', 'All Star', 'angry'),\n",
    "        ('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry'),\n",
    "        \n",
    "        ('Blur', 'Song 2', 'relaxed') # I'm not quite confident about this labeling\n",
    "    ]\n",
    "\n",
    "    count_correct = 0\n",
    "    for s in songs:\n",
    "        # Download the lyric\n",
    "        lyric = lyricwikia.get_lyrics(s[0], s[1])\n",
    "        # Convert lyric to spacy Doc and preproces it\n",
    "        doc = nlp(lyric)\n",
    "        doc = doc_preprocess(doc)\n",
    "        # Classify\n",
    "        vect = np.array([doc.vector])\n",
    "        label = classifier.predict(sc.transform(vect))\n",
    "        lbl = np.argmax(label[0])\n",
    "        if emotion_labels[lbl] == s[2]:\n",
    "            count_correct += 1\n",
    "        print(s, '->', emotion_labels[lbl], '(was supposed to be {})'.format(s[2]))\n",
    "    print('We got {} predictions our of {} songs'.format(count_correct, len(songs)))\n",
    "    print('Accuracy: %0.2f' % (count_correct / len(songs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:27:57.894355Z",
     "start_time": "2018-04-11T13:27:51.751289Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> sad (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> sad (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> happy (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 4 predictions our of 14 songs\n",
      "Accuracy: 0.29\n"
     ]
    }
   ],
   "source": [
    "extra_test_ann(best_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we just consider the song title? \n",
    "Now let's try to perform the classification just considering the song title. We'll try SVM, Gradient Boost and an ANN to compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:45:48.337796Z",
     "start_time": "2018-06-05T17:45:48.326187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index      Artist                Song  Emotion\n",
      "0   ML1       Usher  There Goes My Baby  relaxed\n",
      "1   ML2    Da'Ville          On My Mind  relaxed\n",
      "2   ML3     Rihanna       Rockstar 101   relaxed\n",
      "3   ML4  J. Holiday                 Bed  relaxed\n"
     ]
    }
   ],
   "source": [
    "print(moodyLyricsDF[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:45:49.143245Z",
     "start_time": "2018-06-05T17:45:49.127620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index      Artist                Song  Emotion\n",
      "0   ML1       Usher  There Goes My Baby        2\n",
      "1   ML2    Da'Ville          On My Mind        2\n",
      "2   ML3     Rihanna       Rockstar 101         2\n",
      "3   ML4  J. Holiday                 Bed        2\n"
     ]
    }
   ],
   "source": [
    "# First we map the emotion labels into number: Happy=0, Sad=1, Relaxed=2, Angry=3\n",
    "mapping = dict(zip(emotion_labels, range(len(emotion_labels)))) \n",
    "moodyLyricsDF['Emotion'] = moodyLyricsDF['Emotion'].map(mapping)\n",
    "print(moodyLyricsDF[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:46:35.507364Z",
     "start_time": "2018-06-05T17:45:49.789076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then we transform the Song title with spaCy\n",
    "rows = list()\n",
    "for index,row in moodyLyricsDF.iterrows():\n",
    "    doc = nlp(row['Song'])\n",
    "    rows.append((\n",
    "                    row['Artist'],\n",
    "                    row['Song'],\n",
    "                    row['Emotion'], doc.vector,\n",
    "                    doc.vector_norm\n",
    "                ))\n",
    "\n",
    "dataset = pd.DataFrame(rows, columns=['Artist', 'Song', 'Emotion','Vector', 'Vector_Norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:46:35.535246Z",
     "start_time": "2018-06-05T17:46:35.509946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Artist                Song  Emotion  \\\n",
      "0     Usher  There Goes My Baby        2   \n",
      "1  Da'Ville          On My Mind        2   \n",
      "\n",
      "                                              Vector  Vector_Norm  \n",
      "0  [-0.0121174995, 0.27217752, -0.34575003, 0.096...     4.293632  \n",
      "1  [0.048868, 0.19073336, -0.32884666, 0.10816, 0...     4.351134  \n"
     ]
    }
   ],
   "source": [
    "print(dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:48:21.773169Z",
     "start_time": "2018-06-05T17:48:21.759773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare array for sklearn classifiers\n",
    "X_vect_name = dataset['Vector'].as_matrix().T\n",
    "X_vect_name = np.array([np.array(x) for x in X_vect_name])\n",
    "X_norm_name = dataset['Vector_Norm'].as_matrix()\n",
    "y_name = dataset['Emotion'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 29 candidates, totalling 290 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 290 out of 290 | elapsed:  9.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'C': 100, 'kernel': 'sigmoid'}\n",
      "Accuracy: 0.67 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['linear'], 'C': [ 0.01, 0.05, 1, 10, 100 ]},\n",
    "    { 'kernel': ['rbf', 'sigmoid'], 'C': [ 0.01, 0.05, 0.1, 0.3, 0.8, 1, 3, 10, 50, 100, 150, 200 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params = parameters_grid_search(SVC, params, X_vect_name, y_name, verbose=1)\n",
    "print('Parameters:', best_params)\n",
    "scores = cross_val_score(svm_best, X_vect_name, y_name, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> sad (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> sad (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> sad (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> angry (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> sad (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> sad (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 6 predictions our of 14 songs\n",
      "Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "extra_test(svm_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost with title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "clf = GradientBoostingClassifier(learning_rate=0.7, n_estimators=50)\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect_name, y_name, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.7, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_vect_name, y_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> angry (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> relaxed (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> angry (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> angry (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> happy (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> happy (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 6 predictions our of 14 songs\n",
      "Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "extra_test(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network with title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:48:35.723780Z",
     "start_time": "2018-06-05T17:48:35.712123Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "# we need to scale because we don't want one feature to predomine the others\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X_vect_name = sc.fit_transform(X_vect_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:48:36.155600Z",
     "start_time": "2018-06-05T17:48:36.140576Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_name)\n",
    "encoded_Y = encoder.transform(y_name)\n",
    "y_nn = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:49:36.154119Z",
     "start_time": "2018-06-05T17:48:37.341796Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2335/2335 [==============================] - 1s 310us/step - loss: 1.3845 - acc: 0.2882\n",
      "Epoch 2/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 1.3563 - acc: 0.3293\n",
      "Epoch 3/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 1.3291 - acc: 0.3683\n",
      "Epoch 4/50\n",
      "2335/2335 [==============================] - 0s 44us/step - loss: 1.2791 - acc: 0.4287\n",
      "Epoch 5/50\n",
      "2335/2335 [==============================] - 0s 45us/step - loss: 1.2263 - acc: 0.4677\n",
      "Epoch 6/50\n",
      "2335/2335 [==============================] - 0s 56us/step - loss: 1.1639 - acc: 0.5191\n",
      "Epoch 7/50\n",
      "2335/2335 [==============================] - 0s 48us/step - loss: 1.1133 - acc: 0.5388\n",
      "Epoch 8/50\n",
      "2335/2335 [==============================] - 0s 41us/step - loss: 1.0706 - acc: 0.5503\n",
      "Epoch 9/50\n",
      "2335/2335 [==============================] - 0s 47us/step - loss: 1.0377 - acc: 0.5820\n",
      "Epoch 10/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.9945 - acc: 0.6283\n",
      "Epoch 11/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.9675 - acc: 0.6176\n",
      "Epoch 12/50\n",
      "2335/2335 [==============================] - 0s 48us/step - loss: 0.9500 - acc: 0.6488\n",
      "Epoch 13/50\n",
      "2335/2335 [==============================] - 0s 41us/step - loss: 0.9104 - acc: 0.6582\n",
      "Epoch 14/50\n",
      "2335/2335 [==============================] - 0s 51us/step - loss: 0.8981 - acc: 0.6660\n",
      "Epoch 15/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.8800 - acc: 0.6719\n",
      "Epoch 16/50\n",
      "2335/2335 [==============================] - 0s 46us/step - loss: 0.8633 - acc: 0.6771\n",
      "Epoch 17/50\n",
      "2335/2335 [==============================] - 0s 49us/step - loss: 0.8412 - acc: 0.6908\n",
      "Epoch 18/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.8436 - acc: 0.6801\n",
      "Epoch 19/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.8225 - acc: 0.7122\n",
      "Epoch 20/50\n",
      "2335/2335 [==============================] - 0s 43us/step - loss: 0.8303 - acc: 0.6938\n",
      "Epoch 21/50\n",
      "2335/2335 [==============================] - 0s 49us/step - loss: 0.7970 - acc: 0.7066\n",
      "Epoch 22/50\n",
      "2335/2335 [==============================] - 0s 55us/step - loss: 0.7997 - acc: 0.7028\n",
      "Epoch 23/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 0.7929 - acc: 0.7101\n",
      "Epoch 24/50\n",
      "2335/2335 [==============================] - 0s 53us/step - loss: 0.7782 - acc: 0.7096\n",
      "Epoch 25/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 0.7614 - acc: 0.7238\n",
      "Epoch 26/50\n",
      "2335/2335 [==============================] - 0s 41us/step - loss: 0.7770 - acc: 0.7203\n",
      "Epoch 27/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 0.7636 - acc: 0.7216\n",
      "Epoch 28/50\n",
      "2335/2335 [==============================] - 0s 48us/step - loss: 0.7525 - acc: 0.7208\n",
      "Epoch 29/50\n",
      "2335/2335 [==============================] - 0s 61us/step - loss: 0.7341 - acc: 0.7366\n",
      "Epoch 30/50\n",
      "2335/2335 [==============================] - 0s 53us/step - loss: 0.7383 - acc: 0.7396\n",
      "Epoch 31/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 0.7274 - acc: 0.7392\n",
      "Epoch 32/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.7357 - acc: 0.7499\n",
      "Epoch 33/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.7184 - acc: 0.7516\n",
      "Epoch 34/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.7154 - acc: 0.7392\n",
      "Epoch 35/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.6993 - acc: 0.7572\n",
      "Epoch 36/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.6966 - acc: 0.7469\n",
      "Epoch 37/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.7035 - acc: 0.7495\n",
      "Epoch 38/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.7047 - acc: 0.7388\n",
      "Epoch 39/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.7077 - acc: 0.7435\n",
      "Epoch 40/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.6783 - acc: 0.7563\n",
      "Epoch 41/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.6928 - acc: 0.7525\n",
      "Epoch 42/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.6770 - acc: 0.7546\n",
      "Epoch 43/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.6701 - acc: 0.7585\n",
      "Epoch 44/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.6768 - acc: 0.7576\n",
      "Epoch 45/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.6735 - acc: 0.7615\n",
      "Epoch 46/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.6572 - acc: 0.7692\n",
      "Epoch 47/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.6577 - acc: 0.7700\n",
      "Epoch 48/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.6596 - acc: 0.7675\n",
      "Epoch 49/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.6708 - acc: 0.7602\n",
      "Epoch 50/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.6686 - acc: 0.7640\n",
      "260/260 [==============================] - 0s 846us/step\n",
      "Epoch 1/50\n",
      "2335/2335 [==============================] - 1s 279us/step - loss: 1.3939 - acc: 0.2792\n",
      "Epoch 2/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 1.3606 - acc: 0.3246\n",
      "Epoch 3/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 1.3276 - acc: 0.3734\n",
      "Epoch 4/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 1.2829 - acc: 0.4291\n",
      "Epoch 5/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 1.2245 - acc: 0.4707\n",
      "Epoch 6/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 1.1740 - acc: 0.4959\n",
      "Epoch 7/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 1.1215 - acc: 0.5298\n",
      "Epoch 8/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 1.0742 - acc: 0.5516\n",
      "Epoch 9/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 1.0414 - acc: 0.5786\n",
      "Epoch 10/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 1.0192 - acc: 0.5777\n",
      "Epoch 11/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.9879 - acc: 0.5983\n",
      "Epoch 12/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.9666 - acc: 0.6321\n",
      "Epoch 13/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.9443 - acc: 0.6394\n",
      "Epoch 14/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.9149 - acc: 0.6450\n",
      "Epoch 15/50\n",
      "2335/2335 [==============================] - 0s 48us/step - loss: 0.9102 - acc: 0.6535\n",
      "Epoch 16/50\n",
      "2335/2335 [==============================] - 0s 49us/step - loss: 0.8939 - acc: 0.6655\n",
      "Epoch 17/50\n",
      "2335/2335 [==============================] - 0s 41us/step - loss: 0.8796 - acc: 0.6698\n",
      "Epoch 18/50\n",
      "2335/2335 [==============================] - 0s 46us/step - loss: 0.8650 - acc: 0.6857\n",
      "Epoch 19/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 0.8450 - acc: 0.6857\n",
      "Epoch 20/50\n",
      "2335/2335 [==============================] - 0s 43us/step - loss: 0.8383 - acc: 0.6831\n",
      "Epoch 21/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 0.8416 - acc: 0.6925\n",
      "Epoch 22/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.8207 - acc: 0.6985\n",
      "Epoch 23/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.8097 - acc: 0.7019\n",
      "Epoch 24/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.8011 - acc: 0.7169\n",
      "Epoch 25/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.8063 - acc: 0.6985\n",
      "Epoch 26/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.8092 - acc: 0.7054\n",
      "Epoch 27/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.7998 - acc: 0.7178\n",
      "Epoch 28/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7710 - acc: 0.7131\n",
      "Epoch 29/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.7854 - acc: 0.7032\n",
      "Epoch 30/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7601 - acc: 0.7186\n",
      "Epoch 31/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.7628 - acc: 0.7148\n",
      "Epoch 32/50\n",
      "2335/2335 [==============================] - 0s 41us/step - loss: 0.7549 - acc: 0.7255\n",
      "Epoch 33/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.7650 - acc: 0.7229\n",
      "Epoch 34/50\n",
      "2335/2335 [==============================] - 0s 43us/step - loss: 0.7336 - acc: 0.7448\n",
      "Epoch 35/50\n",
      "2335/2335 [==============================] - 0s 47us/step - loss: 0.7366 - acc: 0.7328\n",
      "Epoch 36/50\n",
      "2335/2335 [==============================] - 0s 49us/step - loss: 0.7420 - acc: 0.7358\n",
      "Epoch 37/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.7263 - acc: 0.7413\n",
      "Epoch 38/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.7317 - acc: 0.7388\n",
      "Epoch 39/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 0.7304 - acc: 0.7413\n",
      "Epoch 40/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.7009 - acc: 0.7503\n",
      "Epoch 41/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.7128 - acc: 0.7443\n",
      "Epoch 42/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.7159 - acc: 0.7503\n",
      "Epoch 43/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.7119 - acc: 0.7319\n",
      "Epoch 44/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.6985 - acc: 0.7503\n",
      "Epoch 45/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.6874 - acc: 0.7675\n",
      "Epoch 46/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.6759 - acc: 0.7615\n",
      "Epoch 47/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.6940 - acc: 0.7490\n",
      "Epoch 48/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 0.7062 - acc: 0.7507\n",
      "Epoch 49/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.6701 - acc: 0.7713\n",
      "Epoch 50/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.6913 - acc: 0.7597\n",
      "260/260 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "2335/2335 [==============================] - 1s 346us/step - loss: 1.3911 - acc: 0.2767\n",
      "Epoch 2/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 1.3696 - acc: 0.3238\n",
      "Epoch 3/50\n",
      "2335/2335 [==============================] - 0s 46us/step - loss: 1.3328 - acc: 0.3696\n",
      "Epoch 4/50\n",
      "2335/2335 [==============================] - 0s 41us/step - loss: 1.2900 - acc: 0.4176\n",
      "Epoch 5/50\n",
      "2335/2335 [==============================] - 0s 62us/step - loss: 1.2309 - acc: 0.4621\n",
      "Epoch 6/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 1.1658 - acc: 0.5071\n",
      "Epoch 7/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 1.1159 - acc: 0.5238\n",
      "Epoch 8/50\n",
      "2335/2335 [==============================] - 0s 43us/step - loss: 1.0797 - acc: 0.5400\n",
      "Epoch 9/50\n",
      "2335/2335 [==============================] - 0s 59us/step - loss: 1.0537 - acc: 0.5679\n",
      "Epoch 10/50\n",
      "2335/2335 [==============================] - 0s 60us/step - loss: 1.0296 - acc: 0.5726\n",
      "Epoch 11/50\n",
      "2335/2335 [==============================] - 0s 60us/step - loss: 0.9889 - acc: 0.6291\n",
      "Epoch 12/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.9683 - acc: 0.6240\n",
      "Epoch 13/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.9474 - acc: 0.6266\n",
      "Epoch 14/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.9251 - acc: 0.6522\n",
      "Epoch 15/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 0.9132 - acc: 0.6467\n",
      "Epoch 16/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.9002 - acc: 0.6621\n",
      "Epoch 17/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.8884 - acc: 0.6634\n",
      "Epoch 18/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.8533 - acc: 0.6869\n",
      "Epoch 19/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.8495 - acc: 0.6891\n",
      "Epoch 20/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.8204 - acc: 0.6981\n",
      "Epoch 21/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.8183 - acc: 0.6929\n",
      "Epoch 22/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.8202 - acc: 0.7002\n",
      "Epoch 23/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.7979 - acc: 0.7062\n",
      "Epoch 24/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.8014 - acc: 0.7084\n",
      "Epoch 25/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7853 - acc: 0.7143\n",
      "Epoch 26/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.7803 - acc: 0.7156\n",
      "Epoch 27/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7763 - acc: 0.7298\n",
      "Epoch 28/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7724 - acc: 0.7118\n",
      "Epoch 29/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7524 - acc: 0.7293\n",
      "Epoch 30/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7408 - acc: 0.7388\n",
      "Epoch 31/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.7298 - acc: 0.7379\n",
      "Epoch 32/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7392 - acc: 0.7319\n",
      "Epoch 33/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.7287 - acc: 0.7362\n",
      "Epoch 34/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7284 - acc: 0.7392\n",
      "Epoch 35/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.7173 - acc: 0.7443\n",
      "Epoch 36/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7331 - acc: 0.7358\n",
      "Epoch 37/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.7132 - acc: 0.7452\n",
      "Epoch 38/50\n",
      "2335/2335 [==============================] - 0s 39us/step - loss: 0.7034 - acc: 0.7439\n",
      "Epoch 39/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 0.7139 - acc: 0.7405\n",
      "Epoch 40/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.6964 - acc: 0.7602\n",
      "Epoch 41/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.6922 - acc: 0.7563\n",
      "Epoch 42/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.6866 - acc: 0.7589\n",
      "Epoch 43/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.6925 - acc: 0.7533\n",
      "Epoch 44/50\n",
      "2335/2335 [==============================] - 0s 57us/step - loss: 0.6901 - acc: 0.7525\n",
      "Epoch 45/50\n",
      "2335/2335 [==============================] - 0s 63us/step - loss: 0.6687 - acc: 0.7606\n",
      "Epoch 46/50\n",
      "2335/2335 [==============================] - 0s 53us/step - loss: 0.6861 - acc: 0.7550\n",
      "Epoch 47/50\n",
      "2335/2335 [==============================] - 0s 42us/step - loss: 0.6763 - acc: 0.7636\n",
      "Epoch 48/50\n",
      "2335/2335 [==============================] - 0s 51us/step - loss: 0.6596 - acc: 0.7619\n",
      "Epoch 49/50\n",
      "2335/2335 [==============================] - 0s 44us/step - loss: 0.6671 - acc: 0.7632\n",
      "Epoch 50/50\n",
      "2335/2335 [==============================] - 0s 43us/step - loss: 0.6584 - acc: 0.7636\n",
      "260/260 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "2335/2335 [==============================] - 1s 390us/step - loss: 1.3927 - acc: 0.2857\n",
      "Epoch 2/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 1.3619 - acc: 0.3388\n",
      "Epoch 3/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 1.3209 - acc: 0.3893\n",
      "Epoch 4/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 1.2682 - acc: 0.4373\n",
      "Epoch 5/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 1.2111 - acc: 0.4771\n",
      "Epoch 6/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 1.1571 - acc: 0.4904\n",
      "Epoch 7/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 1.1171 - acc: 0.5173\n",
      "Epoch 8/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 1.0905 - acc: 0.5409\n",
      "Epoch 9/50\n",
      "2335/2335 [==============================] - 0s 28us/step - loss: 1.0425 - acc: 0.5739\n",
      "Epoch 10/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 1.0331 - acc: 0.5846\n",
      "Epoch 11/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.9989 - acc: 0.6039\n",
      "Epoch 12/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.9770 - acc: 0.6218\n",
      "Epoch 13/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.9522 - acc: 0.6321\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.9404 - acc: 0.6450\n",
      "Epoch 15/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.9199 - acc: 0.6574\n",
      "Epoch 16/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.9001 - acc: 0.6595\n",
      "Epoch 17/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.8679 - acc: 0.6775\n",
      "Epoch 18/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.8695 - acc: 0.6779\n",
      "Epoch 19/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.8581 - acc: 0.6818\n",
      "Epoch 20/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.8329 - acc: 0.6994\n",
      "Epoch 21/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.8283 - acc: 0.6899\n",
      "Epoch 22/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.8088 - acc: 0.7045\n",
      "Epoch 23/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.7955 - acc: 0.7135\n",
      "Epoch 24/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.8126 - acc: 0.6908\n",
      "Epoch 25/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7931 - acc: 0.7092\n",
      "Epoch 26/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.7876 - acc: 0.7161\n",
      "Epoch 27/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.7648 - acc: 0.7315\n",
      "Epoch 28/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.7803 - acc: 0.7148\n",
      "Epoch 29/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.7657 - acc: 0.7255\n",
      "Epoch 30/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.7572 - acc: 0.7272\n",
      "Epoch 31/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.7470 - acc: 0.7306\n",
      "Epoch 32/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.7470 - acc: 0.7229\n",
      "Epoch 33/50\n",
      "2335/2335 [==============================] - 0s 36us/step - loss: 0.7379 - acc: 0.7328\n",
      "Epoch 34/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.7349 - acc: 0.7345\n",
      "Epoch 35/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 0.7250 - acc: 0.7452\n",
      "Epoch 36/50\n",
      "2335/2335 [==============================] - 0s 28us/step - loss: 0.7224 - acc: 0.7426\n",
      "Epoch 37/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.7010 - acc: 0.7400\n",
      "Epoch 38/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.7132 - acc: 0.7375\n",
      "Epoch 39/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.6993 - acc: 0.7546\n",
      "Epoch 40/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.7023 - acc: 0.7542\n",
      "Epoch 41/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7129 - acc: 0.7439\n",
      "Epoch 42/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.7065 - acc: 0.7469\n",
      "Epoch 43/50\n",
      "2335/2335 [==============================] - 0s 28us/step - loss: 0.6846 - acc: 0.7529\n",
      "Epoch 44/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.6987 - acc: 0.7418\n",
      "Epoch 45/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.6828 - acc: 0.7559\n",
      "Epoch 46/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.6854 - acc: 0.7567\n",
      "Epoch 47/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7007 - acc: 0.7456\n",
      "Epoch 48/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.6792 - acc: 0.7525\n",
      "Epoch 49/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.6554 - acc: 0.7683\n",
      "Epoch 50/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.6792 - acc: 0.7559\n",
      "260/260 [==============================] - 0s 984us/step\n",
      "Epoch 1/50\n",
      "2335/2335 [==============================] - 1s 367us/step - loss: 1.3932 - acc: 0.2719\n",
      "Epoch 2/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 1.3616 - acc: 0.3306\n",
      "Epoch 3/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 1.3330 - acc: 0.3675\n",
      "Epoch 4/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 1.2877 - acc: 0.4056\n",
      "Epoch 5/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 1.2356 - acc: 0.4501\n",
      "Epoch 6/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 1.1786 - acc: 0.4865\n",
      "Epoch 7/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 1.1314 - acc: 0.5024\n",
      "Epoch 8/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 1.0844 - acc: 0.5229\n",
      "Epoch 9/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 1.0564 - acc: 0.5495\n",
      "Epoch 10/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 1.0388 - acc: 0.5606\n",
      "Epoch 11/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 1.0055 - acc: 0.5777\n",
      "Epoch 12/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.9905 - acc: 0.6090\n",
      "Epoch 13/50\n",
      "2335/2335 [==============================] - 0s 29us/step - loss: 0.9788 - acc: 0.6146\n",
      "Epoch 14/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.9486 - acc: 0.6253\n",
      "Epoch 15/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.9281 - acc: 0.6325\n",
      "Epoch 16/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.9120 - acc: 0.6445\n",
      "Epoch 17/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.9031 - acc: 0.6480\n",
      "Epoch 18/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 0.8914 - acc: 0.6638\n",
      "Epoch 19/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.8894 - acc: 0.6552\n",
      "Epoch 20/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.8670 - acc: 0.6677\n",
      "Epoch 21/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.8586 - acc: 0.6865\n",
      "Epoch 22/50\n",
      "2335/2335 [==============================] - 0s 40us/step - loss: 0.8455 - acc: 0.6857\n",
      "Epoch 23/50\n",
      "2335/2335 [==============================] - 0s 38us/step - loss: 0.8266 - acc: 0.6818\n",
      "Epoch 24/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.7939 - acc: 0.7152\n",
      "Epoch 25/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7970 - acc: 0.7169\n",
      "Epoch 26/50\n",
      "2335/2335 [==============================] - 0s 34us/step - loss: 0.8017 - acc: 0.7066\n",
      "Epoch 27/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7964 - acc: 0.7036\n",
      "Epoch 28/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7692 - acc: 0.7225\n",
      "Epoch 29/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.7741 - acc: 0.7233\n",
      "Epoch 30/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7721 - acc: 0.7173\n",
      "Epoch 31/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7703 - acc: 0.7178\n",
      "Epoch 32/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7337 - acc: 0.7336\n",
      "Epoch 33/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.7544 - acc: 0.7289\n",
      "Epoch 34/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.7506 - acc: 0.7289\n",
      "Epoch 35/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7345 - acc: 0.7370\n",
      "Epoch 36/50\n",
      "2335/2335 [==============================] - 0s 32us/step - loss: 0.7526 - acc: 0.7306\n",
      "Epoch 37/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7260 - acc: 0.7400\n",
      "Epoch 38/50\n",
      "2335/2335 [==============================] - 0s 37us/step - loss: 0.7163 - acc: 0.7465\n",
      "Epoch 39/50\n",
      "2335/2335 [==============================] - 0s 44us/step - loss: 0.7286 - acc: 0.7366\n",
      "Epoch 40/50\n",
      "2335/2335 [==============================] - 0s 41us/step - loss: 0.7039 - acc: 0.7443\n",
      "Epoch 41/50\n",
      "2335/2335 [==============================] - 0s 33us/step - loss: 0.7160 - acc: 0.7418\n",
      "Epoch 42/50\n",
      "2335/2335 [==============================] - 0s 35us/step - loss: 0.7064 - acc: 0.7490\n",
      "Epoch 43/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.7076 - acc: 0.7443\n",
      "Epoch 44/50\n",
      "2335/2335 [==============================] - 0s 28us/step - loss: 0.7078 - acc: 0.7430\n",
      "Epoch 45/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.7082 - acc: 0.7503\n",
      "Epoch 46/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.6723 - acc: 0.7572\n",
      "Epoch 47/50\n",
      "2335/2335 [==============================] - 0s 27us/step - loss: 0.6740 - acc: 0.7533\n",
      "Epoch 48/50\n",
      "2335/2335 [==============================] - 0s 31us/step - loss: 0.6754 - acc: 0.7563\n",
      "Epoch 49/50\n",
      "2335/2335 [==============================] - 0s 30us/step - loss: 0.6935 - acc: 0.7546\n",
      "Epoch 50/50\n",
      "2335/2335 [==============================] - 0s 28us/step - loss: 0.6727 - acc: 0.7576\n",
      "260/260 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "2336/2336 [==============================] - 1s 421us/step - loss: 1.4017 - acc: 0.2701\n",
      "Epoch 2/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 1.3613 - acc: 0.3386\n",
      "Epoch 3/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.3288 - acc: 0.3626\n",
      "Epoch 4/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 1.2936 - acc: 0.4131\n",
      "Epoch 5/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 1.2449 - acc: 0.4619\n",
      "Epoch 6/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 1.1871 - acc: 0.4910\n",
      "Epoch 7/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 1.1441 - acc: 0.5188\n",
      "Epoch 8/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 1.0997 - acc: 0.5432\n",
      "Epoch 9/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 1.0586 - acc: 0.5676\n",
      "Epoch 10/50\n",
      "2336/2336 [==============================] - 0s 45us/step - loss: 1.0283 - acc: 0.5899\n",
      "Epoch 11/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 1.0010 - acc: 0.6057\n",
      "Epoch 12/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.9618 - acc: 0.6182\n",
      "Epoch 13/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 0.9458 - acc: 0.6413\n",
      "Epoch 14/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.9180 - acc: 0.6588\n",
      "Epoch 15/50\n",
      "2336/2336 [==============================] - 0s 37us/step - loss: 0.8950 - acc: 0.6699\n",
      "Epoch 16/50\n",
      "2336/2336 [==============================] - 0s 41us/step - loss: 0.8730 - acc: 0.6699\n",
      "Epoch 17/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.8661 - acc: 0.6819\n",
      "Epoch 18/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 0.8378 - acc: 0.6901\n",
      "Epoch 19/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.8352 - acc: 0.6982\n",
      "Epoch 20/50\n",
      "2336/2336 [==============================] - 0s 28us/step - loss: 0.8359 - acc: 0.6969\n",
      "Epoch 21/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.8019 - acc: 0.7110\n",
      "Epoch 22/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.8114 - acc: 0.7051\n",
      "Epoch 23/50\n",
      "2336/2336 [==============================] - 0s 44us/step - loss: 0.7884 - acc: 0.7158\n",
      "Epoch 24/50\n",
      "2336/2336 [==============================] - 0s 42us/step - loss: 0.7780 - acc: 0.7183\n",
      "Epoch 25/50\n",
      "2336/2336 [==============================] - 0s 37us/step - loss: 0.7824 - acc: 0.7217\n",
      "Epoch 26/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7639 - acc: 0.7239\n",
      "Epoch 27/50\n",
      "2336/2336 [==============================] - 0s 28us/step - loss: 0.7716 - acc: 0.7200\n",
      "Epoch 28/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 0.7612 - acc: 0.7209\n",
      "Epoch 29/50\n",
      "2336/2336 [==============================] - 0s 28us/step - loss: 0.7511 - acc: 0.7359\n",
      "Epoch 30/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7383 - acc: 0.7303\n",
      "Epoch 31/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.7253 - acc: 0.7397\n",
      "Epoch 32/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7388 - acc: 0.7342\n",
      "Epoch 33/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.7214 - acc: 0.7389\n",
      "Epoch 34/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.7230 - acc: 0.7406\n",
      "Epoch 35/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.7032 - acc: 0.7384\n",
      "Epoch 36/50\n",
      "2336/2336 [==============================] - 0s 40us/step - loss: 0.7211 - acc: 0.7457\n",
      "Epoch 37/50\n",
      "2336/2336 [==============================] - 0s 28us/step - loss: 0.6928 - acc: 0.7513\n",
      "Epoch 38/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.6883 - acc: 0.7680\n",
      "Epoch 39/50\n",
      "2336/2336 [==============================] - 0s 28us/step - loss: 0.6768 - acc: 0.7521\n",
      "Epoch 40/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 0.6898 - acc: 0.7483\n",
      "Epoch 41/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 0.6665 - acc: 0.7590\n",
      "Epoch 42/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.6790 - acc: 0.7624\n",
      "Epoch 43/50\n",
      "2336/2336 [==============================] - 0s 28us/step - loss: 0.6788 - acc: 0.7556\n",
      "Epoch 44/50\n",
      "2336/2336 [==============================] - 0s 28us/step - loss: 0.6715 - acc: 0.7551\n",
      "Epoch 45/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 0.6662 - acc: 0.7573\n",
      "Epoch 46/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 0.6655 - acc: 0.7586\n",
      "Epoch 47/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.6558 - acc: 0.7646\n",
      "Epoch 48/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.6539 - acc: 0.7710\n",
      "Epoch 49/50\n",
      "2336/2336 [==============================] - 0s 41us/step - loss: 0.6594 - acc: 0.7616\n",
      "Epoch 50/50\n",
      "2336/2336 [==============================] - 0s 42us/step - loss: 0.6605 - acc: 0.7534\n",
      "259/259 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "2336/2336 [==============================] - 1s 480us/step - loss: 1.3943 - acc: 0.2714\n",
      "Epoch 2/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 1.3591 - acc: 0.3228\n",
      "Epoch 3/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 1.3307 - acc: 0.3741\n",
      "Epoch 4/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.2866 - acc: 0.4217\n",
      "Epoch 5/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.2356 - acc: 0.4692\n",
      "Epoch 6/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 1.1801 - acc: 0.5000\n",
      "Epoch 7/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.1295 - acc: 0.5398\n",
      "Epoch 8/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.0952 - acc: 0.5467\n",
      "Epoch 9/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.0561 - acc: 0.5792\n",
      "Epoch 10/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 1.0146 - acc: 0.6079\n",
      "Epoch 11/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.9887 - acc: 0.6250\n",
      "Epoch 12/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.9522 - acc: 0.6460\n",
      "Epoch 13/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.9270 - acc: 0.6490\n",
      "Epoch 14/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.9133 - acc: 0.6601\n",
      "Epoch 15/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.8719 - acc: 0.6665\n",
      "Epoch 16/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.8629 - acc: 0.6905\n",
      "Epoch 17/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.8522 - acc: 0.6931\n",
      "Epoch 18/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.8572 - acc: 0.6918\n",
      "Epoch 19/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.8276 - acc: 0.6931\n",
      "Epoch 20/50\n",
      "2336/2336 [==============================] - 0s 29us/step - loss: 0.8221 - acc: 0.7008\n",
      "Epoch 21/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.8202 - acc: 0.7051\n",
      "Epoch 22/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.7997 - acc: 0.7175\n",
      "Epoch 23/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7986 - acc: 0.7059\n",
      "Epoch 24/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.7805 - acc: 0.7252\n",
      "Epoch 25/50\n",
      "2336/2336 [==============================] - 0s 40us/step - loss: 0.7717 - acc: 0.7222\n",
      "Epoch 26/50\n",
      "2336/2336 [==============================] - 0s 43us/step - loss: 0.7717 - acc: 0.7290\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.7754 - acc: 0.7183\n",
      "Epoch 28/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7601 - acc: 0.7290\n",
      "Epoch 29/50\n",
      "2336/2336 [==============================] - 0s 48us/step - loss: 0.7431 - acc: 0.7367\n",
      "Epoch 30/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.7700 - acc: 0.7252\n",
      "Epoch 31/50\n",
      "2336/2336 [==============================] - 0s 50us/step - loss: 0.7378 - acc: 0.7329\n",
      "Epoch 32/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7487 - acc: 0.7290\n",
      "Epoch 33/50\n",
      "2336/2336 [==============================] - 0s 40us/step - loss: 0.7232 - acc: 0.7440\n",
      "Epoch 34/50\n",
      "2336/2336 [==============================] - 0s 48us/step - loss: 0.7277 - acc: 0.7389\n",
      "Epoch 35/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.7202 - acc: 0.7466\n",
      "Epoch 36/50\n",
      "2336/2336 [==============================] - 0s 47us/step - loss: 0.7151 - acc: 0.7440\n",
      "Epoch 37/50\n",
      "2336/2336 [==============================] - 0s 39us/step - loss: 0.7004 - acc: 0.7603\n",
      "Epoch 38/50\n",
      "2336/2336 [==============================] - 0s 37us/step - loss: 0.7023 - acc: 0.7594\n",
      "Epoch 39/50\n",
      "2336/2336 [==============================] - 0s 47us/step - loss: 0.6964 - acc: 0.7513\n",
      "Epoch 40/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.6995 - acc: 0.7539\n",
      "Epoch 41/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.6781 - acc: 0.7568\n",
      "Epoch 42/50\n",
      "2336/2336 [==============================] - 0s 45us/step - loss: 0.6885 - acc: 0.7491\n",
      "Epoch 43/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.6797 - acc: 0.7530\n",
      "Epoch 44/50\n",
      "2336/2336 [==============================] - 0s 40us/step - loss: 0.6814 - acc: 0.7654\n",
      "Epoch 45/50\n",
      "2336/2336 [==============================] - 0s 41us/step - loss: 0.6813 - acc: 0.7654\n",
      "Epoch 46/50\n",
      "2336/2336 [==============================] - 0s 40us/step - loss: 0.6801 - acc: 0.7616\n",
      "Epoch 47/50\n",
      "2336/2336 [==============================] - 0s 43us/step - loss: 0.6881 - acc: 0.7560\n",
      "Epoch 48/50\n",
      "2336/2336 [==============================] - 0s 38us/step - loss: 0.6783 - acc: 0.7646\n",
      "Epoch 49/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.6862 - acc: 0.7568\n",
      "Epoch 50/50\n",
      "2336/2336 [==============================] - 0s 46us/step - loss: 0.6675 - acc: 0.7654\n",
      "259/259 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "2336/2336 [==============================] - 1s 426us/step - loss: 1.3889 - acc: 0.2778\n",
      "Epoch 2/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.3634 - acc: 0.3348\n",
      "Epoch 3/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.3285 - acc: 0.3784\n",
      "Epoch 4/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 1.2790 - acc: 0.4234\n",
      "Epoch 5/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.2166 - acc: 0.4670\n",
      "Epoch 6/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 1.1587 - acc: 0.4897\n",
      "Epoch 7/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.1202 - acc: 0.5128\n",
      "Epoch 8/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.0789 - acc: 0.5291\n",
      "Epoch 9/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 1.0585 - acc: 0.5458\n",
      "Epoch 10/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.0377 - acc: 0.5582\n",
      "Epoch 11/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 1.0122 - acc: 0.5955\n",
      "Epoch 12/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.9818 - acc: 0.6074\n",
      "Epoch 13/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.9547 - acc: 0.6207\n",
      "Epoch 14/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.9330 - acc: 0.6374\n",
      "Epoch 15/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.9128 - acc: 0.6515\n",
      "Epoch 16/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.9083 - acc: 0.6597\n",
      "Epoch 17/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.8803 - acc: 0.6704\n",
      "Epoch 18/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.8629 - acc: 0.6824\n",
      "Epoch 19/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.8473 - acc: 0.6905\n",
      "Epoch 20/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.8510 - acc: 0.6858\n",
      "Epoch 21/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.8381 - acc: 0.6922\n",
      "Epoch 22/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.8173 - acc: 0.6978\n",
      "Epoch 23/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.8075 - acc: 0.6986\n",
      "Epoch 24/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.8034 - acc: 0.7055\n",
      "Epoch 25/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7977 - acc: 0.7222\n",
      "Epoch 26/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7746 - acc: 0.7200\n",
      "Epoch 27/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7760 - acc: 0.7192\n",
      "Epoch 28/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7730 - acc: 0.7295\n",
      "Epoch 29/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7549 - acc: 0.7273\n",
      "Epoch 30/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7634 - acc: 0.7333\n",
      "Epoch 31/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7339 - acc: 0.7299\n",
      "Epoch 32/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7369 - acc: 0.7384\n",
      "Epoch 33/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7487 - acc: 0.7312\n",
      "Epoch 34/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7374 - acc: 0.7312\n",
      "Epoch 35/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7243 - acc: 0.7367\n",
      "Epoch 36/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7193 - acc: 0.7393\n",
      "Epoch 37/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7265 - acc: 0.7453\n",
      "Epoch 38/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7112 - acc: 0.7453\n",
      "Epoch 39/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7117 - acc: 0.7440\n",
      "Epoch 40/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 0.6981 - acc: 0.7496\n",
      "Epoch 41/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.6955 - acc: 0.7483\n",
      "Epoch 42/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7096 - acc: 0.7483\n",
      "Epoch 43/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.6933 - acc: 0.7581\n",
      "Epoch 44/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.6870 - acc: 0.7603\n",
      "Epoch 45/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.6907 - acc: 0.7556\n",
      "Epoch 46/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6866 - acc: 0.7594\n",
      "Epoch 47/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6945 - acc: 0.7551\n",
      "Epoch 48/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.6671 - acc: 0.7641\n",
      "Epoch 49/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.6756 - acc: 0.7568\n",
      "Epoch 50/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6729 - acc: 0.7581\n",
      "259/259 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "2336/2336 [==============================] - 1s 441us/step - loss: 1.3872 - acc: 0.2902\n",
      "Epoch 2/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.3635 - acc: 0.3408\n",
      "Epoch 3/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 1.3311 - acc: 0.3566\n",
      "Epoch 4/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.2847 - acc: 0.4195\n",
      "Epoch 5/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.2387 - acc: 0.4499\n",
      "Epoch 6/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.1770 - acc: 0.4996\n",
      "Epoch 7/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 1.1259 - acc: 0.5261\n",
      "Epoch 8/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 1.0923 - acc: 0.5432\n",
      "Epoch 9/50\n",
      "2336/2336 [==============================] - 0s 30us/step - loss: 1.0410 - acc: 0.5771\n",
      "Epoch 10/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 1.0134 - acc: 0.6032\n",
      "Epoch 11/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.9929 - acc: 0.6070\n",
      "Epoch 12/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.9583 - acc: 0.6387\n",
      "Epoch 13/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.9338 - acc: 0.6464\n",
      "Epoch 14/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.9081 - acc: 0.6610\n",
      "Epoch 15/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.8992 - acc: 0.6644\n",
      "Epoch 16/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.8727 - acc: 0.6708\n",
      "Epoch 17/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.8639 - acc: 0.6819\n",
      "Epoch 18/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.8559 - acc: 0.6939\n",
      "Epoch 19/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.8304 - acc: 0.6901\n",
      "Epoch 20/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.8246 - acc: 0.7008\n",
      "Epoch 21/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.8155 - acc: 0.7003\n",
      "Epoch 22/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.8111 - acc: 0.7055\n",
      "Epoch 23/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7958 - acc: 0.7153\n",
      "Epoch 24/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.7775 - acc: 0.7196\n",
      "Epoch 25/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7855 - acc: 0.7162\n",
      "Epoch 26/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.7638 - acc: 0.7226\n",
      "Epoch 27/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7772 - acc: 0.7188\n",
      "Epoch 28/50\n",
      "2336/2336 [==============================] - 0s 40us/step - loss: 0.7453 - acc: 0.7230\n",
      "Epoch 29/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7587 - acc: 0.7295\n",
      "Epoch 30/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7308 - acc: 0.7333\n",
      "Epoch 31/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7330 - acc: 0.7354\n",
      "Epoch 32/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7327 - acc: 0.7367\n",
      "Epoch 33/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7292 - acc: 0.7329\n",
      "Epoch 34/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7142 - acc: 0.7432\n",
      "Epoch 35/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7090 - acc: 0.7402\n",
      "Epoch 36/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.6940 - acc: 0.7641\n",
      "Epoch 37/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7076 - acc: 0.7449\n",
      "Epoch 38/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.7015 - acc: 0.7594\n",
      "Epoch 39/50\n",
      "2336/2336 [==============================] - 0s 37us/step - loss: 0.7089 - acc: 0.7491\n",
      "Epoch 40/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7041 - acc: 0.7513\n",
      "Epoch 41/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6994 - acc: 0.7470\n",
      "Epoch 42/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.6920 - acc: 0.7521\n",
      "Epoch 43/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.6747 - acc: 0.7564\n",
      "Epoch 44/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6780 - acc: 0.7560\n",
      "Epoch 45/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.6846 - acc: 0.7650\n",
      "Epoch 46/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.6803 - acc: 0.7654\n",
      "Epoch 47/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6765 - acc: 0.7616\n",
      "Epoch 48/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.6624 - acc: 0.7676\n",
      "Epoch 49/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.6600 - acc: 0.7667\n",
      "Epoch 50/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.6649 - acc: 0.7671\n",
      "259/259 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "2336/2336 [==============================] - 1s 466us/step - loss: 1.3820 - acc: 0.2992\n",
      "Epoch 2/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 1.3614 - acc: 0.3215\n",
      "Epoch 3/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 1.3189 - acc: 0.3964\n",
      "Epoch 4/50\n",
      "2336/2336 [==============================] - 0s 41us/step - loss: 1.2610 - acc: 0.4533\n",
      "Epoch 5/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 1.1991 - acc: 0.4696\n",
      "Epoch 6/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 1.1419 - acc: 0.4987\n",
      "Epoch 7/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 1.1013 - acc: 0.5197\n",
      "Epoch 8/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 1.0652 - acc: 0.5484\n",
      "Epoch 9/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 1.0380 - acc: 0.5634\n",
      "Epoch 10/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 1.0229 - acc: 0.5642\n",
      "Epoch 11/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.9909 - acc: 0.6113\n",
      "Epoch 12/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.9772 - acc: 0.6134\n",
      "Epoch 13/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.9457 - acc: 0.6318\n",
      "Epoch 14/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.9387 - acc: 0.6396\n",
      "Epoch 15/50\n",
      "2336/2336 [==============================] - 0s 37us/step - loss: 0.9181 - acc: 0.6528\n",
      "Epoch 16/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.8857 - acc: 0.6691\n",
      "Epoch 17/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.8769 - acc: 0.6742\n",
      "Epoch 18/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.8637 - acc: 0.6764\n",
      "Epoch 19/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.8427 - acc: 0.6875\n",
      "Epoch 20/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.8383 - acc: 0.6871\n",
      "Epoch 21/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.8168 - acc: 0.7025\n",
      "Epoch 22/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.8065 - acc: 0.7016\n",
      "Epoch 23/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.8023 - acc: 0.7119\n",
      "Epoch 24/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.7731 - acc: 0.7235\n",
      "Epoch 25/50\n",
      "2336/2336 [==============================] - 0s 36us/step - loss: 0.7875 - acc: 0.7166\n",
      "Epoch 26/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7659 - acc: 0.7307\n",
      "Epoch 27/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.7617 - acc: 0.7320\n",
      "Epoch 28/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.7595 - acc: 0.7329\n",
      "Epoch 29/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7439 - acc: 0.7316\n",
      "Epoch 30/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7545 - acc: 0.7380\n",
      "Epoch 31/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.7442 - acc: 0.7273\n",
      "Epoch 32/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7307 - acc: 0.7324\n",
      "Epoch 33/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.7330 - acc: 0.7312\n",
      "Epoch 34/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7386 - acc: 0.7372\n",
      "Epoch 35/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7256 - acc: 0.7384\n",
      "Epoch 36/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7207 - acc: 0.7453\n",
      "Epoch 37/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7175 - acc: 0.7440\n",
      "Epoch 38/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.7061 - acc: 0.7461\n",
      "Epoch 39/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.7004 - acc: 0.7573\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6873 - acc: 0.7517\n",
      "Epoch 41/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6921 - acc: 0.7517\n",
      "Epoch 42/50\n",
      "2336/2336 [==============================] - 0s 38us/step - loss: 0.6922 - acc: 0.7521\n",
      "Epoch 43/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.6924 - acc: 0.7551\n",
      "Epoch 44/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.6795 - acc: 0.7491\n",
      "Epoch 45/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.6945 - acc: 0.7539\n",
      "Epoch 46/50\n",
      "2336/2336 [==============================] - 0s 34us/step - loss: 0.6685 - acc: 0.7607\n",
      "Epoch 47/50\n",
      "2336/2336 [==============================] - 0s 32us/step - loss: 0.6633 - acc: 0.7748\n",
      "Epoch 48/50\n",
      "2336/2336 [==============================] - 0s 35us/step - loss: 0.6748 - acc: 0.7616\n",
      "Epoch 49/50\n",
      "2336/2336 [==============================] - 0s 31us/step - loss: 0.6855 - acc: 0.7581\n",
      "Epoch 50/50\n",
      "2336/2336 [==============================] - 0s 33us/step - loss: 0.6472 - acc: 0.7671\n",
      "259/259 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "kclf = KerasClassifier(build_fn = build_ann, batch_size = 128, epochs = 50)\n",
    "accuracies = cross_val_score(estimator = kclf, X = X_vect_name, y = y_nn, cv = 10, n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T17:49:36.170790Z",
     "start_time": "2018-06-05T17:49:36.162347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7009400059975395\n",
      "Standard Deviation: 0.02572823337502852\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy:', accuracies.mean())\n",
    "print('Standard Deviation:', accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:21:51.440556Z",
     "start_time": "2018-04-11T13:21:51.431707Z"
    }
   },
   "outputs": [],
   "source": [
    "X_vect_name = dataset['Vector'].as_matrix().T\n",
    "X_vect_name = np.array([np.array(x) for x in X_vect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:21:59.853022Z",
     "start_time": "2018-04-11T13:21:52.837102Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2453/2453 [==============================] - 2s 759us/step - loss: 1.3868 - acc: 0.2951\n",
      "Epoch 2/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 1.3854 - acc: 0.3004\n",
      "Epoch 3/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.3816 - acc: 0.3033\n",
      "Epoch 4/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 1.3721 - acc: 0.3229\n",
      "Epoch 5/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.3556 - acc: 0.3424\n",
      "Epoch 6/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 1.3475 - acc: 0.3624\n",
      "Epoch 7/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.3283 - acc: 0.3702\n",
      "Epoch 8/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 1.2993 - acc: 0.3946\n",
      "Epoch 9/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.2595 - acc: 0.4350\n",
      "Epoch 10/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 1.1994 - acc: 0.4721\n",
      "Epoch 11/50\n",
      "2453/2453 [==============================] - 0s 45us/step - loss: 1.1424 - acc: 0.4974\n",
      "Epoch 12/50\n",
      "2453/2453 [==============================] - 0s 43us/step - loss: 1.0777 - acc: 0.5145\n",
      "Epoch 13/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 1.0243 - acc: 0.5218\n",
      "Epoch 14/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 0.9653 - acc: 0.5495\n",
      "Epoch 15/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.9282 - acc: 0.5850\n",
      "Epoch 16/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 0.9024 - acc: 0.5879\n",
      "Epoch 17/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.8899 - acc: 0.5874\n",
      "Epoch 18/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.8595 - acc: 0.6254\n",
      "Epoch 19/50\n",
      "2453/2453 [==============================] - 0s 36us/step - loss: 0.8498 - acc: 0.6066\n",
      "Epoch 20/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.8249 - acc: 0.6249\n",
      "Epoch 21/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.8121 - acc: 0.6298\n",
      "Epoch 22/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.8135 - acc: 0.6343\n",
      "Epoch 23/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.7859 - acc: 0.6592\n",
      "Epoch 24/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.7881 - acc: 0.6478\n",
      "Epoch 25/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.7707 - acc: 0.6572\n",
      "Epoch 26/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.7652 - acc: 0.6690\n",
      "Epoch 27/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.7590 - acc: 0.6657\n",
      "Epoch 28/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 0.7464 - acc: 0.6869\n",
      "Epoch 29/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.7368 - acc: 0.6747\n",
      "Epoch 30/50\n",
      "2453/2453 [==============================] - 0s 42us/step - loss: 0.7434 - acc: 0.6596\n",
      "Epoch 31/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.7171 - acc: 0.6918\n",
      "Epoch 32/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.7176 - acc: 0.6869\n",
      "Epoch 33/50\n",
      "2453/2453 [==============================] - 0s 41us/step - loss: 0.7054 - acc: 0.6967\n",
      "Epoch 34/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.6939 - acc: 0.6983\n",
      "Epoch 35/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.7085 - acc: 0.6918\n",
      "Epoch 36/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.6823 - acc: 0.7110\n",
      "Epoch 37/50\n",
      "2453/2453 [==============================] - 0s 35us/step - loss: 0.6822 - acc: 0.7008\n",
      "Epoch 38/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.6705 - acc: 0.7199\n",
      "Epoch 39/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.6731 - acc: 0.7142\n",
      "Epoch 40/50\n",
      "2453/2453 [==============================] - 0s 36us/step - loss: 0.6603 - acc: 0.7256\n",
      "Epoch 41/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.6549 - acc: 0.7195\n",
      "Epoch 42/50\n",
      "2453/2453 [==============================] - 0s 39us/step - loss: 0.6368 - acc: 0.7415\n",
      "Epoch 43/50\n",
      "2453/2453 [==============================] - 0s 36us/step - loss: 0.6409 - acc: 0.7419\n",
      "Epoch 44/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.6265 - acc: 0.7428\n",
      "Epoch 45/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.6412 - acc: 0.7395\n",
      "Epoch 46/50\n",
      "2453/2453 [==============================] - 0s 38us/step - loss: 0.6030 - acc: 0.7546\n",
      "Epoch 47/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.6057 - acc: 0.7583\n",
      "Epoch 48/50\n",
      "2453/2453 [==============================] - 0s 35us/step - loss: 0.6158 - acc: 0.7534\n",
      "Epoch 49/50\n",
      "2453/2453 [==============================] - 0s 37us/step - loss: 0.6147 - acc: 0.7538\n",
      "Epoch 50/50\n",
      "2453/2453 [==============================] - 0s 40us/step - loss: 0.5900 - acc: 0.7664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbda0fe07b8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = build_ann()\n",
    "ann.fit(X_vect_name, y_nn, batch_size = 128, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:35:07.076109Z",
     "start_time": "2018-04-11T13:34:59.611822Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> happy (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> happy (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> sad (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> sad (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> sad (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> sad (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> sad (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> sad (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 8 predictions our of 14 songs\n",
      "Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "extra_test_ann(ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Content + Song Title\n",
    "Here we propose an approach which is a sort of combination of the previous two.\n",
    "\n",
    "We already understood that the best classifiers for our purpose are the SVM and the ANN so we will just use them.\n",
    "\n",
    "First of all we will do our feature engineering, and then we will proceed to the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:06:53.268502Z",
     "start_time": "2018-06-05T18:06:05.886277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare array for sklearn classifiers\n",
    "X_vect = dataset['Vector'].as_matrix()\n",
    "X_vect = np.array([np.array(x) for x in X_vect])\n",
    "\n",
    "#X_vect_title\n",
    "#pca = PCA(n_components=4)\n",
    "#pca.fit(X_vect_pca)\n",
    "#X_vect_comb = pca.transform(X_vect_pca)\n",
    "\n",
    "X_vect_nl = list()\n",
    "\n",
    "for (i, (index,row)) in enumerate(dataset.iterrows()):\n",
    "    song_title = row['Lyric_Path'].split('_')[-1]#row['Song']\n",
    "    title_doc = nlp(song_title)\n",
    "    title_comp = title_doc.vector_norm\n",
    "    X_vect_nl.append(np.concatenate((X_vect[i], [title_comp])))\n",
    "    \n",
    "X_vect_nl = np.array(X_vect_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "We will repeat the same operations with did eariler. If you want more detailed explainations, please refer to the \"Classifiers on Lyrics Content\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:07:35.974324Z",
     "start_time": "2018-06-05T18:07:06.007861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build model\n",
    "clf = SVC()\n",
    "# Evaluate accuracy\n",
    "scores = cross_val_score(clf, X_vect_nl, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:13:01.457988Z",
     "start_time": "2018-06-05T18:12:54.530674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'kernel': 'linear', 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Define the set of parameters we want to test on\n",
    "params = [\n",
    "    { 'kernel': ['linear'], 'C': [ 0.01, 0.05, 1, 10, 100 ]},\n",
    "    { 'kernel': ['rbf', 'sigmoid'], 'C': [ 0.01, 0.05, 0.1, 0.3, 0.8, 1, 3, 10, 50, 100, 150, 200 ] }\n",
    "]\n",
    "\n",
    "# Perform grid search\n",
    "svm_best, best_params = parameters_grid_search(SVC, params, X_vect_nl, y, verbose=1)\n",
    "print('Parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:13:22.760521Z",
     "start_time": "2018-06-05T18:13:15.127472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm_best, X_vect_nl, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T09:49:06.896711Z",
     "start_time": "2018-04-16T09:48:55.571082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bobby McFerrin', \"Don't Worry, Be Happy\", 'happy') -> angry (was supposed to be happy)\n",
      "('Queen', \"Don't Stop me Now\", 'happy') -> sad (was supposed to be happy)\n",
      "('Pharrell Williams', 'Happy', 'happy') -> happy (was supposed to be happy)\n",
      "('The Monkees', \"I'm a believer\", 'happy') -> angry (was supposed to be happy)\n",
      "('R.E.M.', 'Everybody Hurts', 'sad') -> angry (was supposed to be sad)\n",
      "('Adele', 'Someone Like You', 'sad') -> angry (was supposed to be sad)\n",
      "('Pink Floyd', 'Wish you were here', 'sad') -> sad (was supposed to be sad)\n",
      "('Johnny Cash', 'Hurt', 'sad') -> angry (was supposed to be sad)\n",
      "('Nirvana', 'Smells like teen spirit', 'sad') -> sad (was supposed to be sad)\n",
      "('Rage Against the Machine', 'Killing in the name', 'angry') -> sad (was supposed to be angry)\n",
      "('Kanye West', 'Stronger', 'angry') -> sad (was supposed to be angry)\n",
      "('Smash Mouth', 'All Star', 'angry') -> sad (was supposed to be angry)\n",
      "('Bloodhound Gang', 'The Ballad of Chasey Lain', 'angry') -> happy (was supposed to be angry)\n",
      "('Blur', 'Song 2', 'relaxed') -> sad (was supposed to be relaxed)\n",
      "We got 3 predictions our of 14 songs\n",
      "Accuracy: 0.21\n"
     ]
    }
   ],
   "source": [
    "extra_test(svm_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network\n",
    "Again, if you want more detailed information on what we will be doing in this section, please refer to what we wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:08:21.191999Z",
     "start_time": "2018-06-05T18:08:21.177824Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "# we need to scale because we don't want one feature to predomine the others\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X_vect_nl_ann = sc.fit_transform(X_vect_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:08:22.062194Z",
     "start_time": "2018-06-05T18:08:22.046584Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "y_nn = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:09:01.911602Z",
     "start_time": "2018-06-05T18:09:01.877431Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_ann(optimizer='adam', input_size=301):\n",
    "    classifier = Sequential()\n",
    "    #2 Adding first hidden layer\n",
    "    classifier.add(Dense(units = 50, kernel_initializer = 'random_normal', activation = 'sigmoid', input_dim = input_size))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    # Adding hidden layers\n",
    "    classifier.add(Dense(units = 35, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    classifier.add(Dense(units = 15, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    classifier.add(Dense(units = 10, kernel_initializer = 'random_normal', activation = 'sigmoid'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    # Adding output layer\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'random_normal', activation = 'softmax'))\n",
    "\n",
    "    #3 Compiling the ANN\n",
    "    classifier.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:10:34.915913Z",
     "start_time": "2018-06-05T18:09:18.303984Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2202/2202 [==============================] - 1s 603us/step - loss: 1.3887 - acc: 0.2448\n",
      "Epoch 2/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3831 - acc: 0.2893\n",
      "Epoch 3/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3831 - acc: 0.2888\n",
      "Epoch 4/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3786 - acc: 0.2965\n",
      "Epoch 5/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3798 - acc: 0.3111\n",
      "Epoch 6/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3792 - acc: 0.3070\n",
      "Epoch 7/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3772 - acc: 0.3074\n",
      "Epoch 8/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3771 - acc: 0.3120\n",
      "Epoch 9/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3756 - acc: 0.3161\n",
      "Epoch 10/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3779 - acc: 0.3134\n",
      "Epoch 11/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3777 - acc: 0.3093\n",
      "Epoch 12/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3775 - acc: 0.3097\n",
      "Epoch 13/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3811 - acc: 0.3102\n",
      "Epoch 14/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3789 - acc: 0.3134\n",
      "Epoch 15/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3795 - acc: 0.3129\n",
      "Epoch 16/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3812 - acc: 0.3106\n",
      "Epoch 17/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3793 - acc: 0.3120\n",
      "Epoch 18/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3801 - acc: 0.3111\n",
      "Epoch 19/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3780 - acc: 0.3129\n",
      "Epoch 20/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3775 - acc: 0.3120\n",
      "Epoch 21/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3779 - acc: 0.3120\n",
      "Epoch 22/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3779 - acc: 0.3120\n",
      "Epoch 23/100\n",
      "2202/2202 [==============================] - 0s 29us/step - loss: 1.3785 - acc: 0.3120\n",
      "Epoch 24/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3786 - acc: 0.3120\n",
      "Epoch 25/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3773 - acc: 0.3120\n",
      "Epoch 26/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3760 - acc: 0.3120\n",
      "Epoch 27/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3771 - acc: 0.3120\n",
      "Epoch 28/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3777 - acc: 0.3120\n",
      "Epoch 29/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3759 - acc: 0.3120\n",
      "Epoch 30/100\n",
      "2202/2202 [==============================] - 0s 28us/step - loss: 1.3773 - acc: 0.3120\n",
      "Epoch 31/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3749 - acc: 0.3120\n",
      "Epoch 32/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3766 - acc: 0.3120\n",
      "Epoch 33/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3772 - acc: 0.3120\n",
      "Epoch 34/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3764 - acc: 0.3120\n",
      "Epoch 35/100\n",
      "2202/2202 [==============================] - 0s 30us/step - loss: 1.3759 - acc: 0.3120\n",
      "Epoch 36/100\n",
      "2202/2202 [==============================] - 0s 31us/step - loss: 1.3754 - acc: 0.3120\n",
      "Epoch 37/100\n",
      "2202/2202 [==============================] - 0s 35us/step - loss: 1.3779 - acc: 0.3120\n",
      "Epoch 38/100\n",
      "2202/2202 [==============================] - 0s 31us/step - loss: 1.3759 - acc: 0.3120\n",
      "Epoch 39/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3771 - acc: 0.3120\n",
      "Epoch 40/100\n",
      "2202/2202 [==============================] - 0s 28us/step - loss: 1.3751 - acc: 0.3120\n",
      "Epoch 41/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3773 - acc: 0.3120\n",
      "Epoch 42/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3765 - acc: 0.3120\n",
      "Epoch 43/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3760 - acc: 0.3120\n",
      "Epoch 44/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3774 - acc: 0.3120\n",
      "Epoch 45/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3758 - acc: 0.3120\n",
      "Epoch 46/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3749 - acc: 0.3120\n",
      "Epoch 47/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3765 - acc: 0.3120\n",
      "Epoch 48/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3768 - acc: 0.3120\n",
      "Epoch 49/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3757 - acc: 0.3120\n",
      "Epoch 50/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3766 - acc: 0.3120\n",
      "Epoch 51/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3749 - acc: 0.3120\n",
      "Epoch 52/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3750 - acc: 0.3120\n",
      "Epoch 53/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3735 - acc: 0.3120\n",
      "Epoch 54/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3714 - acc: 0.3120\n",
      "Epoch 55/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3727 - acc: 0.3120\n",
      "Epoch 56/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3665 - acc: 0.3120\n",
      "Epoch 57/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3643 - acc: 0.3120\n",
      "Epoch 58/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3547 - acc: 0.3120\n",
      "Epoch 59/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3465 - acc: 0.3120\n",
      "Epoch 60/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3347 - acc: 0.3120\n",
      "Epoch 61/100\n",
      "2202/2202 [==============================] - 0s 33us/step - loss: 1.3210 - acc: 0.3120\n",
      "Epoch 62/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.2987 - acc: 0.3120\n",
      "Epoch 63/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.2765 - acc: 0.3206\n",
      "Epoch 64/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.2634 - acc: 0.3397\n",
      "Epoch 65/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.2368 - acc: 0.3738\n",
      "Epoch 66/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.2161 - acc: 0.3992\n",
      "Epoch 67/100\n",
      "2202/2202 [==============================] - 0s 40us/step - loss: 1.1978 - acc: 0.4101\n",
      "Epoch 68/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1787 - acc: 0.4360\n",
      "Epoch 69/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1673 - acc: 0.4532\n",
      "Epoch 70/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1599 - acc: 0.4596\n",
      "Epoch 71/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1412 - acc: 0.4659\n",
      "Epoch 72/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1332 - acc: 0.4700\n",
      "Epoch 73/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1172 - acc: 0.4873\n",
      "Epoch 74/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1151 - acc: 0.4832\n",
      "Epoch 75/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1199 - acc: 0.4896\n",
      "Epoch 76/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1056 - acc: 0.4927\n",
      "Epoch 77/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0965 - acc: 0.5059\n",
      "Epoch 78/100\n",
      "2202/2202 [==============================] - 0s 17us/step - loss: 1.0933 - acc: 0.5050\n",
      "Epoch 79/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0953 - acc: 0.5000\n",
      "Epoch 80/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0937 - acc: 0.5095\n",
      "Epoch 81/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0872 - acc: 0.5073\n",
      "Epoch 82/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0665 - acc: 0.5132\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0685 - acc: 0.5123\n",
      "Epoch 84/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0569 - acc: 0.5182\n",
      "Epoch 85/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0684 - acc: 0.5218\n",
      "Epoch 86/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0596 - acc: 0.5154\n",
      "Epoch 87/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0622 - acc: 0.5218\n",
      "Epoch 88/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0613 - acc: 0.5204\n",
      "Epoch 89/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0524 - acc: 0.5250\n",
      "Epoch 90/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0448 - acc: 0.5268\n",
      "Epoch 91/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0554 - acc: 0.5250\n",
      "Epoch 92/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0421 - acc: 0.5272\n",
      "Epoch 93/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0296 - acc: 0.5259\n",
      "Epoch 94/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0454 - acc: 0.5327\n",
      "Epoch 95/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0288 - acc: 0.5359\n",
      "Epoch 96/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0244 - acc: 0.5381\n",
      "Epoch 97/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0286 - acc: 0.5313\n",
      "Epoch 98/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0186 - acc: 0.5395\n",
      "Epoch 99/100\n",
      "2202/2202 [==============================] - 0s 17us/step - loss: 1.0197 - acc: 0.5354\n",
      "Epoch 100/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0292 - acc: 0.5332\n",
      "245/245 [==============================] - 0s 2ms/step\n",
      "Epoch 1/100\n",
      "2202/2202 [==============================] - 1s 550us/step - loss: 1.3782 - acc: 0.3074\n",
      "Epoch 2/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3777 - acc: 0.3093\n",
      "Epoch 3/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3780 - acc: 0.3156\n",
      "Epoch 4/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3758 - acc: 0.3147\n",
      "Epoch 5/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3771 - acc: 0.3174\n",
      "Epoch 6/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3752 - acc: 0.3174\n",
      "Epoch 7/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3743 - acc: 0.3183\n",
      "Epoch 8/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3748 - acc: 0.3174\n",
      "Epoch 9/100\n",
      "2202/2202 [==============================] - 0s 33us/step - loss: 1.3771 - acc: 0.3170\n",
      "Epoch 10/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3759 - acc: 0.3183\n",
      "Epoch 11/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3726 - acc: 0.3179\n",
      "Epoch 12/100\n",
      "2202/2202 [==============================] - 0s 29us/step - loss: 1.3786 - acc: 0.3170\n",
      "Epoch 13/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3775 - acc: 0.3165\n",
      "Epoch 14/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3747 - acc: 0.3174\n",
      "Epoch 15/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3714 - acc: 0.3174\n",
      "Epoch 16/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3781 - acc: 0.3174\n",
      "Epoch 17/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3736 - acc: 0.3174\n",
      "Epoch 18/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3757 - acc: 0.3174\n",
      "Epoch 19/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3735 - acc: 0.3174\n",
      "Epoch 20/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3755 - acc: 0.3174\n",
      "Epoch 21/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3760 - acc: 0.3174\n",
      "Epoch 22/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3761 - acc: 0.3174\n",
      "Epoch 23/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3764 - acc: 0.3174\n",
      "Epoch 24/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3755 - acc: 0.3174\n",
      "Epoch 25/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3741 - acc: 0.3174\n",
      "Epoch 26/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3765 - acc: 0.3174\n",
      "Epoch 27/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3764 - acc: 0.3174\n",
      "Epoch 28/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3748 - acc: 0.3174\n",
      "Epoch 29/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3731 - acc: 0.3174\n",
      "Epoch 30/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3760 - acc: 0.3174\n",
      "Epoch 31/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3747 - acc: 0.3174\n",
      "Epoch 32/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3745 - acc: 0.3174\n",
      "Epoch 33/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3743 - acc: 0.3174\n",
      "Epoch 34/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3755 - acc: 0.3174\n",
      "Epoch 35/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3755 - acc: 0.3174\n",
      "Epoch 36/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3755 - acc: 0.3174\n",
      "Epoch 37/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3742 - acc: 0.3174\n",
      "Epoch 38/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3748 - acc: 0.3174\n",
      "Epoch 39/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3751 - acc: 0.3174\n",
      "Epoch 40/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3763 - acc: 0.3174\n",
      "Epoch 41/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3742 - acc: 0.3174\n",
      "Epoch 42/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3743 - acc: 0.3174\n",
      "Epoch 43/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3737 - acc: 0.3174\n",
      "Epoch 44/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3731 - acc: 0.3174\n",
      "Epoch 45/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3734 - acc: 0.3174\n",
      "Epoch 46/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3736 - acc: 0.3174\n",
      "Epoch 47/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3708 - acc: 0.3174\n",
      "Epoch 48/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3655 - acc: 0.3174\n",
      "Epoch 49/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3649 - acc: 0.3174\n",
      "Epoch 50/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3587 - acc: 0.3174\n",
      "Epoch 51/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3526 - acc: 0.3174\n",
      "Epoch 52/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3439 - acc: 0.3174\n",
      "Epoch 53/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3271 - acc: 0.3174\n",
      "Epoch 54/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3131 - acc: 0.3174\n",
      "Epoch 55/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2904 - acc: 0.3174\n",
      "Epoch 56/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2763 - acc: 0.3174\n",
      "Epoch 57/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2559 - acc: 0.3174\n",
      "Epoch 58/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.2343 - acc: 0.3188\n",
      "Epoch 59/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.2171 - acc: 0.3342\n",
      "Epoch 60/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2010 - acc: 0.3719\n",
      "Epoch 61/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1864 - acc: 0.4024\n",
      "Epoch 62/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1853 - acc: 0.4223\n",
      "Epoch 63/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1566 - acc: 0.4505\n",
      "Epoch 64/100\n",
      "2202/2202 [==============================] - 0s 29us/step - loss: 1.1609 - acc: 0.4650\n",
      "Epoch 65/100\n",
      "2202/2202 [==============================] - 0s 28us/step - loss: 1.1440 - acc: 0.4759\n",
      "Epoch 66/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.1354 - acc: 0.4900\n",
      "Epoch 67/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1256 - acc: 0.4941\n",
      "Epoch 68/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1221 - acc: 0.5014\n",
      "Epoch 69/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1209 - acc: 0.4968\n",
      "Epoch 70/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.1117 - acc: 0.5082\n",
      "Epoch 71/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1192 - acc: 0.5041\n",
      "Epoch 72/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1069 - acc: 0.5132\n",
      "Epoch 73/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1075 - acc: 0.5045\n",
      "Epoch 74/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0971 - acc: 0.5109\n",
      "Epoch 75/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0826 - acc: 0.5168\n",
      "Epoch 76/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0862 - acc: 0.5145\n",
      "Epoch 77/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0875 - acc: 0.5082\n",
      "Epoch 78/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0744 - acc: 0.5168\n",
      "Epoch 79/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0743 - acc: 0.5191\n",
      "Epoch 80/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0826 - acc: 0.5123\n",
      "Epoch 81/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0652 - acc: 0.5241\n",
      "Epoch 82/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0628 - acc: 0.5254\n",
      "Epoch 83/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0612 - acc: 0.5327\n",
      "Epoch 84/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0667 - acc: 0.5118\n",
      "Epoch 85/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0648 - acc: 0.5236\n",
      "Epoch 86/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.0697 - acc: 0.5104\n",
      "Epoch 87/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0598 - acc: 0.5241\n",
      "Epoch 88/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0511 - acc: 0.5300\n",
      "Epoch 89/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0613 - acc: 0.5141\n",
      "Epoch 90/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0428 - acc: 0.5322\n",
      "Epoch 91/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0467 - acc: 0.5322\n",
      "Epoch 92/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0476 - acc: 0.5245\n",
      "Epoch 93/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0308 - acc: 0.5381\n",
      "Epoch 94/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0343 - acc: 0.5395\n",
      "Epoch 95/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0305 - acc: 0.5400\n",
      "Epoch 96/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0356 - acc: 0.5341\n",
      "Epoch 97/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0286 - acc: 0.5359\n",
      "Epoch 98/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0302 - acc: 0.5295\n",
      "Epoch 99/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0278 - acc: 0.5441\n",
      "Epoch 100/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0164 - acc: 0.5413\n",
      "245/245 [==============================] - 0s 2ms/step\n",
      "Epoch 1/100\n",
      "2202/2202 [==============================] - 1s 514us/step - loss: 1.3875 - acc: 0.2470\n",
      "Epoch 2/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3831 - acc: 0.2866\n",
      "Epoch 3/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3812 - acc: 0.3029\n",
      "Epoch 4/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3828 - acc: 0.3074\n",
      "Epoch 5/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3780 - acc: 0.3102\n",
      "Epoch 6/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3800 - acc: 0.3120\n",
      "Epoch 7/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3777 - acc: 0.3115\n",
      "Epoch 8/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3788 - acc: 0.3111\n",
      "Epoch 9/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3776 - acc: 0.3124\n",
      "Epoch 10/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3744 - acc: 0.3120\n",
      "Epoch 11/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3791 - acc: 0.3120\n",
      "Epoch 12/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3799 - acc: 0.3120\n",
      "Epoch 13/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3789 - acc: 0.3120\n",
      "Epoch 14/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3774 - acc: 0.3120\n",
      "Epoch 15/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3771 - acc: 0.3120\n",
      "Epoch 16/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3775 - acc: 0.3120\n",
      "Epoch 17/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3767 - acc: 0.3120\n",
      "Epoch 18/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3781 - acc: 0.3120\n",
      "Epoch 19/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3774 - acc: 0.3120\n",
      "Epoch 20/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3763 - acc: 0.3120\n",
      "Epoch 21/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3744 - acc: 0.3120\n",
      "Epoch 22/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3783 - acc: 0.3120\n",
      "Epoch 23/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3770 - acc: 0.3120\n",
      "Epoch 24/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3769 - acc: 0.3120\n",
      "Epoch 25/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3787 - acc: 0.3120\n",
      "Epoch 26/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3780 - acc: 0.3120\n",
      "Epoch 27/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3768 - acc: 0.3120\n",
      "Epoch 28/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3782 - acc: 0.3120\n",
      "Epoch 29/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3765 - acc: 0.3120\n",
      "Epoch 30/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3772 - acc: 0.3120\n",
      "Epoch 31/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3772 - acc: 0.3120\n",
      "Epoch 32/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3777 - acc: 0.3120\n",
      "Epoch 33/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3773 - acc: 0.3120\n",
      "Epoch 34/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3765 - acc: 0.3120\n",
      "Epoch 35/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3779 - acc: 0.3120\n",
      "Epoch 36/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3765 - acc: 0.3120\n",
      "Epoch 37/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3775 - acc: 0.3120\n",
      "Epoch 38/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3763 - acc: 0.3120\n",
      "Epoch 39/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3765 - acc: 0.3120\n",
      "Epoch 40/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3772 - acc: 0.3120\n",
      "Epoch 41/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3771 - acc: 0.3120\n",
      "Epoch 42/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3761 - acc: 0.3120\n",
      "Epoch 43/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3785 - acc: 0.3120\n",
      "Epoch 44/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3768 - acc: 0.3120\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3752 - acc: 0.3120\n",
      "Epoch 46/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3747 - acc: 0.3120\n",
      "Epoch 47/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3741 - acc: 0.3120\n",
      "Epoch 48/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3739 - acc: 0.3120\n",
      "Epoch 49/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3741 - acc: 0.3120\n",
      "Epoch 50/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3729 - acc: 0.3120\n",
      "Epoch 51/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3693 - acc: 0.3120\n",
      "Epoch 52/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3678 - acc: 0.3120\n",
      "Epoch 53/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3636 - acc: 0.3120\n",
      "Epoch 54/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3585 - acc: 0.3120\n",
      "Epoch 55/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3514 - acc: 0.3120\n",
      "Epoch 56/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3394 - acc: 0.3120\n",
      "Epoch 57/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3259 - acc: 0.3120\n",
      "Epoch 58/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3167 - acc: 0.3120\n",
      "Epoch 59/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3008 - acc: 0.3120\n",
      "Epoch 60/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2841 - acc: 0.3120\n",
      "Epoch 61/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.2666 - acc: 0.3120\n",
      "Epoch 62/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.2562 - acc: 0.3120\n",
      "Epoch 63/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2403 - acc: 0.3143\n",
      "Epoch 64/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.2260 - acc: 0.3306\n",
      "Epoch 65/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.2153 - acc: 0.3656\n",
      "Epoch 66/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.2064 - acc: 0.4019\n",
      "Epoch 67/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.1877 - acc: 0.4242\n",
      "Epoch 68/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1770 - acc: 0.4478\n",
      "Epoch 69/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1717 - acc: 0.4564\n",
      "Epoch 70/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1645 - acc: 0.4864\n",
      "Epoch 71/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1572 - acc: 0.4787\n",
      "Epoch 72/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1530 - acc: 0.4923\n",
      "Epoch 73/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1482 - acc: 0.4868\n",
      "Epoch 74/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.1305 - acc: 0.4936\n",
      "Epoch 75/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.1341 - acc: 0.4886\n",
      "Epoch 76/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1185 - acc: 0.5073\n",
      "Epoch 77/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1210 - acc: 0.5018\n",
      "Epoch 78/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.1144 - acc: 0.4986\n",
      "Epoch 79/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1071 - acc: 0.5036\n",
      "Epoch 80/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1116 - acc: 0.5045\n",
      "Epoch 81/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0997 - acc: 0.5073\n",
      "Epoch 82/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0990 - acc: 0.5086\n",
      "Epoch 83/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0864 - acc: 0.5104\n",
      "Epoch 84/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0838 - acc: 0.5168\n",
      "Epoch 85/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.0789 - acc: 0.5163\n",
      "Epoch 86/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0758 - acc: 0.5173\n",
      "Epoch 87/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0796 - acc: 0.5163\n",
      "Epoch 88/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0659 - acc: 0.5259\n",
      "Epoch 89/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0626 - acc: 0.5186\n",
      "Epoch 90/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0711 - acc: 0.5223\n",
      "Epoch 91/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0561 - acc: 0.5213\n",
      "Epoch 92/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0563 - acc: 0.5263\n",
      "Epoch 93/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0483 - acc: 0.5204\n",
      "Epoch 94/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0516 - acc: 0.5291\n",
      "Epoch 95/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0547 - acc: 0.5236\n",
      "Epoch 96/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0491 - acc: 0.5245\n",
      "Epoch 97/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0541 - acc: 0.5250\n",
      "Epoch 98/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0490 - acc: 0.5114\n",
      "Epoch 99/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0307 - acc: 0.5300\n",
      "Epoch 100/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0376 - acc: 0.5259\n",
      "245/245 [==============================] - 0s 2ms/step\n",
      "Epoch 1/100\n",
      "2202/2202 [==============================] - 1s 523us/step - loss: 1.3896 - acc: 0.2457\n",
      "Epoch 2/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3856 - acc: 0.2748\n",
      "Epoch 3/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3812 - acc: 0.2943\n",
      "Epoch 4/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3808 - acc: 0.2965\n",
      "Epoch 5/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3786 - acc: 0.3056\n",
      "Epoch 6/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3802 - acc: 0.3079\n",
      "Epoch 7/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3792 - acc: 0.3079\n",
      "Epoch 8/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3796 - acc: 0.3061\n",
      "Epoch 9/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3771 - acc: 0.3093\n",
      "Epoch 10/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3799 - acc: 0.3102\n",
      "Epoch 11/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3761 - acc: 0.3088\n",
      "Epoch 12/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3791 - acc: 0.3074\n",
      "Epoch 13/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3776 - acc: 0.3102\n",
      "Epoch 14/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3793 - acc: 0.3097\n",
      "Epoch 15/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3788 - acc: 0.3093\n",
      "Epoch 16/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3775 - acc: 0.3093\n",
      "Epoch 17/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3789 - acc: 0.3106\n",
      "Epoch 18/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3785 - acc: 0.3106\n",
      "Epoch 19/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3771 - acc: 0.3102\n",
      "Epoch 20/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3780 - acc: 0.3102\n",
      "Epoch 21/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3793 - acc: 0.3111\n",
      "Epoch 22/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3793 - acc: 0.3102\n",
      "Epoch 23/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3780 - acc: 0.3102\n",
      "Epoch 24/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3786 - acc: 0.3102\n",
      "Epoch 25/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3792 - acc: 0.3102\n",
      "Epoch 26/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3763 - acc: 0.3102\n",
      "Epoch 27/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3779 - acc: 0.3102\n",
      "Epoch 28/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3764 - acc: 0.3102\n",
      "Epoch 29/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3755 - acc: 0.3102\n",
      "Epoch 30/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3767 - acc: 0.3102\n",
      "Epoch 31/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3778 - acc: 0.3102\n",
      "Epoch 32/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3760 - acc: 0.3102\n",
      "Epoch 33/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3765 - acc: 0.3102\n",
      "Epoch 34/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3772 - acc: 0.3102\n",
      "Epoch 35/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3762 - acc: 0.3102\n",
      "Epoch 36/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3764 - acc: 0.3102\n",
      "Epoch 37/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3774 - acc: 0.3102\n",
      "Epoch 38/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3777 - acc: 0.3102\n",
      "Epoch 39/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3773 - acc: 0.3102\n",
      "Epoch 40/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3783 - acc: 0.3102\n",
      "Epoch 41/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3793 - acc: 0.3102\n",
      "Epoch 42/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3768 - acc: 0.3102\n",
      "Epoch 43/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3763 - acc: 0.3102\n",
      "Epoch 44/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3775 - acc: 0.3102\n",
      "Epoch 45/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3776 - acc: 0.3102\n",
      "Epoch 46/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3781 - acc: 0.3102\n",
      "Epoch 47/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3779 - acc: 0.3102\n",
      "Epoch 48/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3769 - acc: 0.3102\n",
      "Epoch 49/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3772 - acc: 0.3102\n",
      "Epoch 50/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3761 - acc: 0.3102\n",
      "Epoch 51/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3773 - acc: 0.3102\n",
      "Epoch 52/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3760 - acc: 0.3102\n",
      "Epoch 53/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3751 - acc: 0.3102\n",
      "Epoch 54/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3751 - acc: 0.3102\n",
      "Epoch 55/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3723 - acc: 0.3102\n",
      "Epoch 56/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3706 - acc: 0.3102\n",
      "Epoch 57/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3653 - acc: 0.3102\n",
      "Epoch 58/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3610 - acc: 0.3102\n",
      "Epoch 59/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3515 - acc: 0.3102\n",
      "Epoch 60/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.3422 - acc: 0.3102\n",
      "Epoch 61/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3290 - acc: 0.3102\n",
      "Epoch 62/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3088 - acc: 0.3102\n",
      "Epoch 63/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.2968 - acc: 0.3102\n",
      "Epoch 64/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.2683 - acc: 0.3102\n",
      "Epoch 65/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2536 - acc: 0.3093\n",
      "Epoch 66/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2327 - acc: 0.3152\n",
      "Epoch 67/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.2139 - acc: 0.3370\n",
      "Epoch 68/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.2010 - acc: 0.3683\n",
      "Epoch 69/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1914 - acc: 0.3983\n",
      "Epoch 70/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1721 - acc: 0.4214\n",
      "Epoch 71/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.1624 - acc: 0.4464\n",
      "Epoch 72/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1507 - acc: 0.4619\n",
      "Epoch 73/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.1440 - acc: 0.4696\n",
      "Epoch 74/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.1337 - acc: 0.4764\n",
      "Epoch 75/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.1269 - acc: 0.4918\n",
      "Epoch 76/100\n",
      "2202/2202 [==============================] - 0s 48us/step - loss: 1.1160 - acc: 0.4936\n",
      "Epoch 77/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.1059 - acc: 0.4950\n",
      "Epoch 78/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1057 - acc: 0.4886\n",
      "Epoch 79/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1047 - acc: 0.4991\n",
      "Epoch 80/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0905 - acc: 0.4986\n",
      "Epoch 81/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0869 - acc: 0.5032\n",
      "Epoch 82/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0711 - acc: 0.5082\n",
      "Epoch 83/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0784 - acc: 0.5041\n",
      "Epoch 84/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0719 - acc: 0.5241\n",
      "Epoch 85/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0642 - acc: 0.5100\n",
      "Epoch 86/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0448 - acc: 0.5177\n",
      "Epoch 87/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0631 - acc: 0.5073\n",
      "Epoch 88/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0595 - acc: 0.4986\n",
      "Epoch 89/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0405 - acc: 0.5232\n",
      "Epoch 90/100\n",
      "2202/2202 [==============================] - 0s 18us/step - loss: 1.0514 - acc: 0.5114\n",
      "Epoch 91/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0567 - acc: 0.5141\n",
      "Epoch 92/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0444 - acc: 0.5136\n",
      "Epoch 93/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.0432 - acc: 0.5177\n",
      "Epoch 94/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0456 - acc: 0.5159\n",
      "Epoch 95/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0261 - acc: 0.5204\n",
      "Epoch 96/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0362 - acc: 0.5204\n",
      "Epoch 97/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0302 - acc: 0.5227\n",
      "Epoch 98/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0378 - acc: 0.5236\n",
      "Epoch 99/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0171 - acc: 0.5250\n",
      "Epoch 100/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0247 - acc: 0.5227\n",
      "245/245 [==============================] - 0s 2ms/step\n",
      "Epoch 1/100\n",
      "2202/2202 [==============================] - 1s 671us/step - loss: 1.3934 - acc: 0.2439\n",
      "Epoch 2/100\n",
      "2202/2202 [==============================] - 0s 29us/step - loss: 1.3884 - acc: 0.2552\n",
      "Epoch 3/100\n",
      "2202/2202 [==============================] - 0s 30us/step - loss: 1.3855 - acc: 0.2748\n",
      "Epoch 4/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3830 - acc: 0.2866\n",
      "Epoch 5/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3806 - acc: 0.3002\n",
      "Epoch 6/100\n",
      "2202/2202 [==============================] - 0s 36us/step - loss: 1.3806 - acc: 0.3052\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202/2202 [==============================] - 0s 29us/step - loss: 1.3800 - acc: 0.3084\n",
      "Epoch 8/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3788 - acc: 0.3093\n",
      "Epoch 9/100\n",
      "2202/2202 [==============================] - 0s 28us/step - loss: 1.3796 - acc: 0.3093\n",
      "Epoch 10/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3806 - acc: 0.3084\n",
      "Epoch 11/100\n",
      "2202/2202 [==============================] - 0s 33us/step - loss: 1.3771 - acc: 0.3079\n",
      "Epoch 12/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3790 - acc: 0.3088\n",
      "Epoch 13/100\n",
      "2202/2202 [==============================] - 0s 28us/step - loss: 1.3818 - acc: 0.3097\n",
      "Epoch 14/100\n",
      "2202/2202 [==============================] - 0s 31us/step - loss: 1.3780 - acc: 0.3088\n",
      "Epoch 15/100\n",
      "2202/2202 [==============================] - 0s 31us/step - loss: 1.3787 - acc: 0.3088\n",
      "Epoch 16/100\n",
      "2202/2202 [==============================] - 0s 31us/step - loss: 1.3783 - acc: 0.3093\n",
      "Epoch 17/100\n",
      "2202/2202 [==============================] - 0s 30us/step - loss: 1.3788 - acc: 0.3088\n",
      "Epoch 18/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3776 - acc: 0.3084\n",
      "Epoch 19/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3770 - acc: 0.3102\n",
      "Epoch 20/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3788 - acc: 0.3093\n",
      "Epoch 21/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3792 - acc: 0.3093\n",
      "Epoch 22/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3793 - acc: 0.3093\n",
      "Epoch 23/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3775 - acc: 0.3093\n",
      "Epoch 24/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3784 - acc: 0.3093\n",
      "Epoch 25/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3777 - acc: 0.3093\n",
      "Epoch 26/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3806 - acc: 0.3093\n",
      "Epoch 27/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3780 - acc: 0.3093\n",
      "Epoch 28/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3787 - acc: 0.3093\n",
      "Epoch 29/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3791 - acc: 0.3093\n",
      "Epoch 30/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3767 - acc: 0.3093\n",
      "Epoch 31/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3770 - acc: 0.3093\n",
      "Epoch 32/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3779 - acc: 0.3093\n",
      "Epoch 33/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3775 - acc: 0.3093\n",
      "Epoch 34/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3763 - acc: 0.3093\n",
      "Epoch 35/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3774 - acc: 0.3093\n",
      "Epoch 36/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3782 - acc: 0.3093\n",
      "Epoch 37/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3775 - acc: 0.3093\n",
      "Epoch 38/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3789 - acc: 0.3093\n",
      "Epoch 39/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3784 - acc: 0.3093\n",
      "Epoch 40/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3774 - acc: 0.3093\n",
      "Epoch 41/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3771 - acc: 0.3093\n",
      "Epoch 42/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3787 - acc: 0.3093\n",
      "Epoch 43/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3781 - acc: 0.3093\n",
      "Epoch 44/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3774 - acc: 0.3093\n",
      "Epoch 45/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3787 - acc: 0.3093\n",
      "Epoch 46/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3783 - acc: 0.3093\n",
      "Epoch 47/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3780 - acc: 0.3093\n",
      "Epoch 48/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3777 - acc: 0.3093\n",
      "Epoch 49/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3785 - acc: 0.3093\n",
      "Epoch 50/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3780 - acc: 0.3093\n",
      "Epoch 51/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3770 - acc: 0.3093\n",
      "Epoch 52/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3774 - acc: 0.3093\n",
      "Epoch 53/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3776 - acc: 0.3093\n",
      "Epoch 54/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3771 - acc: 0.3093\n",
      "Epoch 55/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3774 - acc: 0.3093\n",
      "Epoch 56/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3779 - acc: 0.3093\n",
      "Epoch 57/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3778 - acc: 0.3093\n",
      "Epoch 58/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3764 - acc: 0.3093\n",
      "Epoch 59/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3779 - acc: 0.3093\n",
      "Epoch 60/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3777 - acc: 0.3093\n",
      "Epoch 61/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3784 - acc: 0.3093\n",
      "Epoch 62/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3772 - acc: 0.3093\n",
      "Epoch 63/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3791 - acc: 0.3093\n",
      "Epoch 64/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3770 - acc: 0.3093\n",
      "Epoch 65/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3765 - acc: 0.3093\n",
      "Epoch 66/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3770 - acc: 0.3093\n",
      "Epoch 67/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3751 - acc: 0.3093\n",
      "Epoch 68/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3761 - acc: 0.3093\n",
      "Epoch 69/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3754 - acc: 0.3093\n",
      "Epoch 70/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3737 - acc: 0.3093\n",
      "Epoch 71/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3740 - acc: 0.3093\n",
      "Epoch 72/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3717 - acc: 0.3093\n",
      "Epoch 73/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3672 - acc: 0.3093\n",
      "Epoch 74/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3655 - acc: 0.3093\n",
      "Epoch 75/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3564 - acc: 0.3093\n",
      "Epoch 76/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3502 - acc: 0.3093\n",
      "Epoch 77/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3358 - acc: 0.3111\n",
      "Epoch 78/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3196 - acc: 0.3261\n",
      "Epoch 79/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3044 - acc: 0.3629\n",
      "Epoch 80/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.2820 - acc: 0.3733\n",
      "Epoch 81/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.2652 - acc: 0.3955\n",
      "Epoch 82/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.2456 - acc: 0.4133\n",
      "Epoch 83/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.2261 - acc: 0.4323\n",
      "Epoch 84/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.2002 - acc: 0.4514\n",
      "Epoch 85/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.1818 - acc: 0.4559\n",
      "Epoch 86/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1606 - acc: 0.4696\n",
      "Epoch 87/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.1466 - acc: 0.4664\n",
      "Epoch 88/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1372 - acc: 0.4764\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1177 - acc: 0.4814\n",
      "Epoch 90/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1062 - acc: 0.4950\n",
      "Epoch 91/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1119 - acc: 0.4823\n",
      "Epoch 92/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.1056 - acc: 0.4946\n",
      "Epoch 93/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.0829 - acc: 0.4959\n",
      "Epoch 94/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0838 - acc: 0.4923\n",
      "Epoch 95/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.0748 - acc: 0.5077\n",
      "Epoch 96/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.0762 - acc: 0.5077\n",
      "Epoch 97/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.0576 - acc: 0.5082\n",
      "Epoch 98/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.0593 - acc: 0.5000\n",
      "Epoch 99/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0487 - acc: 0.5114\n",
      "Epoch 100/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.0394 - acc: 0.5168\n",
      "245/245 [==============================] - 0s 2ms/step\n",
      "Epoch 1/100\n",
      "2202/2202 [==============================] - 1s 591us/step - loss: 1.3854 - acc: 0.2729\n",
      "Epoch 2/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3847 - acc: 0.2734\n",
      "Epoch 3/100\n",
      "2202/2202 [==============================] - 0s 28us/step - loss: 1.3799 - acc: 0.3002\n",
      "Epoch 4/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3822 - acc: 0.2947\n",
      "Epoch 5/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3807 - acc: 0.3065\n",
      "Epoch 6/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3764 - acc: 0.3070\n",
      "Epoch 7/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3805 - acc: 0.3056\n",
      "Epoch 8/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3817 - acc: 0.3025\n",
      "Epoch 9/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3826 - acc: 0.3029\n",
      "Epoch 10/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3800 - acc: 0.3061\n",
      "Epoch 11/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3802 - acc: 0.3047\n",
      "Epoch 12/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3787 - acc: 0.3074\n",
      "Epoch 13/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3789 - acc: 0.3088\n",
      "Epoch 14/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3807 - acc: 0.3034\n",
      "Epoch 15/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3785 - acc: 0.3088\n",
      "Epoch 16/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3807 - acc: 0.3093\n",
      "Epoch 17/100\n",
      "2202/2202 [==============================] - 0s 38us/step - loss: 1.3797 - acc: 0.3065\n",
      "Epoch 18/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.3773 - acc: 0.3093\n",
      "Epoch 19/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3774 - acc: 0.3070\n",
      "Epoch 20/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3778 - acc: 0.3074\n",
      "Epoch 21/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3800 - acc: 0.3084\n",
      "Epoch 22/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3791 - acc: 0.3088\n",
      "Epoch 23/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3788 - acc: 0.3084\n",
      "Epoch 24/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3766 - acc: 0.3088\n",
      "Epoch 25/100\n",
      "2202/2202 [==============================] - 0s 36us/step - loss: 1.3786 - acc: 0.3088\n",
      "Epoch 26/100\n",
      "2202/2202 [==============================] - 0s 46us/step - loss: 1.3792 - acc: 0.3084\n",
      "Epoch 27/100\n",
      "2202/2202 [==============================] - 0s 33us/step - loss: 1.3788 - acc: 0.3084\n",
      "Epoch 28/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3785 - acc: 0.3084\n",
      "Epoch 29/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3796 - acc: 0.3084\n",
      "Epoch 30/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3769 - acc: 0.3084\n",
      "Epoch 31/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3790 - acc: 0.3084\n",
      "Epoch 32/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3797 - acc: 0.3084\n",
      "Epoch 33/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3790 - acc: 0.3084\n",
      "Epoch 34/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3801 - acc: 0.3084\n",
      "Epoch 35/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3792 - acc: 0.3084\n",
      "Epoch 36/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3785 - acc: 0.3084\n",
      "Epoch 37/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3784 - acc: 0.3084\n",
      "Epoch 38/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3793 - acc: 0.3084\n",
      "Epoch 39/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3781 - acc: 0.3084\n",
      "Epoch 40/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3788 - acc: 0.3084\n",
      "Epoch 41/100\n",
      "2202/2202 [==============================] - 0s 44us/step - loss: 1.3786 - acc: 0.3084\n",
      "Epoch 42/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3774 - acc: 0.3084\n",
      "Epoch 43/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3756 - acc: 0.3084\n",
      "Epoch 44/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3763 - acc: 0.3084\n",
      "Epoch 45/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3767 - acc: 0.3084\n",
      "Epoch 46/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3754 - acc: 0.3084\n",
      "Epoch 47/100\n",
      "2202/2202 [==============================] - 0s 51us/step - loss: 1.3726 - acc: 0.3084\n",
      "Epoch 48/100\n",
      "2202/2202 [==============================] - 0s 49us/step - loss: 1.3718 - acc: 0.3084\n",
      "Epoch 49/100\n",
      "2202/2202 [==============================] - 0s 36us/step - loss: 1.3663 - acc: 0.3084\n",
      "Epoch 50/100\n",
      "2202/2202 [==============================] - 0s 34us/step - loss: 1.3609 - acc: 0.3084\n",
      "Epoch 51/100\n",
      "2202/2202 [==============================] - 0s 19us/step - loss: 1.3541 - acc: 0.3084\n",
      "Epoch 52/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3445 - acc: 0.3084\n",
      "Epoch 53/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3343 - acc: 0.3084\n",
      "Epoch 54/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3119 - acc: 0.3084\n",
      "Epoch 55/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3056 - acc: 0.3084\n",
      "Epoch 56/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.2834 - acc: 0.3084\n",
      "Epoch 57/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.2700 - acc: 0.3084\n",
      "Epoch 58/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.2464 - acc: 0.3084\n",
      "Epoch 59/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.2418 - acc: 0.3143\n",
      "Epoch 60/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.2218 - acc: 0.3342\n",
      "Epoch 61/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.2054 - acc: 0.3647\n",
      "Epoch 62/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1901 - acc: 0.3937\n",
      "Epoch 63/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1861 - acc: 0.4196\n",
      "Epoch 64/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1629 - acc: 0.4391\n",
      "Epoch 65/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1634 - acc: 0.4559\n",
      "Epoch 66/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.1604 - acc: 0.4650\n",
      "Epoch 67/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1541 - acc: 0.4668\n",
      "Epoch 68/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1358 - acc: 0.4718\n",
      "Epoch 69/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1340 - acc: 0.4846\n",
      "Epoch 70/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1225 - acc: 0.4900\n",
      "Epoch 71/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1263 - acc: 0.4796\n",
      "Epoch 72/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1319 - acc: 0.4800\n",
      "Epoch 73/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1209 - acc: 0.4977\n",
      "Epoch 74/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.1182 - acc: 0.4818\n",
      "Epoch 75/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1068 - acc: 0.5059\n",
      "Epoch 76/100\n",
      "2202/2202 [==============================] - 0s 33us/step - loss: 1.1062 - acc: 0.5050\n",
      "Epoch 77/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1008 - acc: 0.4877\n",
      "Epoch 78/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0890 - acc: 0.5100\n",
      "Epoch 79/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0871 - acc: 0.5064\n",
      "Epoch 80/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0834 - acc: 0.5077\n",
      "Epoch 81/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0836 - acc: 0.5073\n",
      "Epoch 82/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0742 - acc: 0.5104\n",
      "Epoch 83/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0785 - acc: 0.5173\n",
      "Epoch 84/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0798 - acc: 0.5145\n",
      "Epoch 85/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0688 - acc: 0.5136\n",
      "Epoch 86/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.0563 - acc: 0.5163\n",
      "Epoch 87/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0666 - acc: 0.5104\n",
      "Epoch 88/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0552 - acc: 0.5259\n",
      "Epoch 89/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0621 - acc: 0.5200\n",
      "Epoch 90/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0564 - acc: 0.5163\n",
      "Epoch 91/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0513 - acc: 0.5195\n",
      "Epoch 92/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0495 - acc: 0.5114\n",
      "Epoch 93/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.0500 - acc: 0.5173\n",
      "Epoch 94/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0414 - acc: 0.5168\n",
      "Epoch 95/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.0478 - acc: 0.5313\n",
      "Epoch 96/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0462 - acc: 0.5173\n",
      "Epoch 97/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.0466 - acc: 0.5250\n",
      "Epoch 98/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0523 - acc: 0.5195\n",
      "Epoch 99/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0348 - acc: 0.5277\n",
      "Epoch 100/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0497 - acc: 0.5182\n",
      "245/245 [==============================] - 1s 2ms/step\n",
      "Epoch 1/100\n",
      "2202/2202 [==============================] - 2s 691us/step - loss: 1.3851 - acc: 0.2593\n",
      "Epoch 2/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3823 - acc: 0.2838\n",
      "Epoch 3/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3795 - acc: 0.3034\n",
      "Epoch 4/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3807 - acc: 0.2979\n",
      "Epoch 5/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3819 - acc: 0.3020\n",
      "Epoch 6/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3788 - acc: 0.3056\n",
      "Epoch 7/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3816 - acc: 0.3056\n",
      "Epoch 8/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3791 - acc: 0.3074\n",
      "Epoch 9/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.3781 - acc: 0.3088\n",
      "Epoch 10/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3811 - acc: 0.3093\n",
      "Epoch 11/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3784 - acc: 0.3074\n",
      "Epoch 12/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3809 - acc: 0.3084\n",
      "Epoch 13/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3797 - acc: 0.3079\n",
      "Epoch 14/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3775 - acc: 0.3084\n",
      "Epoch 15/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3785 - acc: 0.3079\n",
      "Epoch 16/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3790 - acc: 0.3074\n",
      "Epoch 17/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3787 - acc: 0.3079\n",
      "Epoch 18/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3793 - acc: 0.3084\n",
      "Epoch 19/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3783 - acc: 0.3084\n",
      "Epoch 20/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.3784 - acc: 0.3084\n",
      "Epoch 21/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3799 - acc: 0.3084\n",
      "Epoch 22/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3792 - acc: 0.3084\n",
      "Epoch 23/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3773 - acc: 0.3084\n",
      "Epoch 24/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3780 - acc: 0.3084\n",
      "Epoch 25/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3792 - acc: 0.3084\n",
      "Epoch 26/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3780 - acc: 0.3084\n",
      "Epoch 27/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3775 - acc: 0.3084\n",
      "Epoch 28/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3766 - acc: 0.3084\n",
      "Epoch 29/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3797 - acc: 0.3084\n",
      "Epoch 30/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3780 - acc: 0.3084\n",
      "Epoch 31/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3783 - acc: 0.3084\n",
      "Epoch 32/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3789 - acc: 0.3084\n",
      "Epoch 33/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3771 - acc: 0.3084\n",
      "Epoch 34/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3791 - acc: 0.3084\n",
      "Epoch 35/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3787 - acc: 0.3084\n",
      "Epoch 36/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3786 - acc: 0.3084\n",
      "Epoch 37/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3782 - acc: 0.3084\n",
      "Epoch 38/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3801 - acc: 0.3084\n",
      "Epoch 39/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3778 - acc: 0.3084\n",
      "Epoch 40/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3788 - acc: 0.3084\n",
      "Epoch 41/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3794 - acc: 0.3084\n",
      "Epoch 42/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3780 - acc: 0.3084\n",
      "Epoch 43/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3781 - acc: 0.3084\n",
      "Epoch 44/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3786 - acc: 0.3084\n",
      "Epoch 45/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3784 - acc: 0.3084\n",
      "Epoch 46/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3786 - acc: 0.3084\n",
      "Epoch 47/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3766 - acc: 0.3084\n",
      "Epoch 48/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3792 - acc: 0.3084\n",
      "Epoch 49/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3787 - acc: 0.3084\n",
      "Epoch 50/100\n",
      "2202/2202 [==============================] - 0s 25us/step - loss: 1.3783 - acc: 0.3084\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3766 - acc: 0.3084\n",
      "Epoch 52/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3781 - acc: 0.3084\n",
      "Epoch 53/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3772 - acc: 0.3084\n",
      "Epoch 54/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3771 - acc: 0.3084\n",
      "Epoch 55/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3771 - acc: 0.3084\n",
      "Epoch 56/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3766 - acc: 0.3084\n",
      "Epoch 57/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.3754 - acc: 0.3084\n",
      "Epoch 58/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3736 - acc: 0.3084\n",
      "Epoch 59/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3698 - acc: 0.3084\n",
      "Epoch 60/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3665 - acc: 0.3084\n",
      "Epoch 61/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3612 - acc: 0.3084\n",
      "Epoch 62/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.3504 - acc: 0.3084\n",
      "Epoch 63/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3387 - acc: 0.3084\n",
      "Epoch 64/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.3252 - acc: 0.3084\n",
      "Epoch 65/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.3058 - acc: 0.3084\n",
      "Epoch 66/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.2845 - acc: 0.3084\n",
      "Epoch 67/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.2648 - acc: 0.3084\n",
      "Epoch 68/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.2461 - acc: 0.3084\n",
      "Epoch 69/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.2265 - acc: 0.3084\n",
      "Epoch 70/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.2165 - acc: 0.3179\n",
      "Epoch 71/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1971 - acc: 0.3506\n",
      "Epoch 72/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1896 - acc: 0.3946\n",
      "Epoch 73/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1730 - acc: 0.4210\n",
      "Epoch 74/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1600 - acc: 0.4469\n",
      "Epoch 75/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1613 - acc: 0.4609\n",
      "Epoch 76/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.1623 - acc: 0.4682\n",
      "Epoch 77/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1487 - acc: 0.4741\n",
      "Epoch 78/100\n",
      "2202/2202 [==============================] - 0s 28us/step - loss: 1.1336 - acc: 0.4827\n",
      "Epoch 79/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1342 - acc: 0.4759\n",
      "Epoch 80/100\n",
      "2202/2202 [==============================] - 0s 20us/step - loss: 1.1215 - acc: 0.4918\n",
      "Epoch 81/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.1107 - acc: 0.4918\n",
      "Epoch 82/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1145 - acc: 0.4950\n",
      "Epoch 83/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0940 - acc: 0.5009\n",
      "Epoch 84/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.1002 - acc: 0.5023\n",
      "Epoch 85/100\n",
      "2202/2202 [==============================] - 0s 27us/step - loss: 1.0981 - acc: 0.5027\n",
      "Epoch 86/100\n",
      "2202/2202 [==============================] - 0s 24us/step - loss: 1.0928 - acc: 0.5077\n",
      "Epoch 87/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0990 - acc: 0.4914\n",
      "Epoch 88/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0843 - acc: 0.5054\n",
      "Epoch 89/100\n",
      "2202/2202 [==============================] - 0s 23us/step - loss: 1.0881 - acc: 0.5077\n",
      "Epoch 90/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0853 - acc: 0.5173\n",
      "Epoch 91/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0912 - acc: 0.5014\n",
      "Epoch 92/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0796 - acc: 0.5154\n",
      "Epoch 93/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0706 - acc: 0.5086\n",
      "Epoch 94/100\n",
      "2202/2202 [==============================] - 0s 26us/step - loss: 1.0674 - acc: 0.5141\n",
      "Epoch 95/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0594 - acc: 0.5104\n",
      "Epoch 96/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0604 - acc: 0.5036\n",
      "Epoch 97/100\n",
      "2202/2202 [==============================] - 0s 22us/step - loss: 1.0497 - acc: 0.5177\n",
      "Epoch 98/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0595 - acc: 0.5095\n",
      "Epoch 99/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0534 - acc: 0.5041\n",
      "Epoch 100/100\n",
      "2202/2202 [==============================] - 0s 21us/step - loss: 1.0552 - acc: 0.5114\n",
      "245/245 [==============================] - 1s 2ms/step\n",
      "Epoch 1/100\n",
      "2203/2203 [==============================] - 2s 734us/step - loss: 1.3997 - acc: 0.2424\n",
      "Epoch 2/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3938 - acc: 0.2379\n",
      "Epoch 3/100\n",
      "2203/2203 [==============================] - 0s 38us/step - loss: 1.3884 - acc: 0.2610\n",
      "Epoch 4/100\n",
      "2203/2203 [==============================] - 0s 32us/step - loss: 1.3824 - acc: 0.2833\n",
      "Epoch 5/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3810 - acc: 0.2960\n",
      "Epoch 6/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3802 - acc: 0.3055\n",
      "Epoch 7/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3778 - acc: 0.3109\n",
      "Epoch 8/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3770 - acc: 0.3155\n",
      "Epoch 9/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3771 - acc: 0.3118\n",
      "Epoch 10/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3785 - acc: 0.3114\n",
      "Epoch 11/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3754 - acc: 0.3109\n",
      "Epoch 12/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3784 - acc: 0.3123\n",
      "Epoch 13/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3765 - acc: 0.3132\n",
      "Epoch 14/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3774 - acc: 0.3146\n",
      "Epoch 15/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3765 - acc: 0.3137\n",
      "Epoch 16/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3794 - acc: 0.3132\n",
      "Epoch 17/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3783 - acc: 0.3132\n",
      "Epoch 18/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3767 - acc: 0.3146\n",
      "Epoch 19/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3762 - acc: 0.3137\n",
      "Epoch 20/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3768 - acc: 0.3146\n",
      "Epoch 21/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3772 - acc: 0.3141\n",
      "Epoch 22/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3759 - acc: 0.3141\n",
      "Epoch 23/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3763 - acc: 0.3141\n",
      "Epoch 24/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3761 - acc: 0.3141\n",
      "Epoch 25/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3771 - acc: 0.3141\n",
      "Epoch 26/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3768 - acc: 0.3141\n",
      "Epoch 27/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3774 - acc: 0.3141\n",
      "Epoch 28/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3740 - acc: 0.3141\n",
      "Epoch 29/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3744 - acc: 0.3141\n",
      "Epoch 30/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3773 - acc: 0.3141\n",
      "Epoch 31/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3764 - acc: 0.3141\n",
      "Epoch 32/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3769 - acc: 0.3141\n",
      "Epoch 33/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3755 - acc: 0.3141\n",
      "Epoch 34/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3781 - acc: 0.3141\n",
      "Epoch 35/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3751 - acc: 0.3141\n",
      "Epoch 36/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3772 - acc: 0.3141\n",
      "Epoch 37/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3778 - acc: 0.3141\n",
      "Epoch 38/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.3762 - acc: 0.3141\n",
      "Epoch 39/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3763 - acc: 0.3141\n",
      "Epoch 40/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3761 - acc: 0.3141\n",
      "Epoch 41/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3774 - acc: 0.3141\n",
      "Epoch 42/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3765 - acc: 0.3141\n",
      "Epoch 43/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3771 - acc: 0.3141\n",
      "Epoch 44/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3763 - acc: 0.3141\n",
      "Epoch 45/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3759 - acc: 0.3141\n",
      "Epoch 46/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3768 - acc: 0.3141\n",
      "Epoch 47/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3762 - acc: 0.3141\n",
      "Epoch 48/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3759 - acc: 0.3141\n",
      "Epoch 49/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3758 - acc: 0.3141\n",
      "Epoch 50/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3767 - acc: 0.3141\n",
      "Epoch 51/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3768 - acc: 0.3141\n",
      "Epoch 52/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3764 - acc: 0.3141\n",
      "Epoch 53/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3764 - acc: 0.3141\n",
      "Epoch 54/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3760 - acc: 0.3141\n",
      "Epoch 55/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3747 - acc: 0.3141\n",
      "Epoch 56/100\n",
      "2203/2203 [==============================] - 0s 26us/step - loss: 1.3767 - acc: 0.3141\n",
      "Epoch 57/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.3748 - acc: 0.3141\n",
      "Epoch 58/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3755 - acc: 0.3141\n",
      "Epoch 59/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3748 - acc: 0.3141\n",
      "Epoch 60/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.3759 - acc: 0.3141\n",
      "Epoch 61/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3740 - acc: 0.3141\n",
      "Epoch 62/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3753 - acc: 0.3141\n",
      "Epoch 63/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3734 - acc: 0.3141\n",
      "Epoch 64/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3717 - acc: 0.3141\n",
      "Epoch 65/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3721 - acc: 0.3141\n",
      "Epoch 66/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3676 - acc: 0.3141\n",
      "Epoch 67/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3638 - acc: 0.3141\n",
      "Epoch 68/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3570 - acc: 0.3141\n",
      "Epoch 69/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3494 - acc: 0.3141\n",
      "Epoch 70/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3403 - acc: 0.3141\n",
      "Epoch 71/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3240 - acc: 0.3141\n",
      "Epoch 72/100\n",
      "2203/2203 [==============================] - 0s 34us/step - loss: 1.3110 - acc: 0.3141\n",
      "Epoch 73/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.2916 - acc: 0.3141\n",
      "Epoch 74/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.2740 - acc: 0.3141\n",
      "Epoch 75/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.2563 - acc: 0.3141\n",
      "Epoch 76/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.2443 - acc: 0.3146\n",
      "Epoch 77/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.2254 - acc: 0.3191\n",
      "Epoch 78/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.2072 - acc: 0.3473\n",
      "Epoch 79/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1987 - acc: 0.3749\n",
      "Epoch 80/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.1878 - acc: 0.4026\n",
      "Epoch 81/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1780 - acc: 0.4235\n",
      "Epoch 82/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1616 - acc: 0.4503\n",
      "Epoch 83/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1693 - acc: 0.4503\n",
      "Epoch 84/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1487 - acc: 0.4626\n",
      "Epoch 85/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.1397 - acc: 0.4721\n",
      "Epoch 86/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1421 - acc: 0.4685\n",
      "Epoch 87/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1198 - acc: 0.4830\n",
      "Epoch 88/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1235 - acc: 0.4866\n",
      "Epoch 89/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1221 - acc: 0.4848\n",
      "Epoch 90/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1105 - acc: 0.4893\n",
      "Epoch 91/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1009 - acc: 0.5007\n",
      "Epoch 92/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.0980 - acc: 0.4993\n",
      "Epoch 93/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1014 - acc: 0.5061\n",
      "Epoch 94/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1027 - acc: 0.4925\n",
      "Epoch 95/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0826 - acc: 0.5016\n",
      "Epoch 96/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.0917 - acc: 0.5034\n",
      "Epoch 97/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.0805 - acc: 0.5102\n",
      "Epoch 98/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0804 - acc: 0.5125\n",
      "Epoch 99/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0686 - acc: 0.5179\n",
      "Epoch 100/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0797 - acc: 0.5179\n",
      "244/244 [==============================] - 1s 2ms/step\n",
      "Epoch 1/100\n",
      "2203/2203 [==============================] - 2s 777us/step - loss: 1.3957 - acc: 0.2202\n",
      "Epoch 2/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3873 - acc: 0.2542\n",
      "Epoch 3/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3837 - acc: 0.2801\n",
      "Epoch 4/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3792 - acc: 0.3019\n",
      "Epoch 5/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3787 - acc: 0.3177\n",
      "Epoch 6/100\n",
      "2203/2203 [==============================] - 0s 26us/step - loss: 1.3795 - acc: 0.3150\n",
      "Epoch 7/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3791 - acc: 0.3146\n",
      "Epoch 8/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3779 - acc: 0.3177\n",
      "Epoch 9/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3761 - acc: 0.3173\n",
      "Epoch 10/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3765 - acc: 0.3182\n",
      "Epoch 11/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3782 - acc: 0.3177\n",
      "Epoch 12/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3758 - acc: 0.3177\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3769 - acc: 0.3177\n",
      "Epoch 14/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3747 - acc: 0.3177\n",
      "Epoch 15/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3756 - acc: 0.3177\n",
      "Epoch 16/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3748 - acc: 0.3177\n",
      "Epoch 17/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3745 - acc: 0.3177\n",
      "Epoch 18/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3736 - acc: 0.3177\n",
      "Epoch 19/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3764 - acc: 0.3177\n",
      "Epoch 20/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3755 - acc: 0.3177\n",
      "Epoch 21/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3758 - acc: 0.3177\n",
      "Epoch 22/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3730 - acc: 0.3177\n",
      "Epoch 23/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3760 - acc: 0.3177\n",
      "Epoch 24/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3767 - acc: 0.3177\n",
      "Epoch 25/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3754 - acc: 0.3177\n",
      "Epoch 26/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3766 - acc: 0.3177\n",
      "Epoch 27/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3767 - acc: 0.3177\n",
      "Epoch 28/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3756 - acc: 0.3177\n",
      "Epoch 29/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3764 - acc: 0.3177\n",
      "Epoch 30/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3748 - acc: 0.3177\n",
      "Epoch 31/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3748 - acc: 0.3177\n",
      "Epoch 32/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3768 - acc: 0.3177\n",
      "Epoch 33/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3737 - acc: 0.3177\n",
      "Epoch 34/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3747 - acc: 0.3177\n",
      "Epoch 35/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3745 - acc: 0.3177\n",
      "Epoch 36/100\n",
      "2203/2203 [==============================] - 0s 28us/step - loss: 1.3771 - acc: 0.3177\n",
      "Epoch 37/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3751 - acc: 0.3177\n",
      "Epoch 38/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3749 - acc: 0.3177\n",
      "Epoch 39/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3745 - acc: 0.3177\n",
      "Epoch 40/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3751 - acc: 0.3177\n",
      "Epoch 41/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3743 - acc: 0.3177\n",
      "Epoch 42/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3736 - acc: 0.3177\n",
      "Epoch 43/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3733 - acc: 0.3177\n",
      "Epoch 44/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3725 - acc: 0.3177\n",
      "Epoch 45/100\n",
      "2203/2203 [==============================] - 0s 27us/step - loss: 1.3719 - acc: 0.3177\n",
      "Epoch 46/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3712 - acc: 0.3177\n",
      "Epoch 47/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3676 - acc: 0.3177\n",
      "Epoch 48/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3636 - acc: 0.3177\n",
      "Epoch 49/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3594 - acc: 0.3177\n",
      "Epoch 50/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3490 - acc: 0.3177\n",
      "Epoch 51/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3388 - acc: 0.3177\n",
      "Epoch 52/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3251 - acc: 0.3177\n",
      "Epoch 53/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3097 - acc: 0.3177\n",
      "Epoch 54/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.2869 - acc: 0.3177\n",
      "Epoch 55/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.2713 - acc: 0.3177\n",
      "Epoch 56/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.2499 - acc: 0.3177\n",
      "Epoch 57/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.2311 - acc: 0.3177\n",
      "Epoch 58/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.2219 - acc: 0.3359\n",
      "Epoch 59/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1978 - acc: 0.3745\n",
      "Epoch 60/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1880 - acc: 0.4149\n",
      "Epoch 61/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1787 - acc: 0.4358\n",
      "Epoch 62/100\n",
      "2203/2203 [==============================] - 0s 30us/step - loss: 1.1638 - acc: 0.4626\n",
      "Epoch 63/100\n",
      "2203/2203 [==============================] - 0s 29us/step - loss: 1.1475 - acc: 0.4762\n",
      "Epoch 64/100\n",
      "2203/2203 [==============================] - 0s 32us/step - loss: 1.1413 - acc: 0.4798\n",
      "Epoch 65/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.1277 - acc: 0.4957\n",
      "Epoch 66/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.1249 - acc: 0.4952\n",
      "Epoch 67/100\n",
      "2203/2203 [==============================] - 0s 33us/step - loss: 1.1254 - acc: 0.5016\n",
      "Epoch 68/100\n",
      "2203/2203 [==============================] - 0s 30us/step - loss: 1.1123 - acc: 0.4966\n",
      "Epoch 69/100\n",
      "2203/2203 [==============================] - 0s 27us/step - loss: 1.1006 - acc: 0.5075\n",
      "Epoch 70/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1000 - acc: 0.5007\n",
      "Epoch 71/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.1020 - acc: 0.5070\n",
      "Epoch 72/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0904 - acc: 0.5125\n",
      "Epoch 73/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0919 - acc: 0.5102\n",
      "Epoch 74/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.0910 - acc: 0.5129\n",
      "Epoch 75/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0707 - acc: 0.5220\n",
      "Epoch 76/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.0630 - acc: 0.5225\n",
      "Epoch 77/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0689 - acc: 0.5161\n",
      "Epoch 78/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0743 - acc: 0.5225\n",
      "Epoch 79/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0542 - acc: 0.5166\n",
      "Epoch 80/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0543 - acc: 0.5243\n",
      "Epoch 81/100\n",
      "2203/2203 [==============================] - 0s 29us/step - loss: 1.0533 - acc: 0.5202\n",
      "Epoch 82/100\n",
      "2203/2203 [==============================] - 0s 26us/step - loss: 1.0464 - acc: 0.5170\n",
      "Epoch 83/100\n",
      "2203/2203 [==============================] - 0s 28us/step - loss: 1.0416 - acc: 0.5334\n",
      "Epoch 84/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.0445 - acc: 0.5188\n",
      "Epoch 85/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0357 - acc: 0.5293\n",
      "Epoch 86/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.0492 - acc: 0.5134\n",
      "Epoch 87/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0315 - acc: 0.5311\n",
      "Epoch 88/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0311 - acc: 0.5379\n",
      "Epoch 89/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0399 - acc: 0.5256\n",
      "Epoch 90/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.0250 - acc: 0.5374\n",
      "Epoch 91/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0267 - acc: 0.5297\n",
      "Epoch 92/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0286 - acc: 0.5361\n",
      "Epoch 93/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.0323 - acc: 0.5256\n",
      "Epoch 94/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.0169 - acc: 0.5424\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0270 - acc: 0.5388\n",
      "Epoch 96/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0223 - acc: 0.5329\n",
      "Epoch 97/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.0137 - acc: 0.5456\n",
      "Epoch 98/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0152 - acc: 0.5347\n",
      "Epoch 99/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0171 - acc: 0.5447\n",
      "Epoch 100/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0104 - acc: 0.5443\n",
      "244/244 [==============================] - 1s 2ms/step\n",
      "Epoch 1/100\n",
      "2203/2203 [==============================] - 2s 795us/step - loss: 1.3852 - acc: 0.2633\n",
      "Epoch 2/100\n",
      "2203/2203 [==============================] - 0s 27us/step - loss: 1.3822 - acc: 0.2969\n",
      "Epoch 3/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3809 - acc: 0.2932\n",
      "Epoch 4/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3786 - acc: 0.3100\n",
      "Epoch 5/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3782 - acc: 0.3128\n",
      "Epoch 6/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3803 - acc: 0.3100\n",
      "Epoch 7/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3789 - acc: 0.3128\n",
      "Epoch 8/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3797 - acc: 0.3141\n",
      "Epoch 9/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3754 - acc: 0.3123\n",
      "Epoch 10/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3776 - acc: 0.3123\n",
      "Epoch 11/100\n",
      "2203/2203 [==============================] - 0s 28us/step - loss: 1.3773 - acc: 0.3118\n",
      "Epoch 12/100\n",
      "2203/2203 [==============================] - 0s 29us/step - loss: 1.3768 - acc: 0.3128\n",
      "Epoch 13/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3780 - acc: 0.3132\n",
      "Epoch 14/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3787 - acc: 0.3132\n",
      "Epoch 15/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3788 - acc: 0.3128\n",
      "Epoch 16/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3765 - acc: 0.3128\n",
      "Epoch 17/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3775 - acc: 0.3128\n",
      "Epoch 18/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3780 - acc: 0.3128\n",
      "Epoch 19/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3770 - acc: 0.3128\n",
      "Epoch 20/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3788 - acc: 0.3128\n",
      "Epoch 21/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3772 - acc: 0.3128\n",
      "Epoch 22/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3777 - acc: 0.3128\n",
      "Epoch 23/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3782 - acc: 0.3128\n",
      "Epoch 24/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3764 - acc: 0.3128\n",
      "Epoch 25/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3780 - acc: 0.3128\n",
      "Epoch 26/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3791 - acc: 0.3128\n",
      "Epoch 27/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3764 - acc: 0.3128\n",
      "Epoch 28/100\n",
      "2203/2203 [==============================] - 0s 43us/step - loss: 1.3768 - acc: 0.3128\n",
      "Epoch 29/100\n",
      "2203/2203 [==============================] - 0s 27us/step - loss: 1.3769 - acc: 0.3128\n",
      "Epoch 30/100\n",
      "2203/2203 [==============================] - 0s 26us/step - loss: 1.3767 - acc: 0.3128\n",
      "Epoch 31/100\n",
      "2203/2203 [==============================] - 0s 32us/step - loss: 1.3763 - acc: 0.3128\n",
      "Epoch 32/100\n",
      "2203/2203 [==============================] - 0s 32us/step - loss: 1.3777 - acc: 0.3128\n",
      "Epoch 33/100\n",
      "2203/2203 [==============================] - 0s 26us/step - loss: 1.3770 - acc: 0.3128\n",
      "Epoch 34/100\n",
      "2203/2203 [==============================] - 0s 28us/step - loss: 1.3764 - acc: 0.3128\n",
      "Epoch 35/100\n",
      "2203/2203 [==============================] - 0s 29us/step - loss: 1.3772 - acc: 0.3128\n",
      "Epoch 36/100\n",
      "2203/2203 [==============================] - 0s 33us/step - loss: 1.3771 - acc: 0.3128\n",
      "Epoch 37/100\n",
      "2203/2203 [==============================] - 0s 26us/step - loss: 1.3770 - acc: 0.3128\n",
      "Epoch 38/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3779 - acc: 0.3128\n",
      "Epoch 39/100\n",
      "2203/2203 [==============================] - 0s 34us/step - loss: 1.3769 - acc: 0.3128\n",
      "Epoch 40/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3775 - acc: 0.3128\n",
      "Epoch 41/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3765 - acc: 0.3128\n",
      "Epoch 42/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3773 - acc: 0.3128\n",
      "Epoch 43/100\n",
      "2203/2203 [==============================] - 0s 30us/step - loss: 1.3767 - acc: 0.3128\n",
      "Epoch 44/100\n",
      "2203/2203 [==============================] - 0s 27us/step - loss: 1.3771 - acc: 0.3128\n",
      "Epoch 45/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3772 - acc: 0.3128\n",
      "Epoch 46/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3768 - acc: 0.3128\n",
      "Epoch 47/100\n",
      "2203/2203 [==============================] - 0s 35us/step - loss: 1.3763 - acc: 0.3128\n",
      "Epoch 48/100\n",
      "2203/2203 [==============================] - 0s 36us/step - loss: 1.3764 - acc: 0.3128\n",
      "Epoch 49/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.3772 - acc: 0.3128\n",
      "Epoch 50/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3760 - acc: 0.3128\n",
      "Epoch 51/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3758 - acc: 0.3128\n",
      "Epoch 52/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3772 - acc: 0.3128\n",
      "Epoch 53/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3768 - acc: 0.3128\n",
      "Epoch 54/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.3754 - acc: 0.3128\n",
      "Epoch 55/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3760 - acc: 0.3128\n",
      "Epoch 56/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3758 - acc: 0.3128\n",
      "Epoch 57/100\n",
      "2203/2203 [==============================] - 0s 30us/step - loss: 1.3765 - acc: 0.3128\n",
      "Epoch 58/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.3745 - acc: 0.3128\n",
      "Epoch 59/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3750 - acc: 0.3128\n",
      "Epoch 60/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3752 - acc: 0.3128\n",
      "Epoch 61/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3741 - acc: 0.3128\n",
      "Epoch 62/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3702 - acc: 0.3128\n",
      "Epoch 63/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3661 - acc: 0.3128\n",
      "Epoch 64/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.3641 - acc: 0.3128\n",
      "Epoch 65/100\n",
      "2203/2203 [==============================] - 0s 26us/step - loss: 1.3553 - acc: 0.3128\n",
      "Epoch 66/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.3435 - acc: 0.3128\n",
      "Epoch 67/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3312 - acc: 0.3128\n",
      "Epoch 68/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.3166 - acc: 0.3128\n",
      "Epoch 69/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.2941 - acc: 0.3128\n",
      "Epoch 70/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.2780 - acc: 0.3128\n",
      "Epoch 71/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.2536 - acc: 0.3128\n",
      "Epoch 72/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.2381 - acc: 0.3128\n",
      "Epoch 73/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.2285 - acc: 0.3132\n",
      "Epoch 74/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.2120 - acc: 0.3364\n",
      "Epoch 75/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1980 - acc: 0.3745\n",
      "Epoch 76/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1792 - acc: 0.4126\n",
      "Epoch 77/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.1810 - acc: 0.4276\n",
      "Epoch 78/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.1610 - acc: 0.4666\n",
      "Epoch 79/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1516 - acc: 0.4644\n",
      "Epoch 80/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.1403 - acc: 0.4816\n",
      "Epoch 81/100\n",
      "2203/2203 [==============================] - 0s 20us/step - loss: 1.1322 - acc: 0.4875\n",
      "Epoch 82/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1321 - acc: 0.4857\n",
      "Epoch 83/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.1289 - acc: 0.4943\n",
      "Epoch 84/100\n",
      "2203/2203 [==============================] - 0s 24us/step - loss: 1.1148 - acc: 0.4980\n",
      "Epoch 85/100\n",
      "2203/2203 [==============================] - 0s 25us/step - loss: 1.1054 - acc: 0.5002\n",
      "Epoch 86/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.1125 - acc: 0.5075\n",
      "Epoch 87/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.1053 - acc: 0.5002\n",
      "Epoch 88/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.0934 - acc: 0.5107\n",
      "Epoch 89/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.0959 - acc: 0.5107\n",
      "Epoch 90/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0883 - acc: 0.5166\n",
      "Epoch 91/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0899 - acc: 0.4998\n",
      "Epoch 92/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0816 - acc: 0.5125\n",
      "Epoch 93/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0773 - acc: 0.5138\n",
      "Epoch 94/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0763 - acc: 0.5166\n",
      "Epoch 95/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0688 - acc: 0.5293\n",
      "Epoch 96/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0716 - acc: 0.5229\n",
      "Epoch 97/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.0620 - acc: 0.5152\n",
      "Epoch 98/100\n",
      "2203/2203 [==============================] - 0s 23us/step - loss: 1.0674 - acc: 0.5225\n",
      "Epoch 99/100\n",
      "2203/2203 [==============================] - 0s 21us/step - loss: 1.0665 - acc: 0.5184\n",
      "Epoch 100/100\n",
      "2203/2203 [==============================] - 0s 22us/step - loss: 1.0574 - acc: 0.5184\n",
      "244/244 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "kclf = KerasClassifier(build_fn = build_ann, batch_size = 256, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = kclf, X = X_vect_nl_ann, y = y_nn, cv = 10, n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T18:10:34.927457Z",
     "start_time": "2018-06-05T18:10:34.918810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.5414653718471527\n",
      "Standard Deviation: 0.03926198650268906\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy:', accuracies.mean())\n",
    "print('Standard Deviation:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have very bad performances also on the training set. There is no reason in attempting any further tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": "1",
   "nav_menu": {
    "height": "260px",
    "width": "346px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "500px",
    "left": "45px",
    "top": "111px",
    "width": "221px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
