\chapter{Basic Modeling Approaches}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Initial Classification}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{POS Tagger Validity Check} 

Before digging into more complex types of analysis, we took the time to check if
the tools we were using could be really good for our purpose. 

Specifically, we had some doubts on the Part-Of-Speech (POS) tagger. Indeed, those kind of systems are generarly 
trained on texts coming from sources whose type of language is very different from those
we would expect to find in lyrics. In fact, the SpaCy's POS tagger implemented in the language
model we are using is trained on OntoNotes 5\cite{ontonotes5} and on Common Crawl\cite{common-crawl},
which are both made of pieces of text taken from news, conversational telephone speech, weblogs, 
usenet newsgroups, broadcast and talk shows. Obviously, this type of natural language texts are 
much different from a lyric and we just wanted to make sure that we were using a appropriate tool.

Before going into the details of what we did, we must state that SpaCy's POS tagger provides two tags per words, which
will both be considered in our analysis. Those two type of tags are:

\begin{itemize}
\item \textbf{POS}: coarse-grained part-of-speech e.g. VERB
\item \textbf{TAG}: fine-grained part-of-speech e.g. PAST\_TENSE
\end{itemize}

In order to obtain reliable insights of the functionalities of our POS tagger we considered three songs:
one with a common language and very few slang words, one filled with slangs and another one with some
vulgar words.

The first song we considered was "Polly" from Nirvana.

As a first approach we tried to tag the words considering one line of the lyric at the time. Here is an example of
what we obtained:

\begin{lstlisting}
Polly wants a cracker
PROPN VERB DET NOUN 

Polly = PROPN NNP -> noun, proper singular
wants = VERB VBZ -> verb, 3rd person singular present
a = DET DT -> determiner
cracker = NOUN NN -> noun, singular or mass
\end{lstlisting}

As a first attempt, our POS tagger completelly succedeed in recognizing the phrase the 
exact way we were expecting. In fact, the absence of weird words e.g. slangs made the
task much easier.

Because of the almost complete absence of punctuation marks in our lyric, we expected
the POS tagger to fail while analyzing the entire song as whole. Instead, we were quite
surprised to see that SpaCy's POS tagger does one desirable thing for our goal:
it treats each line as a standalone sentence, evern though they are not specifically separed by
a stopping mark. Therefore, the same positive behaviour we observed on the first line
was totally replicated on the other lines, giving us the exact tagging we were
expecting by visually inspecting our song's lyrics.

We omitted the entire tagging process output for brevity reasons.

Our first experiment served to the purpose of arriving to a conclusion: SpaCy's POS tagger
is a good tool for lyrics. However it did not solve our doubts about its ability of recognizing
"weirder" words such as slang words or abbreviations. For this reason, we moved on 
analysing the lyrics of "Kiss You Back" from Underground Kiss.

The first interesting thing we noticed was that the POS tagger properly recognized abbreviations such as "'ll'".
Moreover, another important feature we noticed was clearly visible while tagging this line: "Yeah, we chocolate cross-over".
Indeed, here the word "chocolate" is used as a verb (even though chocolate is clearly not defined as a verb in 
the dictionary) and the POS tagger was able to recognize this exception. 
This is quite important because, in songs, those situations happen very often.

Other additional things we noticed while analyzing this song came our of the tagging output of the following line:

\begin{lstlisting}
Jus't havin' fun with it, man, know what I'm sayin'?
NOUN VERB NOUN ADP PRON PUNCT INTJ PUNCT VERB NOUN PRON VERB VERB PUNCT PUNCT 

Jus't = NOUN NNS -> noun, plural
havin' = VERB VBG -> verb, gerund or present participle
fun = NOUN NN -> noun, singular or mass
with = ADP IN -> conjunction, subordinating or preposition
it = PRON PRP -> pronoun, personal
, = PUNCT , -> punctuation mark, comma
man = INTJ UH -> interjection
, = PUNCT , -> punctuation mark, comma
know = VERB VB -> verb, base form
what = NOUN WP -> wh-pronoun, personal
I = PRON PRP -> pronoun, personal
'm = VERB VBP -> verb, non-3rd person singular present
sayin = VERB VBG -> verb, gerund or present participle
' = PUNCT '' -> closing quotation mark
? = PUNCT . -> punctuation mark, sentence closer
\end{lstlisting}

One very interesting result we can notice comes from the following two lines:
\begin{lstlisting}
havin' = VERB VBG -> verb, gerund or present participle and
ayin = VERB VBG -> verb, gerund or present participle
\end{lstlisting}

In fact it looks like our POS tagger is able to recognize verbs in their correct tense
even if they are abbreviated in an unconventional way.

One thing which really impressed us, was the word "man" being recognized to be an interjection from time to time. 
"An interjection is a part of speech that shows the emotion or feeling of the author. These words or phrases can 
stand alone or be placed before or after a sentence. Many times an interjection is followed 
by a punctuation mark, often an exclamation point"\footnote{http://examples.yourdictionary.com/examples-of-interjections.html}. 
This description perfectly fits with the usage of the word "man" in their contextes when it was recognized to be an interjection. 

Those kind of things are not trivial to detect and this ability of our POS tagger convinced us even more
of its impressive skills.

The last thing we were left to analyze at this point was a vulgar song. For this purpose we considered 
"The Ballad Of Chasey Lain", from Bloodhound Gang. 

In this case we have no special remarks to report. We can just say that everything was tagged and
recognized in the exact way we expected.

We did not report entire lyrics nor the full POS tagger output for the interest of brevity. However, those
analysis are available on the public GitHub repository for this project\footnote{https://github.com/sgiammy/emotion-patterns-in-music-playlists/blob/master/Notebook/3\_POS\_tagger\_verification.ipynb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Beyond the lyrics dataset: EmoInt}

One major limitation we had to face during our work on this project was the shortage in terms of data.
Indeed, most of the already labeled datasets which can be found online have been created for type of texts
which were much different from lyrics i.e. news items, blog posts, Facebook posts, tweets, etc.

In order to overcome this limitation we tought that, if we found a dataset whose items language is close enough
to the common lyrics language, we would have had some performances improvements in our classifiers. This is the
reason why we tried to combine our dataset together with EmoInt\cite{emoint}.

EmoInt provides several tweets annotated according to an emotion (anger, fear, joy, sadness) and to the degree 
at which the emotion is expressed in text. As EmoInt provide intensity levels together with emotion labels, 
we decided to take into account only those tweets for which the intensity was greater that 0.50 (50\%). 

Our original dataset, MoodyLyrics, contains "happy", "sad", "angry" and "relaxed" as labels. 
Therefore, in order to perform a sort of interjection with EmoInt, we used only the tweets corresponding to the anger, 
joy and sadness emotions, discarding those belonging to the fear emotion as we would not have been able to map them into
our original work.

Moreover, it is important to mention that EmoInt was manually annotated using Best-Worst Scaling (BWS)\cite{bws}, 
an annotation scheme proved to obtain very reliable scores. Therefore, we choose EmoInt because it looked
like a realiable choice.

As a single preprocessing technique, we dropped hashtags and remove the tag characters 
(e.g. "Hey @MrTwitter how are you? \#cool" became "Hey MrTwitter how are you?") because 
we had to compare tweets with songs and songs do not have those kind of things. Also, 
this sort of preprocessing should maximize the chances that everything is properly recognized by our POS tagger.

After having preprocessed EmoInt, we combined it to our lyrics based dataset (MoodyLyrics) and tried some different
modeling approaches to see if we could obtain any performance improvements.

After having performed several different trials, we came to the conclusion that the best subset of features
to use for our new dataset (MoodyLyrics + EmoInt) was the following: \textit{Lyric\_Vector}, \textit{Word\_Count}, 
\textit{\%Echoisms}, \textit{Selfish\_degree}, \textit{\%Duplicate\_Lines}, \textit{isTitleInLyric}, 
\textit{\%Present\_tense\_verbs}, \textit{\%Past\_tense\_verbs}, \textit{\%Future\_tense\_verbs},
\textit{\%ADJ}, \textit{\%PUNCT}, \textit{Sentiment} and \textit{Subjectivity\_degree}.

\begin{table}[]
\centering
\label{table:emoint-results}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Classifier} & \textbf{Accuracy})   \\ \midrule
k-Nearest Neighbour & 46\%  \\
Support Vector Machine & 48\%  \\
Gradient Boost & 46\%  \\
Neural Network & 82.38\%  \\
Multinomial Na\"{i}ve Bayes Classifier & 49\%  \\ \bottomrule
\end{tabular}
\caption{Accuracy results for different classifiers on MoodyLyrics and EmoInt combined}
\end{table}

Using the data obtained as just explained, we built several classifiers with which we obtained the results
shown in table \ref{table:emoint-results}. Those results were obtained by performing a leave-one-out
validation on the built model.

As it was when we used MoodyLyrics alone, the best results were obtained in using a Neural Network also in this case.
This network had a very simple architecture: one input layer with sigmoid activation function and 120 neurons,
one hidden layer with softmax activation function and 60 neurons and an output layer with 4 neurons and softmax
activation function.

Anyway, it is quite clear that this approach did not lead us to real improvements over our previous cases.
This experiment served to the purpose of understanding that EmoInt is probably too much different from what 
we have classify and it may not improve our predictive abilities at all. Therefore we believed that the best
choice was to keep using a lyrics based dataset as it is MoodyLyrics.

